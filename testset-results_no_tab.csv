,input_function,expected_if_statement,predicted_if_statement,code_bleu,exact_match,bleu_4
0,"def read ( self , count = True , timeout = None , ignore_non_errors = True , ignore_timeouts = True ) : try : return self . _read ( count , timeout ) except usb . USBError as e : if DEBUG_COMM : log . info ( "" read: e.errno= %s  e.strerror= %s  e.message= %s  repr= %s "" % ( e . errno , e . strerror , e . message , repr ( e ) ) ) <MASK> return [ ] if ignore_non_errors and is_noerr ( e ) : return [ ] raise",if ignore_timeouts and is_timeout ( e ) :,if ignore_timeouts and is_noerr ( e ) :,98.75831429,FALSE,98.12
1,"def _cache_mem ( curr_out , prev_mem , mem_len , reuse_len = None ) : """"""cache hidden states into memory."""""" if mem_len is None or mem_len == 0 : return None else : if reuse_len is not None and reuse_len > 0 : curr_out = curr_out [ : reuse_len ] <MASK> new_mem = curr_out [ - mem_len : ] else : new_mem = tf . concat ( [ prev_mem , curr_out ] , 0 ) [ - mem_len : ] new_mem . stop_gradient = True return new_mem",if prev_mem is None :,if mem_len is not None and curr_out [ - mem_len : ] ==,90.05093074,FALSE,87.66
2,def filtered ( gen ) : for example in gen : example_len = length_fn ( example ) # Checking max length boundary. if max_length is not None : <MASK> continue # Checking min length boundary. if min_length is not None : if example_len < min_length : continue # Within bounds. yield example,if example_len > max_length :,if example_len > max_length :,100,TRUE,100
3,"def search ( self , query ) : # ""Search.ashx?query="" + query + filterVal if not query : logger . debug ( "" Empty search query "" ) return [ ] logger . debug ( ' Searching TuneIn for  "" %s "" ' % query ) args = "" &query= "" + query search_results = self . _tunein ( "" Search.ashx "" , args ) results = [ ] for item in self . _flatten ( search_results ) : <MASK> # Only return stations self . _stations [ item [ "" guide_id "" ] ] = item results . append ( item ) return results","if item . get ( ""type"" , """" ) == ""audio"" :","if ""guide_id"" in item :",70.17049505,FALSE,88.01
4,"def _check_script ( self , script , directive ) : for var in compile_script ( script ) : <MASK> # Skip variable checks return False if var . can_contain ( "" . "" ) : # Yay! Our variable can contain any symbols! reason = ( ' At least variable  "" $ {var} ""  can contain untrusted user input ' . format ( var = var . name ) ) self . add_issue ( directive = [ directive ] + var . providers , reason = reason ) return True return False","if var . must_contain ( ""/"" ) :",if not var . check_script ( directive ) :,56.6709333,FALSE,91.87
5,"def getAllDataLinkIDs ( ) : linkDataIDs = set ( ) dataType = _forestData . dataTypeBySocket for socketID , linkedIDs in _forestData . linkedSockets . items ( ) : for linkedID in linkedIDs : <MASK> # check which one is origin/target linkDataIDs . add ( ( socketID , linkedID , dataType [ socketID ] , dataType [ linkedID ] ) ) else : linkDataIDs . add ( ( linkedID , socketID , dataType [ linkedID ] , dataType [ socketID ] ) ) return linkDataIDs",if socketID [ 1 ] :,"if linkedID == ""origin"" :",91.19979694,FALSE,91.95
6,"def _stderr_supports_color ( ) : try : if hasattr ( sys . stderr , "" isatty "" ) and sys . stderr . isatty ( ) : if curses : curses . setupterm ( ) <MASK> return True elif colorama : if sys . stderr is getattr ( colorama . initialise , "" wrapped_stderr "" , object ( ) ) : return True except Exception : # Very broad exception handling because it's always better to # fall back to non-colored logs than to break at startup. pass return False","if curses . tigetnum ( ""colors"" ) > 0 :","if sys . stderr is getattr ( curses . initialise , ""wrapped_stderr"" , object (",91.10066497,FALSE,84.21
7,"def offsets ( self ) : offsets = { } offset_so_far = 0 for name , ty in self . fields . items ( ) : if isinstance ( ty , SimTypeBottom ) : l . warning ( "" Found a bottom field in struct  %s . Ignore and increment the offset using the default  "" "" element size. "" , self . name , ) continue if not self . _pack : align = ty . alignment <MASK> offset_so_far + = align - offset_so_far % align offsets [ name ] = offset_so_far offset_so_far + = ty . size / / self . _arch . byte_width return offsets",if offset_so_far % align != 0 :,if align :,93.85393,FALSE,92.02
8,"def Restore ( self ) : picker , obj = self . _window , self . _pObject value = obj . RestoreValue ( PERSIST_FILEDIRPICKER_PATH ) if value is not None : if issubclass ( picker . __class__ , wx . FileDialog ) : <MASK> value = value [ - 1 ] picker . SetPath ( value ) return True return False",if type ( value ) == list :,"if value [ 0 ] == ""file"" :",90.79223576,FALSE,87.96
9,"def dt_s_tup_to_string ( dt_s_tup ) : dt_string = dt_s_tup [ 0 ] # string for identifying the file to parse. if dt_s_tup [ 1 ] > 0 : # if there are seasons in the model <MASK> dt_string = dt_string [ : 2 ] + "" s "" + dt_string [ 2 : ] else : dt_string = "" s "" + dt_string return dt_string","if ""co"" in dt_string or ""ci"" in dt_string or ""nc"" in dt_string :","if dt_string [ : 2 ] == ""s"" :",91.20309406,FALSE,81.3
10,"def writer ( stream , items ) : sep = "" "" for item in items : stream . write ( sep ) sep = ""   "" <MASK> item = str ( item ) if not PY3K : if not isinstance ( item , unicode ) : item = str ( item ) stream . write ( item ) stream . write ( "" \n "" )","if not isinstance ( item , str ) :","if not isinstance ( item , str ) :",100,TRUE,100
11,"def _get_result_keys ( self , config ) : result_key = config . get ( "" result_key "" ) if result_key is not None : <MASK> result_key = [ result_key ] result_key = [ jmespath . compile ( rk ) for rk in result_key ] return result_key","if not isinstance ( result_key , list ) :","if not isinstance ( result_key , list ) :",100,TRUE,100
12,"def _download_build_artifacts ( self , build : Dict [ str , Any ] ) - > None : arch = build [ "" arch_tag "" ] snap_build = self . _lp_load_url ( build [ "" self_link "" ] ) urls = snap_build . getFileUrls ( ) if not urls : logger . error ( f "" Snap file not available for arch  { arch !r} . "" ) return for url in urls : file_name = _get_url_basename ( url ) self . _download_file ( url = url , dst = file_name ) <MASK> logger . info ( f "" Snapped  { file_name } "" ) else : logger . info ( f "" Fetched  { file_name } "" )","if file_name . endswith ( "".snap"" ) :",if snap_build . is_snapped ( ) :,94.13749596,FALSE,93.63
13,"def _add_custom_statement ( self , custom_statements ) : if custom_statements is None : return self . resource_policy [ "" Version "" ] = "" 2012-10-17 "" if self . resource_policy . get ( "" Statement "" ) is None : self . resource_policy [ "" Statement "" ] = custom_statements else : if not isinstance ( custom_statements , list ) : custom_statements = [ custom_statements ] statement = self . resource_policy [ "" Statement "" ] if not isinstance ( statement , list ) : statement = [ statement ] for s in custom_statements : <MASK> statement . append ( s ) self . resource_policy [ "" Statement "" ] = statement",if s not in statement :,"if not isinstance ( s , list ) :",91.68455739,FALSE,94.7
14,"def display_failures_for_single_test ( result : TestResult ) - > None : """"""Display a failure for a single method / endpoint."""""" display_subsection ( result ) checks = _get_unique_failures ( result . checks ) for idx , check in enumerate ( checks , 1 ) : message : Optional [ str ] <MASK> message = f "" { idx } .  { check . message } "" else : message = None example = cast ( Case , check . example ) # filtered in `_get_unique_failures` display_example ( example , check . name , message , result . seed ) # Display every time except the last check if idx != len ( checks ) : click . echo ( "" \n "" )",if check . message :,if check . message is not None :,94.04839225,FALSE,97.19
15,"def build ( opt ) : dpath = os . path . join ( opt [ "" datapath "" ] , "" qangaroo "" ) version = "" v1.1 "" if not build_data . built ( dpath , version_string = version ) : print ( "" [building data:  "" + dpath + "" ] "" ) <MASK> # An older version exists, so remove these outdated files. build_data . remove_dir ( dpath ) build_data . make_dir ( dpath ) # Download the data. for downloadable_file in RESOURCES : downloadable_file . download_file ( dpath ) # Mark the data as built. build_data . mark_done ( dpath , version_string = version )",if build_data . built ( dpath ) :,if build_data . built ( dpath ) :,100,TRUE,100
16,"def call ( self , step_input , states ) : new_states = [ ] for i in range ( self . num_layers ) : out , new_state = self . lstm_cells [ i ] ( step_input , states [ i ] ) step_input = ( layers . dropout ( out , self . dropout_prob , dropout_implementation = "" upscale_in_train "" ) <MASK> else out ) new_states . append ( new_state ) return step_input , new_states",if self . dropout_prob > 0.0,if self . dropout_prob is not None,92.48914424,FALSE,95.95
17,"def jupyter_progress_bar ( min = 0 , max = 1.0 ) : """"""Returns an ipywidget progress bar or None if we can't import it"""""" widgets = wandb . util . get_module ( "" ipywidgets "" ) try : <MASK> # TODO: this currently works in iPython but it's deprecated since 4.0 from IPython . html import widgets # type: ignore assert hasattr ( widgets , "" VBox "" ) assert hasattr ( widgets , "" Label "" ) assert hasattr ( widgets , "" FloatProgress "" ) return ProgressWidget ( widgets , min = min , max = max ) except ( ImportError , AssertionError ) : return None",if widgets is None :,"if hasattr ( widgets , ""ProgressWidget"" ) :",92.88646539,FALSE,92.89
18,"def _record_event ( self , path , fsevent_handle , filename , events , error ) : with self . lock : self . events [ path ] . append ( events ) <MASK> if not os . path . exists ( path ) : self . watches . pop ( path ) . close ( )",if events | pyuv . fs . UV_RENAME :,if error :,88.6706648,FALSE,84.45
19,"def _get_v1_id_from_tags ( self , tags_obj , tag ) : """"""Get image id from array of tags"""""" if isinstance ( tags_obj , dict ) : try : return tags_obj [ tag ] except KeyError : pass elif isinstance ( tags_obj , [ ] ) : try : for tag_dict in tags_obj : <MASK> return tag_dict [ "" layer "" ] except KeyError : pass return "" ""","if tag_dict [ ""name"" ] == tag :","if ""layer"" in tag_dict :",88.4904864,FALSE,90.05
20,"def query_lister ( domain , query = "" "" , max_items = None , attr_names = None ) : more_results = True num_results = 0 next_token = None while more_results : rs = domain . connection . query_with_attributes ( domain , query , attr_names , next_token = next_token ) for item in rs : <MASK> if num_results == max_items : raise StopIteration yield item num_results + = 1 next_token = rs . next_token more_results = next_token != None",if max_items :,if max_items is not None :,96.04299108,FALSE,96.34
21,"def filter ( this , args ) : array = to_object ( this , args . space ) callbackfn = get_arg ( args , 0 ) arr_len = js_arr_length ( array ) if not is_callable ( callbackfn ) : raise MakeError ( "" TypeError "" , "" callbackfn must be a function "" ) _this = get_arg ( args , 1 ) k = 0 res = [ ] while k < arr_len : <MASK> kValue = array . get ( unicode ( k ) ) if to_boolean ( callbackfn . call ( _this , ( kValue , float ( k ) , array ) ) ) : res . append ( kValue ) k + = 1 return args . space . ConstructArray ( res )",if array . has_property ( unicode ( k ) ) :,if array . has_property ( unicode ( k ) ) :,100,TRUE,100
22,"def every_one_is ( self , dst ) : msg = "" all members of  %r  should be  %r , but the  %d th is  %r "" for index , item in enumerate ( self . _src ) : if self . _range : if index < self . _range [ 0 ] or index > self . _range [ 1 ] : continue error = msg % ( self . _src , dst , index , item ) <MASK> raise AssertionError ( error ) return True",if item != dst :,if error not in self . _src :,92.23750732,FALSE,91.92
23,"def schedule_logger ( job_id = None , delete = False ) : if not job_id : return getLogger ( "" fate_flow_schedule "" ) else : if delete : with LoggerFactory . lock : try : for key in LoggerFactory . schedule_logger_dict . keys ( ) : <MASK> del LoggerFactory . schedule_logger_dict [ key ] except : pass return True key = job_id + "" schedule "" if key in LoggerFactory . schedule_logger_dict : return LoggerFactory . schedule_logger_dict [ key ] return LoggerFactory . get_schedule_logger ( job_id )",if job_id in key :,"if key == job_id + ""schedule"" :",94.38538491,FALSE,92.49
24,"def Tokenize ( s ) : # type: (str) -> Iterator[Token] for item in TOKEN_RE . findall ( s ) : # The type checker can't know the true type of item! item = cast ( TupleStr4 , item ) if item [ 0 ] : typ = "" number "" val = item [ 0 ] elif item [ 1 ] : typ = "" name "" val = item [ 1 ] <MASK> typ = item [ 2 ] val = item [ 2 ] elif item [ 3 ] : typ = item [ 3 ] val = item [ 3 ] yield Token ( typ , val )",elif item [ 2 ] :,elif item [ 2 ] :,75,TRUE,100
25,"def _read_data_from_all_categories ( self , directory , config , categories ) : lines = [ ] for category in categories : data_file = os . path . join ( directory , _DATASET_VERSION , category , config ) <MASK> with open ( data_file ) as f : ls = f . read ( ) . split ( "" \n "" ) for l in ls [ : : - 1 ] : if not l : ls . remove ( l ) lines . extend ( ls ) return lines",if os . path . exists ( data_file ) :,if os . path . exists ( data_file ) :,100,TRUE,100
26,"def find_handlers ( self , forms ) : handlers = { } for form in forms . itervalues ( ) : for action_name , _action_label in form . actions : <MASK> handlers [ action_name ] = form else : raise HandlerError ( "" More than one form defines the handler  %s "" % action_name ) return handlers",if action_name not in handlers :,if action_name in self . handlers :,90.00456618,FALSE,94.32
27,"def get_story_task_completed_body ( payload : Dict [ str , Any ] ) - > Optional [ str ] : action = get_action_with_primary_id ( payload ) kwargs = { "" task_description "" : action [ "" description "" ] , } story_id = action [ "" story_id "" ] for ref in payload [ "" references "" ] : <MASK> kwargs [ "" name_template "" ] = STORY_NAME_TEMPLATE . format ( name = ref [ "" name "" ] , app_url = ref [ "" app_url "" ] , ) if action [ "" changes "" ] [ "" complete "" ] [ "" new "" ] : return STORY_TASK_COMPLETED_TEMPLATE . format ( * * kwargs ) else : return None","if ref [ ""id"" ] == story_id :","if ref [ ""id"" ] == story_id :",100,TRUE,100
28,"def _create_valid_graph ( graph ) : nodes = graph . nodes ( ) for i in range ( len ( nodes ) ) : for j in range ( len ( nodes ) ) : <MASK> continue edge = ( nodes [ i ] , nodes [ j ] ) if graph . has_edge ( edge ) : graph . del_edge ( edge ) graph . add_edge ( edge , 1 )",if i == j :,if i == j :,100,TRUE,100
29,"def _post_order ( op ) : if isinstance ( op , tvm . tir . Allocate ) : lift_stmt [ - 1 ] . append ( op ) return op . body if isinstance ( op , tvm . tir . AttrStmt ) : <MASK> lift_stmt [ - 1 ] . append ( op ) return op . body if op . attr_key == "" virtual_thread "" : return _merge_block ( lift_stmt . pop ( ) + [ op ] , op . body ) return op if isinstance ( op , tvm . tir . For ) : return _merge_block ( lift_stmt . pop ( ) + [ op ] , op . body ) raise RuntimeError ( "" not reached "" )","if op . attr_key == ""storage_scope"" :","if op . attr_key == ""virtual_thread"" :",99.03493175,FALSE,97.29
30,"def format_lazy_import ( names ) : """"""Formats lazy import lines"""""" lines = "" "" for _ , name , asname in names : pkg , _ , _ = name . partition ( "" . "" ) <MASK> line = "" {pkg}  = _LazyModule.load( {pkg!r} ,  {mod!r} ) \n "" else : line = "" {asname}  = _LazyModule.load( {pkg!r} ,  {mod!r} ,  {asname!r} ) \n "" lines + = line . format ( pkg = pkg , mod = name , asname = asname ) return lines",if asname is None :,"if pkg == ""__init__"" :",83.41419147,FALSE,91.8
31,"def evaluateWord ( self , argument ) : wildcard_count = argument [ 0 ] . count ( "" * "" ) if wildcard_count > 0 : if wildcard_count == 1 and argument [ 0 ] . startswith ( "" * "" ) : return self . GetWordWildcard ( argument [ 0 ] [ 1 : ] , method = "" endswith "" ) if wildcard_count == 1 and argument [ 0 ] . endswith ( "" * "" ) : return self . GetWordWildcard ( argument [ 0 ] [ : - 1 ] , method = "" startswith "" ) else : _regex = argument [ 0 ] . replace ( "" * "" , "" .+ "" ) matched = False for w in self . words : matched = bool ( re . search ( _regex , w ) ) <MASK> break return matched return self . GetWord ( argument [ 0 ] )",if matched :,if matched :,100,TRUE,100
32,"def setup ( self , ir : "" IR "" , aconf : Config ) - > bool : if self . kind == "" ConsulResolver "" : self . resolve_with = "" consul "" <MASK> self . post_error ( "" ConsulResolver is required to have a datacenter "" ) return False elif self . kind == "" KubernetesServiceResolver "" : self . resolve_with = "" k8s "" elif self . kind == "" KubernetesEndpointResolver "" : self . resolve_with = "" k8s "" else : self . post_error ( f "" Resolver kind  { self . kind }  unknown "" ) return False return True","if not self . get ( ""datacenter"" ) :","if not aconf . get ( ""datacenter"" ) :",91.75613493,FALSE,98.03
33,"def get_success_url ( self ) : """"""Continue to the flow index or redirect according `?back` parameter."""""" if "" back "" in self . request . GET : back_url = self . request . GET [ "" back "" ] <MASK> back_url = "" / "" return back_url return reverse ( self . success_url )","if not is_safe_url ( url = back_url , allowed_hosts = { self . request . get_host ( ) } ) :","if not back_url . startswith ( ""/"" ) :",57.20757035,FALSE,73.4
34,"def download_main ( download , download_playlist , urls , playlist , output_dir , merge , info_only ) : for url in urls : if url . startswith ( "" https:// "" ) : url = url [ 8 : ] <MASK> url = "" http:// "" + url if playlist : download_playlist ( url , output_dir = output_dir , merge = merge , info_only = info_only ) else : download ( url , output_dir = output_dir , merge = merge , info_only = info_only )","if not url . startswith ( ""http://"" ) :","if not url . startswith ( ""http://"" ) :",100,TRUE,100
35,"def __str__ ( self ) : buf = [ "" "" ] if self . fileName : buf . append ( self . fileName + "" : "" ) if self . line != - 1 : <MASK> buf . append ( "" line  "" ) buf . append ( str ( self . line ) ) if self . column != - 1 : buf . append ( "" : "" + str ( self . column ) ) buf . append ( "" : "" ) buf . append ( ""   "" ) return str ( "" "" ) . join ( buf )",if not self . fileName :,if self . line != - 1 :,94.69453083,FALSE,93.43
36,"def parse_bash_set_output ( output ) : """"""Parse Bash-like 'set' output"""""" if not sys . platform . startswith ( "" win "" ) : # Replace ""\""-continued lines in *Linux* environment dumps. # Cannot do this on Windows because a ""\"" at the end of the # line does not imply a continuation. output = output . replace ( "" \\ \n "" , "" "" ) environ = { } for line in output . splitlines ( 0 ) : line = line . rstrip ( ) if not line : continue # skip black lines item = _ParseBashEnvStr ( line ) <MASK> environ [ item [ 0 ] ] = item [ 1 ] return environ",if item :,if item :,75,TRUE,100
37,"def remove_selected ( self ) : """"""Removes selected items from list."""""" to_delete = [ ] for i in range ( len ( self ) ) : if self [ i ] . selected : to_delete . append ( i ) to_delete . reverse ( ) for i in to_delete : self . pop ( i ) if len ( to_delete ) > 0 : first_to_delete = to_delete [ - 1 ] <MASK> self [ 0 ] . selected = True elif first_to_delete > 0 : self [ first_to_delete - 1 ] . selected = True",if first_to_delete == 0 and len ( self ) > 0 :,if first_to_delete == 1 :,87.55350399,FALSE,93.26
38,"def update ( self , update_tracks = True ) : self . enable_update_metadata_images ( False ) old_album_title = self . metadata [ "" album "" ] self . metadata [ "" album "" ] = config . setting [ "" nat_name "" ] for track in self . tracks : <MASK> track . metadata [ "" album "" ] = self . metadata [ "" album "" ] for file in track . linked_files : track . update_file_metadata ( file ) self . enable_update_metadata_images ( True ) super ( ) . update ( update_tracks )","if old_album_title == track . metadata [ ""album"" ] :","if track . metadata [ ""album"" ] != old_album_title :",94.97427396,FALSE,95.39
39,"def on_input ( self , target , message ) : if message . strip ( ) == "" "" : self . panel ( "" No commit message provided "" ) return if target : command = [ "" git "" , "" add "" ] <MASK> command . append ( "" --all "" ) else : command . extend ( ( "" -- "" , target ) ) self . run_command ( command , functools . partial ( self . add_done , message ) ) else : self . add_done ( message , "" "" )","if target == ""*"" :","if target == ""all"" :",98.7470517,FALSE,97.74
40,"def go_to_last_edit_location ( self ) : if self . last_edit_cursor_pos is not None : filename , position = self . last_edit_cursor_pos <MASK> self . last_edit_cursor_pos = None return else : self . load ( filename ) editor = self . get_current_editor ( ) if position < editor . document ( ) . characterCount ( ) : editor . set_cursor_position ( position )",if not osp . isfile ( filename ) :,if filename is None :,64.92239082,FALSE,91.9
41,"def returnByType ( self , results ) : new_results = { } for r in results : type_name = r . get ( "" type "" , "" movie "" ) + "" s "" <MASK> new_results [ type_name ] = [ ] new_results [ type_name ] . append ( r ) # Combine movies, needs a cleaner way.. if "" movies "" in new_results : new_results [ "" movies "" ] = self . combineOnIMDB ( new_results [ "" movies "" ] ) return new_results",if type_name not in new_results :,if type_name not in new_results :,100,TRUE,100
42,"def cache_sns_topics_across_accounts ( ) - > bool : function : str = f "" { __name__ } . { sys . _getframe ( ) . f_code . co_name } "" # First, get list of accounts accounts_d : list = async_to_sync ( get_account_id_to_name_mapping ) ( ) for account_id in accounts_d . keys ( ) : if config . get ( "" environment "" ) == "" prod "" : cache_sns_topics_for_account . delay ( account_id ) else : <MASK> cache_sns_topics_for_account . delay ( account_id ) stats . count ( f "" { function } .success "" ) return True","if account_id in config . get ( ""celery.test_account_ids"" , [ ] ) :","if config . get ( ""environment"" ) == ""prod"" :",95.39327816,FALSE,89.55
43,"def get ( self , subject , topic ) : """"""Handles GET requests."""""" if subject in feconf . AVAILABLE_LANDING_PAGES : <MASK> self . render_template ( "" topic-landing-page.mainpage.html "" ) else : raise self . PageNotFoundException else : raise self . PageNotFoundException",if topic in feconf . AVAILABLE_LANDING_PAGES [ subject ] :,if topic in feconf . AVAILABLE_LANDING_PAGES [ subject ],61.86303934,FALSE,96.38
44,"def callback ( compiled ) : <MASK> logger . show_tabulated ( "" Compiled "" , showpath ( codepath ) , "" without writing to file. "" ) else : with univ_open ( destpath , "" w "" ) as opened : writefile ( opened , compiled ) logger . show_tabulated ( "" Compiled to "" , showpath ( destpath ) , "" . "" ) if self . show : print ( compiled ) if run : <MASK> self . execute ( compiled , path = codepath , allow_show = False ) else : self . execute_file ( destpath )",if destpath is None :,if not self . writefile :,85.37846746,FALSE,90.76
45,"def _find_start_index ( self , string , start , end ) : while True : index = string . find ( "" { "" , start , end ) - 1 if index < 0 : return - 1 <MASK> return index start = index + 2","if self . _start_index_is_ok ( string , index ) :","if string . get ( ""{"" , """" ) == ""}"" :",60.75086904,FALSE,75.7
46,"def _get_nlu_target_format ( export_path : Text ) - > Text : guessed_format = loading . guess_format ( export_path ) if guessed_format not in { MARKDOWN , RASA , RASA_YAML } : if rasa . shared . data . is_likely_json_file ( export_path ) : guessed_format = RASA elif rasa . shared . data . is_likely_markdown_file ( export_path ) : guessed_format = MARKDOWN <MASK> guessed_format = RASA_YAML return guessed_format",elif rasa . shared . data . is_likely_yaml_file ( export_path ) :,elif rasa . shared . data . is_likely_yaml_file ( export_,93.63447674,FALSE,96.51
47,"def moveToThreadNext ( self ) : """"""Move a position to threadNext position."""""" p = self if p . v : if p . v . children : p . moveToFirstChild ( ) el<MASK> p . moveToNext ( ) else : p . moveToParent ( ) while p : <MASK> p . moveToNext ( ) break # found p . moveToParent ( ) # not found. return p",if p . hasNext ( ) :,if p . v . next :,91.35481275,FALSE,89.4
48,"def copy_attributes ( info_add , obj , name_fmt , attributes , formatter = None ) : for attr in attributes : value = getattr ( obj , attr , None ) <MASK> continue name = name_fmt % attr if formatter is not None : value = formatter ( attr , value ) info_add ( name , value )",if value is None :,if value is None :,100,TRUE,100
49,"def getElement ( self , aboutUri , namespace , name ) : for desc in self . rdfRoot . getElementsByTagNameNS ( RDF_NAMESPACE , "" Description "" ) : <MASK> attr = desc . getAttributeNodeNS ( namespace , name ) if attr != None : yield attr for element in desc . getElementsByTagNameNS ( namespace , name ) : yield element","if desc . getAttributeNS ( RDF_NAMESPACE , ""about"" ) == aboutUri :","if desc . getAttributeNS ( namespace , None ) == aboutUri :",93.06661418,FALSE,89.27
50,def run ( self ) : while not self . completed : if self . block : time . sleep ( self . period ) else : self . _completed . wait ( self . period ) self . counter + = 1 try : self . callback ( self . counter ) except Exception : self . stop ( ) <MASK> dt = time . time ( ) - self . _start_time if dt > self . timeout : self . stop ( ) if self . counter == self . count : self . stop ( ),if self . timeout is not None :,if self . counter == self . count :,95.09432029,FALSE,93.26
51,"def _parse_fixits ( message , titer , line ) : """"""Parses fixit messages."""""" while ( OutputParser . message_line_re . match ( line ) is None and OutputParser . note_line_re . match ( line ) is None ) : message_text = line . strip ( ) <MASK> message . fixits . append ( Note ( message . path , message . line , line . find ( message_text ) + 1 , message_text , ) ) line = next ( titer ) return line","if message_text != """" :",if message_text :,95.48410379,FALSE,95.12
52,"def _connect_db ( self , force_reconnect = False ) : thread_id = thread . get_ident ( ) if force_reconnect and thread_id in ENGINES : del ENGINES [ thread_id ] conn = None try : engine = ENGINES [ thread_id ] conn = engine . connect ( ) _test = conn . execute ( "" SELECT 1 "" ) _test . fetchall ( ) except ( KeyError , MySQLdb . OperationalError ) : <MASK> conn . close ( ) engine = sqla . create_engine ( self . db_url , pool_recycle = 3600 ) ENGINES [ thread_id ] = engine conn = engine . connect ( ) return conn",if conn :,if conn :,100,TRUE,100
53,"def read ( self , n ) : if self . current_frame : data = self . current_frame . read ( n ) <MASK> self . current_frame = None return self . file_read ( n ) if len ( data ) < n : raise UnpicklingError ( "" pickle exhausted before end of frame "" ) return data else : return self . file_read ( n )",if not data and n != 0 :,if not data :,77.35733299,FALSE,92.12
54,"def __setLoadCmd ( self ) : base = self . __rawLoadCmd for _ in range ( self . __machHeader . ncmds ) : command = LOAD_COMMAND . from_buffer_copy ( base ) <MASK> segment = SEGMENT_COMMAND . from_buffer_copy ( base ) self . __setSections ( segment , base [ 56 : ] , 32 ) elif command . cmd == MACHOFlags . LC_SEGMENT_64 : segment = SEGMENT_COMMAND64 . from_buffer_copy ( base ) self . __setSections ( segment , base [ 72 : ] , 64 ) base = base [ command . cmdsize : ]",if command . cmd == MACHOFlags . LC_SEGMENT :,if command . cmd == MACHOFlags . LC_SEGMENT_32 :,82.30905583,FALSE,97.46
55,"def emit_post_sync_signal ( created_models , verbosity , interactive , db ) : # Emit the post_sync signal for every application. for app in models . get_apps ( ) : app_name = app . __name__ . split ( "" . "" ) [ - 2 ] <MASK> print ( "" Running post-sync handlers for application  %s "" % app_name ) models . signals . post_syncdb . send ( sender = app , app = app , created_models = created_models , verbosity = verbosity , interactive = interactive , db = db , )",if verbosity >= 2 :,if verbosity >= 2 :,75,TRUE,100
56,"def git_pull ( args ) : if len ( args ) < = 1 : repo = _get_repo ( ) _confirm_dangerous ( ) url = args [ 0 ] if len ( args ) == 1 else repo . remotes . get ( "" origin "" , "" "" ) if url in repo . remotes : origin = url url = repo . remotes . get ( origin ) <MASK> repo . pull ( origin_uri = url ) else : print ( "" No pull URL. "" ) else : print ( command_help [ "" git pull "" ] )",if url :,if url :,100,TRUE,100
57,"def version ( self ) : try : return self . _version except AttributeError : for line in self . _get_metadata ( self . PKG_INFO ) : <MASK> self . _version = safe_version ( line . split ( "" : "" , 1 ) [ 1 ] . strip ( ) ) return self . _version else : tmpl = "" Missing  ' Version: '  header and/or  %s  file "" raise ValueError ( tmpl % self . PKG_INFO , self )","if line . lower ( ) . startswith ( ""version:"" ) :","if line . startswith ( ""version:"" ) :",92.63509086,FALSE,95.64
58,"def increment ( self , metric , labels , delta ) : """"""Increment a value by |delta|."""""" with self . _lock : key = self . _get_key ( metric . name , labels ) <MASK> start_time = self . _store [ key ] . start_time value = self . _store [ key ] . value + delta else : start_time = time . time ( ) value = metric . default_value + delta self . _store [ key ] = _StoreValue ( metric , labels , start_time , value )",if key in self . _store :,if key in self . _store :,100,TRUE,100
59,"def get_current_connections ( session ) : """"""Retrieves open connections using the the given session"""""" # Use Show process list to count the open sesions. res = session . sql ( "" SHOW PROCESSLIST "" ) . execute ( ) rows = res . fetch_all ( ) connections = { } for row in rows : <MASK> connections [ row . get_string ( "" User "" ) ] = [ row . get_string ( "" Host "" ) ] else : connections [ row . get_string ( "" User "" ) ] . append ( row . get_string ( "" Host "" ) ) return connections","if row . get_string ( ""User"" ) not in connections :",if not connections :,70.80107274,FALSE,90.19
60,"def asset ( * paths ) : for path in paths : fspath = www_root + "" /assets/ "" + path etag = "" "" try : <MASK> etag = asset_etag ( fspath ) else : os . stat ( fspath ) except FileNotFoundError as e : if path == paths [ - 1 ] : if not os . path . exists ( fspath + "" .spt "" ) : tell_sentry ( e , { } ) else : continue except Exception as e : tell_sentry ( e , { } ) return asset_url + path + ( etag and "" ?etag= "" + etag )",if env . cache_static :,"if os . name == ""nt"" :",96.14305244,FALSE,92.89
61,def thread_loop ( self ) - > None : while not self . stop_event . is_set ( ) : time . sleep ( 1 ) new_trials = self . study . trials with self . lock : need_to_add_callback = self . new_trials is None self . new_trials = new_trials <MASK> self . doc . add_next_tick_callback ( self . update_callback ),if need_to_add_callback :,if need_to_add_callback :,100,TRUE,100
62,"def _cache_db_tables_iterator ( tables , cache_alias , db_alias ) : no_tables = not tables cache_aliases = settings . CACHES if cache_alias is None else ( cache_alias , ) db_aliases = settings . DATABASES if db_alias is None else ( db_alias , ) for db_alias in db_aliases : if no_tables : tables = connections [ db_alias ] . introspection . table_names ( ) <MASK> for cache_alias in cache_aliases : yield cache_alias , db_alias , tables",if tables :,if cache_aliases :,97.41627334,FALSE,96.22
63,"def remove_subscriber ( self , topic , subscriber ) : if subscriber in self . subscribers [ topic ] : if hasattr ( subscriber , "" _pyroRelease "" ) : subscriber . _pyroRelease ( ) <MASK> try : proxy = self . proxy_cache [ subscriber . _pyroUri ] proxy . _pyroRelease ( ) del self . proxy_cache [ subscriber . _pyroUri ] except KeyError : pass self . subscribers [ topic ] . discard ( subscriber )","if hasattr ( subscriber , ""_pyroUri"" ) :",if self . proxy_cache . get ( subscriber . _pyroUri ) :,93.25042053,FALSE,87.98
64,"def test_constructor ( job_id ) : with patch ( "" apscheduler.job.Job._modify "" ) as _modify : scheduler_mock = MagicMock ( BaseScheduler ) job = Job ( scheduler_mock , id = job_id ) assert job . _scheduler is scheduler_mock assert job . _jobstore_alias is None modify_kwargs = _modify . call_args [ 1 ] <MASK> assert len ( modify_kwargs [ "" id "" ] ) == 32 else : assert modify_kwargs [ "" id "" ] == job_id",if job_id is None :,"if modify_kwargs [ ""type"" ] == ""function"" :",89.64093549,FALSE,88.63
65,"def get_connection ( self ) : if self . config . proxy_host != "" "" : return httplib . HTTPConnection ( self . config . proxy_host , self . config . proxy_port ) else : <MASK> return httplib . HTTPSConnection ( self . config . simpledb_host ) else : return httplib . HTTPConnection ( self . config . simpledb_host )",if self . config . use_https :,"if self . config . simpledb_port != """" :",70.21781873,FALSE,90.07
66,"def notify_login ( self , ipaddress = "" "" ) : if app . NOTIFY_ON_LOGIN : update_text = common . notifyStrings [ common . NOTIFY_LOGIN_TEXT ] title = common . notifyStrings [ common . NOTIFY_LOGIN ] <MASK> self . _notify_pht ( title , update_text . format ( ipaddress ) )",if update_text and title and ipaddress :,if title :,67.33822976,FALSE,88.87
67,"def _getItemHeight ( self , item , ctrl = None ) : """"""Returns the full height of the item to be inserted in the form"""""" if type ( ctrl ) == psychopy . visual . TextBox2 : return ctrl . size [ 1 ] if type ( ctrl ) == psychopy . visual . Slider : # Set radio button layout if item [ "" layout "" ] == "" horiz "" : return 0.03 + ctrl . labelHeight * 3 <MASK> # for vertical take into account the nOptions return ctrl . labelHeight * len ( item [ "" options "" ] )","elif item [ ""layout"" ] == ""vert"" :","elif item [ ""layout"" ] == ""vertical"" :",98.86606557,FALSE,97.98
68,"def _get_errors_lines ( self ) : """"""Return the number of lines that contains errors to highlight."""""" errors_lines = [ ] block = self . document ( ) . begin ( ) while block . isValid ( ) : user_data = get_user_data ( block ) <MASK> errors_lines . append ( block . blockNumber ( ) ) block = block . next ( ) return errors_lines",if user_data . error :,if user_data . error :,100,TRUE,100
69,"def set_pbar_fraction ( self , frac , progress , stage = None ) : gtk . gdk . threads_enter ( ) try : self . is_pulsing = False self . set_stage_text ( stage or _ ( "" Processing... "" ) ) self . pbar . set_text ( progress ) if frac > 1 : frac = 1.0 <MASK> frac = 0 self . pbar . set_fraction ( frac ) finally : gtk . gdk . threads_leave ( )",if frac < 0 :,if frac == 0 :,98.45291226,FALSE,96.57
70,"def list_files ( basedir ) : """"""List files in the directory rooted at |basedir|."""""" if not os . path . isdir ( basedir ) : raise NoSuchDirectory ( basedir ) directories = [ "" "" ] while directories : d = directories . pop ( ) for basename in os . listdir ( os . path . join ( basedir , d ) ) : filename = os . path . join ( d , basename ) if os . path . isdir ( os . path . join ( basedir , filename ) ) : directories . append ( filename ) <MASK> yield filename","elif os . path . exists ( os . path . join ( basedir , filename ) ) :",elif os . path . isfile ( filename ) :,90.853047,FALSE,90.2
71,"def assistive ( self ) : """"""Detects if item can be used as assistance"""""" # Make sure we cache results if self . __assistive is None : assistive = False # Go through all effects and find first assistive for effect in self . effects . values ( ) : <MASK> # If we find one, stop and mark item as assistive assistive = True break self . __assistive = assistive return self . __assistive",if effect . isAssistance is True :,if effect . item is not None :,71.92148893,FALSE,94.35
72,"def closest_unseen ( self , row1 , col1 , filter = None ) : # find the closest unseen from this row/col min_dist = maxint closest_unseen = None for row in range ( self . height ) : for col in range ( self . width ) : if filter is None or ( row , col ) not in filter : if self . map [ row ] [ col ] == UNSEEN : dist = self . distance ( row1 , col1 , row , col ) <MASK> min_dist = dist closest_unseen = ( row , col ) return closest_unseen",if dist < min_dist :,if dist < min_dist :,100,TRUE,100
73,"def _maybe_has_default_route ( self ) : for route in self . iter_routes ( ) : <MASK> return True for iface in self . iter_interfaces ( ) : for subnet in iface . get ( "" subnets "" , [ ] ) : for route in subnet . get ( "" routes "" , [ ] ) : <MASK> return True return False",if self . _is_default_route ( route ) :,"if route . get ( ""default"" ) :",86.75308685,FALSE,77.66
74,"def data ( self , data ) : if data is None : raise Exception ( "" Data cannot be None "" ) val = [ ] for d in data : if isinstance ( d , str ) : val . append ( bytes ( d , "" utf-8 "" ) ) <MASK> val . append ( d ) else : raise Exception ( "" Invalid type, data can only be an str or a bytes not  {} :  {} "" . format ( type ( data ) , d ) ) self . __data = val","elif isinstance ( d , bytes ) :","elif isinstance ( d , bytes ) :",100,TRUE,100
75,"def get_one_segment_function ( data , context , echoerr ) : ext = data [ "" ext "" ] function_name = context [ - 2 ] [ 1 ] . get ( "" function "" ) if function_name : module , function_name = get_function_strings ( function_name , context , ext ) func = import_segment ( function_name , data , context , echoerr , module = module ) <MASK> yield func",if func :,if func :,100,TRUE,100
76,"def generic_visit ( self , node , parents = None ) : parents = ( parents or [ ] ) + [ node ] for field , value in iter_fields ( node ) : if isinstance ( value , list ) : for item in value : <MASK> self . visit ( item , parents ) elif isinstance ( value , AST ) : self . visit ( value , parents )","if isinstance ( item , AST ) :","if isinstance ( item , AST ) :",100,TRUE,100
77,"def find_scintilla_constants ( f ) : lexers = [ ] states = [ ] for name in f . order : v = f . features [ name ] <MASK> if v [ "" FeatureType "" ] == "" val "" : if name . startswith ( "" SCE_ "" ) : states . append ( ( name , v [ "" Value "" ] ) ) elif name . startswith ( "" SCLEX_ "" ) : lexers . append ( ( name , v [ "" Value "" ] ) ) return ( lexers , states )","if v [ ""Category"" ] != ""Deprecated"" :","if v [ ""Type"" ] == ""constant"" :",97.49080169,FALSE,93.89
78,"def things ( self , query ) : limit = query . pop ( "" limit "" , 100 ) offset = query . pop ( "" offset "" , 0 ) keys = set ( self . docs ) for k , v in query . items ( ) : <MASK> # query keys need to be flattened properly, # this corrects any nested keys that have been included # in values. flat = common . flatten_dict ( v ) [ 0 ] k + = "" . "" + web . rstrips ( flat [ 0 ] , "" .key "" ) v = flat [ 1 ] keys = set ( k for k in self . filter_index ( self . index , k , v ) if k in keys ) keys = sorted ( keys ) return keys [ offset : offset + limit ]","if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",100,TRUE,100
79,"def del_ ( self , key ) : initial_hash = hash_ = self . hash ( key ) while True : if self . _keys [ hash_ ] is self . _empty : # That key was never assigned return None <MASK> # key found, assign with deleted sentinel self . _keys [ hash_ ] = self . _deleted self . _values [ hash_ ] = self . _deleted self . _len - = 1 return hash_ = self . _rehash ( hash_ ) if initial_hash == hash_ : # table is full and wrapped around return None",elif self . _keys [ hash_ ] == key :,if self . _values [ hash_ ] is self . _empty :,95.55927487,FALSE,91.26
80,"def test_204_invalid_content_length ( self ) : # 204 status with non-zero content length is malformed with ExpectLog ( gen_log , "" .*Response with code 204 should not have body "" ) : response = self . fetch ( "" /?error=1 "" ) <MASK> self . skipTest ( "" requires HTTP/1.x "" ) if self . http_client . configured_class != SimpleAsyncHTTPClient : self . skipTest ( "" curl client accepts invalid headers "" ) self . assertEqual ( response . code , 599 )",if not self . http1 :,if response . status_code == 200 :,71.54021159,FALSE,91.97
81,"def __str__ ( self ) - > str : text = "" \n "" for k , r in self . result . items ( ) : text + = "" {} \n "" . format ( "" # "" * 40 ) <MASK> text + = "" #  {}  (failed) \n "" . format ( k ) else : text + = "" #  {}  (succeeded) \n "" . format ( k ) text + = "" {} \n "" . format ( "" # "" * 40 ) for sub_r in r : text + = "" ****  {} \n "" . format ( sub_r . name ) text + = "" {} \n "" . format ( sub_r ) return text",if r . failed :,if r is None :,96.03232861,FALSE,97.76
82,"def DeleteTask ( ) : oid = request . form . get ( "" oid "" , "" "" ) if oid : result = Mongo . coll [ "" Task "" ] . delete_one ( { "" _id "" : ObjectId ( oid ) } ) <MASK> result = Mongo . coll [ "" Result "" ] . delete_many ( { "" task_id "" : ObjectId ( oid ) } ) if result : return "" success "" return "" fail """,if result . deleted_count > 0 :,if result :,95.1912413,FALSE,92.11
83,"def _replace_vars ( self , line , extracted , env_variables ) : for e in extracted : <MASK> value = env_variables . get ( e ) if isinstance ( value , dict ) or isinstance ( value , list ) : value = pprint . pformat ( value ) decorated = self . _decorate_var ( e ) line = line . replace ( decorated , str ( value ) ) return line",if e in env_variables :,if e in env_variables :,100,TRUE,100
84,"def should_include ( service ) : for f in filt : if f == "" status "" : state = filt [ f ] containers = project . containers ( [ service . name ] , stopped = True ) if not has_container_with_state ( containers , state ) : return False elif f == "" source "" : source = filt [ f ] if source == "" image "" or source == "" build "" : <MASK> return False else : raise UserError ( "" Invalid value for source filter:  %s "" % source ) else : raise UserError ( "" Invalid filter:  %s "" % f ) return True",if source not in service . options :,"if not has_image_with_build ( containers , source ) :",93.25441012,FALSE,89.94
85,def state_callback_loop ( ) : if usercallback : when = 1 while ( when and not self . future_removed . done ( ) and not self . session . shutdownstarttime ) : result = usercallback ( self . get_state ( ) ) when = ( await result ) if iscoroutine ( result ) else result <MASK> await sleep ( when ),if when > 0.0 and not self . session . shutdownstarttime :,if when :,86.99456991,FALSE,86.16
86,"def __get_new_timeout ( self , timeout ) : """"""When using --timeout_multiplier=#.#"""""" self . __check_scope ( ) try : timeout_multiplier = float ( self . timeout_multiplier ) <MASK> timeout_multiplier = 0.5 timeout = int ( math . ceil ( timeout_multiplier * timeout ) ) return timeout except Exception : # Wrong data type for timeout_multiplier (expecting int or float) return timeout",if timeout_multiplier <= 0.5 :,"if timeout_multiplier == float ( ""inf"" ) :",93.37836216,FALSE,91.12
87,"def readexactly ( self , n ) : buf = b "" "" while n : yield IORead ( self . s ) res = self . s . read ( n ) assert res is not None <MASK> yield IOReadDone ( self . s ) break buf + = res n - = len ( res ) return buf",if not res :,if not res :,100,TRUE,100
88,"def contract_rendering_pane ( event ) : """"""Expand the rendering pane."""""" c = event . get ( "" c "" ) if c : vr = c . frame . top . findChild ( QtWidgets . QWidget , "" viewrendered_pane "" ) <MASK> vr . contract ( ) else : # Just open the pane. viewrendered ( event )",if vr :,if vr :,100,TRUE,100
89,"def translate_headers ( self , environ ) : """"""Translate CGI-environ header names to HTTP header names."""""" for cgiName in environ : # We assume all incoming header keys are uppercase already. <MASK> yield self . headerNames [ cgiName ] , environ [ cgiName ] elif cgiName [ : 5 ] == "" HTTP_ "" : # Hackish attempt at recovering original header names. translatedHeader = cgiName [ 5 : ] . replace ( "" _ "" , "" - "" ) yield translatedHeader , environ [ cgiName ]",if cgiName in self . headerNames :,if cgiName in self . headerNames :,100,TRUE,100
90,"def get_value_from_string ( self , string_value ) : """"""Return internal representation starting from CFN/user-input value."""""" param_value = self . get_default_value ( ) try : <MASK> string_value = str ( string_value ) . strip ( ) if string_value != "" NONE "" : param_value = int ( string_value ) except ValueError : self . pcluster_config . warn ( "" Unable to convert the value  ' {0} '  to an Integer.  "" "" Using default value for parameter  ' {1} ' "" . format ( string_value , self . key ) ) return param_value",if string_value is not None :,if string_value :,91.13282538,FALSE,96.83
91,"def monitor_filter ( self ) : """"""Return filtered service objects list"""""" services = self . client . services . list ( filters = { "" label "" : "" com.ouroboros.enable "" } ) monitored_services = [ ] for service in services : ouro_label = service . attrs [ "" Spec "" ] [ "" Labels "" ] . get ( "" com.ouroboros.enable "" ) <MASK> monitored_services . append ( service ) self . data_manager . monitored_containers [ self . socket ] = len ( monitored_services ) self . data_manager . set ( self . socket ) return monitored_services","if not self . config . label_enable or ouro_label . lower ( ) in [ ""true"" , ""yes"" ] :",if ouro_label :,86.71483291,FALSE,82.3
92,"def nextEditable ( self ) : """"""Moves focus of the cursor to the next editable window"""""" if self . currentEditable is None : if len ( self . _editableChildren ) : self . _currentEditableRef = self . _editableChildren [ 0 ] else : for ref in weakref . getweakrefs ( self . currentEditable ) : if ref in self . _editableChildren : cei = self . _editableChildren . index ( ref ) nei = cei + 1 <MASK> nei = 0 self . _currentEditableRef = self . _editableChildren [ nei ] return self . currentEditable",if nei >= len ( self . _editableChildren ) :,if nei >= len ( self . _editableChildren ) :,100,TRUE,100
93,"def linkify_cm_by_tp ( self , timeperiods ) : for rm in self : mtp_name = rm . modulation_period . strip ( ) # The new member list, in id mtp = timeperiods . find_by_name ( mtp_name ) <MASK> err = ( "" Error: the business impact modulation  ' %s '  got an unknown  "" "" modulation_period  ' %s ' "" % ( rm . get_name ( ) , mtp_name ) ) rm . configuration_errors . append ( err ) rm . modulation_period = mtp","if mtp_name != """" and mtp is None :",if mtp is None :,96.81282268,FALSE,92.83
94,def close_open_fds ( keep = None ) : # noqa keep = [ maybe_fileno ( f ) for f in ( keep or [ ] ) if maybe_fileno ( f ) is not None ] for fd in reversed ( range ( get_fdmax ( default = 2048 ) ) ) : <MASK> try : os . close ( fd ) except OSError as exc : if exc . errno != errno . EBADF : raise,if fd not in keep :,if fd not in keep :,100,TRUE,100
95,"def _append_child_from_unparsed_xml ( father_node , unparsed_xml ) : """"""Append child xml nodes to a node."""""" dom_tree = parseString ( unparsed_xml ) if dom_tree . hasChildNodes ( ) : first_child = dom_tree . childNodes [ 0 ] <MASK> child_nodes = first_child . childNodes for _ in range ( len ( child_nodes ) ) : childNode = child_nodes . item ( 0 ) father_node . appendChild ( childNode ) return raise DistutilsInternalError ( "" Could not Append append elements to  "" "" the Windows msi descriptor. "" )",if first_child . hasChildNodes ( ) :,if first_child . nodeType == dom_node . ELEMENT_NODE :,83.38793349,FALSE,91.8
96,"def process_request ( self , request ) : for old , new in self . names_name : request . uri = request . uri . replace ( old , new ) <MASK> body = six . ensure_str ( request . body ) if old in body : request . body = body . replace ( old , new ) return request",if is_text_payload ( request ) and request . body :,if request . body :,65.10397716,FALSE,86.16
97,"def __init__ ( self , * * options ) : self . func_name_highlighting = get_bool_opt ( options , "" func_name_highlighting "" , True ) self . disabled_modules = get_list_opt ( options , "" disabled_modules "" , [ ] ) self . _functions = set ( ) if self . func_name_highlighting : from pygments . lexers . _luabuiltins import MODULES for mod , func in MODULES . iteritems ( ) : <MASK> self . _functions . update ( func ) RegexLexer . __init__ ( self , * * options )",if mod not in self . disabled_modules :,if mod in self . disabled_modules :,98.50542391,FALSE,98.09
98,"def GetBestSizeForParentSize ( self , parentSize ) : """"""Finds the best width and height given the parent's width and height."""""" if len ( self . GetChildren ( ) ) == 1 : win = self . GetChildren ( ) [ 0 ] <MASK> temp_dc = wx . ClientDC ( self ) childSize = win . GetBestSizeForParentSize ( parentSize ) clientParentSize = self . _art . GetPanelClientSize ( temp_dc , self , wx . Size ( * parentSize ) , None ) overallSize = self . _art . GetPanelSize ( temp_dc , self , wx . Size ( * clientParentSize ) , None ) return overallSize return self . GetSize ( )","if isinstance ( win , RibbonControl ) :",if win . GetWidth ( ) == self . GetHeight ( ) :,91.07850484,FALSE,91.31
99,"def pid_from_name ( name ) : processes = [ ] for pid in os . listdir ( "" /proc "" ) : try : pid = int ( pid ) pname , cmdline = SunProcess . _name_args ( pid ) <MASK> return pid if name in cmdline . split ( ""   "" , 1 ) [ 0 ] : return pid except : pass raise ProcessException ( "" No process with such name:  %s "" % name )",if name in pname :,"if pname == ""pid"" :",94.04647396,FALSE,92.21
100,"def __get_file_by_num ( self , num , file_list , idx = 0 ) : for element in file_list : if idx == num : return element <MASK> i = self . __get_file_by_num ( num , element [ 3 ] , idx + 1 ) if not isinstance ( i , int ) : return i idx = i else : idx + = 1 return idx",if element [ 3 ] and element [ 4 ] :,if element [ 3 ] :,91.80312388,FALSE,93.91
101,"def scan_block_scalar_indentation ( self ) : # See the specification for details. chunks = [ ] max_indent = 0 end_mark = self . get_mark ( ) while self . peek ( ) in ""   \r \n \x85 \u2028 \u2029 "" : if self . peek ( ) != ""   "" : chunks . append ( self . scan_line_break ( ) ) end_mark = self . get_mark ( ) else : self . forward ( ) <MASK> max_indent = self . column return chunks , max_indent , end_mark",if self . column > max_indent :,if self . column > max_indent :,75,TRUE,100
102,"def ant_map ( m ) : tmp = "" rows  %s \n cols  %s \n "" % ( len ( m ) , len ( m [ 0 ] ) ) players = { } for row in m : tmp + = "" m  "" for col in row : if col == LAND : tmp + = "" . "" elif col == BARRIER : tmp + = "" % "" <MASK> tmp + = "" * "" elif col == UNSEEN : tmp + = "" ? "" else : players [ col ] = True tmp + = chr ( col + 97 ) tmp + = "" \n "" tmp = ( "" players  %s \n "" % len ( players ) ) + tmp return tmp",elif col == FOOD :,elif col == AND :,87.59064183,FALSE,98.3
103,"def prepare_data ( entry ) : branch_wise_entries = { } gross_pay = 0 for d in entry : gross_pay + = d . gross_pay <MASK> branch_wise_entries [ d . branch ] [ d . mode_of_payment ] = d . net_pay else : branch_wise_entries . setdefault ( d . branch , { } ) . setdefault ( d . mode_of_payment , d . net_pay ) return branch_wise_entries , gross_pay",if branch_wise_entries . get ( d . branch ) :,if d . branch in branch_wise_entries :,82.7746874,FALSE,92.69
104,"def __init__ ( self , uuid = None , cluster_state = None , children = None , * * kwargs ) : self . uuid = uuid self . cluster_state = cluster_state if self . cluster_state is not None : self . children = WeakSet ( self . cluster_state . tasks . get ( task_id ) for task_id in children or ( ) <MASK> ) else : self . children = WeakSet ( ) self . _serializer_handlers = { "" children "" : self . _serializable_children , "" root "" : self . _serializable_root , "" parent "" : self . _serializable_parent , } if kwargs : self . __dict__ . update ( kwargs )",if task_id in self . cluster_state . tasks,if task_id is not None,84.37355647,FALSE,93.98
105,"def listdir ( self , d ) : try : return [ p for p in os . listdir ( d ) <MASK> ] except OSError : return [ ]","if os . path . basename ( p ) != ""CVS"" and os . path . isdir ( os . path . join ( d , p ) )",if p . startswith ( self . prefix ) and p . endswith ( self . prefix ),36.13638321,FALSE,48.85
106,"def send_packed_command ( self , command , check_health = True ) : if not self . _sock : self . connect ( ) try : <MASK> command = [ command ] for item in command : self . _sock . sendall ( item ) except socket . error as e : self . disconnect ( ) if len ( e . args ) == 1 : _errno , errmsg = "" UNKNOWN "" , e . args [ 0 ] else : _errno , errmsg = e . args raise ConnectionError ( "" Error  %s  while writing to socket.  %s . "" % ( _errno , errmsg ) ) except Exception : self . disconnect ( ) raise","if isinstance ( command , str ) :",if not check_health :,90.15223881,FALSE,94.58
107,"def run ( self ) : """"""Start the scanner"""""" logging . info ( "" Dirscanner starting up "" ) self . shutdown = False while not self . shutdown : # Wait to be woken up or triggered with self . loop_condition : self . loop_condition . wait ( self . dirscan_speed ) <MASK> self . scan ( )",if self . dirscan_speed and not self . shutdown :,if self . dirscan_speed :,94.74144484,FALSE,91.82
108,"def __aexit__ ( self , exc_type : type , exc_value : BaseException , tb : TracebackType ) - > None : if exc_type is not None : await self . close ( ) await self . _task while not self . _receive_queue . empty ( ) : data = await self . _receive_queue . get ( ) if isinstance ( data , bytes ) : self . response_data . extend ( data ) <MASK> raise data","elif not isinstance ( data , HTTPDisconnect ) :",elif exc_type is not None :,87.62423984,FALSE,91.85
109,"def f ( msg ) : text = extractor ( msg ) for px in prefix : <MASK> chunks = text [ len ( px ) : ] . split ( separator ) return chunks [ 0 ] , ( chunks [ 1 : ] , ) if pass_args else ( ) return ( ( None , ) , ) # to distinguish with `None`",if text . startswith ( px ) :,if text . startswith ( px ) :,100,TRUE,100
110,"def _flatten ( * args ) : ahs = set ( ) if len ( args ) > 0 : for item in args : if type ( item ) is ActionHandle : ahs . add ( item ) <MASK> for ah in item : if type ( ah ) is not ActionHandle : # pragma:nocover raise ActionManagerError ( "" Bad argument type  %s "" % str ( ah ) ) ahs . add ( ah ) else : # pragma:nocover raise ActionManagerError ( "" Bad argument type  %s "" % str ( item ) ) return ahs","elif type ( item ) in ( list , tuple , dict , set ) :","elif isinstance ( item , ( list , tuple ) ) :",83.03208719,FALSE,90.83
111,"def find_class ( self , module , name ) : # Subclasses may override this. sys . audit ( "" pickle.find_class "" , module , name ) if self . proto < 3 and self . fix_imports : if ( module , name ) in _compat_pickle . NAME_MAPPING : module , name = _compat_pickle . NAME_MAPPING [ ( module , name ) ] <MASK> module = _compat_pickle . IMPORT_MAPPING [ module ] __import__ ( module , level = 0 ) if self . proto > = 4 : return _getattribute ( sys . modules [ module ] , name ) [ 0 ] else : return getattr ( sys . modules [ module ] , name )",elif module in _compat_pickle . IMPORT_MAPPING :,if module in _compat_pickle . IMPORT_MAPPING :,73.82891097,FALSE,98.38
112,"def _send_until_done ( self , data ) : while True : try : return self . connection . send ( data ) except OpenSSL . SSL . WantWriteError : wr = util . wait_for_write ( self . socket , self . socket . gettimeout ( ) ) <MASK> raise timeout ( ) continue except OpenSSL . SSL . SysCallError as e : raise SocketError ( str ( e ) )",if not wr :,if wr is None :,94.08437579,FALSE,94.74
113,"def __new__ ( cls , * args , * * kwargs ) : """"""Hack to ensure method defined as async are implemented as such."""""" coroutines = inspect . getmembers ( BaseManager , predicate = inspect . iscoroutinefunction ) for coroutine in coroutines : implemented_method = getattr ( cls , coroutine [ 0 ] ) <MASK> raise RuntimeError ( "" The method  %s  must be a coroutine "" % implemented_method ) return super ( ) . __new__ ( cls , * args , * * kwargs )",if not inspect . iscoroutinefunction ( implemented_method ) :,"if not implemented_method . startswith ( ""coroutine"" ) :",91.15352098,FALSE,92.64
114,"def add_directive ( self , name , obj , content = None , arguments = None , * * options ) : if isinstance ( obj , clstypes ) and issubclass ( obj , Directive ) : <MASK> raise ExtensionError ( "" when adding directive classes, no  "" "" additional arguments may be given "" ) directives . register_directive ( name , directive_dwim ( obj ) ) else : obj . content = content obj . arguments = arguments obj . options = options directives . register_directive ( name , obj )",if content or arguments or options :,if arguments is None :,93.68423837,FALSE,94.06
115,"def create ( self , w ) : if w . use_eventloop : # does not use dedicated timer thread. w . timer = _Timer ( max_interval = 10.0 ) else : <MASK> # Default Timer is set by the pool, as for example, the # eventlet pool needs a custom timer implementation. w . timer_cls = w . pool_cls . Timer w . timer = self . instantiate ( w . timer_cls , max_interval = w . timer_precision , on_error = self . on_timer_error , on_tick = self . on_timer_tick , )",if not w . timer_cls :,if w . timer_cls is None :,97.48777583,FALSE,96.14
116,"def _config ( _molecule_file , request ) : with open ( _molecule_file ) as f : d = util . safe_load ( f ) if hasattr ( request , "" param "" ) : <MASK> d2 = util . safe_load ( request . getfixturevalue ( request . param ) ) else : d2 = request . getfixturevalue ( request . param ) # print(100, d) # print(200, d2) d = util . merge_dicts ( d , d2 ) # print(300, d) return d","if isinstance ( request . getfixturevalue ( request . param ) , str ) :","if isinstance ( request . param , dict ) :",77.82349741,FALSE,93.1
117,"def _instrument_model ( self , model ) : for key , value in list ( model . __dict__ . items ( ) ) : # avoid ""dictionary keys changed during iteration"" <MASK> new_layer = self . _instrument ( value ) if new_layer is not value : setattr ( model , key , new_layer ) elif isinstance ( value , list ) : for i , item in enumerate ( value ) : if isinstance ( item , tf . keras . layers . Layer ) : value [ i ] = self . _instrument ( item ) return model","if isinstance ( value , tf . keras . layers . Layer ) :","if isinstance ( value , dict ) :",96.24188003,FALSE,93.27
118,"def is_accepted_drag_event ( self , event ) : if event . source ( ) == self . table : return True mime = event . mimeData ( ) if mime . hasUrls ( ) : for url in mime . urls ( ) : # Only support local files. <MASK> break # And only allow supported extensions. filename = url . toLocalFile ( ) extension = os . path . splitext ( filename ) [ 1 ] . lower ( ) [ 1 : ] if extension not in _dictionary_formats ( ) : break else : return True return False",if not url . isLocalFile ( ) :,if url . isLocalFile ( ) :,98.69715338,FALSE,97.87
119,"def explain ( self , other , depth = 0 ) : exp = super ( UnionType , self ) . explain ( other , depth ) for ndx , subtype in enumerate ( self . params [ "" allowed_types "" ] ) : <MASK> exp + = "" \n {} and "" . format ( "" "" . join ( [ "" \t "" ] * depth ) ) exp + = "" \n "" + subtype . explain ( other , depth = depth + 1 ) return exp",if ndx > 0 :,if ndx == 0 :,98.66492143,FALSE,96.5
120,"def test_k_is_stochastic_parameter ( self ) : # k as stochastic parameter aug = iaa . MedianBlur ( k = iap . Choice ( [ 3 , 5 ] ) ) seen = [ False , False ] for i in sm . xrange ( 100 ) : observed = aug . augment_image ( self . base_img ) if np . array_equal ( observed , self . blur3x3 ) : seen [ 0 ] + = True <MASK> seen [ 1 ] + = True else : raise Exception ( "" Unexpected result in MedianBlur@2 "" ) if all ( seen ) : break assert np . all ( seen )","elif np . array_equal ( observed , self . blur5x5 ) :","elif np . array_equal ( observed , self . blur5x5 ) :",75,TRUE,100
121,"def test_get_message ( self ) : async with self . chat_client : await self . _create_thread ( ) async with self . chat_thread_client : message_id = await self . _send_message ( ) message = await self . chat_thread_client . get_message ( message_id ) assert message . id == message_id assert message . type == ChatMessageType . TEXT assert message . content . message == "" hello world "" # delete chat threads <MASK> await self . chat_client . delete_chat_thread ( self . thread_id )",if not self . is_playback ( ) :,if self . thread_id :,96.13848169,FALSE,93.45
122,"def do_write_property ( self , device , callback = None ) : try : iocb = ( device <MASK> else self . form_iocb ( device , request_type = "" writeProperty "" ) ) deferred ( self . request_io , iocb ) self . requests_in_progress . update ( { iocb : { "" callback "" : callback } } ) iocb . add_callback ( self . __general_cb ) except Exception as error : log . exception ( "" exception:  %r "" , error )","if isinstance ( device , IOCB )","if not device . startswith ( ""http"" )",78.55248406,FALSE,91.97
123,"def fit ( self , dataset , force_retrain ) : if force_retrain : self . sub_unit_1 [ "" fitted "" ] = True self . sub_unit_1 [ "" calls "" ] + = 1 self . sub_unit_2 [ "" fitted "" ] = True self . sub_unit_2 [ "" calls "" ] + = 1 else : if not self . sub_unit_1 [ "" fitted "" ] : self . sub_unit_1 [ "" fitted "" ] = True self . sub_unit_1 [ "" calls "" ] + = 1 <MASK> self . sub_unit_2 [ "" fitted "" ] = True self . sub_unit_2 [ "" calls "" ] + = 1 return self","if not self . sub_unit_2 [ ""fitted"" ] :","if not self . sub_unit_2 [ ""fitted"" ] :",100,TRUE,100
124,"def _insert_with_loop ( self ) : id_list = [ ] last_id = None return_id_list = self . _return_id_list for row in self . _rows : last_id = InsertQuery ( self . model_class , row ) . upsert ( self . _upsert ) . execute ( ) <MASK> id_list . append ( last_id ) <MASK> return id_list else : return last_id",if return_id_list :,if return_id_list :,100,TRUE,100
125,"def merge_block ( self ) : """"""merges a block in the map"""""" for i in range ( self . block . x ) : for j in range ( self . block . x ) : c = self . block . get ( i , j ) <MASK> self . map [ ( i + self . block . pos . x , j + self . block . pos . y ) ] = c",if c :,if c is not None :,78.02684512,FALSE,94.89
126,"def configure_plex ( config ) : core . PLEX_SSL = int ( config [ "" Plex "" ] [ "" plex_ssl "" ] ) core . PLEX_HOST = config [ "" Plex "" ] [ "" plex_host "" ] core . PLEX_PORT = config [ "" Plex "" ] [ "" plex_port "" ] core . PLEX_TOKEN = config [ "" Plex "" ] [ "" plex_token "" ] plex_section = config [ "" Plex "" ] [ "" plex_sections "" ] or [ ] if plex_section : <MASK> plex_section = "" , "" . join ( plex_section ) # fix in case this imported as list. plex_section = [ tuple ( item . split ( "" , "" ) ) for item in plex_section . split ( "" | "" ) ] core . PLEX_SECTION = plex_section","if isinstance ( plex_section , list ) :","if isinstance ( plex_section , list ) :",100,TRUE,100
127,"def select ( self ) : e = xlib . XEvent ( ) while xlib . XPending ( self . _display ) : xlib . XNextEvent ( self . _display , e ) # Key events are filtered by the xlib window event # handler so they get a shot at the prefiltered event. <MASK> if xlib . XFilterEvent ( e , e . xany . window ) : continue try : dispatch = self . _window_map [ e . xany . window ] except KeyError : continue dispatch ( e )","if e . xany . type not in ( xlib . KeyPress , xlib . KeyRelease ) :",if xlib . XFilterEvent ( e ) :,92.64170846,FALSE,86.25
128,"def format_message ( self ) : bits = [ self . message ] if self . possibilities : <MASK> bits . append ( "" Did you mean  %s ? "" % self . possibilities [ 0 ] ) else : possibilities = sorted ( self . possibilities ) bits . append ( "" (Possible options:  %s ) "" % "" ,  "" . join ( possibilities ) ) return ""    "" . join ( bits )",if len ( self . possibilities ) == 1 :,if len ( self . possibilities ) == 1 :,100,TRUE,100
129,"def _collect_logs ( model ) : page_token = None all_logs = [ ] while True : paginated_logs = model . lookup_logs ( now , later , page_token = page_token ) page_token = paginated_logs . next_page_token all_logs . extend ( paginated_logs . logs ) <MASK> break return all_logs",if page_token is None :,if not paginated_logs . logs :,90.55900922,FALSE,91.06
130,"def run ( self ) : while True : context_id_list_tuple = self . _inflated_addresses . get ( block = True ) <MASK> break c_id , inflated_address_list = context_id_list_tuple inflated_value_map = dict ( inflated_address_list ) if c_id in self . _contexts : self . _contexts [ c_id ] . set_from_tree ( inflated_value_map )",if context_id_list_tuple is _SHUTDOWN_SENTINEL :,if context_id_list_tuple is None :,91.28601402,FALSE,94.82
131,"def _setup_prefix ( self ) : # we assume here that our metadata may be nested inside a ""basket"" # of multiple eggs; that's why we use module_path instead of .archive path = self . module_path old = None while path != old : <MASK> self . egg_name = os . path . basename ( path ) self . egg_info = os . path . join ( path , "" EGG-INFO "" ) self . egg_root = path break old = path path , base = os . path . split ( path )","if path . lower ( ) . endswith ( "".egg"" ) :",if os . path . isdir ( path ) :,70.54570714,FALSE,90.08
132,"def get_filename ( self , prompt ) : okay = False val = "" "" while not okay : val = raw_input ( "" %s :  %s "" % ( prompt , val ) ) val = os . path . expanduser ( val ) if os . path . isfile ( val ) : okay = True <MASK> path = val val = self . choose_from_list ( os . listdir ( path ) ) if val : val = os . path . join ( path , val ) okay = True else : val = "" "" else : print ( "" Invalid value:  %s "" % val ) val = "" "" return val",elif os . path . isdir ( val ) :,elif os . path . isdir ( val ) :,100,TRUE,100
133,"def versions ( self , sitename , data ) : # handle the query of type {""query"": '{""key"": ""/books/ia:foo00bar"", ...}} if "" query "" in data : q = json . loads ( data [ "" query "" ] ) itemid = self . _get_itemid ( q . get ( "" key "" ) ) <MASK> key = q [ "" key "" ] return json . dumps ( [ self . dummy_edit ( key ) ] ) # if not just go the default way return ConnectionMiddleware . versions ( self , sitename , data )",if itemid :,if itemid is not None and itemid != self . _id :,72.14066125,FALSE,90.69
134,"def read_stanza ( self ) : while True : try : stanza_end = self . _buffer . index ( b "" \n "" ) stanza = self . decoder . decode ( self . _buffer [ : stanza_end ] ) self . _buffer = self . _buffer [ stanza_end + 1 : ] colon = stanza . index ( "" : "" ) return stanza [ : colon ] , stanza [ colon + 1 : ] except ValueError : bytes = self . read_bytes ( ) <MASK> return None else : self . _buffer + = bytes",if not bytes :,if bytes is None :,95.91125085,FALSE,96.33
135,def decodeattrs ( attrs ) : names = [ ] for bit in range ( 16 ) : mask = 1 << bit <MASK> if attrnames . has_key ( mask ) : names . append ( attrnames [ mask ] ) else : names . append ( hex ( mask ) ) return names,if attrs & mask :,if attrs & ( 1 << bit ) :,91.70981809,FALSE,88.32
136,"def _set_http_cookie ( ) : if conf . cookie : <MASK> conf . http_headers [ HTTP_HEADER . COOKIE ] = "" ;  "" . join ( map ( lambda x : "" = "" . join ( x ) , conf . cookie . items ( ) ) ) else : conf . http_headers [ HTTP_HEADER . COOKIE ] = conf . cookie","if isinstance ( conf . cookie , dict ) :","if isinstance ( conf . cookie , dict ) :",100,TRUE,100
137,"def __ne__ ( self , other ) : if isinstance ( other , WeakMethod ) : <MASK> return self is not other return weakref . ref . __ne__ ( self , other ) or self . _func_ref != other . _func_ref return True",if not self . _alive or not other . _alive :,if self . _func_ref is None :,76.28744996,FALSE,84.15
138,"def update_unread ( self , order_id , reset = False ) : conn = Database . connect_database ( self . PATH ) with conn : cursor = conn . cursor ( ) <MASK> cursor . execute ( """"""UPDATE sales SET unread = unread + 1 WHERE id=?;"""""" , ( order_id , ) ) else : cursor . execute ( """""" UPDATE sales SET unread=0 WHERE id=?; """""" , ( order_id , ) ) conn . commit ( ) conn . close ( )",if reset is False :,if reset :,90.68872837,FALSE,96.91
139,"def _get_field_value ( self , test , key , match ) : if test . ver == ofproto_v1_0 . OFP_VERSION : members = inspect . getmembers ( match ) for member in members : if member [ 0 ] == key : field_value = member [ 1 ] elif member [ 0 ] == "" wildcards "" : wildcards = member [ 1 ] if key == "" nw_src "" : field_value = test . nw_src_to_str ( wildcards , field_value ) <MASK> field_value = test . nw_dst_to_str ( wildcards , field_value ) else : field_value = match [ key ] return field_value","elif key == ""nw_dst"" :","elif key == ""nw_dst"" :",100,TRUE,100
140,"def nested_filter ( self , items , mask ) : keep_current = self . current_mask ( mask ) keep_nested_lookup = self . nested_masks ( mask ) for k , v in items : keep_nested = keep_nested_lookup . get ( k ) <MASK> if keep_nested is not None : if isinstance ( v , dict ) : yield k , dict ( self . nested_filter ( v . items ( ) , keep_nested ) ) else : yield k , v",if k in keep_current :,if keep_current is not None and k != keep_current :,92.76149996,FALSE,90.32
141,"def goToPrevMarkedHeadline ( self , event = None ) : """"""Select the next marked node."""""" c = self p = c . p if not p : return p . moveToThreadBack ( ) wrapped = False while 1 : <MASK> break elif p : p . moveToThreadBack ( ) elif wrapped : break else : wrapped = True p = c . rootPosition ( ) if not p : g . blue ( "" done "" ) c . treeSelectHelper ( p ) # Sets focus.",if p and p . isMarked ( ) :,if p . isMarked ( ) :,93.52283492,FALSE,97.26
142,"def sample ( self , * * config ) : """"""Sample a configuration from this search space."""""" ret = { } ret . update ( self . data ) kwspaces = self . kwspaces kwspaces . update ( config ) striped_keys = [ k . split ( SPLITTER ) [ 0 ] for k in config . keys ( ) ] for k , v in kwspaces . items ( ) : <MASK> if isinstance ( v , NestedSpace ) : sub_config = _strip_config_space ( config , prefix = k ) ret [ k ] = v . sample ( * * sub_config ) else : ret [ k ] = v return ret",if k in striped_keys :,if k not in striped_keys :,99.13914817,FALSE,98.17
143,"def update_gradients_full ( self , dL_dK , X , X2 = None ) : if self . ARD : phi1 = self . phi ( X ) <MASK> self . variance . gradient = np . einsum ( "" ij,iq,jq->q "" , dL_dK , phi1 , phi1 ) else : phi2 = self . phi ( X2 ) self . variance . gradient = np . einsum ( "" ij,iq,jq->q "" , dL_dK , phi1 , phi2 ) else : self . variance . gradient = np . einsum ( "" ij,ij "" , dL_dK , self . _K ( X , X2 ) ) * self . beta",if X2 is None or X is X2 :,if self . ARD :,79.15743255,FALSE,94.02
144,"def post ( self ) : host_json = json . loads ( request . data ) host_os = host_json . get ( "" os "" ) if host_os : result = get_monkey_executable ( host_os . get ( "" type "" ) , host_os . get ( "" machine "" ) ) if result : # change resulting from new base path executable_filename = result [ "" filename "" ] real_path = MonkeyDownload . get_executable_full_path ( executable_filename ) <MASK> result [ "" size "" ] = os . path . getsize ( real_path ) return result return { }",if os . path . isfile ( real_path ) :,if real_path :,95.65411531,FALSE,92.63
145,"def _encode_data ( self , data , content_type , ) : if content_type is MULTIPART_CONTENT : return encode_multipart ( BOUNDARY , data ) else : # Encode the content so that the byte representation is correct. match = CONTENT_TYPE_RE . match ( content_type ) <MASK> charset = match . group ( 1 ) else : charset = settings . DEFAULT_CHARSET return force_bytes ( data , encoding = charset )",if match :,if match :,75,TRUE,100
146,"def _merge_scientific_float_tokens ( tokens : Iterable [ str ] ) - > List [ str ] : tokens = list ( tokens ) i = 0 while "" e "" in tokens [ i + 1 : ] : i = tokens . index ( "" e "" , i + 1 ) s = i - 1 e = i + 1 if not re . match ( "" [0-9] "" , str ( tokens [ s ] ) ) : continue if re . match ( "" [+-] "" , str ( tokens [ e ] ) ) : e + = 1 <MASK> e + = 1 tokens [ s : e ] = [ "" "" . join ( tokens [ s : e ] ) ] i - = 1 return tokens","if re . match ( ""[0-9]"" , str ( tokens [ e ] ) ) :","if re . match ( ""[+-]"" , str ( tokens [ e ] ) ) :",99.17296909,FALSE,97.5
147,"def convert_with_key ( self , key , value , replace = True ) : result = self . configurator . convert ( value ) # If the converted value is different, save for next time if value is not result : <MASK> self [ key ] = result if type ( result ) in ( ConvertingDict , ConvertingList , ConvertingTuple ) : result . parent = self result . key = key return result",if replace :,if replace :,100,TRUE,100
148,"def OnListEndLabelEdit ( self , std , extra ) : item = extra [ 0 ] text = item [ 4 ] if text is None : return item_id = self . GetItem ( item [ 0 ] ) [ 6 ] from bdb import Breakpoint for bplist in Breakpoint . bplist . itervalues ( ) : for bp in bplist : <MASK> if text . strip ( ) . lower ( ) == "" none "" : text = None bp . cond = text break self . RespondDebuggerData ( )",if id ( bp ) == item_id :,if bp . id == item_id :,94.25834951,FALSE,95.14
149,"def add ( self , url : str , future_nzo : NzbObject , when : Optional [ int ] = None ) : """"""Add an URL to the URLGrabber queue, 'when' is seconds from now"""""" if future_nzo and when : # Always increase counter future_nzo . url_tries + = 1 # Too many tries? Cancel <MASK> self . fail_to_history ( future_nzo , url , T ( "" Maximum retries "" ) ) return future_nzo . url_wait = time . time ( ) + when self . queue . put ( ( url , future_nzo ) )",if future_nzo . url_tries > cfg . max_url_retries ( ) :,if future_nzo . url_tries > self . max_retries :,97.05152709,FALSE,93.9
150,def _is_datetime_string ( series ) : if series . dtype == object : not_numeric = False try : pd . to_numeric ( series ) except Exception as e : not_numeric = True datetime_col = None <MASK> try : datetime_col = pd . to_datetime ( series ) except Exception as e : return False if datetime_col is not None : return True return False,if not_numeric :,if not_numeric :,100,TRUE,100
151,"def _getEventAndObservers ( self , event ) : if isinstance ( event , xpath . XPathQuery ) : # Treat as xpath observers = self . _xpathObservers else : <MASK> # Treat as event observers = self . _eventObservers else : # Treat as xpath event = xpath . internQuery ( event ) observers = self . _xpathObservers return event , observers",if self . prefix == event [ : len ( self . prefix ) ] :,"if isinstance ( event , xpath . Event ) :",90.52937263,FALSE,80.43
152,"def test_wildcard_import ( ) : bonobo = __import__ ( "" bonobo "" ) assert bonobo . __version__ for name in dir ( bonobo ) : # ignore attributes starting by underscores if name . startswith ( "" _ "" ) : continue attr = getattr ( bonobo , name ) <MASK> continue assert name in bonobo . __all__",if inspect . ismodule ( attr ) :,"if not hasattr ( attr , ""__call__"" ) :",95.16891553,FALSE,85.2
153,"def relint_views ( wid = None ) : windows = [ sublime . Window ( wid ) ] if wid else sublime . windows ( ) for window in windows : for view in window . views ( ) : <MASK> hit ( view , "" relint_views "" )",if view . buffer_id ( ) in persist . assigned_linters and view . is_primary ( ) :,"if view . name == ""view"" :",85.13227103,FALSE,71.3
154,def _check_for_unknown_gender ( self ) : if self . obj . get_gender ( ) == Person . UNKNOWN : d = GenderDialog ( parent = self . window ) gender = d . run ( ) d . destroy ( ) <MASK> self . obj . set_gender ( gender ),if gender >= 0 :,if gender is not None :,92.38711955,FALSE,93.08
155,"def add_to_path ( self , fnames ) : """"""Add fnames to path"""""" indexes = [ ] for path in fnames : project = self . get_source_project ( path ) <MASK> self . parent_widget . emit ( SIGNAL ( "" pythonpath_changed() "" ) ) indexes . append ( self . get_index ( path ) ) if indexes : self . reset_icon_provider ( ) for index in indexes : self . update ( index )",if project . add_to_pythonpath ( path ) :,if project and project . is_dir ( ) :,93.62343016,FALSE,92.13
156,"def validate ( self , value ) : if value . grid_id is not None : if not isinstance ( value , self . proxy_class ) : self . error ( "" FileField only accepts GridFSProxy values "" ) <MASK> self . error ( "" Invalid GridFSProxy value "" )","if not isinstance ( value . grid_id , ObjectId ) :",if not value . grid_id :,83.8481865,FALSE,87.35
157,"def shortcut ( self , input , ch_out , stride , name , if_first = False ) : ch_in = input . shape [ 1 ] if ch_in != ch_out or stride != 1 : <MASK> return self . conv_bn_layer ( input , ch_out , 1 , stride , name = name ) else : return self . conv_bn_layer_new ( input , ch_out , 1 , stride , name = name ) else : return input",if if_first :,if if_first :,100,TRUE,100
158,"def convert_path ( ctx , tpath ) : for points , code in tpath . iter_segments ( ) : if code == Path . MOVETO : ctx . move_to ( * points ) elif code == Path . LINETO : ctx . line_to ( * points ) elif code == Path . CURVE3 : ctx . curve_to ( points [ 0 ] , points [ 1 ] , points [ 0 ] , points [ 1 ] , points [ 2 ] , points [ 3 ] ) <MASK> ctx . curve_to ( * points ) elif code == Path . CLOSEPOLY : ctx . close_path ( )",elif code == Path . CURVE4 :,elif code == Path . CURVE4 :,75,TRUE,100
159,"def _get_build_status ( self , job_name , build_number ) : try : build_info = self . server . get_build_info ( job_name , build_number ) <MASK> return "" building "" else : return "" built "" except jenkins . NotFoundException : return "" not found ""","if build_info [ ""building"" ] :","if build_info . get ( ""building_status"" ) == ""built"" :",58.3203217,FALSE,81.89
160,"def _parse_param_value ( name , datatype , default ) : if datatype == "" bool "" : if default . lower ( ) == "" true "" : return True elif default . lower ( ) == "" false "" : return False else : _s = "" {} : Invalid default value  ' {} '  for bool parameter  {} "" raise SyntaxError ( _s . format ( self . name , default , p ) ) elif datatype == "" int "" : if type ( default ) == int : return default else : return int ( default , 0 ) elif datatype == "" real "" : <MASK> return default else : return float ( default ) else : return str ( default )",if type ( default ) == float :,if type ( default ) == float :,100,TRUE,100
161,"def get_fills ( self , exchange_order_id ) : async with aiohttp . ClientSession ( ) as client : response : aiohttp . ClientResponse = await client . get ( f "" { BASE_URL } { FILLS_ROUTE } "" , params = { "" orderId "" : exchange_order_id , "" limit "" : 100 } , ) <MASK> try : msg = await response . json ( ) except ValueError : msg = await response . text ( ) raise DydxAsyncAPIError ( response . status , msg ) return await response . json ( )",if response . status >= 300 :,if response . status != 200 :,98.25506048,FALSE,96.23
162,"def semanticTags ( self , semanticTags ) : if semanticTags is None : self . __semanticTags = OrderedDict ( ) # check for key , value in list ( semanticTags . items ( ) ) : if not isinstance ( key , int ) : raise TypeError ( "" At least one key is not a valid int position "" ) if not isinstance ( value , list ) : raise TypeError ( "" At least one value of the provided dict is not a list of string "" ) for x in value : <MASK> raise TypeError ( "" At least one value of the provided dict is not a list of string "" ) self . __semanticTags = semanticTags","if not isinstance ( x , str ) :","if not isinstance ( x , str ) :",100,TRUE,100
163,"def start_cutting_tool ( self , event , axis , direction ) : toggle = event . EventObject self . cutting = toggle . Value if toggle . Value : # Disable the other toggles for child in self . cutsizer . Children : child = child . Window <MASK> child . Value = False self . cutting_axis = axis self . cutting_direction = direction else : self . cutting_axis = None self . cutting_direction = None self . cutting_dist = None",if child != toggle :,"if child . Name == ""cutting"" :",97.37557882,FALSE,91.92
164,"def decoration_helper ( self , patched , args , keywargs ) : extra_args = [ ] with contextlib . ExitStack ( ) as exit_stack : for patching in patched . patchings : arg = exit_stack . enter_context ( patching ) if patching . attribute_name is not None : keywargs . update ( arg ) <MASK> extra_args . append ( arg ) args + = tuple ( extra_args ) yield ( args , keywargs )",elif patching . new is DEFAULT :,elif patching . is_extra :,94.51518206,FALSE,95.47
165,def decodeattrs ( attrs ) : names = [ ] for bit in range ( 16 ) : mask = 1 << bit if attrs & mask : <MASK> names . append ( attrnames [ mask ] ) else : names . append ( hex ( mask ) ) return names,if attrnames . has_key ( mask ) :,if attrnames . has_key ( mask ) :,100,TRUE,100
166,"def pytest_collection_modifyitems ( items ) : for item in items : if item . nodeid . startswith ( "" tests/params "" ) : if "" stage "" not in item . keywords : item . add_marker ( pytest . mark . stage ( "" unit "" ) ) <MASK> item . add_marker ( pytest . mark . init ( rng_seed = 123 ) )","if ""init"" not in item . keywords :","if ""init"" not in item . keywords :",100,TRUE,100
167,"def handle_socket ( self , request ) : conn = request . connection while True : chunk = conn . recv ( 4 ) <MASK> break slen = struct . unpack ( "" >L "" , chunk ) [ 0 ] chunk = conn . recv ( slen ) while len ( chunk ) < slen : chunk = chunk + conn . recv ( slen - len ( chunk ) ) obj = pickle . loads ( chunk ) record = logging . makeLogRecord ( obj ) self . log_output + = record . msg + "" \n "" self . handled . release ( )",if len ( chunk ) < 4 :,if not chunk :,93.50798144,FALSE,93.85
168,"def on_source_foreach ( self , model , path , iter , id ) : m_id = model . get_value ( iter , self . COLUMN_ID ) if m_id == id : if self . _foreach_mode == "" get "" : self . _foreach_take = model . get_value ( iter , self . COLUMN_ENABLED ) <MASK> self . _foreach_take = iter","elif self . _foreach_mode == ""set"" :","elif self . _foreach_mode == ""set"" :",100,TRUE,100
169,"def parts ( ) : for l in lists . leaves : head_name = l . get_head_name ( ) if head_name == "" System`List "" : yield l . leaves <MASK> raise MessageException ( "" Catenate "" , "" invrp "" , l )","elif head_name != ""System`Missing"" :","elif head_name == ""System`List"" :",96.27923166,FALSE,92.12
170,"def __fill_counter_values ( self , command : str ) : result = [ ] regex = r "" (item[0-9]+ \ .counter_value) "" for token in re . split ( regex , command ) : <MASK> try : result . append ( str ( self . simulator_config . item_dict [ token ] . value ) ) except ( KeyError , ValueError , AttributeError ) : logger . error ( "" Could not get counter value for  "" + token ) else : result . append ( token ) return "" "" . join ( result )","if re . match ( regex , token ) is not None :",if token in self . simulator_config . item_dict :,91.34201405,FALSE,90.49
171,"def IMPORTFROM ( self , node ) : <MASK> if not self . futuresAllowed : self . report ( messages . LateFutureImport , node , [ n . name for n in node . names ] ) else : self . futuresAllowed = False for alias in node . names : if alias . name == "" * "" : self . scope . importStarred = True self . report ( messages . ImportStarUsed , node , node . module ) continue name = alias . asname or alias . name importation = Importation ( name , node ) <MASK> importation . used = ( self . scope , node ) self . addBinding ( node , importation )","if node . module == ""__future__"" :",if self . scope :,85.56884229,FALSE,81
172,"def _split_batch_list ( args , batch_list ) : new_list = [ ] for batch in batch_list . batches : new_list . append ( batch ) <MASK> yield batch_pb2 . BatchList ( batches = new_list ) new_list = [ ] if new_list : yield batch_pb2 . BatchList ( batches = new_list )",if len ( new_list ) == args . batch_size_limit :,if len ( new_list ) == 1 :,80.50010533,FALSE,90.48
173,"def get_branch_or_use_upstream ( branch_name , arg , repo ) : if not branch_name : # use upstream branch current_b = repo . current_branch upstream_b = current_b . upstream <MASK> raise ValueError ( "" No  {0}  branch specified and the current branch has no upstream  "" "" branch set "" . format ( arg ) ) ret = current_b . upstream else : ret = get_branch ( branch_name , repo ) return ret",if not upstream_b :,if not upstream_b :,100,TRUE,100
174,"def __init__ ( self , * * settings ) : default_settings = self . get_default_settings ( ) for name , value in default_settings . items ( ) : <MASK> setattr ( self , name , value ) for name , value in settings . items ( ) : if name not in default_settings : raise ImproperlyConfigured ( "" Invalid setting  ' {} '  for  {} "" . format ( name , self . __class__ . __name__ , ) ) setattr ( self , name , value )","if not hasattr ( self , name ) :",if name not in settings :,92.10033964,FALSE,93.16
175,"def _declare ( self , name , obj , included = False , quals = 0 ) : if name in self . _declarations : prevobj , prevquals = self . _declarations [ name ] if prevobj is obj and prevquals == quals : return <MASK> raise api . FFIError ( "" multiple declarations of  %s  (for interactive usage,  "" "" try cdef(xx, override=True)) "" % ( name , ) ) assert "" __dotdotdot__ "" not in name . split ( ) self . _declarations [ name ] = ( obj , quals ) if included : self . _included_declarations . add ( obj )",if not self . _override :,if len ( prevquals ) > 1 :,93.75356402,FALSE,94.33
176,"def include_file ( name , fdir = tmp_dir , b64 = False ) : try : if fdir is None : fdir = "" "" <MASK> with io . open ( os . path . join ( fdir , name ) , "" rb "" ) as f : return base64 . b64encode ( f . read ( ) ) . decode ( "" utf-8 "" ) else : with io . open ( os . path . join ( fdir , name ) , "" r "" , encoding = "" utf-8 "" ) as f : return f . read ( ) except ( OSError , IOError ) as e : logger . error ( "" Could not include file  ' {} ' :  {} "" . format ( name , e ) )",if b64 :,if b64 :,100,TRUE,100
177,"def to_raw_json ( self ) : parts = { } for p in self . parts : <MASK> parts [ p [ 0 ] ] = [ ] parts [ p [ 0 ] ] . append ( { "" value "" : p [ 2 ] , "" parameters "" : p [ 1 ] } ) children = [ x . to_raw_json ( ) for x in self . children ] return { "" type "" : self . __class__ . __name__ , "" children "" : children , "" parts "" : parts , }",if p [ 0 ] not in parts :,if not p [ 0 ] in parts :,98.17610541,FALSE,96.84
178,"def process_output ( output : str , filename : str , start_line : int ) - > Tuple [ Optional [ str ] , bool ] : error_found = False for line in output . splitlines ( ) : t = get_revealed_type ( line , filename , start_line ) <MASK> return t , error_found elif "" error: "" in line : error_found = True return None , True # finding no reveal_type is an error",if t :,if t is not None :,71.34199916,FALSE,95.36
179,"def __init__ ( self , resize_keyboard = None , one_time_keyboard = None , selective = None , row_width = 3 ) : if row_width > self . max_row_keys : # Todo: Will be replaced with Exception in future releases <MASK> logger . error ( "" Telegram does not support reply keyboard row width over  %d . "" % self . max_row_keys ) row_width = self . max_row_keys self . resize_keyboard = resize_keyboard self . one_time_keyboard = one_time_keyboard self . selective = selective self . row_width = row_width self . keyboard = [ ]",if not DISABLE_KEYLEN_ERROR :,if self . max_row_keys > 0 :,97.25192937,FALSE,93.2
180,"def realizeElementExpressions ( innerElement ) : elementHasBeenRealized = False for exp in innerElement . expressions : if not hasattr ( exp , "" realize "" ) : continue # else: before , during , after = exp . realize ( innerElement ) elementHasBeenRealized = True for n in before : newStream . append ( n ) <MASK> newStream . append ( during ) for n in after : newStream . append ( n ) if elementHasBeenRealized is False : newStream . append ( innerElement )",if during is not None :,if during is not None :,100,TRUE,100
181,"def lex_number ( self , pos ) : # numeric literal start = pos found_dot = False while pos < len ( self . string ) and ( self . string [ pos ] . isdigit ( ) or self . string [ pos ] == "" . "" ) : <MASK> if found_dot is True : raise ValueError ( "" Invalid number. Found multiple  ' . ' "" ) found_dot = True # technically we allow more than one ""."" and let float()'s parsing # complain later pos + = 1 val = self . string [ start : pos ] return Token ( TokenType . LNUM , val , len ( val ) )","if self . string [ pos ] == ""."" :",if self . string [ pos ] . isalpha ( ) :,73.21840969,FALSE,95.57
182,"def rename ( src , dst ) : # Try atomic or pseudo-atomic rename if _rename ( src , dst ) : return # Fall back to ""move away and replace"" try : os . rename ( src , dst ) except OSError as e : <MASK> raise old = "" %s - %08x "" % ( dst , random . randint ( 0 , sys . maxsize ) ) os . rename ( dst , old ) os . rename ( src , dst ) try : os . unlink ( old ) except Exception : pass",if e . errno != errno . EEXIST :,if e . errno != errno . EEXIST :,75,TRUE,100
183,"def _the_callback ( widget , event_id ) : point = widget . GetCenter ( ) index = widget . WIDGET_INDEX if hasattr ( callback , "" __call__ "" ) : if num > 1 : args = [ point , index ] else : args = [ point ] <MASK> args . append ( widget ) try_callback ( callback , * args ) return",if pass_widget :,if event_id == widget . WIDGET_EVENT_ID :,85.01186893,FALSE,85.25
184,"def run ( self ) : for _ in range ( self . n ) : error = True try : self . collection . insert_one ( { "" test "" : "" insert "" } ) error = False except : <MASK> raise if self . expect_exception : assert error",if not self . expect_exception :,if self . expect_exception is True :,88.00201905,FALSE,91.72
185,"def handle ( self , * args : Any , * * options : Any ) - > None : realm = self . get_realm ( options ) if options [ "" all "" ] : <MASK> raise CommandError ( "" You must specify a realm if you choose the --all option. "" ) self . fix_all_users ( realm ) return self . fix_emails ( realm , options [ "" emails "" ] )",if realm is None :,if not realm :,93.59025794,FALSE,95
186,"def recv_tdi ( self , nbits , pos ) : bits = 0 for n in range ( nbits * 2 ) : yield from self . _wait_for_tck ( ) <MASK> bits = ( bits << 1 ) | ( yield self . tdi . o ) return bits",if ( yield self . tck . o ) == pos :,if ( yield self . tck . o ) & 1 :,93.58158862,FALSE,93.31
187,"def _split_head ( self ) : if not hasattr ( self , "" _severed_head "" ) : <MASK> tree = self . _tree . copy ( ) head = tree . get_heading_text ( ) tree . remove_heading ( ) self . _severed_head = ( head , tree ) else : self . _severed_head = ( None , None ) return self . _severed_head",if self . _tree :,if self . _tree :,100,TRUE,100
188,"def buildSearchTrie ( self , choices ) : searchtrie = trie . Trie ( ) for choice in choices : for token in self . tokenizeChoice ( choice ) : <MASK> searchtrie [ token ] = [ ] searchtrie [ token ] . append ( choice ) return searchtrie",if not searchtrie . has_key ( token ) :,if token not in searchtrie :,79.1212812,FALSE,82.73
189,"def format_sql ( sql , params ) : rv = [ ] if isinstance ( params , dict ) : # convert sql with named parameters to sql with unnamed parameters conv = _FormatConverter ( params ) if params : sql = sql_to_string ( sql ) sql = sql % conv params = conv . params else : params = ( ) for param in params or ( ) : <MASK> rv . append ( "" NULL "" ) param = safe_repr ( param ) rv . append ( param ) return sql , rv",if param is None :,if param is None :,100,TRUE,100
190,def on_completed2 ( ) : doner [ 0 ] = True if not qr : if len ( ql ) > 0 : observer . on_next ( False ) observer . on_completed ( ) <MASK> observer . on_next ( True ) observer . on_completed ( ),elif donel [ 0 ] :,elif len ( ql ) == 0 :,84.52376396,FALSE,86.72
191,"def notify_digest ( self , frequency , changes ) : notifications = defaultdict ( list ) users = { } for change in changes : for user in self . get_users ( frequency , change ) : <MASK> notifications [ user . pk ] . append ( change ) users [ user . pk ] = user for user in users . values ( ) : self . send_digest ( user . profile . language , user . email , notifications [ user . pk ] , subscription = user . current_subscription , )",if change . project is None or user . can_access_project ( change . project ) :,if user . email not in notifications [ user . pk ] :,83.64613444,FALSE,84.39
192,"def _any_listener_using ( self , target_group_arn ) : for load_balancer in self . load_balancers . values ( ) : for listener in load_balancer . listeners . values ( ) : for rule in listener . rules : for action in rule . actions : <MASK> return True return False","if action . data . get ( ""target_group_arn"" ) == target_group_arn :",if action . arn == target_group_arn :,64.12134377,FALSE,83.67
193,"def train_dict ( self , triples ) : """"""Train a dict lemmatizer given training (word, pos, lemma) triples."""""" # accumulate counter ctr = Counter ( ) ctr . update ( [ ( p [ 0 ] , p [ 1 ] , p [ 2 ] ) for p in triples ] ) # find the most frequent mappings for p , _ in ctr . most_common ( ) : w , pos , l = p if ( w , pos ) not in self . composite_dict : self . composite_dict [ ( w , pos ) ] = l <MASK> self . word_dict [ w ] = l return",if w not in self . word_dict :,"if ( w , pos ) not in self . word_dict :",73.21032494,FALSE,95.63
194,"def parse_git_config ( path ) : """"""Parse git config file."""""" config = dict ( ) section = None with open ( os . path . join ( path , "" config "" ) , "" r "" ) as f : for line in f : line = line . strip ( ) <MASK> section = line [ 1 : - 1 ] . strip ( ) config [ section ] = dict ( ) elif section : key , value = line . replace ( ""   "" , "" "" ) . split ( "" = "" ) config [ section ] [ key ] = value return config","if line . startswith ( ""["" ) :","if line . startswith ( ""section="" ) :",98.92285276,FALSE,97.29
195,"def send_signal ( self , pid , signum ) : if pid in self . processes : process = self . processes [ pid ] hook_result = self . call_hook ( "" before_signal "" , pid = pid , signum = signum ) <MASK> logger . debug ( "" before_signal hook didn ' t return True  "" "" => signal  %i  is not sent to  %i "" % ( signum , pid ) ) else : process . send_signal ( signum ) self . call_hook ( "" after_signal "" , pid = pid , signum = signum ) else : logger . debug ( "" process  %s  does not exist "" % pid )",if signum != signal . SIGKILL and not hook_result :,if hook_result :,90.15175652,FALSE,93.32
196,"def validate_pos_return ( self ) : if self . is_pos and self . is_return : total_amount_in_payments = 0 for payment in self . payments : total_amount_in_payments + = payment . amount invoice_total = self . rounded_total or self . grand_total <MASK> frappe . throw ( _ ( "" Total payments amount can ' t be greater than  {} "" ) . format ( - invoice_total ) )",if total_amount_in_payments < invoice_total :,if total_amount_in_payments > invoice_total :,98.20510973,FALSE,97.64
197,"def delete ( key , inner_key = None ) : if inner_key is not None : try : del cache [ key ] [ inner_key ] del use_count [ key ] [ inner_key ] <MASK> del cache [ key ] del use_count [ key ] wrapper . cache_size - = 1 except KeyError : return False else : return True else : try : wrapper . cache_size - = len ( cache [ key ] ) del cache [ key ] del use_count [ key ] except KeyError : return False else : return True",if not cache [ key ] :,if not cache [ key ] :,100,TRUE,100
198,"def insertionsort ( array ) : size = array . getsize ( ) array . reset ( "" Insertion sort "" ) for i in range ( 1 , size ) : j = i - 1 while j > = 0 : <MASK> break array . swap ( j , j + 1 ) j = j - 1 array . message ( "" Sorted "" )","if array . compare ( j , j + 1 ) <= 0 :","if array . compare ( j , i ) < 0 :",90.13661409,FALSE,91.55
199,"def publish_state ( cls , payload , state ) : try : if isinstance ( payload , LiveActionDB ) : <MASK> cls . process ( payload ) else : worker . get_worker ( ) . process ( payload ) except Exception : traceback . print_exc ( ) print ( payload )",if state == action_constants . LIVEACTION_STATUS_REQUESTED :,if state :,66.99450635,FALSE,80.78
200,"def change_opacity_function ( self , new_f ) : self . opacity_function = new_f dr = self . radius / self . num_levels sectors = [ ] for submob in self . submobjects : if type ( submob ) == AnnularSector : sectors . append ( submob ) for ( r , submob ) in zip ( np . arange ( 0 , self . radius , dr ) , sectors ) : <MASK> # it's the shadow, don't dim it continue alpha = self . opacity_function ( r ) submob . set_fill ( opacity = alpha )",if type ( submob ) != AnnularSector :,if submob . get_fill ( ) == 0 :,95.51669681,FALSE,91.52
201,"def is_suppressed_warning ( type : str , subtype : str , suppress_warnings : List [ str ] ) - > bool : """"""Check the warning is suppressed or not."""""" if type is None : return False for warning_type in suppress_warnings : <MASK> target , subtarget = warning_type . split ( "" . "" , 1 ) else : target , subtarget = warning_type , None if target == type : if ( subtype is None or subtarget is None or subtarget == subtype or subtarget == "" * "" ) : return True return False","if ""."" in warning_type :","if ""."" in warning_type :",100,TRUE,100
202,"def set_many ( self , mapping , timeout = None ) : timeout = self . _normalize_timeout ( timeout ) # Use transaction=False to batch without calling redis MULTI # which is not supported by twemproxy pipe = self . _client . pipeline ( transaction = False ) for key , value in _items ( mapping ) : dump = self . dump_object ( value ) <MASK> pipe . set ( name = self . key_prefix + key , value = dump ) else : pipe . setex ( name = self . key_prefix + key , value = dump , time = timeout ) return pipe . execute ( )",if timeout == - 1 :,if timeout is None :,98.06991412,FALSE,95.78
203,"def maybe_relative_path ( path ) : if not os . path . isabs ( path ) : return path # already relative dir = path names = [ ] while True : prevdir = dir dir , name = os . path . split ( prevdir ) if dir == prevdir or not dir : return path # failed to make it relative names . append ( name ) try : <MASK> names . reverse ( ) return os . path . join ( * names ) except OSError : pass","if samefile ( dir , os . curdir ) :",if reverse_paths :,95.31342923,FALSE,90.76
204,"def word_range ( word ) : for ind in range ( len ( word ) ) : temp = word [ ind ] for c in [ chr ( x ) for x in range ( ord ( "" a "" ) , ord ( "" z "" ) + 1 ) ] : <MASK> yield word [ : ind ] + c + word [ ind + 1 : ]",if c != temp :,if c in temp :,98.30734653,FALSE,95.41
205,"def validate ( self ) : self . update_soil_edit ( "" sand_composition "" ) for soil_type in self . soil_types : <MASK> frappe . throw ( _ ( "" {0}  should be a value between 0 and 100 "" ) . format ( soil_type ) ) if sum ( self . get ( soil_type ) for soil_type in self . soil_types ) != 100 : frappe . throw ( _ ( "" Soil compositions do not add up to 100 "" ) )",if self . get ( soil_type ) > 100 or self . get ( soil_type ) < 0 :,if self . get ( soil_type ) != 0 :,88.97574853,FALSE,88.92
206,"def on_click ( self , event ) : run = self . _is_running ( ) if event [ "" button "" ] == self . button_activate : self . py3 . command_run ( [ "" xscreensaver-command "" , "" -activate "" ] ) if event [ "" button "" ] == self . button_toggle : <MASK> self . py3 . command_run ( [ "" xscreensaver-command "" , "" -exit "" ] ) else : # Because we want xscreensaver to continue running after # exit, we instead use preexec_fn=setpgrp here. Popen ( [ "" xscreensaver "" , "" -no-splash "" , "" -no-capture-stderr "" ] , stdout = PIPE , stderr = PIPE , preexec_fn = setpgrp , )",if run :,if run :,100,TRUE,100
207,"def maybe_relative_path ( path ) : if not os . path . isabs ( path ) : return path # already relative dir = path names = [ ] while True : prevdir = dir dir , name = os . path . split ( prevdir ) <MASK> return path # failed to make it relative names . append ( name ) try : if samefile ( dir , os . curdir ) : names . reverse ( ) return os . path . join ( * names ) except OSError : pass",if dir == prevdir or not dir :,if not os . path . isabs ( dir ) :,95.67876562,FALSE,90.48
208,"def _format_micros ( self , datestring ) : parts = datestring [ : - 1 ] . split ( "" . "" ) if len ( parts ) == 1 : <MASK> return datestring [ : - 1 ] + "" .000000Z "" else : return datestring + "" .000000Z "" else : micros = parts [ - 1 ] [ : 6 ] if len ( parts [ - 1 ] ) > 6 else parts [ - 1 ] return "" . "" . join ( parts [ : - 1 ] + [ "" {:06d} "" . format ( int ( micros ) ) ] ) + "" Z ""","if datestring . endswith ( ""Z"" ) :",if len ( datestring [ - 1 ] ) > 4 :,75.32276649,FALSE,91.79
209,"def preprocess_raw_enwik9 ( input_filename , output_filename ) : with open ( input_filename , "" r "" ) as f1 : with open ( output_filename , "" w "" ) as f2 : while True : line = f1 . readline ( ) if not line : break line = list ( enwik9_norm_transform ( [ line ] ) ) [ 0 ] <MASK> if line [ 0 ] == ""   "" : line = line [ 1 : ] f2 . writelines ( line + "" \n "" )","if line != "" "" and line != """" :",if line :,91.30509594,FALSE,89.86
210,"def set ( self , item , data ) : if not type ( item ) is slice : item = slice ( item , item + len ( data ) , None ) virt_item = self . item2virtitem ( item ) if not virt_item : return off = 0 for s , n_item in virt_item : <MASK> i = slice ( off , n_item . stop + off - n_item . start , n_item . step ) data_slice = data . __getitem__ ( i ) s . content . __setitem__ ( n_item , data_slice ) off = i . stop else : raise ValueError ( "" TODO XXX "" ) return","if isinstance ( s , ProgBits ) :",if n_item . start < off :,85.97229917,FALSE,94.19
211,"def walk ( msg , callback , data ) : partnum = 0 for part in msg . walk ( ) : # multipart/* are just containers if part . get_content_maintype ( ) == "" multipart "" : continue ctype = part . get_content_type ( ) if ctype is None : ctype = OCTET_TYPE filename = part . get_filename ( ) <MASK> filename = PART_FN_TPL % ( partnum ) headers = dict ( part ) LOG . debug ( headers ) headers [ "" Content-Type "" ] = ctype payload = util . fully_decoded_payload ( part ) callback ( data , filename , payload , headers ) partnum = partnum + 1",if not filename :,if filename is None :,98.19968804,FALSE,96.89
212,"def _run_wes ( args ) : """"""Run CWL using a Workflow Execution Service (WES) endpoint"""""" main_file , json_file , project_name = _get_main_and_json ( args . directory ) main_file = _pack_cwl ( main_file ) if args . host and "" stratus "" in args . host : _run_wes_stratus ( args , main_file , json_file ) else : opts = [ "" --no-wait "" ] <MASK> opts + = [ "" --host "" , args . host ] if args . auth : opts + = [ "" --auth "" , args . auth ] cmd = [ "" wes-client "" ] + opts + [ main_file , json_file ] _run_tool ( cmd )",if args . host :,if args . host :,100,TRUE,100
213,"def insertTestData ( self , rows ) : for row in rows : if isinstance ( row , Worker ) : self . workers [ row . id ] = dict ( id = row . id , name = row . name , paused = 0 , graceful = 0 , info = row . info ) <MASK> row . id = row . buildermasterid * 10000 + row . workerid self . configured [ row . id ] = dict ( buildermasterid = row . buildermasterid , workerid = row . workerid ) elif isinstance ( row , ConnectedWorker ) : self . connected [ row . id ] = dict ( masterid = row . masterid , workerid = row . workerid )","elif isinstance ( row , ConfiguredWorker ) :","if isinstance ( row , ConfiguredWorker ) :",95.05583949,FALSE,98.09
214,"def local_shape_to_shape_i ( node ) : if node . op == T . shape : # This optimization needs ShapeOpt and fgraph.shape_feature <MASK> return shape_feature = node . fgraph . shape_feature ret = shape_feature . make_vector_shape ( node . inputs [ 0 ] ) # We need to copy over stack trace from input to output copy_stack_trace ( node . outputs [ 0 ] , ret ) return [ ret ]","if not hasattr ( node . fgraph , ""shape_feature"" ) :","if not hasattr ( node . fgraph , ""shape_feature"" ) :",100,TRUE,100
215,"def get_config ( ) : """"""Get INI parser with version.ini data."""""" # TODO(hanuszczak): See comment in `setup.py` for `grr-response-proto`. ini_path = os . path . join ( THIS_DIRECTORY , "" version.ini "" ) <MASK> ini_path = os . path . join ( THIS_DIRECTORY , "" ../../version.ini "" ) <MASK> raise RuntimeError ( "" Couldn ' t find version.ini "" ) config = configparser . ConfigParser ( ) config . read ( ini_path ) return config",if not os . path . exists ( ini_path ) :,if not os . path . exists ( ini_path ) :,75,TRUE,100
216,"def init_weights ( self , pretrained = None ) : if isinstance ( pretrained , str ) : logger = logging . getLogger ( ) load_checkpoint ( self , pretrained , strict = False , logger = logger ) elif pretrained is None : for m in self . modules ( ) : <MASK> kaiming_init ( m ) elif isinstance ( m , ( _BatchNorm , nn . GroupNorm ) ) : constant_init ( m , 1 ) else : raise TypeError ( "" pretrained must be a str or None "" )","if isinstance ( m , nn . Conv2d ) :","if isinstance ( m , nn . Conv2d ) :",100,TRUE,100
217,"def isValidDateString ( config_param_name , value , valid_value ) : try : <MASK> return value day , month , year = value . split ( "" - "" ) if int ( day ) < 1 or int ( day ) > 31 : raise DateStringValueError ( config_param_name , value ) if int ( month ) < 1 or int ( month ) > 12 : raise DateStringValueError ( config_param_name , value ) if int ( year ) < 1900 or int ( year ) > 2013 : raise DateStringValueError ( config_param_name , value ) return value except Exception : raise DateStringValueError ( config_param_name , value )","if value == ""DD-MM-YYYY"" :",if valid_value :,92.21167868,FALSE,94.59
218,"def from_obj ( cls , py_obj ) : if not isinstance ( py_obj , Image ) : raise TypeError ( "" py_obj must be a wandb.Image "" ) else : <MASK> box_keys = list ( py_obj . _boxes . keys ( ) ) else : box_keys = [ ] if hasattr ( py_obj , "" masks "" ) and py_obj . masks : mask_keys = list ( py_obj . masks . keys ( ) ) else : mask_keys = [ ] return cls ( box_keys , mask_keys )","if hasattr ( py_obj , ""_boxes"" ) and py_obj . _boxes :","if hasattr ( py_obj , ""_boxes"" ) and py_obj . _boxes :",100,TRUE,100
219,"def _path_type ( st , lst ) : parts = [ ] if st : if stat . S_ISREG ( st . st_mode ) : parts . append ( "" file "" ) <MASK> parts . append ( "" dir "" ) else : parts . append ( "" other "" ) if lst : if stat . S_ISLNK ( lst . st_mode ) : parts . append ( "" link "" ) return ""   "" . join ( parts )",elif stat . S_ISDIR ( st . st_mode ) :,elif stat . S_ISDIR ( st . st_mode ) :,100,TRUE,100
220,"def is_destructive ( queries ) : """"""Returns if any of the queries in *queries* is destructive."""""" keywords = ( "" drop "" , "" shutdown "" , "" delete "" , "" truncate "" , "" alter "" ) for query in sqlparse . split ( queries ) : if query : <MASK> return True elif query_starts_with ( query , [ "" update "" ] ) is True and not query_has_where_clause ( query ) : return True return False","if query_starts_with ( query , keywords ) is True :","if query_starts_with ( query , keywords ) is True and query_has_clause",92.4472567,FALSE,93.55
221,"def _store_gsuite_membership_post ( self ) : """"""Flush storing gsuite memberships."""""" if not self . member_cache : return self . session . flush ( ) # session.execute automatically flushes if self . membership_items : <MASK> # SQLite doesn't support bulk insert for item in self . membership_items : stmt = self . dao . TBL_MEMBERSHIP . insert ( item ) self . session . execute ( stmt ) else : stmt = self . dao . TBL_MEMBERSHIP . insert ( self . membership_items ) self . session . execute ( stmt )","if get_sql_dialect ( self . session ) == ""sqlite"" :","if isinstance ( self . membership_items , ( list , tuple ) ) :",95.19574121,FALSE,89.1
222,"def forward ( self , inputs : paddle . Tensor ) : outputs = [ ] blocks = self . block ( inputs ) route = None for i , block in enumerate ( blocks ) : <MASK> block = paddle . concat ( [ route , block ] , axis = 1 ) route , tip = self . yolo_blocks [ i ] ( block ) block_out = self . block_outputs [ i ] ( tip ) outputs . append ( block_out ) if i < 2 : route = self . route_blocks_2 [ i ] ( route ) route = self . upsample ( route ) return outputs",if i > 0 :,if route is not None :,85.17396581,FALSE,95.6
223,"def deep_dict ( self , root = None ) : if root is None : root = self result = { } for key , value in root . items ( ) : <MASK> result [ key ] = self . deep_dict ( root = self . __class__ . _get_next ( key , root ) ) else : result [ key ] = value return result","if isinstance ( value , dict ) :","if isinstance ( value , dict ) :",100,TRUE,100
224,"def _parse_param_list ( self , content ) : r = Reader ( content ) params = [ ] while not r . eof ( ) : header = r . read ( ) . strip ( ) <MASK> arg_name , arg_type = header . split ( ""  :  "" ) [ : 2 ] else : arg_name , arg_type = header , "" "" desc = r . read_to_next_unindented_line ( ) desc = dedent_lines ( desc ) params . append ( ( arg_name , arg_type , desc ) ) return params","if "" : "" in header :","if "" : "" in header :",100,TRUE,100
225,"def _ungroup ( sequence , groups = None ) : for v in sequence : <MASK> if groups is not None : groups . append ( list ( _ungroup ( v , groups = None ) ) ) for v in _ungroup ( v , groups ) : yield v else : yield v","if isinstance ( v , ( list , tuple ) ) :","if isinstance ( v , list ) :",91.31527645,FALSE,90.49
226,"def _add_resource_group ( obj ) : if isinstance ( obj , list ) : for array_item in obj : _add_resource_group ( array_item ) elif isinstance ( obj , dict ) : try : if "" resourcegroup "" not in [ x . lower ( ) for x in obj . keys ( ) ] : if obj [ "" id "" ] : obj [ "" resourceGroup "" ] = _parse_id ( obj [ "" id "" ] ) [ "" resource-group "" ] except ( KeyError , IndexError , TypeError ) : pass for item_key in obj : <MASK> _add_resource_group ( obj [ item_key ] )","if item_key != ""sourceVault"" :","if item_key in ( ""resource-group"" , ""resource-group"" ) :",92.31964038,FALSE,92.42
227,"def haslayer ( self , cls ) : """"""true if self has a layer that is an instance of cls. Superseded by ""cls in self"" syntax."""""" if self . __class__ == cls or self . __class__ . __name__ == cls : return 1 for f in self . packetfields : fvalue_gen = self . getfieldval ( f . name ) if fvalue_gen is None : continue if not f . islist : fvalue_gen = SetGen ( fvalue_gen , _iterpacket = 0 ) for fvalue in fvalue_gen : <MASK> ret = fvalue . haslayer ( cls ) if ret : return ret return self . payload . haslayer ( cls )","if isinstance ( fvalue , Packet ) :","if hasattr ( fvalue , ""haslayer"" ) :",94.5695388,FALSE,95.39
228,"def _post_attachment ( self , message , channel , color , sub_fields = None ) : if channel is None : message_channels = self . channels else : message_channels = [ channel ] for message_channel in message_channels : attachment = { "" fallback "" : message , "" text "" : message , "" color "" : color , } <MASK> attachment [ "" fields "" ] = sub_fields self . slack_client . api_call ( "" chat.postMessage "" , channel = message_channel , attachments = [ attachment ] , as_user = True , )",if sub_fields is not None :,if sub_fields :,92.85281522,FALSE,96.37
229,"def create ( cls , repository , args ) : key = cls ( ) passphrase = os . environ . get ( "" ATTIC_PASSPHRASE "" ) if passphrase is not None : passphrase2 = passphrase else : passphrase , passphrase2 = 1 , 2 while passphrase != passphrase2 : passphrase = getpass ( "" Enter passphrase:  "" ) <MASK> print ( "" Passphrase must not be blank "" ) continue passphrase2 = getpass ( "" Enter same passphrase again:  "" ) if passphrase != passphrase2 : print ( "" Passphrases do not match "" ) key . init ( repository , passphrase ) if passphrase : print ( "" Remember your passphrase. Your data will be inaccessible without it. "" ) return key",if not passphrase :,if not passphrase :,100,TRUE,100
230,"def _generate_create_date ( self ) : if self . timezone is not None : # First, assume correct capitalization tzinfo = tz . gettz ( self . timezone ) <MASK> # Fall back to uppercase tzinfo = tz . gettz ( self . timezone . upper ( ) ) <MASK> raise util . CommandError ( "" Can ' t locate timezone:  %s "" % self . timezone ) create_date = ( datetime . datetime . utcnow ( ) . replace ( tzinfo = tz . tzutc ( ) ) . astimezone ( tzinfo ) ) else : create_date = datetime . datetime . now ( ) return create_date",if tzinfo is None :,if tzinfo is None :,100,TRUE,100
231,"def _read_header_lines ( fp ) : """"""Read lines with headers until the start of body"""""" lines = deque ( ) for line in fp : if is_empty ( line ) : break # tricky case if it's not a header and not an empty line # usually means that user forgot to separate the body and newlines # so ""unread"" this line here, what means to treat it like a body <MASK> fp . seek ( fp . tell ( ) - len ( line ) ) break lines . append ( line ) return lines",if not _RE_HEADER . match ( line ) :,"if not line . startswith ( ""header"" ) :",97.21226184,FALSE,92.77
232,"def _media_files_drag_received ( widget , context , x , y , data , info , timestamp ) : uris = data . get_uris ( ) files = [ ] for uri in uris : try : uri_tuple = GLib . filename_from_uri ( uri ) except : continue uri , unused = uri_tuple <MASK> if utils . is_media_file ( uri ) == True : files . append ( uri ) if len ( files ) == 0 : return open_dropped_files ( files )",if os . path . exists ( uri ) == True :,if uri :,91.46752667,FALSE,89.49
233,"def remove_importlib ( frame , options ) : if frame is None : return None for child in frame . children : remove_importlib ( child , options = options ) <MASK> # remove this node, moving the self_time and children up to the parent frame . self_time + = child . self_time frame . add_children ( child . children , after = child ) child . remove_from_parent ( ) return frame","if ""<frozen importlib._bootstrap"" in child . file_path :",if child . self_time is not None :,87.66410653,FALSE,85.58
234,"def __call__ ( self , graph ) : for layer_name , data in self . params : <MASK> node = graph . get_node ( layer_name ) node . data = self . adjust_parameters ( node , data ) else : print_stderr ( "" Ignoring parameters for non-existent layer:  %s "" % layer_name ) return graph",if layer_name in graph :,if layer_name in graph :,100,TRUE,100
235,"def test_with_three_points ( self ) : cba = ia . Polygon ( [ ( 1 , 2 ) , ( 3 , 4 ) , ( 5 , 5 ) ] ) for i , xy in enumerate ( cba ) : assert i in [ 0 , 1 , 2 ] if i == 0 : assert np . allclose ( xy , ( 1 , 2 ) ) <MASK> assert np . allclose ( xy , ( 3 , 4 ) ) elif i == 2 : assert np . allclose ( xy , ( 5 , 5 ) ) assert i == 2",elif i == 1 :,elif i == 1 :,100,TRUE,100
236,"def _serve ( self ) : self . _conn = self . manager . request ( REQUEST_DNS_LISTENER , self . domain ) conn = MsgPackMessages ( self . _conn ) while self . active : request = conn . recv ( ) if not request : logger . warning ( "" DNS: Recieved empty request. Shutdown "" ) self . stop ( ) break now = time . time ( ) response = self . handler . process ( request ) if not response : response = [ ] used = time . time ( ) - now <MASK> logger . warning ( "" DNS: Slow processing speed ( %s )s "" , used ) conn . send ( response )",if used > 1 :,if used > 0 :,98.99155282,FALSE,98.14
237,"def read ( cls , fp , * * kwargs ) : major_version , minor_version , count = read_fmt ( "" 2HI "" , fp ) items = [ ] for _ in range ( count ) : length = read_fmt ( "" I "" , fp ) [ 0 ] - 4 <MASK> with io . BytesIO ( fp . read ( length ) ) as f : items . append ( Annotation . read ( f ) ) return cls ( major_version = major_version , minor_version = minor_version , items = items )",if length > 0 :,if length > 0 :,100,TRUE,100
238,"def save_uploaded_files ( ) : files = [ ] unzip = bool ( request . form . get ( "" unzip "" ) in [ "" true "" , "" on "" ] ) for uploaded_file in request . files . getlist ( "" files "" ) : <MASK> with zipfile . ZipFile ( uploaded_file , "" r "" ) as zf : for info in zf . infolist ( ) : name = info . filename size = info . file_size data = zf . read ( name ) if size > 0 : files . append ( save_file ( data , filename = name . split ( "" / "" ) [ - 1 ] ) ) else : files . append ( save_file ( uploaded_file ) ) return files",if unzip and zipfile . is_zipfile ( uploaded_file ) :,if unzip :,90.08808978,FALSE,91.98
239,"def analyze_string_content ( self , string , line_num , filename ) : output = { } if self . keyword_exclude and self . keyword_exclude . search ( string ) : return output for identifier in self . secret_generator ( string , filetype = determine_file_type ( filename ) , ) : <MASK> continue secret = PotentialSecret ( self . secret_type , filename , identifier , line_num , ) output [ secret ] = secret return output",if self . is_secret_false_positive ( identifier ) :,"if identifier . startswith ( ""secret_"" ) :",91.7730181,FALSE,89.59
240,"def _validate_and_set_default_hyperparameters ( self ) : """"""Placeholder docstring"""""" # Check if all the required hyperparameters are set. If there is a default value # for one, set it. for name , definition in self . hyperparameter_definitions . items ( ) : if name not in self . hyperparam_dict : spec = definition [ "" spec "" ] if "" DefaultValue "" in spec : self . hyperparam_dict [ name ] = spec [ "" DefaultValue "" ] <MASK> raise ValueError ( "" Required hyperparameter:  %s  is not set "" % name )","elif ""IsRequired"" in spec and spec [ ""IsRequired"" ] :",if self . hyperparam_dict [ name ] is None :,69.6883798,FALSE,89.17
241,"def get_code ( self , fullname = None ) : fullname = self . _fix_name ( fullname ) if self . code is None : mod_type = self . etc [ 2 ] if mod_type == imp . PY_SOURCE : source = self . get_source ( fullname ) self . code = compile ( source , self . filename , "" exec "" ) elif mod_type == imp . PY_COMPILED : self . _reopen ( ) try : self . code = read_code ( self . file ) finally : self . file . close ( ) <MASK> self . code = self . _get_delegate ( ) . get_code ( ) return self . code",elif mod_type == imp . PKG_DIRECTORY :,if self . _get_delegate ( ) :,73.89942809,FALSE,92.22
242,"def eigh_abstract_eval ( operand , lower ) : if isinstance ( operand , ShapedArray ) : <MASK> raise ValueError ( "" Argument to symmetric eigendecomposition must have shape [..., n, n], "" "" got shape  {} "" . format ( operand . shape ) ) batch_dims = operand . shape [ : - 2 ] n = operand . shape [ - 1 ] v = ShapedArray ( batch_dims + ( n , n ) , operand . dtype ) w = ShapedArray ( batch_dims + ( n , ) , lax . lax . _complex_basetype ( operand . dtype ) ) else : v , w = operand , operand return v , w",if operand . ndim < 2 or operand . shape [ - 2 ] != operand . shape [ - 1 ] :,if lower :,89.13551344,FALSE,84.3
243,"def conninfo_parse ( dsn ) : ret = { } length = len ( dsn ) i = 0 while i < length : if dsn [ i ] . isspace ( ) : i + = 1 continue param_match = PARAMETER_RE . match ( dsn [ i : ] ) <MASK> return param = param_match . group ( 1 ) i + = param_match . end ( ) if i > = length : return value , end = read_param_value ( dsn [ i : ] ) if value is None : return i + = end ret [ param ] = value return ret",if not param_match :,if param_match is None :,96.42753723,FALSE,95.95
244,"def load_weights_from_unsupervised ( self , unsupervised_model ) : update_state_dict = copy . deepcopy ( self . network . state_dict ( ) ) for param , weights in unsupervised_model . network . state_dict ( ) . items ( ) : if param . startswith ( "" encoder "" ) : # Convert encoder's layers name to match new_param = "" tabnet. "" + param else : new_param = param <MASK> # update only common layers update_state_dict [ new_param ] = weights self . network . load_state_dict ( update_state_dict )",if self . network . state_dict ( ) . get ( new_param ) is not None :,"if param . startswith ( ""common"" ) :",91.88579627,FALSE,86.17
245,"def viewer_setup ( self ) : for key , value in DEFAULT_CAMERA_CONFIG . items ( ) : <MASK> getattr ( self . viewer . cam , key ) [ : ] = value else : setattr ( self . viewer . cam , key , value )","if isinstance ( value , np . ndarray ) :","if key . startswith ( ""C"" ) :",75.81663317,FALSE,86.66
246,"def colormap_changed ( change ) : if change [ "" new "" ] : cmap_colors = [ color [ 1 : ] for color in cmap . step . __dict__ [ "" _schemes "" ] [ colormap . value ] ] palette . value = "" ,  "" . join ( cmap_colors ) colorbar = getattr ( cmap . step , colormap . value ) colorbar_output = self . colorbar_widget with colorbar_output : colorbar_output . clear_output ( ) display ( colorbar ) <MASK> labels = [ f "" Class  { i + 1 } "" for i in range ( len ( palette . value . split ( "" , "" ) ) ) ] legend_labels . value = "" ,  "" . join ( labels )","if len ( palette . value ) > 0 and "","" in palette . value :","if change [ ""legend"" ] :",91.24133938,FALSE,89.59
247,"def invalidate ( self , layers = None ) : if layers is None : layers = Layer . AllLayers if layers : layers = set ( layers ) self . invalidLayers . update ( layers ) blockRenderers = [ br for br in self . blockRenderers if br . layer is Layer . Blocks or br . layer not in layers ] <MASK> self . forgetDisplayLists ( ) self . blockRenderers = blockRenderers if self . renderer . showRedraw and Layer . Blocks in layers : self . needsRedisplay = True",if len ( blockRenderers ) < len ( self . blockRenderers ) :,if len ( blockRenderers ) == 0 :,90.70817381,FALSE,91.67
248,"def fromstring ( cls , input ) : productions = [ ] for linenum , line in enumerate ( input . split ( "" \n "" ) ) : line = line . strip ( ) <MASK> continue try : productions + = _read_dependency_production ( line ) except ValueError : raise ValueError ( "" Unable to parse line  %s :  %s "" % ( linenum , line ) ) if len ( productions ) == 0 : raise ValueError ( "" No productions found! "" ) return DependencyGrammar ( productions )","if line . startswith ( ""#"" ) or line == """" :","if not line or line . startswith ( ""#"" ) :",94.28379196,FALSE,92.73
249,"def repl ( m , base_path , rel_path = None ) : if m . group ( "" comments "" ) : tag = m . group ( "" comments "" ) else : tag = m . group ( "" open "" ) <MASK> tag + = RE_TAG_LINK_ATTR . sub ( lambda m2 : repl_absolute ( m2 , base_path ) , m . group ( "" attr "" ) ) else : tag + = RE_TAG_LINK_ATTR . sub ( lambda m2 : repl_relative ( m2 , base_path , rel_path ) , m . group ( "" attr "" ) ) tag + = m . group ( "" close "" ) return tag",if rel_path is None :,if rel_path is None :,100,TRUE,100
250,"def encode ( path ) : if isinstance ( path , str_cls ) : try : path = path . encode ( fs_encoding , "" strict "" ) except UnicodeEncodeError : <MASK> raise path = path . encode ( fs_fallback_encoding , "" strict "" ) return path",if not platform . is_linux ( ) :,if not fs_fallback_encoding :,72.63991993,FALSE,86.74
251,"def __iter__ ( self ) : base_iterator = super ( ProcessIterable , self ) . __iter__ ( ) if getattr ( self . queryset , "" _coerced "" , False ) : for process in base_iterator : <MASK> process = coerce_to_related_instance ( process , process . flow_class . process_class ) yield process else : for process in base_iterator : yield process","if isinstance ( process , self . queryset . model ) :","if isinstance ( process , Process ) :",79.21150603,FALSE,93.15
252,"def footnotes_under ( n : Element ) - > Iterator [ nodes . footnote ] : if isinstance ( n , nodes . footnote ) : yield n else : for c in n . children : <MASK> continue elif isinstance ( c , nodes . Element ) : yield from footnotes_under ( c )","if isinstance ( c , addnodes . start_of_file ) :","if c . tag != ""div"" :",89.83960436,FALSE,81.43
253,"def _process_submissions ( self ) - > None : """"""Process all submissions which have not been processed yet."""""" while self . _to_be_processed : job = self . _to_be_processed [ 0 ] job . process ( ) # trigger computation <MASK> heapq . heappush ( self . _steady_priority_queue , OrderedJobs ( job . release_time , self . _order , job ) , ) self . _to_be_processed . popleft ( ) # remove right after it is added to the heap queue self . _order + = 1",if not self . batch_mode :,if job . release_time is not None :,96.69712837,FALSE,93.1
254,"def valid_localparts ( strip_delimiters = False ) : for line in ABRIDGED_LOCALPART_VALID_TESTS . split ( "" \n "" ) : # strip line, skip over empty lines line = line . strip ( ) if line == "" "" : continue # skip over comments or empty lines match = COMMENT . match ( line ) <MASK> continue # skip over localparts with delimiters if strip_delimiters : if "" , "" in line or "" ; "" in line : continue yield line",if match :,if not match :,98.79742093,FALSE,97.47
255,"def _get_payload_hash ( self , method , data = None ) : if method in ( "" POST "" , "" PUT "" ) : if data : <MASK> # File upload; don't try to read the entire payload return UNSIGNED_PAYLOAD return _hash ( data ) else : return UNSIGNED_PAYLOAD else : return _hash ( "" "" )","if hasattr ( data , ""next"" ) or hasattr ( data , ""__next__"" ) :","if data [ ""type"" ] == ""file"" :",70.13240093,FALSE,76.02
256,"def get_download_info ( self ) : try : download_info = self . api . get_download_info ( self . game ) result = True except NoDownloadLinkFound as e : print ( e ) <MASK> Config . unset ( "" current_download "" ) GLib . idle_add ( self . parent . parent . show_error , _ ( "" Download error "" ) , _ ( "" There was an error when trying to fetch the download link! \n {} "" . format ( e ) ) , ) download_info = False result = False return result , download_info","if Config . get ( ""current_download"" ) == self . game . id :",if e . status == 404 :,87.91505901,FALSE,86.82
257,"def find_id ( self , doc_id ) : self . _lock . acquire ( ) try : doc = self . _docs . get ( doc_id ) <MASK> doc = copy . deepcopy ( doc ) doc [ "" id "" ] = doc_id return doc finally : self . _lock . release ( )",if doc :,if doc :,100,TRUE,100
258,"def assign_art ( self , session , task ) : """"""Place the discovered art in the filesystem."""""" if task in self . art_candidates : candidate = self . art_candidates . pop ( task ) self . _set_art ( task . album , candidate , not self . src_removed ) <MASK> task . prune ( candidate . path )",if self . src_removed :,if self . src_removed :,100,TRUE,100
259,"def _replace_named ( self , named , replace_scalar ) : for item in named : for name , value in self . _get_replaced_named ( item , replace_scalar ) : <MASK> raise DataError ( "" Argument names must be strings. "" ) yield name , value",if not is_string ( name ) :,"if not isinstance ( name , str ) :",64.71904359,FALSE,90.98
260,"def qtTypeIdent ( conn , * args ) : # We're not using the conn object at the moment, but - we will # modify the # logic to use the server version specific keywords later. res = None value = None for val in args : # DataType doesn't have len function then convert it to string if not hasattr ( val , "" __len__ "" ) : val = str ( val ) if len ( val ) == 0 : continue value = val <MASK> value = value . replace ( ' "" ' , ' "" "" ' ) value = ' "" ' + value + ' "" ' res = ( ( res and res + "" . "" ) or "" "" ) + value return res","if Driver . needsQuoting ( val , True ) :","if hasattr ( value , ""__str__"" ) :",72.10687451,FALSE,92.11
261,"def _update_tileable_and_chunk_shape ( self , tileable_graph , chunk_result , failed_ops ) : for n in tileable_graph : <MASK> continue tiled_n = get_tiled ( n ) if has_unknown_shape ( tiled_n ) : if any ( c . key not in chunk_result for c in tiled_n . chunks ) : # some of the chunks has been fused continue new_nsplits = self . get_tileable_nsplits ( n , chunk_result = chunk_result ) for node in ( n , tiled_n ) : node . _update_shape ( tuple ( sum ( nsplit ) for nsplit in new_nsplits ) ) tiled_n . _nsplits = new_nsplits",if n . op in failed_ops :,if not failed_ops ( n ) :,86.80917332,FALSE,95.72
262,"def _read_filter ( self , data ) : if data : <MASK> self . inner_sha . update ( data ) if self . expected_inner_md5sum : self . inner_md5 . update ( data ) return data",if self . expected_inner_sha256 :,if self . expected_inner_sha :,96.64370469,FALSE,95.16
263,"def find_previous_editable ( self , * args ) : if self . editw == 0 : if self . _active_page > 0 : self . switch_page ( self . _active_page - 1 ) if not self . editw == 0 : # remember that xrange does not return the 'last' value, # so go to -1, not 0! (fence post error in reverse) for n in range ( self . editw - 1 , - 1 , - 1 ) : <MASK> self . editw = n break",if self . _widgets__ [ n ] . editable and not self . _widgets__ [ n ] . hidden :,if self . _active_page == n :,91.68283632,FALSE,82.61
264,"def _get_event_for_message ( self , message_id ) : with self . event_lock : <MASK> raise RuntimeError ( "" Event for message[ {} ] should have been created before accessing "" . format ( message_id ) ) return self . _events [ message_id ]",if message_id not in self . _events :,if message_id not in self . _events :,100,TRUE,100
265,"def _get_deepest ( self , t ) : if isinstance ( t , list ) : if len ( t ) == 1 : return t [ 0 ] else : for part in t : res = self . _get_deepest ( part ) <MASK> return res return None return None",if res :,if res is not None :,66.07334223,FALSE,92.63
266,"def _get_notify ( self , action_node ) : if action_node . name not in self . _skip_notify_tasks : <MASK> task_notify = NotificationsHelper . to_model ( action_node . notify ) return task_notify elif self . _chain_notify : return self . _chain_notify return None",if action_node . notify :,if action_node . notify :,100,TRUE,100
267,"def __init__ ( self , centered = None , shape_params = ( ) ) : assert centered is None or isinstance ( centered , ( float , torch . Tensor ) ) assert isinstance ( shape_params , ( tuple , list ) ) assert all ( isinstance ( name , str ) for name in shape_params ) if is_validation_enabled ( ) : if isinstance ( centered , float ) : assert 0 < = centered and centered < = 1 <MASK> assert ( 0 < = centered ) . all ( ) assert ( centered < = 1 ) . all ( ) else : assert centered is None self . centered = centered self . shape_params = shape_params","elif isinstance ( centered , torch . Tensor ) :","elif isinstance ( centered , str ) :",89.96441133,FALSE,96.88
268,"def collect ( self ) : for nickname in self . squid_hosts . keys ( ) : squid_host = self . squid_hosts [ nickname ] fulldata = self . _getData ( squid_host [ "" host "" ] , squid_host [ "" port "" ] ) if fulldata is not None : fulldata = fulldata . splitlines ( ) for data in fulldata : matches = self . stat_pattern . match ( data ) <MASK> self . publish_counter ( "" %s . %s "" % ( nickname , matches . group ( 1 ) ) , float ( matches . group ( 2 ) ) )",if matches :,if matches is not None :,95.04140879,FALSE,96.4
269,"def test_len ( self ) : eq = self . assertEqual eq ( base64MIME . base64_len ( "" hello "" ) , len ( base64MIME . encode ( "" hello "" , eol = "" "" ) ) ) for size in range ( 15 ) : if size == 0 : bsize = 0 elif size < = 3 : bsize = 4 elif size < = 6 : bsize = 8 <MASK> bsize = 12 elif size < = 12 : bsize = 16 else : bsize = 20 eq ( base64MIME . base64_len ( "" x "" * size ) , bsize )",elif size <= 9 :,elif size <= 8 :,92.40183377,FALSE,97.84
270,"def wait_for_initial_conf ( self , timeout = 1.0 ) : logger . info ( "" Waiting for initial configuration "" ) cur_timeout = timeout # Arbiter do not already set our have_conf param while not self . new_conf and not self . interrupted : elapsed , _ , _ = self . handleRequests ( cur_timeout ) if elapsed : cur_timeout - = elapsed <MASK> continue cur_timeout = timeout sys . stdout . write ( "" . "" ) sys . stdout . flush ( )",if cur_timeout > 0 :,if cur_timeout <= 0 :,98.57993902,FALSE,96.76
271,"def __init__ ( self , querylist = None ) : self . query_id = - 1 if querylist is None : self . querylist = [ ] else : self . querylist = querylist for query in self . querylist : if self . query_id == - 1 : self . query_id = query . query_id else : <MASK> raise ValueError ( "" query in list must be same query_id "" )",if self . query_id != query . query_id :,if query . query_id != query . query_id :,98.42169683,FALSE,97.44
272,"def candidates ( ) - > Generator [ "" Symbol "" , None , None ] : s = self if Symbol . debug_lookup : Symbol . debug_print ( "" searching in self: "" ) print ( s . to_string ( Symbol . debug_indent + 1 ) , end = "" "" ) while True : if matchSelf : yield s <MASK> yield from s . children_recurse_anon else : yield from s . _children if s . siblingAbove is None : break s = s . siblingAbove if Symbol . debug_lookup : Symbol . debug_print ( "" searching in sibling: "" ) print ( s . to_string ( Symbol . debug_indent + 1 ) , end = "" "" )",if recurseInAnon :,elif recurseAnon :,96.33435029,FALSE,97.55
273,"def get_default_params ( problem_type : str , penalty : str ) : # TODO: get seed from seeds provider if problem_type == REGRESSION : default_params = { "" C "" : None , "" random_state "" : 0 , "" fit_intercept "" : True } <MASK> default_params [ "" solver "" ] = "" auto "" else : default_params = { "" C "" : None , "" random_state "" : 0 , "" solver "" : _get_solver ( problem_type ) , "" n_jobs "" : - 1 , "" fit_intercept "" : True , } model_params = list ( default_params . keys ( ) ) return model_params , default_params",if penalty == L2 :,"if penalty == ""auto"" :",98.89523214,FALSE,97.08
274,"def _UploadDirectory ( local_dir : str , gcs_bucket : storage . Bucket , gcs_dir : str ) : """"""Upload the contents of a local directory to a GCS Bucket."""""" for file_name in os . listdir ( local_dir ) : path = os . path . join ( local_dir , file_name ) <MASK> logging . info ( "" Skipping  %s  as it ' s not a file. "" , path ) continue logging . info ( "" Uploading:  %s "" , path ) gcs_blob = gcs_bucket . blob ( f "" { gcs_dir } / { file_name } "" ) gcs_blob . upload_from_filename ( path )",if not os . path . isfile ( path ) :,if not os . path . isfile ( path ) :,100,TRUE,100
275,"def decode_query_ids ( self , trans , conditional ) : if conditional . operator == "" and "" : self . decode_query_ids ( trans , conditional . left ) self . decode_query_ids ( trans , conditional . right ) else : left_base = conditional . left . split ( "" . "" ) [ 0 ] if left_base in self . FIELDS : field = self . FIELDS [ left_base ] <MASK> conditional . right = trans . security . decode_id ( conditional . right )",if field . id_decode :,"if field . get ( ""type"" ) == ""query_id"" :",77.16352495,FALSE,88.45
276,"def data_dir ( self ) - > Path : try : from appdirs import user_data_dir except ImportError : # linux path = Path . home ( ) / "" .local "" / "" share "" <MASK> return path / "" dephell "" # mac os path = Path . home ( ) / "" Library "" / "" Application Support "" <MASK> return path / "" dephell "" self . pip_main ( [ "" install "" , "" appdirs "" ] ) from appdirs import user_data_dir return Path ( user_data_dir ( "" dephell "" ) )",if path . exists ( ) :,if path . exists ( ) :,75,TRUE,100
277,"def setGameCard ( self , isGameCard = False ) : if isGameCard : targetValue = 1 else : targetValue = 0 for nca in self : if isinstance ( nca , Nca ) : <MASK> continue Print . info ( "" writing isGameCard for  %s ,  %d "" % ( str ( nca . _path ) , targetValue ) ) nca . header . setIsGameCard ( targetValue )",if nca . header . getIsGameCard ( ) == targetValue :,if nca . header . setIsGameCard ( targetValue ) :,92.75130966,FALSE,91.55
278,"def check_apns_certificate ( ss ) : mode = "" start "" for s in ss . split ( "" \n "" ) : if mode == "" start "" : if "" BEGIN RSA PRIVATE KEY "" in s or "" BEGIN PRIVATE KEY "" in s : mode = "" key "" <MASK> if "" END RSA PRIVATE KEY "" in s or "" END PRIVATE KEY "" in s : mode = "" end "" break elif s . startswith ( "" Proc-Type "" ) and "" ENCRYPTED "" in s : raise ImproperlyConfigured ( "" Encrypted APNS private keys are not supported "" ) if mode != "" end "" : raise ImproperlyConfigured ( "" The APNS certificate doesn ' t contain a private key "" )","elif mode == ""key"" :","elif s . startswith ( ""APNS-Type"" ) :",87.71403022,FALSE,93.75
279,"def register_aggregate_groups ( conn , * groups ) : seen = set ( ) for group in groups : klasses = AGGREGATE_COLLECTION [ group ] for klass in klasses : name = getattr ( klass , "" name "" , klass . __name__ ) <MASK> seen . add ( name ) conn . create_aggregate ( name , - 1 , klass )",if name not in seen :,if name not in seen :,100,TRUE,100
280,"def _impl ( inputs , input_types ) : data = inputs [ 0 ] axis = None keepdims = False if len ( inputs ) > 2 : # default, torch have only data, axis=None, keepdims=False if isinstance ( inputs [ 1 ] , int ) : axis = int ( inputs [ 1 ] ) <MASK> axis = inputs [ 1 ] else : axis = list ( _infer_shape ( inputs [ 1 ] ) ) keepdims = bool ( inputs [ 2 ] ) return get_relay_op ( name ) ( data , axis = axis , keepdims = keepdims )",elif _is_int_seq ( inputs [ 1 ] ) :,"elif isinstance ( inputs [ 1 ] , list ) :",97.71119689,FALSE,93.07
281,"def walks_generator ( ) : if filelist is not None : bucket = [ ] for filename in filelist : with io . open ( filename ) as inf : for line in inf : walk = [ int ( x ) for x in line . strip ( "" \n "" ) . split ( ""   "" ) ] bucket . append ( walk ) if len ( bucket ) == batch_size : yield bucket bucket = [ ] <MASK> yield bucket else : for _ in range ( epoch ) : for nodes in graph . node_batch_iter ( batch_size ) : walks = graph . random_walk ( nodes , walk_len ) yield walks",if len ( bucket ) :,if len ( bucket ) == batch_size :,83.30050322,FALSE,95.33
282,"def _calculate_runtimes ( states ) : results = { "" runtime "" : 0.00 , "" num_failed_states "" : 0 , "" num_passed_states "" : 0 } for state , resultset in states . items ( ) : <MASK> # Count the pass vs failures if resultset [ "" result "" ] : results [ "" num_passed_states "" ] + = 1 else : results [ "" num_failed_states "" ] + = 1 # Count durations results [ "" runtime "" ] + = resultset [ "" duration "" ] log . debug ( "" Parsed state metrics:  {} "" . format ( results ) ) return results","if isinstance ( resultset , dict ) and ""duration"" in resultset :","if resultset [ ""result"" ] :",86.65908024,FALSE,90.85
283,"def _replicator_primary_device ( ) - > snt_replicator . Replicator : # NOTE: The explicit device list is required since currently Replicator # only considers CPU and GPU devices. This means on TPU by default we only # mirror on the local CPU. for device_type in ( "" TPU "" , "" GPU "" , "" CPU "" ) : devices = tf . config . experimental . list_logical_devices ( device_type = device_type ) <MASK> devices = [ d . name for d in devices ] logging . info ( "" Replicating over  %s "" , devices ) return snt_replicator . Replicator ( devices = devices ) assert False , "" No TPU/GPU or CPU found """,if devices :,"if device_type == ""GPU"" :",73.52211319,FALSE,93.59
284,"def get_tag_values ( self , event ) : http = event . interfaces . get ( "" sentry.interfaces.Http "" ) if not http : return [ ] if not http . headers : return [ ] headers = http . headers # XXX: transitional support for workers if isinstance ( headers , dict ) : headers = headers . items ( ) output = [ ] for key , value in headers : <MASK> continue ua = Parse ( value ) if not ua : continue result = self . get_tag_from_ua ( ua ) if result : output . append ( result ) return output","if key != ""User-Agent"" :","if key . startswith ( ""HTTP_"" ) :",97.75176366,FALSE,92.97
285,"def general ( metadata , value ) : if metadata . get ( "" commands "" ) and value : <MASK> v = quote ( value ) else : v = value return u "" {0}   {1} "" . format ( metadata [ "" commands "" ] [ 0 ] , v ) else : if not value : return None el<MASK> return quote ( value ) else : return value","if not metadata . get ( ""nargs"" ) :","if isinstance ( value , ( str , unicode ) ) :",65.76341327,FALSE,78.43
286,"def _actions_read ( self , c ) : self . action_input . handle_read ( c ) if c in [ curses . KEY_ENTER , util . KEY_ENTER2 ] : # take action if self . action_input . selected_index == 0 : # Cancel self . back_to_parent ( ) elif self . action_input . selected_index == 1 : # Apply self . _apply_prefs ( ) client . core . get_config ( ) . addCallback ( self . _update_preferences ) <MASK> # OK self . _apply_prefs ( ) self . back_to_parent ( )",elif self . action_input . selected_index == 2 :,"elif c in [ curses . KEY_OK , util . KEY_OK2 ] :",95.03082809,FALSE,89.31
287,def logic ( ) : if reset == 1 : lfsr . next = 1 else : <MASK> # lfsr.next[24:1] = lfsr[23:0] lfsr . next = lfsr << 1 lfsr . next [ 0 ] = lfsr [ 23 ] ^ lfsr [ 22 ] ^ lfsr [ 21 ] ^ lfsr [ 16 ],if enable :,if reset == 0 :,92.49629426,FALSE,92.37
288,"def action_delete ( self , request , attachments ) : deleted_attachments = [ ] desynced_posts = [ ] for attachment in attachments : <MASK> deleted_attachments . append ( attachment . pk ) desynced_posts . append ( attachment . post_id ) if desynced_posts : with transaction . atomic ( ) : for post in Post . objects . filter ( id__in = desynced_posts ) : self . delete_from_cache ( post , deleted_attachments ) for attachment in attachments : attachment . delete ( ) message = _ ( "" Selected attachments have been deleted. "" ) messages . success ( request , message )",if attachment . post :,if attachment . pk :,98.81078951,FALSE,98.07
289,"def __getitem__ ( self , index ) : if self . _check ( ) : if isinstance ( index , int ) : if index < 0 or index > = len ( self . features ) : raise IndexError ( index ) if self . features [ index ] is None : feature = self . device . feature_request ( FEATURE . FEATURE_SET , 0x10 , index ) if feature : ( feature , ) = _unpack ( "" !H "" , feature [ : 2 ] ) self . features [ index ] = FEATURE [ feature ] return self . features [ index ] <MASK> indices = index . indices ( len ( self . features ) ) return [ self . __getitem__ ( i ) for i in range ( * indices ) ]","elif isinstance ( index , slice ) :","if isinstance ( index , Index ) :",97.03055264,FALSE,96.82
290,"def _skip_start ( self ) : start , stop = self . start , self . stop for chunk in self . app_iter : self . _pos + = len ( chunk ) <MASK> continue elif self . _pos == start : return b "" "" else : chunk = chunk [ start - self . _pos : ] if stop is not None and self . _pos > stop : chunk = chunk [ : stop - self . _pos ] assert len ( chunk ) == stop - start return chunk else : raise StopIteration ( )",if self . _pos < start :,if self . _pos >= start :,98.80617003,FALSE,97.01
291,"def get_files ( d ) : f = [ ] for root , dirs , files in os . walk ( d ) : for name in files : <MASK> continue if "" qemux86copy- "" in root or "" qemux86- "" in root : continue if "" do_build "" not in name and "" do_populate_sdk "" not in name : f . append ( os . path . join ( root , name ) ) return f","if ""meta-environment"" in root or ""cross-canadian"" in root :","if ""qemux86"" in root or ""qemux86"" in root :",97.03887808,FALSE,94.76
292,"def _load_windows_store_certs ( self , storename , purpose ) : certs = bytearray ( ) try : for cert , encoding , trust in enum_certificates ( storename ) : # CA certs are never PKCS#7 encoded <MASK> if trust is True or purpose . oid in trust : certs . extend ( cert ) except PermissionError : warnings . warn ( "" unable to enumerate Windows certificate store "" ) if certs : self . load_verify_locations ( cadata = certs ) return certs","if encoding == ""x509_asn"" :","if encoding == ""PKCS#7"" :",98.5508815,FALSE,95.72
293,"def test_tokenizer_identifier_with_correct_config ( self ) : for tokenizer_class in [ BertTokenizer , BertTokenizerFast , AutoTokenizer ] : tokenizer = tokenizer_class . from_pretrained ( "" wietsedv/bert-base-dutch-cased "" ) self . assertIsInstance ( tokenizer , ( BertTokenizer , BertTokenizerFast ) ) <MASK> self . assertEqual ( tokenizer . basic_tokenizer . do_lower_case , False ) else : self . assertEqual ( tokenizer . do_lower_case , False ) self . assertEqual ( tokenizer . model_max_length , 512 )","if isinstance ( tokenizer , BertTokenizer ) :","if hasattr ( tokenizer , ""basic_tokenizer"" ) :",83.56914629,FALSE,92.17
294,"def run ( self ) : global WAITING_BEFORE_START time . sleep ( WAITING_BEFORE_START ) while self . keep_alive : path_id , module , resolve = self . queue_receive . get ( ) if path_id is None : continue self . lock . acquire ( ) self . modules [ path_id ] = module self . lock . release ( ) <MASK> resolution = self . _resolve_with_other_modules ( resolve ) self . _relations [ path_id ] = [ ] for package in resolution : self . _relations [ path_id ] . append ( resolution [ package ] ) self . queue_send . put ( ( path_id , module , False , resolution ) )",if resolve :,if path_id not in self . _relations :,71.26149263,FALSE,93.28
295,"def __new__ ( mcs , name , bases , attrs ) : include_profile = include_trace = include_garbage = True bases = list ( bases ) if name == "" SaltLoggingClass "" : for base in bases : if hasattr ( base , "" trace "" ) : include_trace = False <MASK> include_garbage = False if include_profile : bases . append ( LoggingProfileMixin ) if include_trace : bases . append ( LoggingTraceMixin ) if include_garbage : bases . append ( LoggingGarbageMixin ) return super ( LoggingMixinMeta , mcs ) . __new__ ( mcs , name , tuple ( bases ) , attrs )","if hasattr ( base , ""garbage"" ) :","if hasattr ( base , ""garbage"" ) :",100,TRUE,100
296,"def __str__ ( self , prefix = "" "" , printElemNumber = 0 ) : res = "" "" if self . has_owner_ : res + = prefix + ( "" owner:  %s \n "" % self . DebugFormatString ( self . owner_ ) ) cnt = 0 for e in self . entries_ : elm = "" "" <MASK> elm = "" ( %d ) "" % cnt res + = prefix + ( "" entries %s  < \n "" % elm ) res + = e . __str__ ( prefix + ""    "" , printElemNumber ) res + = prefix + "" > \n "" cnt + = 1 return res",if printElemNumber :,if printElemNumber :,100,TRUE,100
297,"def parse_tag ( self ) : buf = [ ] escaped = False for c in self . get_next_chars ( ) : if escaped : buf . append ( c ) elif c == "" \\ "" : escaped = True <MASK> return "" "" . join ( buf ) else : buf . append ( c ) raise Exception ( "" Unclosed tag  "" + "" "" . join ( buf ) )","elif c == "">"" :","elif c == ""\\"" :",94.98751214,FALSE,96.02
298,"def get_batches ( train_nodes , train_labels , batch_size = 64 , shuffle = True ) : if shuffle : random . shuffle ( train_nodes ) total = train_nodes . shape [ 0 ] for i in range ( 0 , total , batch_size ) : <MASK> cur_nodes = train_nodes [ i : i + batch_size ] cur_labels = train_labels [ cur_nodes ] yield cur_nodes , cur_labels",if i + batch_size <= total :,if i + batch_size < total :,98.23065172,FALSE,97.58
299,"def _get_all_info_lines ( data ) : infos = [ ] for row in data : splitrow = row . split ( ) if len ( splitrow ) > 0 : <MASK> infos . append ( ""   "" . join ( splitrow [ 1 : ] ) ) return infos","if splitrow [ 0 ] == ""INFO:"" :","if splitrow [ 0 ] == ""info"" :",97.70473935,FALSE,94.63
300,"def _validate_client_public_key ( self , username , key_data ) : """"""Validate a client public key for the specified user"""""" try : key = decode_ssh_public_key ( key_data ) except KeyImportError : return None options = None if self . _client_keys : options = self . _client_keys . validate ( key , self . _peer_addr ) if options is None : result = self . _owner . validate_public_key ( username , key ) if asyncio . iscoroutine ( result ) : result = yield from result <MASK> return None options = { } self . _key_options = options return key",if not result :,if not result :,100,TRUE,100
301,"def attach_related_versions ( addons , addon_dict = None ) : if addon_dict is None : addon_dict = { addon . id : addon for addon in addons } all_ids = set ( filter ( None , ( addon . _current_version_id for addon in addons ) ) ) versions = list ( Version . objects . filter ( id__in = all_ids ) . order_by ( ) ) for version in versions : try : addon = addon_dict [ version . addon_id ] except KeyError : log . info ( "" Version  %s  has an invalid add-on id. "" % version . id ) continue <MASK> addon . _current_version = version version . addon = addon",if addon . _current_version_id == version . id :,if addon . _current_version is None :,93.87091767,FALSE,94.64
302,"def move_view ( obj , evt ) : position = obj . GetCurrentCursorPosition ( ) for other_axis , axis_number in self . _axis_names . iteritems ( ) : <MASK> continue ipw3d = getattr ( self , "" ipw_3d_ %s "" % other_axis ) ipw3d . ipw . slice_position = position [ axis_number ]",if other_axis == axis_name :,if axis_number not in position :,93.08696804,FALSE,88.94
303,"def func_wrapper ( * args , * * kwargs ) : warnings . simplefilter ( "" always "" , DeprecationWarning ) # turn off filter for old , new in arg_mapping . items ( ) : <MASK> warnings . warn ( f "" Keyword argument  ' { old } '  has been  "" f "" deprecated in favour of  ' { new } ' .  "" f "" ' { old } '  will be removed in a future version. "" , category = DeprecationWarning , stacklevel = 2 , ) val = kwargs . pop ( old ) kwargs [ new ] = val # reset filter warnings . simplefilter ( "" default "" , DeprecationWarning ) return func ( * args , * * kwargs )",if old in kwargs :,if old in kwargs :,75,TRUE,100
304,"def inner_connection_checker ( self , * args , * * kwargs ) : LOG . debug ( "" in _connection_checker "" ) for attempts in range ( 5 ) : try : return func ( self , * args , * * kwargs ) except exception . VolumeBackendAPIException as e : pattern = re . compile ( r "" .*Session id expired$ "" ) matches = pattern . match ( six . text_type ( e ) ) if matches : <MASK> LOG . debug ( "" Session might have expired. "" ""  Trying to relogin "" ) self . _login ( ) continue LOG . error ( "" Re-throwing Exception  %s "" , e ) raise",if attempts < 4 :,if len ( matches ) > 0 :,94.68136409,FALSE,94.49
305,"def set ( self , pcount ) : """"""Set channel prefetch_count setting."""""" if pcount != self . prev : new_value = pcount <MASK> logger . warning ( "" QoS: Disabled: prefetch_count exceeds  %r "" , PREFETCH_COUNT_MAX ) new_value = 0 logger . debug ( "" basic.qos: prefetch_count-> %s "" , new_value ) self . callback ( prefetch_count = new_value ) self . prev = pcount return pcount",if pcount > PREFETCH_COUNT_MAX :,if PREFETCH_COUNT_MAX > pcount :,97.24815923,FALSE,95.82
306,"def _build_gcs_object_key ( self , key ) : if self . platform_specific_separator : <MASK> gcs_object_key = os . path . join ( self . prefix , self . _convert_key_to_filepath ( key ) ) else : gcs_object_key = self . _convert_key_to_filepath ( key ) else : <MASK> gcs_object_key = "" / "" . join ( ( self . prefix , self . _convert_key_to_filepath ( key ) ) ) else : gcs_object_key = self . _convert_key_to_filepath ( key ) return gcs_object_key",if self . prefix :,if self . prefix :,100,TRUE,100
307,"def number_operators ( self , a , b , skip = [ ] ) : dict = { "" a "" : a , "" b "" : b } for name , expr in self . binops . items ( ) : <MASK> name = "" __ %s __ "" % name if hasattr ( a , name ) : res = eval ( expr , dict ) self . binop_test ( a , b , res , expr , name ) for name , expr in self . unops . items ( ) : <MASK> name = "" __ %s __ "" % name if hasattr ( a , name ) : res = eval ( expr , dict ) self . unop_test ( a , res , expr , name )",if name not in skip :,if name not in skip :,100,TRUE,100
308,def isCurveMonotonic ( set_ ) : for i in range ( len ( set_ ) - 1 ) : # ==== added by zli ======= <MASK> return False # ==== added by zli ======= # ==== added by zli ======= # if set_[i][1] > set_[i + 1][1]: if set_ [ i ] [ 1 ] > = set_ [ i + 1 ] [ 1 ] : # ==== added by zli ======= return False return True,if set_ [ i ] [ 0 ] >= set_ [ i + 1 ] [ 0 ] :,if set_ [ i ] [ 0 ] <= set_ [ i + 1 ] [ 0,96.82690895,FALSE,96.02
309,"def show_topics ( ) : """"""prints all available miscellaneous help topics."""""" print ( _stash . text_color ( "" Miscellaneous Topics: "" , "" yellow "" ) ) for pp in PAGEPATHS : <MASK> continue content = os . listdir ( pp ) for pn in content : if "" . "" in pn : name = pn [ : pn . index ( "" . "" ) ] else : name = pn print ( name )",if not os . path . isdir ( pp ) :,"if pp == ""topics"" :",74.6590288,FALSE,89.56
310,"def test_send_error ( self ) : allow_transfer_encoding_codes = ( 205 , 304 ) for code in ( 101 , 102 , 204 , 205 , 304 ) : self . con . request ( "" SEND_ERROR "" , "" / {} "" . format ( code ) ) res = self . con . getresponse ( ) self . assertEqual ( code , res . status ) self . assertEqual ( None , res . getheader ( "" Content-Length "" ) ) self . assertEqual ( None , res . getheader ( "" Content-Type "" ) ) <MASK> self . assertEqual ( None , res . getheader ( "" Transfer-Encoding "" ) ) data = res . read ( ) self . assertEqual ( b "" "" , data )",if code not in allow_transfer_encoding_codes :,if code in allow_transfer_encoding_codes :,98.92362413,FALSE,98.36
311,"def _length_hint ( obj ) : """"""Returns the length hint of an object."""""" try : return len ( obj ) except ( AttributeError , TypeError ) : try : get_hint = type ( obj ) . __length_hint__ except AttributeError : return None try : hint = get_hint ( obj ) except TypeError : return None <MASK> return None return hint","if hint is NotImplemented or not isinstance ( hint , int_types ) or hint < 0 :",if not hint :,64.83055202,FALSE,80.35
312,"def _rmtree ( self , path ) : # Essentially a stripped down version of shutil.rmtree.  We can't # use globals because they may be None'ed out at shutdown. for name in self . _listdir ( path ) : fullname = self . _path_join ( path , name ) try : isdir = self . _isdir ( fullname ) except self . _os_error : isdir = False <MASK> self . _rmtree ( fullname ) else : try : self . _remove ( fullname ) except self . _os_error : pass try : self . _rmdir ( path ) except self . _os_error : pass",if isdir :,if isdir :,75,TRUE,100
313,"def get_sources ( self , sources = None ) : """"""Returns all sources from this provider."""""" self . _load ( ) if sources is None : sources = list ( self . data . keys ( ) ) elif not isinstance ( sources , ( list , tuple ) ) : sources = [ sources ] for source in sources : <MASK> raise KeyError ( "" Invalid data key:  {} . Valid keys are:  {} "" . format ( source , "" ,  "" . join ( str ( k ) for k in self . data ) ) ) return { k : self . data [ k ] for k in sources }",if source not in self . data :,if source not in self . data :,100,TRUE,100
314,"def do_shorts ( opts : List [ Tuple [ str , str ] ] , optstring : str , shortopts : str , args : List [ str ] ) - > Tuple [ List [ Tuple [ str , str ] ] , List [ str ] ] : while optstring != "" "" : opt , optstring = optstring [ 0 ] , optstring [ 1 : ] if short_has_arg ( opt , shortopts ) : <MASK> if not args : raise GetoptError ( "" option - %s  requires argument "" % opt , opt ) optstring , args = args [ 0 ] , args [ 1 : ] optarg , optstring = optstring , "" "" else : optarg = "" "" opts . append ( ( "" - "" + opt , optarg ) ) return opts , args","if optstring == """" :","if optstring == """" :",100,TRUE,100
315,"def _sanitize_dict ( self , config_dict , allow_val_change = None , ignore_keys : set = None ) : sanitized = { } for k , v in six . iteritems ( config_dict ) : <MASK> continue k , v = self . _sanitize ( k , v , allow_val_change ) sanitized [ k ] = v return sanitized",if ignore_keys and k in ignore_keys :,if k in ignore_keys :,91.88929972,FALSE,93.44
316,def x ( data ) : count = 0 while count < 10 : data . start_example ( SOME_LABEL ) b = data . draw_bits ( 1 ) <MASK> count + = 1 data . stop_example ( discard = not b ) data . mark_interesting ( ),if b :,if b :,100,TRUE,100
317,"def prompt_for_resume ( config ) : logger = logging . getLogger ( "" changeme "" ) logger . error ( "" A previous scan was interrupted. Type R to resume or F to start a fresh scan "" ) answer = "" "" while not ( answer == "" R "" or answer == "" F "" ) : prompt = "" (R/F)>  "" answer = "" "" try : answer = raw_input ( prompt ) except NameError : answer = input ( prompt ) if answer . upper ( ) == "" F "" : logger . debug ( "" Forcing a fresh scan "" ) <MASK> logger . debug ( "" Resuming previous scan "" ) config . resume = True return config . resume","elif answer . upper ( ) == ""R"" :","elif answer . upper ( ) == ""R"" :",100,TRUE,100
318,"def _evaluate_local_single ( self , iterator ) : for batch in iterator : in_arrays = convert . _call_converter ( self . converter , batch , self . device ) with function . no_backprop_mode ( ) : <MASK> results = self . calc_local ( * in_arrays ) elif isinstance ( in_arrays , dict ) : results = self . calc_local ( * * in_arrays ) else : results = self . calc_local ( in_arrays ) if self . _progress_hook : self . _progress_hook ( batch ) yield results","if isinstance ( in_arrays , tuple ) :","if isinstance ( in_arrays , list ) :",98.65526439,FALSE,98.04
319,"def _send_until_done ( self , data ) : while True : try : return self . connection . send ( data ) except OpenSSL . SSL . WantWriteError : <MASK> raise timeout ( ) continue except OpenSSL . SSL . SysCallError as e : raise SocketError ( str ( e ) )","if not util . wait_for_write ( self . socket , self . socket . gettimeout ( ) ) :",if self . timeout is not None :,55.79915713,FALSE,70.76
320,"def _read_jtl_chunk ( self , jtl ) : data = jtl . read ( 1024 * 1024 * 10 ) if data : parts = data . rsplit ( "" \n "" , 1 ) <MASK> ready_chunk = self . buffer + parts [ 0 ] + "" \n "" self . buffer = parts [ 1 ] df = string_to_df ( ready_chunk ) self . stat_queue . put ( df ) return df else : self . buffer + = parts [ 0 ] else : if self . jmeter_finished : self . agg_finished = True jtl . readline ( ) return None",if len ( parts ) > 1 :,if len ( parts ) == 2 :,82.51383524,FALSE,96.57
321,"def __new__ ( mcl , classname , bases , dictionary ) : slots = list ( dictionary . get ( "" __slots__ "" , [ ] ) ) for getter_name in [ key for key in dictionary if key . startswith ( "" get_ "" ) ] : name = getter_name slots . append ( "" __ "" + name ) getter = dictionary . pop ( getter_name ) setter = dictionary . get ( setter_name , None ) <MASK> del dictionary [ setter_name ] dictionary [ name ] = property ( getter . setter ) dictionary [ "" __slots__ "" ] = tuple ( slots ) return super ( ) . __new__ ( mcl , classname , bases , dictionary )","if setter is not None and isinstance ( setter , collections . Callable ) :",if getter is None :,91.36490383,FALSE,91.24
322,"def tex_coords ( self ) : """"""Array of texture coordinate data."""""" if "" multi_tex_coords "" not in self . domain . attribute_names : <MASK> domain = self . domain attribute = domain . attribute_names [ "" tex_coords "" ] self . _tex_coords_cache = attribute . get_region ( attribute . buffer , self . start , self . count ) self . _tex_coords_cache_version = domain . _version region = self . _tex_coords_cache region . invalidate ( ) return region . array else : return None",if self . _tex_coords_cache_version != self . domain . _version :,if self . _tex_coords_cache is None :,86.00294996,FALSE,91.71
323,"def index ( self , sub , start = 0 ) : """"""Returns the index of the closing bracket"""""" br = "" ([ { < "" [ "" )]}> "" . index ( sub ) ] count = 0 for i in range ( start , len ( self . string ) ) : char = self . string [ i ] <MASK> count + = 1 elif char == sub : if count > 0 : count - = 1 else : return i err = "" Closing bracket  {!r}  missing in string  {!r} "" . format ( sub , "" "" . join ( self . original ) ) raise ParseError ( err )",if char == br :,if char == br :,100,TRUE,100
324,"def test_createFile ( self ) : text = "" This is a test! "" path = tempfile . mktemp ( ) try : koDoc = self . _koDocFromPath ( path , load = False ) koDoc . buffer = text koDoc . save ( 0 ) del koDoc koDoc2 = self . _koDocFromPath ( path ) assert koDoc2 . buffer == text finally : <MASK> os . unlink ( path ) # clean up",if os . path . exists ( path ) :,if os . path . exists ( path ) :,100,TRUE,100
325,"def __editScopeHasEdit ( self , attributeHistory ) : with attributeHistory . context : tweak = GafferScene . EditScopeAlgo . acquireParameterEdit ( attributeHistory . scene . node ( ) , attributeHistory . context [ "" scene:path "" ] , attributeHistory . attributeName , IECoreScene . ShaderNetwork . Parameter ( "" "" , self . __parameter ) , createIfNecessary = False , ) <MASK> return False return tweak [ "" enabled "" ] . getValue ( )",if tweak is None :,"if not tweak [ ""enabled"" ] . getValue ( ) :",92.12345322,FALSE,86.57
326,"def mail_migrator ( app , schema_editor ) : Event_SettingsStore = app . get_model ( "" pretixbase "" , "" Event_SettingsStore "" ) for ss in Event_SettingsStore . objects . filter ( key__in = [ "" mail_text_order_approved "" , "" mail_text_order_placed "" , "" mail_text_order_placed_require_approval "" , ] ) : chgd = ss . value . replace ( "" {date} "" , "" {expire_date} "" ) <MASK> ss . value = chgd ss . save ( ) cache . delete ( "" hierarkey_ {} _ {} "" . format ( "" event "" , ss . object_id ) )",if chgd != ss . value :,if chgd :,93.82554906,FALSE,95.68
327,"def __get_limits ( self ) : dimension = len ( self . __tree . get_root ( ) . data ) nodes = self . __get_all_nodes ( ) max , min = [ float ( "" -inf "" ) ] * dimension , [ float ( "" +inf "" ) ] * dimension for node in nodes : for d in range ( dimension ) : if max [ d ] < node . data [ d ] : max [ d ] = node . data [ d ] <MASK> min [ d ] = node . data [ d ] return min , max",if min [ d ] > node . data [ d ] :,if min [ d ] > node . data [ d ] :,100,TRUE,100
328,"def get_complete_position ( self , context : UserContext ) - > int : # Check member prefix pattern. for prefix_pattern in convert2list ( self . get_filetype_var ( context [ "" filetype "" ] , "" prefix_patterns "" ) ) : m = re . search ( self . _object_pattern + prefix_pattern + r "" \ w*$ "" , context [ "" input "" ] ) <MASK> continue self . _prefix = re . sub ( r "" \ w*$ "" , "" "" , m . group ( 0 ) ) m = re . search ( r "" \ w*$ "" , context [ "" input "" ] ) if m : return m . start ( ) return - 1","if m is None or prefix_pattern == """" :",if not m :,71.41419179,FALSE,92.1
329,"def _stderr_supports_color ( ) : try : if hasattr ( sys . stderr , "" isatty "" ) and sys . stderr . isatty ( ) : if curses : curses . setupterm ( ) if curses . tigetnum ( "" colors "" ) > 0 : return True <MASK> if sys . stderr is getattr ( colorama . initialise , "" wrapped_stderr "" , object ( ) ) : return True except Exception : # Very broad exception handling because it's always better to # fall back to non-colored logs than to break at startup. pass return False",elif colorama :,elif colorama :,100,TRUE,100
330,"def setLabelColumnWidth ( self , panel , width ) : for child in panel . GetChildren ( ) : <MASK> size = child . GetSize ( ) size [ 0 ] = width child . SetBestSize ( size )","if isinstance ( child , wx . lib . stattext . GenStaticText ) :",if child . GetWidth ( ) == width :,74.04143843,FALSE,74.86
331,"def update ( self , other ) : if other . M is None : <MASK> self . items . update ( other . items ) else : for i in other . items : self . add ( i ) return <MASK> self . convert ( ) self . M = array . array ( "" B "" , list ( map ( max , list ( zip ( self . M , other . M ) ) ) ) )",if self . M is None :,if self . items is None :,97.31377773,FALSE,94.53
332,"def on_end_epoch ( self , state ) : if self . write_epoch_metrics : <MASK> self . writer . add_text ( "" epoch "" , "" <h4>Epoch  {} </h4> "" . format ( state [ torchbearer . EPOCH ] ) + self . table_formatter ( str ( state [ torchbearer . METRICS ] ) ) , 1 , ) else : self . writer . add_text ( "" epoch "" , self . table_formatter ( str ( state [ torchbearer . METRICS ] ) ) , state [ torchbearer . EPOCH ] , )",if self . visdom :,if self . use_epoch :,73.68932694,FALSE,96.25
333,"def is_listening_for_message ( conversation_id : Text , endpoint : EndpointConfig ) - > bool : """"""Check if the conversation is in need for a user message."""""" tracker = await retrieve_tracker ( endpoint , conversation_id , EventVerbosity . APPLIED ) for i , e in enumerate ( reversed ( tracker . get ( "" events "" , [ ] ) ) ) : <MASK> return False elif e . get ( "" event "" ) == ActionExecuted . type_name : return e . get ( "" name "" ) == ACTION_LISTEN_NAME return False","if e . get ( ""event"" ) == UserUttered . type_name :","if e . get ( ""event"" ) == ActionExecuted . type_unknown :",97.93595567,FALSE,96.23
334,"def filter_ports ( self , dpid , in_port , nw_id , allow_nw_id_external = None ) : assert nw_id != self . nw_id_unknown ret = [ ] for port in self . get_ports ( dpid ) : nw_id_ = port . network_id <MASK> continue if nw_id_ == nw_id : ret . append ( port . port_no ) elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external : ret . append ( port . port_no ) return ret",if port . port_no == in_port :,if in_port and port . port_no not in in_port :,95.29972039,FALSE,93.61
335,"def next_month ( billing_cycle_anchor : datetime , dt : datetime ) - > datetime : estimated_months = round ( ( dt - billing_cycle_anchor ) . days * 12.0 / 365 ) for months in range ( max ( estimated_months - 1 , 0 ) , estimated_months + 2 ) : proposed_next_month = add_months ( billing_cycle_anchor , months ) <MASK> return proposed_next_month raise AssertionError ( "" Something wrong in next_month calculation with  "" f "" billing_cycle_anchor:  { billing_cycle_anchor } , dt:  { dt } "" )",if 20 < ( proposed_next_month - dt ) . days < 40 :,if proposed_next_month . date ( ) . year == dt . year :,90.81922096,FALSE,90.99
336,"def wait_complete ( self ) : """"""Wait for futures complete done."""""" for future in concurrent . futures . as_completed ( self . _futures . keys ( ) ) : try : error = future . exception ( ) except concurrent . futures . CancelledError : break name = self . _futures [ future ] <MASK> err_msg = ' Extracting  "" {0} "" , got:  {1} ' . format ( name , error ) logger . error ( err_msg )",if error is not None :,if error :,80.59632135,FALSE,95.68
337,"def _accept_with ( cls , orm , target ) : if target is orm . mapper : return mapperlib . Mapper elif isinstance ( target , type ) : if issubclass ( target , mapperlib . Mapper ) : return target else : mapper = _mapper_or_none ( target ) <MASK> return mapper else : return _MapperEventsHold ( target ) else : return target",if mapper is not None :,if mapper is not None :,100,TRUE,100
338,"def gvariant_args ( args : List [ Any ] ) - > str : """"""Convert args into gvariant."""""" gvariant = "" "" for arg in args : <MASK> gvariant + = ""   {} "" . format ( str ( arg ) . lower ( ) ) elif isinstance ( arg , ( int , float ) ) : gvariant + = f ""   { arg } "" elif isinstance ( arg , str ) : gvariant + = f '   "" { arg } "" ' else : gvariant + = f ""   { arg !s} "" return gvariant . lstrip ( )","if isinstance ( arg , bool ) :","if isinstance ( arg , ( int , float ) ) :",94.72935162,FALSE,94.77
339,"def _list_cases ( suite ) : for test in suite : if isinstance ( test , unittest . TestSuite ) : _list_cases ( test ) <MASK> if support . match_test ( test ) : print ( test . id ( ) )","elif isinstance ( test , unittest . TestCase ) :","elif isinstance ( test , unittest . TestCase ) :",100,TRUE,100
340,def get_and_set_all_disambiguation ( self ) : all_disambiguations = [ ] for page in self . pages : <MASK> all_disambiguations . extend ( page . relations . disambiguation_links_norm ) if page . relations . disambiguation_links is not None : all_disambiguations . extend ( page . relations . disambiguation_links ) return set ( all_disambiguations ),if page . relations . disambiguation_links_norm is not None :,if page . relations . disambiguation_links_norm is not None :,100,TRUE,100
341,"def test_decode_invalid ( self ) : testcases = [ ( b "" xn--w& "" , "" strict "" , UnicodeError ( ) ) , ( b "" xn--w& "" , "" ignore "" , "" xn- "" ) , ] for puny , errors , expected in testcases : with self . subTest ( puny = puny , errors = errors ) : <MASK> self . assertRaises ( UnicodeError , puny . decode , "" punycode "" , errors ) else : self . assertEqual ( puny . decode ( "" punycode "" , errors ) , expected )","if isinstance ( expected , Exception ) :",if expected is None :,93.89189339,FALSE,93.41
342,"def find_globs ( walker , patterns , matches ) : for root , dirs , files in walker : for d in dirs : d = join ( root , d ) for pattern in patterns : for p in Path ( d ) . glob ( pattern ) : matches . add ( str ( p ) ) sub_files = set ( ) for p in matches : <MASK> for f in files : sub_files . add ( join ( root , f ) ) matches . update ( sub_files )",if root . startswith ( p ) :,if p . is_file ( ) :,94.56969198,FALSE,93.62
343,"def parse_stack_trace ( self , it , line ) : """"""Iterate over lines and parse stack traces."""""" events = [ ] stack_traces = [ ] while self . stack_trace_re . match ( line ) : event = self . parse_stack_trace_line ( line ) <MASK> events . append ( event ) stack_traces . append ( line ) line = get_next ( it ) events . reverse ( ) return stack_traces , events , line",if event :,if event :,100,TRUE,100
344,"def process ( self ) : """"""Do processing necessary, storing result in feature."""""" summation = 0 # count of all histo = self . data [ "" flat.notes.quarterLengthHistogram "" ] if not histo : raise NativeFeatureException ( "" input lacks notes "" ) maxKey = 0 # max found for any one key for key in histo : # all defined keys should be greater than zero, but just in case if histo [ key ] > 0 : summation + = histo [ key ] <MASK> maxKey = histo [ key ] self . feature . vector [ 0 ] = maxKey / summation",if histo [ key ] >= maxKey :,if histo [ key ] > maxKey :,98.87630727,FALSE,97.99
345,"def load_resource ( name ) : """"""return file contents for files within the package root folder"""""" try : <MASK> return sublime . load_resource ( "" Packages/Markdown Preview/ {0} "" . format ( name ) ) else : filename = os . path . join ( sublime . packages_path ( ) , INSTALLED_DIRECTORY , os . path . normpath ( name ) ) return load_utf8 ( filename ) except : print ( "" Error while load_resource( ' %s ' ) "" % name ) traceback . print_exc ( ) return "" """,if is_ST3 ( ) :,"if name . startswith ( ""markdown-preview"" ) :",95.3960577,FALSE,93.41
346,"def get_password ( self , service , repo_url ) : if self . is_unlocked : asyncio . set_event_loop ( asyncio . new_event_loop ( ) ) collection = secretstorage . get_default_collection ( self . connection ) attributes = { "" application "" : "" Vorta "" , "" service "" : service , "" repo_url "" : repo_url } items = list ( collection . search_items ( attributes ) ) logger . debug ( "" Found  %i  passwords matching repo URL. "" , len ( items ) ) <MASK> return items [ 0 ] . get_secret ( ) . decode ( "" utf-8 "" ) return None",if len ( items ) > 0 :,if len ( items ) == 1 :,98.44162765,FALSE,96.79
347,"def get_files ( d ) : res = [ ] for p in glob . glob ( os . path . join ( d , "" * "" ) ) : if not p : continue ( pth , fname ) = os . path . split ( p ) if fname == "" output "" : continue if fname == "" PureMVC_Python_1_0 "" : continue if fname [ - 4 : ] == "" .pyc "" : # ehmm.. no. continue <MASK> get_dir ( p ) else : res . append ( p ) return res",if os . path . isdir ( p ) :,if os . path . isdir ( p ) :,100,TRUE,100
348,"def test_nic_names ( self ) : p = subprocess . Popen ( [ "" ipconfig "" , "" /all "" ] , stdout = subprocess . PIPE ) out = p . communicate ( ) [ 0 ] if PY3 : out = str ( out , sys . stdout . encoding ) nics = psutil . net_io_counters ( pernic = True ) . keys ( ) for nic in nics : <MASK> continue if nic not in out : self . fail ( "" %r  nic wasn ' t found in  ' ipconfig /all '  output "" % nic )","if ""pseudo-interface"" in nic . replace ( "" "" , ""-"" ) . lower ( ) :",if not nic :,89.71569681,FALSE,83.77
349,"def vexop_to_simop ( op , extended = True , fp = True ) : res = operations . get ( op ) if res is None and extended : attrs = op_attrs ( op ) <MASK> raise UnsupportedIROpError ( "" Operation not implemented "" ) res = SimIROp ( op , * * attrs ) if res is None : raise UnsupportedIROpError ( "" Operation not implemented "" ) if res . _float and not fp : raise UnsupportedIROpError ( "" Floating point support disabled "" ) return res",if attrs is None :,if not attrs :,96.06614505,FALSE,95.67
350,"def rule_builder_add_value ( self , value , screenshot_name = None ) : rule_builder = self . components . rule_builder rule_builder . menu_button_column . wait_for_and_click ( ) with self . rule_builder_rule_editor ( "" add-column-value "" ) as editor_element : filter_input = editor_element . find_element_by_css_selector ( "" input[type= ' text ' ] "" ) filter_input . clear ( ) filter_input . send_keys ( value ) <MASK> self . screenshot ( screenshot_name )",if screenshot_name :,if screenshot_name :,100,TRUE,100
351,"def make_open_socket ( self ) : s = socket . socket ( ) try : s . bind ( DEFAULT_BIND_ADDR_TUPLE ) <MASK> # Windows and linux (with psutil) doesn't show as open until # we call listen (linux with lsof accepts either) s . listen ( 1 ) self . assert_open ( s , s . fileno ( ) ) except : s . close ( ) s = None raise return s",if WIN or greentest . LINUX :,"if sys . platform == ""win32"" :",88.71188892,FALSE,90.38
352,"def handle_ray_task_error ( e ) : for s in e . traceback_str . split ( "" \n "" ) [ : : - 1 ] : <MASK> try : raise getattr ( builtins , s . split ( "" : "" ) [ 0 ] ) ( "" "" . join ( s . split ( "" : "" ) [ 1 : ] ) ) except AttributeError as att_err : if "" module "" in str ( att_err ) and builtins . __name__ in str ( att_err ) : pass else : raise att_err raise e","if ""Error"" in s or ""Exception"" in s :","if s . startswith ( ""ray:"" ) :",92.61500623,FALSE,90.83
353,"def compare_multiple_events ( i , expected_results , actual_results ) : events_in_a_row = [ ] j = i while j < len ( expected_results ) and isinstance ( actual_results [ j ] , actual_results [ i ] . __class__ ) : events_in_a_row . append ( actual_results [ j ] ) j + = 1 message = "" "" for event in events_in_a_row : for k in range ( i , j ) : passed , message = compare_events ( expected_results [ k ] , event ) <MASK> expected_results [ k ] = None break else : return i , False , message return j , True , "" """,if passed :,if not passed :,96.76777081,FALSE,98.36
354,"def ListSubscriptions ( self , params ) : queryreturn = sqlQuery ( """""" SELECT label, address, enabled FROM subscriptions """""" ) data = ' { "" subscriptions "" :[ ' for row in queryreturn : label , address , enabled = row label = shared . fixPotentiallyInvalidUTF8Data ( label ) <MASK> data + = "" , "" data + = json . dumps ( { "" label "" : label . encode ( "" base64 "" ) , "" address "" : address , "" enabled "" : enabled == 1 , } , indent = 4 , separators = ( "" , "" , "" :  "" ) , ) data + = "" ]} "" return data",if len ( data ) > 20 :,if enabled :,93.53150161,FALSE,94.34
355,"def compile ( self , args ) : compiled_args = { } for key , value in six . iteritems ( args ) : <MASK> compiled_args [ key ] = str ( value ) else : compiled_args [ key ] = sjson_dumps ( value ) return self . _minified_code % compiled_args",if key in self . clean_args :,"if isinstance ( value , six . string_types ) :",73.84373904,FALSE,84.95
356,"def insert ( self , pack_id , data ) : if ( pack_id not in self . queue ) and pack_id > self . begin_id : self . queue [ pack_id ] = PacketInfo ( data ) if self . end_id == pack_id : self . end_id = pack_id + 1 <MASK> eid = self . end_id while eid < pack_id : self . miss_queue . add ( eid ) eid + = 1 self . end_id = pack_id + 1 else : self . miss_queue . remove ( pack_id )",elif self . end_id < pack_id :,elif self . end_id != pack_id :,98.69749587,FALSE,97.37
357,"def _target_generator ( self ) : # since we do not have predictions yet, so we ignore sampling here if self . _internal_target_generator is None : <MASK> return None from . . . . model_zoo . ssd . target import SSDTargetGenerator self . _internal_target_generator = SSDTargetGenerator ( iou_thresh = self . _iou_thresh , stds = self . _box_norm , negative_mining_ratio = - 1 , * * self . _kwargs ) return self . _internal_target_generator else : return self . _internal_target_generator",if self . _anchors_none :,if self . _iou_thresh == 0 :,72.92449945,FALSE,94.3
358,"def test_heapsort ( self ) : # Exercise everything with repeated heapsort checks for trial in range ( 100 ) : size = random . randrange ( 50 ) data = [ random . randrange ( 25 ) for i in range ( size ) ] <MASK> # Half of the time, use heapify heap = data [ : ] self . module . heapify ( heap ) else : # The rest of the time, use heappush heap = [ ] for item in data : self . module . heappush ( heap , item ) heap_sorted = [ self . module . heappop ( heap ) for i in range ( size ) ] self . assertEqual ( heap_sorted , sorted ( data ) )",if trial & 1 :,if trial == 1 :,74.04160659,FALSE,97.46
359,"def wait ( self , timeout = None ) : if self . returncode is None : if timeout is None : msecs = _subprocess . INFINITE else : msecs = max ( 0 , int ( timeout * 1000 + 0.5 ) ) res = _subprocess . WaitForSingleObject ( int ( self . _handle ) , msecs ) <MASK> code = _subprocess . GetExitCodeProcess ( self . _handle ) if code == TERMINATE : code = - signal . SIGTERM self . returncode = code return self . returncode",if res == _subprocess . WAIT_OBJECT_0 :,if res == _subprocess . WAIT_OBJECT_0 :,100,TRUE,100
360,"def _on_change ( self ) : changed = False self . save ( ) for key , value in self . data . items ( ) : if isinstance ( value , bool ) : if value : changed = True break if isinstance ( value , int ) : if value != 1 : changed = True break <MASK> continue elif len ( value ) != 0 : changed = True break self . _reset_button . disabled = not changed",elif value is None :,"if key == ""button"" :",88.15043006,FALSE,90.98
361,"def isnotsurplus ( self , item : T ) - > bool : if not self . matchers : <MASK> self . mismatch_description . append_text ( "" not matched:  "" ) . append_description_of ( item ) return False return True",if self . mismatch_description :,if self . mismatch_description :,75,TRUE,100
362,"def resolve_env_secrets ( config , environ ) : """"""Create copy that recursively replaces {""$env"": ""NAME""} with values from environ"""""" if isinstance ( config , dict ) : <MASK> return environ . get ( list ( config . values ( ) ) [ 0 ] ) elif list ( config . keys ( ) ) == [ "" $file "" ] : return open ( list ( config . values ( ) ) [ 0 ] ) . read ( ) else : return { key : resolve_env_secrets ( value , environ ) for key , value in config . items ( ) } elif isinstance ( config , list ) : return [ resolve_env_secrets ( value , environ ) for value in config ] else : return config","if list ( config . keys ( ) ) == [ ""$env"" ] :","if list ( config . keys ( ) ) == [ ""$name"" ] :",74.11378326,FALSE,98.54
363,"def __open__ ( filename , * args , * * kwargs ) : if os . path . isfile ( filename ) : return __realopen__ ( filename , * args , * * kwargs ) if not os . path . isabs ( filename ) : datafilename = __papplet__ . dataPath ( filename ) <MASK> return __realopen__ ( datafilename , * args , * * kwargs ) sketchfilename = __papplet__ . sketchPath ( filename ) if os . path . isfile ( sketchfilename ) : return __realopen__ ( sketchfilename , * args , * * kwargs ) # Fail naturally return __realopen__ ( filename , * args , * * kwargs )",if os . path . isfile ( datafilename ) :,if os . path . isfile ( datafilename ) :,100,TRUE,100
364,def run ( self ) : while not self . completed : <MASK> time . sleep ( self . period ) else : self . _completed . wait ( self . period ) self . counter + = 1 try : self . callback ( self . counter ) except Exception : self . stop ( ) if self . timeout is not None : dt = time . time ( ) - self . _start_time if dt > self . timeout : self . stop ( ) if self . counter == self . count : self . stop ( ),if self . block :,if self . _completed . is_set ( ) :,96.27509915,FALSE,91.83
365,"def remove ( self , path , config = None , error_on_path = False , defaults = None ) : if not path : <MASK> raise NoSuchSettingsPath ( ) return if config is not None or defaults is not None : if config is None : config = self . _config if defaults is None : defaults = dict ( self . _map . parents ) chain = HierarchicalChainMap ( config , defaults ) else : chain = self . _map try : chain . del_by_path ( path ) self . _mark_dirty ( ) except KeyError : <MASK> raise NoSuchSettingsPath ( ) pass",if error_on_path :,if error_on_path :,100,TRUE,100
366,"def structured_dot_grad ( sparse_A , dense_B , ga ) : if sparse_A . type . format in ( "" csc "" , "" csr "" ) : <MASK> sdgcsx = sdg_csc CSx = CSC else : sdgcsx = sdg_csr CSx = CSR g_A_data = sdgcsx ( csm_indices ( sparse_A ) , csm_indptr ( sparse_A ) , dense_B , ga ) return CSx ( g_A_data , csm_indices ( sparse_A ) , csm_indptr ( sparse_A ) , csm_shape ( sparse_A ) ) else : raise NotImplementedError ( )","if sparse_A . type . format == ""csc"" :","if sparse_B . type . format == ""csc"" :",95.0823384,FALSE,98.22
367,"def step_async ( self , actions ) : listify = True try : <MASK> listify = False except TypeError : pass if not listify : self . actions = actions else : assert ( self . num_envs == 1 ) , f "" actions  { actions }  is either not a list or has a wrong size - cannot match to  { self . num_envs }  environments "" self . actions = [ actions ]",if len ( actions ) == self . num_envs :,if not self . list_async ( actions ) :,69.44610611,FALSE,89.08
368,"def tempFailureRetry ( func , * args , * * kwargs ) : while True : try : return func ( * args , * * kwargs ) except ( os . error , IOError ) as ex : <MASK> continue else : raise",if ex . errno == errno . EINTR :,if ex . errno == errno . EINTR :,75,TRUE,100
369,"def test_learning_always_changes_generation ( chars , order ) : learner = LStar ( lambda s : len ( s ) == 1 and s [ 0 ] in chars ) for c in order : prev = learner . generation s = bytes ( [ c ] ) <MASK> learner . learn ( s ) assert learner . generation > prev",if learner . dfa . matches ( s ) != learner . member ( s ) :,if s [ 0 ] in chars :,82.92161596,FALSE,79.09
370,"def test_costs_5D_noisy_names ( signal_bkps_5D_noisy , cost_name ) : signal , bkps = signal_bkps_5D_noisy cost = cost_factory ( cost_name ) cost . fit ( signal ) cost . error ( 0 , 100 ) cost . error ( 100 , signal . shape [ 0 ] ) cost . error ( 10 , 50 ) cost . sum_of_costs ( bkps ) with pytest . raises ( NotEnoughPoints ) : <MASK> cost . min_size = 4 cost . error ( 1 , 2 ) else : cost . error ( 1 , 2 )","if cost_name == ""cosine"" :","if cost_name == ""noisy"" :",98.67541501,FALSE,98.09
371,"def remove_empty_dirs ( dirname ) : logger . debug ( "" remove_empty_dirs  ' %s ' "" % ( dirname ) ) try : <MASK> dirname = dirname . encode ( "" utf-8 "" ) os . removedirs ( dirname ) logger . debug ( "" remove_empty_dirs  ' %s '  done "" % ( dirname ) ) except OSError as exc : # Python >2.5 if exc . errno == errno . ENOTEMPTY : logger . debug ( "" remove_empty_dirs  ' %s '  not empty "" % ( dirname ) ) pass else : raise except Exception as e : logger . exception ( e ) logger . error ( "" remove_empty_dirs exception:  "" + dirname ) raise e","if not isinstance ( dirname , str ) :","if isinstance ( dirname , str ) :",85.16244832,FALSE,98.33
372,"def get_unique_attribute ( self , name : str ) : feat = None for f in self . features : <MASK> if feat is not None : raise RuntimeError ( "" The attribute was not unique. "" ) feat = f if feat is None : raise RuntimeError ( "" The attribute did not exist "" ) return getattr ( feat , name )","if self . _return_feature ( f ) and hasattr ( f , name ) :",if f . name == name :,85.59653795,FALSE,79.64
373,"def get_allocated_address ( self , config : ActorPoolConfig , allocated : allocated_type ) - > str : addresses = config . get_external_addresses ( label = self . label ) for addr in addresses : occupied = False for strategy , _ in allocated . get ( addr , dict ( ) ) . values ( ) : if strategy == self : occupied = True break <MASK> return addr raise NoIdleSlot ( f "" No idle slot for creating actor  "" f "" with label  { self . label } , mark  { self . mark } "" )",if not occupied :,if occupied :,97.32523833,FALSE,97.75
374,"def __deepcopy__ ( self , memo ) : cls = self . __class__ result = cls . __new__ ( cls ) memo [ id ( self ) ] = result for key , value in self . __dict__ . items ( ) : <MASK> setattr ( result , key , copy . copy ( value ) ) else : setattr ( result , key , copy . deepcopy ( value , memo ) ) return result",if key in cls . dynamic_methods :,"if isinstance ( value , dict ) :",93.08470542,FALSE,91.36
375,def restore_forward ( model ) : for child in model . children ( ) : # leaf node <MASK> child . forward = child . old_forward child . old_forward = None else : restore_forward ( child ),"if is_leaf ( child ) and hasattr ( child , ""old_forward"" ) :","if isinstance ( child , Leaf ) and child . old_forward is not None :",88.6110404,FALSE,76.26
376,"def add ( self , obj , allow_duplicates = False ) : if allow_duplicates or obj not in self . _constants : self . _constant_pool . append ( obj ) self . _constants [ obj ] = len ( self ) <MASK> self . _constant_pool . append ( None )","if obj . __class__ in ( Double , Long ) :",if self . _constants [ obj ] == 0 :,67.91815878,FALSE,81.76
377,"def find_file_copyright_notices ( fname ) : ret = set ( ) f = open ( fname ) lines = f . readlines ( ) for l in lines [ : 80 ] : # hmmm, assume copyright to be in first 80 lines idx = l . lower ( ) . find ( "" copyright "" ) if idx < 0 : continue copyright = l [ idx + 9 : ] . strip ( ) if not copyright : continue copyright = sanitise ( copyright ) # hmm, do a quick check to see if there's a year, # if not, skip it <MASK> continue ret . add ( copyright ) return ret","if not copyright . find ( ""200"" ) >= 0 and not copyright . find ( ""199"" ) >= 0 :",if not copyright . isdigit ( ) :,93.45473003,FALSE,83.81
378,"def callback ( lexer , match , context ) : text = match . group ( ) extra = "" "" if start : context . next_indent = len ( text ) if context . next_indent < context . indent : while context . next_indent < context . indent : context . indent = context . indent_stack . pop ( ) <MASK> extra = text [ context . indent : ] text = text [ : context . indent ] else : context . next_indent + = len ( text ) if text : yield match . start ( ) , TokenClass , text if extra : yield match . start ( ) + len ( text ) , TokenClass . Error , extra context . pos = match . end ( )",if context . next_indent > context . indent :,if context . next_indent < context . indent :,99.10866036,FALSE,98.34
379,"def queries ( self ) : if DEV : cmd = ShellCommand ( "" docker "" , "" ps "" , "" -qf "" , "" name= %s "" % self . path . k8s ) if not cmd . check ( f "" docker check for  { self . path . k8s } "" ) : if not cmd . stdout . strip ( ) : log_cmd = ShellCommand ( "" docker "" , "" logs "" , self . path . k8s , stderr = subprocess . STDOUT ) <MASK> print ( cmd . stdout ) pytest . exit ( f "" container failed to start for  { self . path . k8s } "" ) return ( )","if log_cmd . check ( f""docker logs for {self.path.k8s}"" ) :",if log_cmd . run ( ) :,91.54468428,FALSE,88.43
380,"def nodes ( self ) : if not self . _nodes : nodes = self . cluster_group . instances ( ) self . _nodes = [ ] master = self . master_node nodeid = 1 for node in nodes : if node . state not in [ "" pending "" , "" running "" ] : continue <MASK> self . _nodes . insert ( 0 , master ) continue self . _nodes . append ( Node ( node , self . key_location , "" node %.3d "" % nodeid ) ) nodeid + = 1 else : for node in self . _nodes : log . debug ( "" refreshing instance  %s "" % node . id ) node . update ( ) return self . _nodes",if node . id == master . id :,if nodeid == 0 :,94.13452871,FALSE,94.12
381,"def match ( cls , agent_name , guid , uri , media = None ) : # Retrieve `Agent` for provided `guid` agent = Agents . get ( agent_name ) if agent is None : <MASK> # First occurrence of unsupported agent log . warn ( "" Unsupported metadata agent:  %s "" % agent_name ) # Mark unsupported agent as ""seen"" unsupported_agents [ agent_name ] = True return False # Duplicate occurrence of unsupported agent log . warn ( "" Unsupported metadata agent:  %s "" % agent_name , extra = { "" duplicate "" : True } ) return False # Fill `guid` with details from agent return agent . fill ( guid , uri , media )",if agent_name not in unsupported_agents :,if agent_name not in unsupported_agents :,100,TRUE,100
382,"def __createRandom ( plug ) : node = plug . node ( ) parentNode = node . ancestor ( Gaffer . Node ) with Gaffer . UndoScope ( node . scriptNode ( ) ) : randomNode = Gaffer . Random ( ) parentNode . addChild ( randomNode ) if isinstance ( plug , ( Gaffer . FloatPlug , Gaffer . IntPlug ) ) : plug . setInput ( randomNode [ "" outFloat "" ] ) <MASK> plug . setInput ( randomNode [ "" outColor "" ] ) GafferUI . NodeEditor . acquire ( randomNode )","elif isinstance ( plug , Gaffer . Color3fPlug ) :","elif isinstance ( plug , Gaffer . ColorPlug ) :",98.77907867,FALSE,97.59
383,"def post_arrow ( self , arr : pa . Table , graph_type : str , opts : str = "" "" ) : dataset_id = self . dataset_id tok = self . token sub_path = f "" api/v2/upload/datasets/ { dataset_id } / { graph_type } /arrow "" try : resp = self . post_arrow_generic ( sub_path , tok , arr , opts ) out = resp . json ( ) <MASK> raise Exception ( "" No success indicator in server response "" ) return out except Exception as e : logger . error ( "" Failed to post arrow to  %s "" , sub_path , exc_info = True ) raise e","if not ( ""success"" in out ) or not out [ ""success"" ] :",if resp . status_code != 200 :,88.21967364,FALSE,88.95
384,"def dict_to_XML ( tag , dictionary , * * kwargs ) : """"""Return XML element converting dicts recursively."""""" elem = Element ( tag , * * kwargs ) for key , val in dictionary . items ( ) : <MASK> child = dict_to_XML ( "" layer "" , val , name = key ) elif isinstance ( val , MutableMapping ) : child = dict_to_XML ( key , val ) else : if tag == "" config "" : child = Element ( "" variable "" , name = key ) else : child = Element ( key ) child . text = str ( val ) elem . append ( child ) return elem","if tag == ""layers"" :","if isinstance ( val , MutableLayer ) :",95.15424094,FALSE,94.65
385,"def apply_incpaths_ml ( self ) : inc_lst = self . includes . split ( ) lst = self . incpaths_lst for dir in inc_lst : node = self . path . find_dir ( dir ) <MASK> error ( "" node not found:  "" + str ( dir ) ) continue if not node in lst : lst . append ( node ) self . bld_incpaths_lst . append ( node )",if not node :,if not node :,100,TRUE,100
386,"def _table_reprfunc ( self , row , col , val ) : if self . _table . column_names [ col ] . endswith ( "" Size "" ) : if isinstance ( val , compat . string_types ) : return ""    %s "" % val elif val < 1024 * * 2 : return ""    %.1f  KB "" % ( val / 1024.0 * * 1 ) <MASK> return ""    %.1f  MB "" % ( val / 1024.0 * * 2 ) else : return ""    %.1f  GB "" % ( val / 1024.0 * * 3 ) if col in ( 0 , "" "" ) : return str ( val ) else : return ""    %s "" % val",elif val < 1024 ** 3 :,elif val < 1024 * 3 :,74.01341085,FALSE,98.77
387,"def _cache_mem ( curr_out , prev_mem , mem_len , reuse_len = None ) : """"""cache hidden states into memory."""""" if mem_len is None or mem_len == 0 : return None else : <MASK> curr_out = curr_out [ : reuse_len ] if prev_mem is None : new_mem = curr_out [ - mem_len : ] else : new_mem = tf . concat ( [ prev_mem , curr_out ] , 0 ) [ - mem_len : ] new_mem . stop_gradient = True return new_mem",if reuse_len is not None and reuse_len > 0 :,if reuse_len is not None :,93.22103306,FALSE,94.72
388,"def GROUP_CONCAT ( builder , distinct , expr , sep = None ) : assert distinct in ( None , True , False ) result = distinct and "" GROUP_CONCAT(DISTINCT  "" or "" GROUP_CONCAT( "" , builder ( expr ) if sep is not None : <MASK> result = result , ""  SEPARATOR  "" , builder ( sep ) else : result = result , "" ,  "" , builder ( sep ) return result , "" ) ""","if builder . provider . dialect == ""MySQL"" :",if distinct :,76.14303747,FALSE,87.9
389,"def __init__ ( self , * args , * * kwargs ) : super ( ) . __init__ ( * args , * * kwargs ) self . custom_fields = [ ] self . obj_type = ContentType . objects . get_for_model ( self . model ) # Add all applicable CustomFields to the form custom_fields = CustomField . objects . filter ( content_types = self . obj_type ) for cf in custom_fields : # Annotate non-required custom fields as nullable <MASK> self . nullable_fields . append ( cf . name ) self . fields [ cf . name ] = cf . to_form_field ( set_initial = False , enforce_required = False ) # Annotate this as a custom field self . custom_fields . append ( cf . name )",if not cf . required :,if cf . required :,98.91577389,FALSE,98.51
390,"def is_child_of ( self , item_hash , possible_child_hash ) : if self . get_last ( item_hash ) != self . get_last ( possible_child_hash ) : return None while True : <MASK> return True if possible_child_hash not in self . items : return False possible_child_hash = self . items [ possible_child_hash ] . previous_hash",if possible_child_hash == item_hash :,if self . get_first ( item_hash ) == self . get_first ( possible,80.77420557,FALSE,83.27
391,"def validate ( self ) : self . assertEqual ( len ( self . inputs ) , len ( self . outputs ) ) for batch_in , batch_out in zip ( self . inputs , self . outputs ) : self . assertEqual ( len ( batch_in ) , len ( batch_out ) ) <MASK> self . validate_unordered_batch ( batch_in , batch_out ) else : for in_data , out_data in zip ( batch_in , batch_out ) : self . assertEqual ( in_data . shape , out_data . shape ) if not self . use_parallel_executor : self . assertTrue ( ( in_data == out_data ) . all ( ) )",if self . use_parallel_executor and not self . use_double_buffer :,if self . use_unordered_batch :,92.97091269,FALSE,91.68
392,"def add_cells ( self , cells ) : for cell in cells : <MASK> id = len ( self . cell_id_map ) self . cell_id_map [ cell ] = id self . id_cell_map [ id ] = cell",if cell not in self . cell_id_map :,if cell not in self . cell_id_map :,100,TRUE,100
393,"def _verify_out ( marker = "" >> "" ) : if shared : self . assertIn ( "" libapp_lib.dylib "" , self . client . out ) else : <MASK> self . assertIn ( "" libapp_lib.a "" , self . client . out ) else : # Incremental build not the same msg self . assertIn ( "" Built target app_lib "" , self . client . out ) out = str ( self . client . out ) . splitlines ( ) for k , v in vals . items ( ) : self . assertIn ( "" %s   %s :  %s "" % ( marker , k , v ) , out )","if marker == "">>"" :",if shared :,94.54111978,FALSE,93.81
394,"def Visit_expr ( self , node ) : # pylint: disable=invalid-name # expr ::= xor_expr ('|' xor_expr)* for child in node . children : self . Visit ( child ) <MASK> _AppendTokenSubtype ( child , format_token . Subtype . BINARY_OPERATOR )","if isinstance ( child , pytree . Leaf ) and child . value == ""|"" :","if isinstance ( child , pytree . Leaf ) and child . value == ""|"" :",75,TRUE,100
395,"def fill_members ( self ) : if self . _get_retrieve ( ) : after = self . after . id if self . after else None data = await self . get_members ( self . guild . id , self . retrieve , after ) if not data : # no data, terminate return <MASK> self . limit = 0 # terminate loop self . after = Object ( id = int ( data [ - 1 ] [ "" user "" ] [ "" id "" ] ) ) for element in reversed ( data ) : await self . members . put ( self . create_member ( element ) )",if len ( data ) < 1000 :,"if data [ - 1 ] [ ""user"" ] [ ""id"" ] == self .",95.26500057,FALSE,85.47
396,"def assert_warns ( expected ) : with warnings . catch_warnings ( record = True ) as w : warnings . simplefilter ( "" always "" ) yield # Python 2 does not raise warnings multiple times from the same stack # frame. if sys . version_info > = ( 3 , 0 ) : <MASK> try : exc_name = expected . __name__ except AttributeError : exc_name = str ( expected ) raise AssertionError ( "" %s  not triggerred "" % exc_name )","if not any ( isinstance ( m . message , expected ) for m in w ) :","if isinstance ( expected , type ) :",66.89227254,FALSE,86.49
397,"def __init__ ( self , measures ) : """"""Constructs a ContingencyMeasures given a NgramAssocMeasures class"""""" self . __class__ . __name__ = "" Contingency "" + measures . __class__ . __name__ for k in dir ( measures ) : <MASK> continue v = getattr ( measures , k ) if not k . startswith ( "" _ "" ) : v = self . _make_contingency_fn ( measures , v ) setattr ( self , k , v )","if k . startswith ( ""__"" ) :","if k . startswith ( ""_"" ) :",98.48858082,FALSE,98.5
398,"def _omit_keywords ( self , context ) : omitted_kws = 0 for event , elem in context : # Teardowns aren't omitted to allow checking suite teardown status. omit = elem . tag == "" kw "" and elem . get ( "" type "" ) != "" teardown "" start = event == "" start "" <MASK> omitted_kws + = 1 if not omitted_kws : yield event , elem elif not start : elem . clear ( ) if omit and not start : omitted_kws - = 1",if omit and start :,if omit and start :,100,TRUE,100
399,"def read_block ( buffer , i ) : offset = i * BLOCK_LENGTH % config . CAPTURE_BUFFER while True : if buffer [ offset ] == BLOCK_MARKER . END : return None while buffer [ offset ] == BLOCK_MARKER . WRITE : time . sleep ( SHORT_SENSOR_SLEEP_TIME ) buffer [ offset ] = BLOCK_MARKER . READ buffer . seek ( offset + 1 ) length = struct . unpack ( "" =H "" , buffer . read ( 2 ) ) [ 0 ] retval = buffer . read ( length ) <MASK> break buffer [ offset ] = BLOCK_MARKER . NOP return retval",if buffer [ offset ] == BLOCK_MARKER . READ :,if retval == BLOCK_MARKER . NOP :,92.8719606,FALSE,94.15
400,def _start ( self ) : try : instance_info = self . _get_instance_info ( ) <MASK> self . _multipass_cmd . start ( instance_name = self . instance_name ) except errors . ProviderInfoError as instance_error : # Until we have proper multipass error codes to know if this # was a communication error we should keep this error tracking # and generation here. raise errors . ProviderInstanceNotFoundError ( instance_name = self . instance_name ) from instance_error,if not instance_info . is_running ( ) :,"if instance_info . get ( ""type"" ) == ""multipass"" :",90.59589566,FALSE,88.04
401,"def _river_driver ( self ) : if self . _cached_river_driver : return self . _cached_river_driver else : <MASK> self . _cached_river_driver = MsSqlDriver ( self . workflow , self . wokflow_object_class , self . field_name ) else : self . _cached_river_driver = OrmDriver ( self . workflow , self . wokflow_object_class , self . field_name ) return self . _cached_river_driver",if app_config . IS_MSSQL :,if self . is_mysql :,95.2298476,FALSE,92.46
402,"def __LazyMap__ ( self , attr ) : try : <MASK> debug_attr_print ( "" %s .__LazyMap__( %s ) added something "" % ( self . _username_ , attr ) ) return 1 except AttributeError : return 0",if self . _LazyAddAttr_ ( attr ) :,if self . _username_ in attr :,63.33072629,FALSE,90.32
403,"def prepare ( self , data = None , user = None ) : """"""Prepare activation for execution."""""" super ( ManagedStartViewActivation , self ) . prepare . original ( ) self . task . owner = user management_form_class = self . get_management_form_class ( ) self . management_form = management_form_class ( data = data , instance = self . task ) if data : <MASK> raise FlowRuntimeError ( "" Activation metadata is broken  {} "" . format ( self . management_form . errors ) ) self . task = self . management_form . save ( commit = False )",if not self . management_form . is_valid ( ) :,if self . management_form . errors :,94.15035544,FALSE,93.4
404,"def PreprocessConditionalStatement ( self , IfList , ReplacedLine ) : while self : if self . __Token : x = 1 elif not IfList : if self < = 2 : continue RegionSizeGuid = 3 <MASK> RegionLayoutLine = 5 continue RegionLayoutLine = self . CurrentLineNumber return 1",if not RegionSizeGuid :,if ReplacedLine :,69.06882891,FALSE,93.02
405,"def _get_completion ( self , document ) : try : completion_header = document . xpath ( "" //div[@id= ' complete_day ' ] "" ) [ 0 ] completion_message = completion_header . getchildren ( ) [ 0 ] <MASK> return False elif "" day_complete_message "" in completion_message . classes : return True except IndexError : return False # Who knows, probably not my diary.","if ""day_incomplete_message"" in completion_message . classes :","if completion_message . get_class ( ) == ""completion"" :",87.63618137,FALSE,87.83
406,"def run ( self ) : DISPATCH_SYNC = components . interfaces . nsIEventTarget . DISPATCH_SYNC try : <MASK> return for match in findlib2 . find_all_matches ( self . regex , self . text ) : <MASK> return self . target . dispatch ( lambda : self . callback ( match ) , DISPATCH_SYNC ) <MASK> return self . target . dispatch ( lambda : self . callback ( None ) , DISPATCH_SYNC ) finally : self . callback = None self . target = None",if self . _stopped :,if self . callback is None :,92.68004423,FALSE,88.02
407,"def to_key ( literal_or_identifier ) : """"""returns string representation of this object"""""" if literal_or_identifier [ "" type "" ] == "" Identifier "" : return literal_or_identifier [ "" name "" ] elif literal_or_identifier [ "" type "" ] == "" Literal "" : k = literal_or_identifier [ "" value "" ] if isinstance ( k , float ) : return unicode ( float_repr ( k ) ) <MASK> return compose_regex ( k ) elif isinstance ( k , bool ) : return "" true "" if k else "" false "" elif k is None : return "" null "" else : return unicode ( k )","elif ""regex"" in literal_or_identifier :","elif isinstance ( k , regex_types ) :",92.71937369,FALSE,93.3
408,"def process_image_pre_creation ( sender , instance : Image , * * kwargs ) : # FIXME(winkidney): May have issue on determining if it #  is created or not if instance . pk is not None : return for plugin in _plugin_instances : process_fn = getattr ( plugin , "" process_image_pre_creation "" , None ) <MASK> continue try : process_fn ( django_settings = settings , image_instance = instance , ) except Exception : logging . exception ( "" Error occurs while trying to access plugin ' s pin_pre_save  "" "" for plugin  %s "" % plugin )",if process_fn is None :,if process_fn is None :,75,TRUE,100
409,"def check_screenshots ( self ) : # If we arrive here, there have not been any failures yet if self . interactive : self . _commit_screenshots ( ) else : <MASK> self . _validate_screenshots ( ) # Always commit the screenshots here. They can be used for the next test run. # If reference screenshots were already present and there was a mismatch, it should # have failed above. self . _commit_screenshots ( ) elif self . allow_missing_screenshots : warnings . warn ( "" No committed reference screenshots available. Ignoring. "" ) else : self . fail ( "" No committed reference screenshots available. Run interactive first. "" )",if self . _has_reference_screenshots ( ) :,if self . _is_available ( ) :,73.90389732,FALSE,95.6
410,"def on_task_abort ( self , task , config ) : if "" abort "" in config : <MASK> return log . debug ( "" sending abort notification "" ) self . send_notification ( config [ "" abort "" ] [ "" title "" ] , config [ "" abort "" ] [ "" message "" ] , config [ "" abort "" ] [ "" via "" ] , template_renderer = task . render , )",if task . silent_abort :,"if config [ ""abort"" ] [ ""via"" ] is None :",67.92556759,FALSE,85.2
411,"def block_users ( self , user_ids ) : broken_items = [ ] self . logger . info ( "" Going to block  %d  users. "" % len ( user_ids ) ) for user_id in tqdm ( user_ids ) : <MASK> self . error_delay ( ) broken_items = user_ids [ user_ids . index ( user_id ) : ] break self . logger . info ( "" DONE: Total blocked  %d  users. "" % self . total [ "" blocks "" ] ) return broken_items",if not self . block ( user_id ) :,"if not self . total [ ""blocks"" ] :",94.29901516,FALSE,93.76
412,"def find_widget_by_id ( self , id , parent = None ) : """"""Recursively searches for widget with specified ID"""""" if parent == None : if id in self : return self [ id ] # Do things fast if possible parent = self [ "" editor "" ] for c in parent . get_children ( ) : if hasattr ( c , "" get_id "" ) : if c . get_id ( ) == id : return c if isinstance ( c , Gtk . Container ) : r = self . find_widget_by_id ( id , c ) <MASK> return r return None",if not r is None :,if r is not None :,98.33344443,FALSE,97.33
413,"def addClasses ( self , name ) : # Result: void - None # In: name: string for n in name . split ( ) : try : k , method = n . split ( "" . "" ) except ValueError : k = n method = None self . classes [ k ] = 1 <MASK> self . methods . setdefault ( k , { } ) [ method ] = 1",if method is not None :,if method :,71.54845722,FALSE,94.38
414,"def Read ( self , lex_mode ) : while True : t = self . _Read ( lex_mode ) self . was_line_cont = t . id == Id . Ignored_LineCont # TODO: Change to ALL IGNORED types, once you have SPACE_TOK.  This means # we don't have to handle them in the VS_1/VS_2/etc. states. <MASK> break # log('Read() Returning %s', t) return t",if t . id != Id . Ignored_LineCont :,if t . id == Id . IGNORED_LineCont :,96.5854945,FALSE,95.26
415,"def _dir_guildfile ( dir , ctx ) : from guild import guildfile try : return guildfile . for_dir ( dir ) except guildfile . NoModels : <MASK> help_suffix = ""  or  ' %s '  for help "" % click_util . cmd_help ( ctx ) else : help_suffix = "" "" cli . error ( "" %s  does not contain a Guild file (guild.yml) \n "" "" Try specifying a project path or package name %s . "" % ( cwd_desc ( dir ) , help_suffix ) ) except guildfile . GuildfileError as e : cli . error ( str ( e ) )",if ctx :,if ctx :,100,TRUE,100
416,"def check_response ( self , response ) : """"""Specialized version of check_response()."""""" for line in response : # Skip blank lines: if not line . strip ( ) : continue <MASK> return elif line . startswith ( b "" Benutzer/Passwort Fehler "" ) : raise BadLogin ( line ) else : raise FailedPost ( "" Server returned  ' %s ' "" % six . ensure_text ( line ) )","if line . startswith ( b""OK"" ) :","if line . startswith ( b""User"" ) :",98.33537535,FALSE,97.39
417,"def ParseResponses ( self , knowledge_base : rdf_client . KnowledgeBase , responses : Iterable [ rdfvalue . RDFValue ] , ) - > Iterator [ rdf_client . User ] : for response in responses : if not isinstance ( response , rdf_client_fs . StatEntry ) : raise TypeError ( f "" Unexpected response type: ` { type ( response ) } ` "" ) # TODO: `st_mode` has to be an `int`, not `StatMode`. <MASK> homedir = response . pathspec . path username = os . path . basename ( homedir ) if username not in self . _ignore_users : yield rdf_client . User ( username = username , homedir = homedir )",if stat . S_ISDIR ( int ( response . st_mode ) ) :,if response . st_mode == rdf_client_fs . ST_MODE_INT :,70.71546746,FALSE,90.12
418,"def __call__ ( self , x , uttid = None ) : if self . utt2spk is not None : spk = self . utt2spk [ uttid ] else : spk = uttid if not self . reverse : <MASK> x = np . add ( x , self . bias [ spk ] ) if self . norm_vars : x = np . multiply ( x , self . scale [ spk ] ) else : if self . norm_vars : x = np . divide ( x , self . scale [ spk ] ) <MASK> x = np . subtract ( x , self . bias [ spk ] ) return x",if self . norm_means :,if self . bias is not None :,94.07558771,FALSE,91.85
419,"def hasFixtures ( self , ctx_callback = None ) : context = self . context if context is None : return False if self . implementsAnyFixture ( context , ctx_callback = ctx_callback ) : return True # My context doesn't have any, but its ancestors might factory = self . factory if factory : ancestors = factory . context . get ( self , [ ] ) for ancestor in ancestors : <MASK> return True return False","if self . implementsAnyFixture ( ancestor , ctx_callback = ctx_callback ) :","if self . implementsAnyFixture ( ancestor , ctx_callback = ctx_callback ) :",100,TRUE,100
420,def UpdateControlState ( self ) : active = self . demoModules . GetActiveID ( ) # Update the radio/restore buttons for moduleID in self . radioButtons : btn = self . radioButtons [ moduleID ] if moduleID == active : btn . SetValue ( True ) else : btn . SetValue ( False ) if self . demoModules . Exists ( moduleID ) : btn . Enable ( True ) <MASK> self . btnRestore . Enable ( True ) else : btn . Enable ( False ) <MASK> self . btnRestore . Enable ( False ),if moduleID == modModified :,if self . demoModules . Restore ( moduleID ) :,93.860873,FALSE,83.91
421,"def ignore_proxy_host ( self ) : """"""Check if self.host is in the $no_proxy ignore list."""""" if urllib . proxy_bypass ( self . host ) : return True no_proxy = os . environ . get ( "" no_proxy "" ) if no_proxy : entries = [ parse_host_port ( x ) for x in no_proxy . split ( "" , "" ) ] for host , port in entries : <MASK> return True return False",if host . lower ( ) == self . host and port == self . port :,if host in self . ignore_list :,86.73880584,FALSE,85.72
422,"def run ( self , _ ) : view = self . view if not view . settings ( ) . get ( "" terminus_view "" ) : return terminal = Terminal . from_id ( view . id ( ) ) if terminal : terminal . close ( ) panel_name = terminal . panel_name <MASK> window = panel_window ( view ) if window : window . destroy_output_panel ( panel_name ) else : view . close ( )",if panel_name :,if panel_name :,100,TRUE,100
423,"def get_docname_for_node ( self , node : Node ) - > str : while node : <MASK> return self . env . path2doc ( node [ "" source "" ] ) elif isinstance ( node , addnodes . start_of_file ) : return node [ "" docname "" ] else : node = node . parent return None # never reached here. only for type hinting","if isinstance ( node , nodes . document ) :","if isinstance ( node , addnodes . start_of_file ) :",97.09805291,FALSE,90.53
424,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 10 : self . add_version ( d . getPrefixedString ( ) ) continue <MASK> raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 0 :,if tt == 0 :,100,TRUE,100
425,"def _maybe_female ( self , path_elements , female , strict ) : if female : if self . has_gender_differences : elements = path_elements + [ "" female "" ] try : return self . _get_file ( elements , "" .png "" , strict = strict ) except ValueError : <MASK> raise el<MASK> raise ValueError ( "" Pokemon  %s  has no gender differences "" % self . species_id ) return self . _get_file ( path_elements , "" .png "" , strict = strict )",if strict :,if self . species_id not in self . gender_differences :,84.88038634,FALSE,79.43
426,"def OnKeyUp ( self , event ) : if self . _properties . modifiable : if event . GetKeyCode ( ) == wx . WXK_ESCAPE : self . _cancel_editing ( ) elif event . GetKeyCode ( ) == wx . WXK_RETURN : self . _update_value ( ) <MASK> self . SetValue ( "" "" ) if event . GetKeyCode ( ) != wx . WXK_RETURN : # Don't send skip event if enter key is pressed # On some platforms this event is sent too late and causes crash event . Skip ( )",elif event . GetKeyCode ( ) == wx . WXK_DELETE :,elif event . GetKeyCode ( ) == wx . WXK_ESCAPE :,98.78761533,FALSE,97.93
427,"def sync_up_to_new_location ( self , worker_ip ) : if worker_ip != self . worker_ip : logger . debug ( "" Setting new worker IP to  %s "" , worker_ip ) self . set_worker_ip ( worker_ip ) self . reset ( ) <MASK> logger . warning ( "" Sync up to new location skipped. This should not occur. "" ) else : logger . warning ( "" Sync attempted to same IP  %s . "" , worker_ip )",if not self . sync_up ( ) :,if self . sync_up_to_new_location :,64.13703131,FALSE,92.23
428,"def _get_download_link ( self , url , download_type = "" torrent "" ) : links = { "" torrent "" : "" "" , "" magnet "" : "" "" , } try : data = self . session . get ( url ) . text with bs4_parser ( data ) as html : downloads = html . find ( "" div "" , { "" class "" : "" download "" } ) <MASK> for download in downloads . findAll ( "" a "" ) : link = download [ "" href "" ] if link . startswith ( "" magnet "" ) : links [ "" magnet "" ] = link else : links [ "" torrent "" ] = urljoin ( self . urls [ "" base_url "" ] , link ) except Exception : pass return links [ download_type ]",if downloads :,if downloads :,100,TRUE,100
429,"def force_ipv4 ( self , * args ) : """"""only ipv4 localhost in /etc/hosts"""""" logg . debug ( "" checking /etc/hosts for  ' ::1 localhost ' "" ) lines = [ ] for line in open ( self . etc_hosts ( ) ) : if "" ::1 "" in line : newline = re . sub ( "" \\ slocalhost \\ s "" , ""   "" , line ) <MASK> logg . info ( "" /etc/hosts:  ' %s '  =>  ' %s ' "" , line . rstrip ( ) , newline . rstrip ( ) ) line = newline lines . append ( line ) f = open ( self . etc_hosts ( ) , "" w "" ) for line in lines : f . write ( line ) f . close ( )",if line != newline :,"if newline . startswith ( ""ipv4"" ) :",95.91153203,FALSE,94.69
430,"def prepare ( self ) : # Maybe the brok is a old daemon one or was already prepared # if so, the data is already ok if hasattr ( self , "" prepared "" ) and not self . prepared : self . data = SafeUnpickler . loads ( self . data ) <MASK> self . data [ "" instance_id "" ] = self . instance_id self . prepared = True","if hasattr ( self , ""instance_id"" ) :",if self . instance_id :,70.42456979,FALSE,88.57
431,"def _test_compute_q0 ( self ) : # Stub code to search a logq space and figure out logq0 by eyeballing # results. This code does not run with the tests. Remove underscore to run. sigma = 15 order = 250 logqs = np . arange ( - 290 , - 270 , 1 ) count = 0 for logq in logqs : count + = 1 sys . stdout . write ( "" \t %0.5g :  %0.10g "" % ( logq , pate . rdp_gaussian ( logq , sigma , order ) ) ) sys . stdout . flush ( ) <MASK> print ( "" "" )",if count % 5 == 0 :,if count % 100 == 0 :,73.88191737,FALSE,98
432,"def valid_fieldnames ( fieldnames ) : """"""check if fieldnames are valid"""""" for fieldname in fieldnames : <MASK> return True elif fieldname in fieldname_map and fieldname_map [ fieldname ] == "" source "" : return True return False","if fieldname in canonical_field_names and fieldname == ""source"" :","if fieldname == ""type"" :",85.74919586,FALSE,81.69
433,"def ns_provide ( self , id_ ) : global controllers , layouts if id_ == "" _leo_viewrendered "" : c = self . c vr = controllers . get ( c . hash ( ) ) or ViewRenderedController ( c ) h = c . hash ( ) controllers [ h ] = vr <MASK> layouts [ h ] = c . db . get ( "" viewrendered_default_layouts "" , ( None , None ) ) # return ViewRenderedController(self.c) return vr",if not layouts . get ( h ) :,if h not in layouts :,74.96340566,FALSE,92.53
434,"def remove ( self , path , config = None , error_on_path = False , defaults = None ) : if not path : if error_on_path : raise NoSuchSettingsPath ( ) return if config is not None or defaults is not None : if config is None : config = self . _config <MASK> defaults = dict ( self . _map . parents ) chain = HierarchicalChainMap ( config , defaults ) else : chain = self . _map try : chain . del_by_path ( path ) self . _mark_dirty ( ) except KeyError : if error_on_path : raise NoSuchSettingsPath ( ) pass",if defaults is None :,if defaults is None :,100,TRUE,100
435,"def _mongo_query_and ( self , queries ) : if len ( queries ) == 1 : return queries [ 0 ] query = { } for q in queries : for k , v in q . items ( ) : if k not in query : query [ k ] = { } <MASK> # TODO check exists of k in query, may be it should be update query [ k ] = v else : query [ k ] . update ( v ) return query","if isinstance ( v , list ) :","elif isinstance ( v , dict ) :",95.20824216,FALSE,94.97
436,"def write ( self , data ) : self . size - = len ( data ) passon = None if self . size > 0 : self . data . append ( data ) else : <MASK> data , passon = data [ : self . size ] , data [ self . size : ] else : passon = b "" "" if data : self . data . append ( data ) return passon",if self . size :,if len ( data ) > self . size :,94.30062506,FALSE,92.28
437,"def updateVar ( name , data , mode = None ) : if mode : if mode == "" append "" : core . config . globalVariables [ name ] . append ( data ) <MASK> core . config . globalVariables [ name ] . add ( data ) else : core . config . globalVariables [ name ] = data","elif mode == ""add"" :","elif mode == ""add"" :",100,TRUE,100
438,"def vi_pos_back_short ( line , index = 0 , count = 1 ) : line = vi_list ( line ) try : for i in range ( count ) : index - = 1 while vi_is_space ( line [ index ] ) : index - = 1 in_word = vi_is_word ( line [ index ] ) <MASK> while vi_is_word ( line [ index ] ) : index - = 1 else : while not vi_is_word_or_space ( line [ index ] ) : index - = 1 return index + 1 except IndexError : return 0",if in_word :,if in_word :,100,TRUE,100
439,"def _truncate_to_length ( generator , len_map = None ) : for example in generator : example = list ( example ) <MASK> for key , max_len in len_map . items ( ) : example_len = example [ key ] . shape if example_len > max_len : example [ key ] = np . resize ( example [ key ] , max_len ) yield tuple ( example )",if len_map is not None :,if len_map :,86.08090675,FALSE,95.06
440,"def decorate ( f ) : # call-signature of f is exposed via __wrapped__. # we want it to mimic Obj.__init__ f . __wrapped__ = Obj . __init__ f . _uses_signature = Obj # Supplement the docstring of f with information from Obj if Obj . __doc__ : doclines = Obj . __doc__ . splitlines ( ) <MASK> doc = f . __doc__ + "" \n "" . join ( doclines [ 1 : ] ) else : doc = "" \n "" . join ( doclines ) try : f . __doc__ = doc except AttributeError : # __doc__ is not modifiable for classes in Python < 3.3 pass return f",if f . __doc__ :,"if doclines [ 0 ] == ""doc"" :",72.46827816,FALSE,93.76
441,"def IncrementErrorCount ( self , category ) : """"""Bumps the module's error statistic."""""" self . error_count + = 1 if self . counting in ( "" toplevel "" , "" detailed "" ) : if self . counting != "" detailed "" : category = category . split ( "" / "" ) [ 0 ] <MASK> self . errors_by_category [ category ] = 0 self . errors_by_category [ category ] + = 1",if category not in self . errors_by_category :,if category not in self . errors_by_category :,100,TRUE,100
442,"def _delete_fields ( self , data ) : data = self . _del ( data , [ "" speaker_ids "" , "" track_id "" , "" microlocation_id "" , "" session_type_id "" ] ) # convert datetime fields for _ in [ "" start_time_tz "" , "" end_time_tz "" ] : <MASK> data [ _ ] = SESSION_POST [ _ [ 0 : - 3 ] ] . from_str ( data [ _ ] ) data [ _ [ 0 : - 3 ] ] = data . pop ( _ ) return data",if _ in data :,if _ in data :,100,TRUE,100
443,"def get_strings_of_set ( word , char_set , threshold = 20 ) : count = 0 letters = "" "" strings = [ ] for char in word : if char in char_set : letters + = char count + = 1 else : <MASK> strings . append ( letters ) letters = "" "" count = 0 <MASK> strings . append ( letters ) return strings",if count > threshold :,if count > threshold :,100,TRUE,100
444,"def _ArgumentListHasDictionaryEntry ( self , token ) : """"""Check if the function argument list has a dictionary as an arg."""""" if _IsArgumentToFunction ( token ) : while token : if token . value == "" { "" : length = token . matching_bracket . total_length - token . total_length return length + self . stack [ - 2 ] . indent > self . column_limit <MASK> break if token . OpensScope ( ) : token = token . matching_bracket token = token . next_token return False",if token . ClosesScope ( ) :,"if token . value == ""}"" :",96.05454511,FALSE,93.49
445,"def check_apns_certificate ( ss ) : mode = "" start "" for s in ss . split ( "" \n "" ) : if mode == "" start "" : if "" BEGIN RSA PRIVATE KEY "" in s or "" BEGIN PRIVATE KEY "" in s : mode = "" key "" elif mode == "" key "" : if "" END RSA PRIVATE KEY "" in s or "" END PRIVATE KEY "" in s : mode = "" end "" break <MASK> raise ImproperlyConfigured ( "" Encrypted APNS private keys are not supported "" ) if mode != "" end "" : raise ImproperlyConfigured ( "" The APNS certificate doesn ' t contain a private key "" )","elif s . startswith ( ""Proc-Type"" ) and ""ENCRYPTED"" in s :","if mode != ""start"" :",81.94084909,FALSE,88.66
446,"def main ( self ) : self . model . clear ( ) self . callman . unregister_all ( ) active_handle = self . get_active ( "" Person "" ) if active_handle : active = self . dbstate . db . get_person_from_handle ( active_handle ) <MASK> self . callman . register_obj ( active ) self . display_citations ( active ) else : self . set_has_data ( False ) else : self . set_has_data ( False )",if active :,if active :,100,TRUE,100
447,"def _validate ( self ) - > None : # Paren validation and such super ( Tuple , self ) . _validate ( ) if len ( self . elements ) == 0 : <MASK> # assumes len(lpar) == len(rpar), via superclass raise CSTValidationError ( "" A zero-length tuple must be wrapped in parentheses. "" )",if len ( self . lpar ) == 0 :,if len ( self . elements ) == 1 :,71.26074916,FALSE,93.37
448,"def _session_from_arg ( self , session_obj , lock_type = None ) : if not isinstance ( session_obj , self . ISession ) : vm = self . _machine_from_arg ( session_obj ) lock_type = lock_type or self . LockType . null <MASK> return vm . create_session ( lock_type ) return None return session_obj",if vm :,if vm :,100,TRUE,100
449,"def _decorator ( cls ) : for name , meth in inspect . getmembers ( cls , inspect . isroutine ) : if name not in cls . __dict__ : continue <MASK> if not private and name . startswith ( "" _ "" ) : continue if name in butnot : continue setattr ( cls , name , decorator ( meth ) ) return cls","if name != ""__init__"" :",if meth is not inspect . __call__ :,90.79381279,FALSE,87.3
450,"def pdb ( message = "" "" ) : """"""Fall into pdb."""""" import pdb # Required: we have just defined pdb as a function! if app and not app . useIpython : # from leo.core.leoQt import QtCore # This is more portable. try : import PyQt5 . QtCore as QtCore except ImportError : try : import PyQt4 . QtCore as QtCore except ImportError : QtCore = None <MASK> # pylint: disable=no-member QtCore . pyqtRemoveInputHook ( ) if message : print ( message ) pdb . set_trace ( )",if QtCore :,if QtCore :,100,TRUE,100
451,"def get_s3_bucket_locations ( buckets , self_log = False ) : """"""return (bucket_name, prefix) for all s3 logging targets"""""" for b in buckets : if b . get ( "" Logging "" ) : <MASK> if b [ "" Name "" ] != b [ "" Logging "" ] [ "" TargetBucket "" ] : continue yield ( b [ "" Logging "" ] [ "" TargetBucket "" ] , b [ "" Logging "" ] [ "" TargetPrefix "" ] ) if not self_log and b [ "" Name "" ] . startswith ( "" cf-templates- "" ) : yield ( b [ "" Name "" ] , "" "" )",if self_log :,"if ""TargetPrefix"" in b [ ""Logging"" ] :",96.52452267,FALSE,91.92
452,"def prepare_fields ( self ) : # See clean() for k , v in self . fields . items ( ) : v . _required = v . required v . required = False v . widget . is_required = False <MASK> v . _required = v . one_required v . one_required = False v . widget . enabled_locales = self . locales","if isinstance ( v , I18nFormField ) :",if v . one_required :,69.79654174,FALSE,91.18
453,"def __pack__ ( self ) : new_values = [ ] for i in xrange ( len ( self . __unpacked_data_elms__ ) ) : for key in self . __keys__ [ i ] : new_val = getattr ( self , key ) old_val = self . __unpacked_data_elms__ [ i ] # In the case of Unions, when the first changed value # is picked the loop is exited <MASK> break new_values . append ( new_val ) return struct . pack ( self . __format__ , * new_values )",if new_val != old_val :,if new_val != old_val :,100,TRUE,100
454,"def run ( self ) : pwd_found = [ ] if constant . user_dpapi and constant . user_dpapi . unlocked : main_vault_directory = os . path . join ( constant . profile [ "" APPDATA "" ] , u "" .. "" , u "" Local "" , u "" Microsoft "" , u "" Vault "" ) <MASK> for vault_directory in os . listdir ( main_vault_directory ) : cred = constant . user_dpapi . decrypt_vault ( os . path . join ( main_vault_directory , vault_directory ) ) if cred : pwd_found . append ( cred ) return pwd_found",if os . path . exists ( main_vault_directory ) :,if os . path . exists ( main_vault_directory ) :,100,TRUE,100
455,"def on_revision_plugin_revision_pre_save ( * * kwargs ) : instance = kwargs [ "" instance "" ] if kwargs . get ( "" created "" , False ) : update_previous_revision = ( not instance . previous_revision and instance . plugin and instance . plugin . current_revision and instance . plugin . current_revision != instance ) <MASK> instance . previous_revision = instance . plugin . current_revision if not instance . revision_number : try : previous_revision = instance . plugin . revision_set . latest ( ) instance . revision_number = previous_revision . revision_number + 1 except RevisionPluginRevision . DoesNotExist : instance . revision_number = 1",if update_previous_revision :,if update_previous_revision :,100,TRUE,100
456,"def __setattr__ ( self , name , value ) : super ( ) . __setattr__ ( name , value ) field = self . _fields . get ( name ) if field : self . check_field_type ( field , value ) <MASK> raise TypeError ( f "" cannot set immutable  { name }  on  { self !r} "" )",if name in self . __ast_frozen_fields__ :,if self . immutable :,89.64213661,FALSE,83.59
457,"def _check_for_req_data ( data ) : required_args = [ "" columns "" ] for arg in required_args : <MASK> return True , make_json_response ( status = 400 , success = 0 , errormsg = gettext ( "" Could not find required parameter ( {} ). "" ) . format ( arg ) , ) return False , "" ""","if arg not in data or ( isinstance ( data [ arg ] , list ) and len ( data [ arg ] ) < 1 ) :",if arg in data :,75.11091435,FALSE,72.34
458,"def train_dict ( self , triples ) : """"""Train a dict lemmatizer given training (word, pos, lemma) triples."""""" # accumulate counter ctr = Counter ( ) ctr . update ( [ ( p [ 0 ] , p [ 1 ] , p [ 2 ] ) for p in triples ] ) # find the most frequent mappings for p , _ in ctr . most_common ( ) : w , pos , l = p <MASK> self . composite_dict [ ( w , pos ) ] = l if w not in self . word_dict : self . word_dict [ w ] = l return","if ( w , pos ) not in self . composite_dict :","if ( w , pos ) not in self . composite_dict :",75,TRUE,100
459,"def render ( type_ , obj , context ) : if type_ == "" foreign_key "" : return None if type_ == "" column "" : if obj . name == "" y "" : return None <MASK> return False else : return "" col( %s ) "" % obj . name if type_ == "" type "" and isinstance ( obj , MySpecialType ) : context . imports . add ( "" from mypackage import MySpecialType "" ) return "" MySpecialType() "" return "" render: %s "" % type_","elif obj . name == ""q"" :","elif obj . name == ""no"" :",98.6945175,FALSE,97.8
460,"def test_knows_when_stepping_back_possible ( self ) : iterator = bidirectional_iterator . BidirectionalIterator ( [ 0 , 1 , 2 , 3 ] ) commands = [ 0 , 1 , 0 , 0 , 1 , 1 , 0 , 0 , 0 , 0 ] command_count = 0 results = [ ] for _ in iterator : <MASK> iterator . step_back_on_next_iteration ( ) results . append ( iterator . can_step_back ( ) ) command_count + = 1 assert results == [ False , True , False , True , True , True , False , True , True , True ]",if commands [ command_count ] :,if command_count in commands :,94.33107193,FALSE,95.73
461,"def flask_debug_true ( context ) : if context . is_module_imported_like ( "" flask "" ) : if context . call_function_name_qual . endswith ( "" .run "" ) : <MASK> return bandit . Issue ( severity = bandit . HIGH , confidence = bandit . MEDIUM , text = "" A Flask app appears to be run with debug=True,  "" "" which exposes the Werkzeug debugger and allows  "" "" the execution of arbitrary code. "" , lineno = context . get_lineno_for_call_arg ( "" debug "" ) , )","if context . check_call_arg_value ( ""debug"" , ""True"" ) :","if context . get_lineno_for_call_arg ( ""debug"" ) :",68.94938222,FALSE,92.14
462,"def __exit__ ( self , exc_type , exc_val , exc_tb ) : if self . _should_meta_profile : end_time = timezone . now ( ) exception_raised = exc_type is not None if exception_raised : Logger . error ( "" Exception when performing meta profiling, dumping trace below "" ) traceback . print_exception ( exc_type , exc_val , exc_tb ) request = getattr ( DataCollector ( ) . local , "" request "" , None ) <MASK> curr = request . meta_time or 0 request . meta_time = curr + _time_taken ( self . start_time , end_time )",if request :,if request :,100,TRUE,100
463,"def get_job_offer ( ja_list ) : ja_joff_map = { } offers = frappe . get_all ( "" Job Offer "" , filters = [ [ "" job_applicant "" , "" IN "" , ja_list ] ] , fields = [ "" name "" , "" job_applicant "" , "" status "" , "" offer_date "" , "" designation "" ] , ) for offer in offers : <MASK> ja_joff_map [ offer . job_applicant ] = [ offer ] else : ja_joff_map [ offer . job_applicant ] . append ( offer ) return ja_joff_map",if offer . job_applicant not in ja_joff_map . keys ( ) :,if offer . job_applicant not in ja_joff_map :,92.2534923,FALSE,96.18
464,"def _get_deepest ( self , t ) : if isinstance ( t , list ) : <MASK> return t [ 0 ] else : for part in t : res = self . _get_deepest ( part ) if res : return res return None return None",if len ( t ) == 1 :,if len ( t ) == 1 :,100,TRUE,100
465,"def test_main ( self ) : root = os . path . dirname ( mutagen . __path__ [ 0 ] ) skip = [ os . path . join ( root , "" docs "" ) , os . path . join ( root , "" venv "" ) ] for dirpath , dirnames , filenames in os . walk ( root ) : <MASK> continue for filename in filenames : if filename . endswith ( "" .py "" ) : path = os . path . join ( dirpath , filename ) self . _check_encoding ( path )",if any ( ( dirpath . startswith ( s + os . sep ) or s == dirpath ) for s in skip ) :,if dirnames . startswith ( skip ) :,85.86639716,FALSE,82.6
466,"def xview ( self , mode = None , value = None , units = None ) : if type ( value ) == str : value = float ( value ) if mode is None : return self . hsb . get ( ) elif mode == "" moveto "" : frameWidth = self . innerframe . winfo_reqwidth ( ) self . _startX = value * float ( frameWidth ) else : # mode == 'scroll' clipperWidth = self . _clipper . winfo_width ( ) <MASK> jump = int ( clipperWidth * self . _jfraction ) else : jump = clipperWidth self . _startX = self . _startX + value * jump self . reposition ( )","if units == ""units"" :",if self . _jfraction is not None :,97.58652884,FALSE,93.76
467,"def test_training_script_with_max_history_set ( tmpdir ) : train_dialogue_model ( DEFAULT_DOMAIN_PATH , DEFAULT_STORIES_FILE , tmpdir . strpath , interpreter = RegexInterpreter ( ) , policy_config = "" data/test_config/max_hist_config.yml "" , kwargs = { } , ) agent = Agent . load ( tmpdir . strpath ) for policy in agent . policy_ensemble . policies : <MASK> if type ( policy ) == FormPolicy : assert policy . featurizer . max_history == 2 else : assert policy . featurizer . max_history == 5","if hasattr ( policy . featurizer , ""max_history"" ) :",if policy . featurizer :,91.9565575,FALSE,91.17
468,"def generate_auto_complete ( self , base , iterable_var ) : sugg = [ ] for entry in iterable_var : compare_entry = entry compare_base = base <MASK> compare_entry = compare_entry . lower ( ) compare_base = compare_base . lower ( ) if self . compare_entries ( compare_entry , compare_base ) : if entry not in sugg : sugg . append ( entry ) return sugg",if self . settings . get ( IGNORE_CASE_SETTING ) :,if self . compare_lower :,92.24269416,FALSE,88.76
469,"def marker_expr ( remaining ) : if remaining and remaining [ 0 ] == "" ( "" : result , remaining = marker ( remaining [ 1 : ] . lstrip ( ) ) <MASK> raise SyntaxError ( "" unterminated parenthesis:  %s "" % remaining ) remaining = remaining [ 1 : ] . lstrip ( ) else : lhs , remaining = marker_var ( remaining ) while remaining : m = MARKER_OP . match ( remaining ) if not m : break op = m . groups ( ) [ 0 ] remaining = remaining [ m . end ( ) : ] rhs , remaining = marker_var ( remaining ) lhs = { "" op "" : op , "" lhs "" : lhs , "" rhs "" : rhs } result = lhs return result , remaining","if remaining [ 0 ] != "")"" :","if result == "")"" :",89.40078926,FALSE,95.92
470,"def __repr__ ( self ) : """"""Dump the class data in the format of a .netrc file."""""" rep = "" "" for host in self . hosts . keys ( ) : attrs = self . hosts [ host ] rep = rep + "" machine  "" + host + "" \n \t login  "" + repr ( attrs [ 0 ] ) + "" \n "" <MASK> rep = rep + "" account  "" + repr ( attrs [ 1 ] ) rep = rep + "" \t password  "" + repr ( attrs [ 2 ] ) + "" \n "" for macro in self . macros . keys ( ) : rep = rep + "" macdef  "" + macro + "" \n "" for line in self . macros [ macro ] : rep = rep + line rep = rep + "" \n "" return rep",if attrs [ 1 ] :,if attrs [ 1 ] :,100,TRUE,100
471,"def _parse_policies ( self , policies_yaml ) : for item in policies_yaml : id_ = required_key ( item , "" id "" ) controls_ids = required_key ( item , "" controls "" ) <MASK> if controls_ids != "" all "" : msg = "" Policy  {id_}  contains invalid controls list  {controls} . "" . format ( id_ = id_ , controls = str ( controls_ids ) ) raise ValueError ( msg ) self . policies [ id_ ] = controls_ids","if not isinstance ( controls_ids , list ) :",if id_ not in self . policies :,89.39158112,FALSE,91.47
472,"def __set__ ( self , obj , value ) : # noqa if ( value is not None and self . field . _currency_field . null and not isinstance ( value , MONEY_CLASSES + ( Decimal , ) ) ) : # For nullable fields we need either both NULL amount and currency or both NOT NULL raise ValueError ( "" Missing currency value "" ) if isinstance ( value , BaseExpression ) : <MASK> value = self . prepare_value ( obj , value . value ) elif not isinstance ( value , Func ) : validate_money_expression ( obj , value ) prepare_expression ( value ) else : value = self . prepare_value ( obj , value ) obj . __dict__ [ self . field . name ] = value","if isinstance ( value , Value ) :",if value . value is not None :,72.38775741,FALSE,95.41
473,"def Children ( self ) : """"""Returns a list of all of this object's owned (strong) children."""""" children = [ ] for property , attributes in self . _schema . iteritems ( ) : ( is_list , property_type , is_strong ) = attributes [ 0 : 3 ] <MASK> if not is_list : children . append ( self . _properties [ property ] ) else : children . extend ( self . _properties [ property ] ) return children",if is_strong and property in self . _properties :,if is_strong :,88.61612696,FALSE,92.03
474,"def next_item ( self , direction ) : """"""Selects next menu item, based on self._direction"""""" start , i = - 1 , 0 try : start = self . items . index ( self . _selected ) i = start + direction except : pass while True : if i == start : # Cannot find valid menu item self . select ( start ) break if i > = len ( self . items ) : i = 0 continue if i < 0 : i = len ( self . items ) - 1 continue if self . select ( i ) : break i + = direction <MASK> start = 0",if start < 0 :,if i == 0 :,98.52573991,FALSE,96.57
475,"def setup_displace ( self ) : self . displace_mod = None self . displace_strength = 0.020 for mod in self . obj . modifiers : <MASK> self . displace_mod = mod self . displace_strength = mod . strength if not self . displace_mod : bpy . ops . object . modifier_add ( type = "" DISPLACE "" ) self . displace_mod = self . obj . modifiers [ - 1 ] self . displace_mod . show_expanded = False self . displace_mod . strength = self . displace_strength self . displace_mod . show_render = False self . displace_mod . show_viewport = False","if mod . type == ""DISPLACE"" :","if isinstance ( mod , bpy . ops . object . modifier_add ) :",93.83101934,FALSE,89.74
476,"def set_json_body ( cls , request_builder ) : old_body = request_builder . info . pop ( "" data "" , { } ) if isinstance ( old_body , abc . Mapping ) : body = request_builder . info . setdefault ( "" json "" , { } ) for path in old_body : <MASK> cls . _sequence_path_resolver ( path , old_body [ path ] , body ) else : body [ path ] = old_body [ path ] else : request_builder . info . setdefault ( "" json "" , old_body )","if isinstance ( path , tuple ) :","if isinstance ( old_body [ path ] , abc . Mapping ) :",93.26365241,FALSE,91.74
477,"def build ( opt ) : dpath = os . path . join ( opt [ "" datapath "" ] , "" DBLL "" ) version = None if not build_data . built ( dpath , version_string = version ) : print ( "" [building data:  "" + dpath + "" ] "" ) <MASK> # An older version exists, so remove these outdated files. build_data . remove_dir ( dpath ) build_data . make_dir ( dpath ) # Download the data. for downloadable_file in RESOURCES : downloadable_file . download_file ( dpath ) # Mark the data as built. build_data . mark_done ( dpath , version_string = version )",if build_data . built ( dpath ) :,if build_data . built ( dpath ) :,100,TRUE,100
478,"def test_prefix_lm ( self ) : num_tries = 100 original = "" This is a long test with lots of words to see if it works ok. "" dataset = tf . data . Dataset . from_tensor_slices ( { "" text "" : [ original ] * num_tries } ) dataset = prep . prefix_lm ( dataset ) for data in test_utils . dataset_as_text ( dataset ) : inputs = data [ "" inputs "" ] . replace ( "" prefix:  "" , "" "" ) targets = data [ "" targets "" ] reconstructed = "" "" . join ( inputs ) <MASK> reconstructed + = ""   "" reconstructed + = "" "" . join ( targets ) self . assertEqual ( reconstructed , original )",if inputs :,if len ( targets ) > 1 :,96.48548467,FALSE,95.14
479,"def leading_whitespace ( self , inputstring ) : """"""Get leading whitespace."""""" leading_ws = [ ] for i , c in enumerate ( inputstring ) : if c in legal_indent_chars : leading_ws . append ( c ) else : break <MASK> self . indchar = c elif c != self . indchar : self . strict_err_or_warn ( "" found mixing of tabs and spaces "" , inputstring , i ) return "" "" . join ( leading_ws )",if self . indchar is None :,if i == 0 :,90.45384034,FALSE,93.94
480,"def __init__ ( self , text ) : self . mappings = { } self . attributes = collections . defaultdict ( set ) for stanza in _ParseTextProperties ( text ) : processor_id , single_values , multiple_values = self . _ParseStanza ( stanza ) if processor_id is None : # can be 0 continue <MASK> logging . warn ( "" Processor id  %s  seen twice in  %s "" , processor_id , text ) continue self . mappings [ processor_id ] = single_values for key , value in multiple_values . items ( ) : self . attributes [ key ] . add ( value )",if processor_id in self . mappings :,if processor_id in self . mappings :,100,TRUE,100
481,"def __iter__ ( self ) : for chunk in self . source : <MASK> self . wait_counter = 0 yield chunk elif self . wait_counter < self . wait_cntr_max : self . wait_counter + = 1 else : logger . warning ( "" Data poller has been receiving no data for  {}  seconds. \n "" "" Closing data poller "" . format ( self . wait_cntr_max * self . poll_period ) ) break time . sleep ( self . poll_period )",if chunk is not None :,if self . wait_counter == self . wait_cntr_max :,82.52854874,FALSE,87.01
482,"def download ( self , prefetch = False ) : while self . running : try : <MASK> ( path , start , end ) = self . prefetch_queue . get ( True , 1 ) # 1 second time-out else : ( path , start , end ) = self . download_queue . get ( True , 1 ) # 1 second time-out self . download_data ( path , start , end ) <MASK> self . prefetch_queue . task_done ( ) else : self . download_queue . task_done ( ) except Queue . Empty : pass",if prefetch :,if prefetch :,100,TRUE,100
483,"def process_messages ( self , found_files , messages ) : for message in messages : <MASK> message . to_absolute_path ( self . config . workdir ) else : message . to_relative_path ( self . config . workdir ) if self . config . blending : messages = blender . blend ( messages ) filepaths = found_files . iter_module_paths ( abspath = False ) return postfilter . filter_messages ( filepaths , self . config . workdir , messages )",if self . config . absolute_paths :,if self . config . absolute :,98.49046422,FALSE,96.64
484,"def set_indentation_params ( self , ispythonsource , guess = 1 ) : if guess and ispythonsource : i = self . guess_indent ( ) <MASK> self . indentwidth = i if self . indentwidth != self . tabwidth : self . usetabs = 0 self . editwin . set_tabwidth ( self . tabwidth )",if 2 <= i <= 8 :,if i is not None :,75.54169826,FALSE,88.06
485,"def to_tree ( self , tagname = None , value = None , namespace = None ) : namespace = getattr ( self , "" namespace "" , namespace ) if value is not None : <MASK> tagname = "" { %s } %s "" % ( namespace , tagname ) el = Element ( tagname ) el . text = safe_string ( value ) return el",if namespace is not None :,if namespace is not None :,100,TRUE,100
486,"def execute ( self , argv : List ) - > bool : if not argv : print ( "" ERROR: You must give at least one module to download. "" ) return False for _arg in argv : result = module_server . search_module ( _arg ) CacheUpdater ( "" hub_download "" , _arg ) . start ( ) <MASK> url = result [ 0 ] [ "" url "" ] with log . ProgressBar ( "" Download  {} "" . format ( url ) ) as bar : for file , ds , ts in utils . download_with_progress ( url ) : bar . update ( float ( ds ) / ts ) else : print ( "" ERROR: Could not find a HubModule named  {} "" . format ( _arg ) ) return True",if result :,if result :,100,TRUE,100
487,"def visit_type_type ( self , t : TypeType ) - > ProperType : if isinstance ( self . s , TypeType ) : typ = self . meet ( t . item , self . s . item ) <MASK> typ = TypeType . make_normalized ( typ , line = t . line ) return typ elif isinstance ( self . s , Instance ) and self . s . type . fullname == "" builtins.type "" : return t elif isinstance ( self . s , CallableType ) : return self . meet ( t , self . s ) else : return self . default ( self . s )","if not isinstance ( typ , NoneType ) :",if self . s . normalize :,82.39298015,FALSE,93.33
488,"def run ( self , paths = [ ] ) : items = [ ] for item in SideBarSelection ( paths ) . getSelectedItems ( ) : items . append ( item . name ( ) ) if len ( items ) > 0 : sublime . set_clipboard ( "" \n "" . join ( items ) ) <MASK> sublime . status_message ( "" Items copied "" ) else : sublime . status_message ( "" Item copied "" )",if len ( items ) > 1 :,if len ( items ) > 1 :,100,TRUE,100
489,"def get_icon ( self ) : if self . icon is not None : # Load it from an absolute filename <MASK> try : return GdkPixbuf . Pixbuf . new_from_file_at_size ( self . icon , 24 , 24 ) except GObject . GError as ge : pass # Load it from the current icon theme ( icon_name , extension ) = os . path . splitext ( os . path . basename ( self . icon ) ) theme = Gtk . IconTheme ( ) if theme . has_icon ( icon_name ) : return theme . load_icon ( icon_name , 24 , 0 )",if os . path . exists ( self . icon ) :,if os . path . isabs ( self . icon ) :,98.90948588,FALSE,98.12
490,"def setup_logger ( ) : """"""Set up logger and add stdout handler"""""" logging . setLoggerClass ( IPDLogger ) logger = logging . getLogger ( "" icloudpd "" ) has_stdout_handler = False for handler in logger . handlers : <MASK> has_stdout_handler = True if not has_stdout_handler : formatter = logging . Formatter ( fmt = "" %(asctime)s   %(levelname)-8s   %(message)s "" , datefmt = "" % Y- % m- %d   % H: % M: % S "" ) stdout_handler = logging . StreamHandler ( stream = sys . stdout ) stdout_handler . setFormatter ( formatter ) stdout_handler . name = "" stdoutLogger "" logger . addHandler ( stdout_handler ) return logger","if handler . name == ""stdoutLogger"" :","if handler . name == ""stdout"" :",98.90494923,FALSE,98.43
491,"def process_extra_fields ( self ) : if self . instance . pk is not None : if self . cleaned_data . get ( "" initialize "" , None ) : self . instance . initialize ( ) <MASK> self . instance . update_from_templates ( )","if self . cleaned_data . get ( ""update"" , None ) or not self . instance . stores . count ( ) :","if self . cleaned_data . get ( ""update_from_templates"" , None )",79.21744458,FALSE,80.75
492,"def testFunctions ( self ) : from zim . formats . wiki import match_url , is_url for input , input_is_url , tail in self . examples : if input_is_url : <MASK> self . assertEqual ( match_url ( input ) , input [ : - len ( tail ) ] ) self . assertFalse ( is_url ( input ) ) else : self . assertEqual ( match_url ( input ) , input ) self . assertTrue ( is_url ( input ) ) else : self . assertEqual ( match_url ( input ) , None ) self . assertFalse ( is_url ( input ) )",if tail :,if tail :,100,TRUE,100
493,"def _SetUser ( self , users ) : for user in users . items ( ) : username = user [ 0 ] settings = user [ 1 ] room = settings [ "" room "" ] [ "" name "" ] if "" room "" in settings else None file_ = settings [ "" file "" ] if "" file "" in settings else None <MASK> if "" joined "" in settings [ "" event "" ] : self . _client . userlist . addUser ( username , room , file_ ) elif "" left "" in settings [ "" event "" ] : self . _client . removeUser ( username ) else : self . _client . userlist . modUser ( username , room , file_ )","if ""event"" in settings :","if ""event"" in settings :",100,TRUE,100
494,"def restoreTerminals ( self , state ) : for name in list ( self . terminals . keys ( ) ) : <MASK> self . removeTerminal ( name ) for name , opts in state . items ( ) : if name in self . terminals : term = self [ name ] term . setOpts ( * * opts ) continue try : opts = strDict ( opts ) self . addTerminal ( name , * * opts ) except : printExc ( "" Error restoring terminal  %s  ( %s ): "" % ( str ( name ) , str ( opts ) ) )",if name not in state :,if name in state :,98.71777571,FALSE,97.77
495,"def htmlify ( path , text ) : fname = os . path . basename ( path ) if any ( ( fnmatch . fnmatchcase ( fname , p ) for p in _patterns ) ) : # Get file_id, skip if not in database sql = "" SELECT files.id FROM files WHERE path = ? LIMIT 1 "" row = _conn . execute ( sql , ( path , ) ) . fetchone ( ) <MASK> return ClangHtmlifier ( _tree , _conn , path , text , row [ 0 ] ) return None",if row :,if row :,100,TRUE,100
496,"def autoformat_filter_conv2d ( fsize , in_depth , out_depth ) : if isinstance ( fsize , int ) : return [ fsize , fsize , in_depth , out_depth ] elif isinstance ( fsize , ( tuple , list , tf . TensorShape ) ) : <MASK> return [ fsize [ 0 ] , fsize [ 1 ] , in_depth , out_depth ] else : raise Exception ( "" filter length error:  "" + str ( len ( fsize ) ) + "" , only a length of 2 is supported. "" ) else : raise Exception ( "" filter format error:  "" + str ( type ( fsize ) ) )",if len ( fsize ) == 2 :,if len ( fsize ) == 2 :,100,TRUE,100
497,"def _rle_encode ( string ) : new = b "" "" count = 0 for cur in string : <MASK> count + = 1 else : if count : new + = b "" \0 "" + bytes ( [ count ] ) count = 0 new + = bytes ( [ cur ] ) return new",if not cur :,"if cur == b""x"" :",86.84208853,FALSE,87.71
498,"def is_clean ( self ) : acceptable_statuses = { "" external "" , "" unversioned "" } root = self . _capture_output ( "" status "" , "" --quiet "" ) for elem in root . findall ( "" ./target/entry "" ) : status = elem . find ( "" ./wc-status "" ) <MASK> continue log . debug ( "" Path  %s  is  %s "" , elem . get ( "" path "" ) , status . get ( "" item "" ) ) return False return True","if status . get ( ""item"" , None ) in acceptable_statuses :","if status . get ( ""status"" ) not in acceptable_statuses :",94.12730977,FALSE,94.83
499,"def process ( self , body , message ) : try : <MASK> raise TypeError ( ' Received an unexpected type  "" %s ""  for payload. ' % type ( body ) ) response = self . _handler . pre_ack_process ( body ) self . _dispatcher . dispatch ( self . _process_message , response ) except : LOG . exception ( "" %s  failed to process message:  %s "" , self . __class__ . __name__ , body ) finally : # At this point we will always ack a message. message . ack ( )","if not isinstance ( body , self . _handler . message_type ) :","if not isinstance ( body , Message ) :",92.52736326,FALSE,92.68
500,"def page_file ( self , page ) : try : page = self . notebook . get_page ( page ) <MASK> return page . source else : return None except PageNotFoundError : return None","if hasattr ( page , ""source"" ) and isinstance ( page . source , File ) :",if page . source :,44.79950697,FALSE,66.1
501,"def _optimize ( self , solutions ) : best_a = None best_silhouette = None best_k = None for a , silhouette , k in solutions ( ) : <MASK> pass elif silhouette < = best_silhouette : break best_silhouette = silhouette best_a = a best_k = k return best_a , best_silhouette , best_k",if best_silhouette is None :,if a >= best_a :,88.73899735,FALSE,91.33
502,"def _cancel_tasks_for_partitions ( self , to_cancel_partitions ) : # type: (Iterable[str]) -> None with self . _lock : _LOGGER . debug ( "" EventProcessor  %r  tries to cancel partitions  %r "" , self . _id , to_cancel_partitions , ) for partition_id in to_cancel_partitions : <MASK> self . _consumers [ partition_id ] . stop = True _LOGGER . info ( "" EventProcessor  %r  has cancelled partition  %r "" , self . _id , partition_id , )",if partition_id in self . _consumers :,if partition_id in self . _consumers :,75,TRUE,100
503,"def get_intersect_all ( self , refine = False ) : result = None for source , parts in self . _per_source . items ( ) : <MASK> result = parts else : result . intersection_update ( parts ) if not result : return None elif len ( result ) == 1 : return list ( result ) [ 0 ] . item else : solids = [ p . item for p in result ] solid = solids [ 0 ] . fuse ( solids [ 1 : ] ) if refine : solid = solid . removeSplitter ( ) return solid",if result is None :,"if source == ""intersect"" :",80.03364882,FALSE,93.6
504,"def geli_detach ( self , pool , clear = False ) : failed = 0 for ed in self . middleware . call_sync ( "" datastore.query "" , "" storage.encrypteddisk "" , [ ( "" encrypted_volume "" , "" = "" , pool [ "" id "" ] ) ] , ) : dev = ed [ "" encrypted_provider "" ] try : self . geli_detach_single ( dev ) except Exception as ee : self . logger . warn ( str ( ee ) ) failed + = 1 <MASK> try : self . geli_clear ( dev ) except Exception as e : self . logger . warn ( "" Failed to clear  %s :  %s "" , dev , e ) return failed",if clear :,if clear :,100,TRUE,100
505,def compute_lengths ( batch_sizes ) : tmp_batch_sizes = np . copy ( batch_sizes ) lengths = [ ] while True : c = np . count_nonzero ( tmp_batch_sizes > 0 ) <MASK> break lengths . append ( c ) tmp_batch_sizes = np . array ( [ b - 1 for b in tmp_batch_sizes ] ) return np . array ( lengths ),if c == 0 :,if c == 0 :,100,TRUE,100
506,"def _render_raw_list ( bytes_items ) : flatten_items = [ ] for item in bytes_items : <MASK> flatten_items . append ( b "" "" ) elif isinstance ( item , bytes ) : flatten_items . append ( item ) elif isinstance ( item , int ) : flatten_items . append ( str ( item ) . encode ( ) ) elif isinstance ( item , list ) : flatten_items . append ( _render_raw_list ( item ) ) return b "" \n "" . join ( flatten_items )",if item is None :,"if isinstance ( item , ( bytes , list ) ) :",90.95091109,FALSE,90.8
507,"def update ( self , new_config ) : jsonschema . validate ( new_config , self . schema ) config = { } for k , v in new_config . items ( ) : <MASK> config [ k ] = self [ k ] else : config [ k ] = v self . _config = config self . changed ( )","if k in self . schema . get ( ""secret"" , [ ] ) and v == SECRET_PLACEHOLDER :",if k in self :,76.89822438,FALSE,75.14
508,"def _encode_numpy ( values , uniques = None , encode = False , check_unknown = True ) : # only used in _encode below, see docstring there for details if uniques is None : if encode : uniques , encoded = np . unique ( values , return_inverse = True ) return uniques , encoded else : # unique sorts return np . unique ( values ) if encode : <MASK> diff = _encode_check_unknown ( values , uniques ) if diff : raise ValueError ( "" y contains previously unseen labels:  %s "" % str ( diff ) ) encoded = np . searchsorted ( uniques , values ) return uniques , encoded else : return uniques",if check_unknown :,if check_unknown :,100,TRUE,100
509,"def restore_dtype_and_merge ( arr , input_dtype ) : if isinstance ( arr , list ) : arr = [ restore_dtype_and_merge ( arr_i , input_dtype ) for arr_i in arr ] shapes = [ arr_i . shape for arr_i in arr ] <MASK> arr = np . array ( arr ) if ia . is_np_array ( arr ) : arr = iadt . restore_dtypes_ ( arr , input_dtype ) return arr",if len ( set ( shapes ) ) == 1 :,"if not np . allclose ( arr , shape ) :",66.78394863,FALSE,90.2
510,"def proc_minute ( d ) : if expanded [ 0 ] [ 0 ] != "" * "" : diff_min = nearest_diff_method ( d . minute , expanded [ 0 ] , 60 ) if diff_min is not None and diff_min != 0 : <MASK> d + = relativedelta ( minutes = diff_min , second = 59 ) else : d + = relativedelta ( minutes = diff_min , second = 0 ) return True , d return False , d",if is_prev :,if diff_min < 59 :,94.60995796,FALSE,93.93
511,"def _populate_tree ( self , element , d ) : """"""Populates an etree with attributes & elements, given a dict."""""" for k , v in d . iteritems ( ) : <MASK> self . _populate_dict ( element , k , v ) elif isinstance ( v , list ) : self . _populate_list ( element , k , v ) elif isinstance ( v , bool ) : self . _populate_bool ( element , k , v ) elif isinstance ( v , basestring ) : self . _populate_str ( element , k , v ) elif type ( v ) in [ int , float , long , complex ] : self . _populate_number ( element , k , v )","if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",100,TRUE,100
512,"def __createItemAttribute ( self , item , function , preload ) : """"""Create the new widget, add it, and remove the old one"""""" try : self . __stack . addWidget ( function ( item , preload ) ) # Remove the widget <MASK> oldWidget = self . __stack . widget ( 0 ) self . __stack . removeWidget ( oldWidget ) oldWidget . setParent ( QtWidgets . QWidget ( ) ) except Exception as e : list ( map ( logger . warning , cuegui . Utils . exceptionOutput ( e ) ) )",if self . __stack . count ( ) > 1 :,if self . __stack . widget ( 0 ) . exists ( ) :,71.53680875,FALSE,92.89
513,"def download_main ( download , download_playlist , urls , playlist , output_dir , merge , info_only ) : for url in urls : <MASK> url = url [ 8 : ] if not url . startswith ( "" http:// "" ) : url = "" http:// "" + url if playlist : download_playlist ( url , output_dir = output_dir , merge = merge , info_only = info_only ) else : download ( url , output_dir = output_dir , merge = merge , info_only = info_only )","if url . startswith ( ""https://"" ) :","if url . startswith ( ""http"" ) :",98.63904904,FALSE,95.71
514,"def add_enc_zero ( obj , enc_zero ) : if isinstance ( obj , np . ndarray ) : return obj + enc_zero elif isinstance ( obj , Iterable ) : return type ( obj ) ( EncryptModeCalculator . add_enc_zero ( o , enc_zero ) <MASK> else o + enc_zero for o in obj ) else : return obj + enc_zero","if isinstance ( o , Iterable )","if isinstance ( o , EncryptModeCalculator )",91.82481983,FALSE,96.94
515,"def ensemble ( self , pairs , other_preds ) : """"""Ensemble the dict with statistical model predictions."""""" lemmas = [ ] assert len ( pairs ) == len ( other_preds ) for p , pred in zip ( pairs , other_preds ) : w , pos = p if ( w , pos ) in self . composite_dict : lemma = self . composite_dict [ ( w , pos ) ] elif w in self . word_dict : lemma = self . word_dict [ w ] else : lemma = pred <MASK> lemma = w lemmas . append ( lemma ) return lemmas",if lemma is None :,elif pred is None :,95.36436752,FALSE,97.24
516,"def replace_to_6hex ( color ) : """"""Validate and replace 3hex colors to 6hex ones."""""" if match ( r "" ^#(?:[0-9a-fA-F] {3} ) { 1,2}$ "" , color ) : <MASK> color = "" # {0} {0} {1} {1} {2} {2} "" . format ( color [ 1 ] , color [ 2 ] , color [ 3 ] ) return color else : exit ( _ ( "" Invalid color  {} "" ) . format ( color ) )",if len ( color ) == 4 :,"if color [ 0 ] == ""6hex"" :",86.27427749,FALSE,92.33
517,"def computeMachineName ( self ) : """"""Return the name of the current machine, i.e, HOSTNAME."""""" # This is prepended to leoSettings.leo or myLeoSettings.leo # to give the machine-specific setting name. # How can this be worth doing?? try : import os name = os . getenv ( "" HOSTNAME "" ) <MASK> name = os . getenv ( "" COMPUTERNAME "" ) <MASK> import socket name = socket . gethostname ( ) except Exception : name = "" "" return name",if not name :,if not name :,75,TRUE,100
518,"def _git_dirty_working_directory ( q , include_untracked ) : try : cmd = [ "" git "" , "" status "" , "" --porcelain "" ] if include_untracked : cmd + = [ "" --untracked-files=normal "" ] else : cmd + = [ "" --untracked-files=no "" ] status = _run_git_cmd ( cmd ) <MASK> q . put ( bool ( status ) ) else : q . put ( None ) except ( subprocess . CalledProcessError , OSError , FileNotFoundError ) : q . put ( None )",if status is not None :,if status :,90.68409671,FALSE,95.91
519,"def runAndWaitWork ( server , work ) : work . touch ( ) thr = threading . Thread ( target = workThread , args = ( server , work ) ) thr . setDaemon ( True ) thr . start ( ) # Wait around for done or timeout while True : if work . isTimedOut ( ) : break # If the thread is done, lets get out. if not thr . isAlive ( ) : break # If our parent, or some thread closes stdin, # time to pack up and go. <MASK> break time . sleep ( 2 )",if sys . stdin . closed :,if thr . start ( ) == 0 :,96.70273361,FALSE,91.9
520,"def read ( self , count = True , timeout = None , ignore_non_errors = True , ignore_timeouts = True ) : try : return self . _read ( count , timeout ) except usb . USBError as e : if DEBUG_COMM : log . info ( "" read: e.errno= %s  e.strerror= %s  e.message= %s  repr= %s "" % ( e . errno , e . strerror , e . message , repr ( e ) ) ) if ignore_timeouts and is_timeout ( e ) : return [ ] <MASK> return [ ] raise",if ignore_non_errors and is_noerr ( e ) :,if ignore_non_errors and is_non_errors ( e ) :,98.75831429,FALSE,96.67
521,"def PrintHeader ( self ) : # print the header array if self . draw == False : return for val in self . parent . header : self . SetPrintFont ( val [ "" Font "" ] ) header_indent = val [ "" Indent "" ] * self . pwidth text = val [ "" Text "" ] htype = val [ "" Type "" ] <MASK> addtext = self . GetDate ( ) elif htype == "" Date & Time "" : addtext = self . GetDateTime ( ) else : addtext = "" "" self . OutTextPageWidth ( text + addtext , self . pheader_margin , val [ "" Align "" ] , header_indent , True )","if htype == ""Date"" :","if htype == ""Date & Date"" :",73.59186599,FALSE,97.96
522,"def get_intersect_all ( self , refine = False ) : result = None for source , parts in self . _per_source . items ( ) : if result is None : result = parts else : result . intersection_update ( parts ) if not result : return None elif len ( result ) == 1 : return list ( result ) [ 0 ] . item else : solids = [ p . item for p in result ] solid = solids [ 0 ] . fuse ( solids [ 1 : ] ) <MASK> solid = solid . removeSplitter ( ) return solid",if refine :,if refine :,100,TRUE,100
523,"def captured_updateNode ( self , context ) : if not self . updating_name_from_pointer : font_datablock = self . get_bpy_data_from_name ( self . fontname , bpy . data . fonts ) <MASK> self . font_pointer = font_datablock updateNode ( self , context )",if font_datablock :,if font_datablock :,100,TRUE,100
524,"def __add__ ( self , other ) : if isinstance ( other , Vector2 ) : # Vector + Vector -> Vector # Vector + Point -> Point # Point + Point -> Vector <MASK> _class = Vector2 else : _class = Point2 return _class ( self . x + other . x , self . y + other . y ) else : assert hasattr ( other , "" __len__ "" ) and len ( other ) == 2 return Vector2 ( self . x + other [ 0 ] , self . y + other [ 1 ] )",if self . __class__ is other . __class__ :,if self . __class__ is None :,97.94580095,FALSE,93.42
525,"def _flatten_settings_from_form ( self , settings , form , form_values ) : """"""Take a nested dict and return a flat dict of setting values."""""" setting_values = { } for field in form . c : <MASK> setting_values . update ( self . _flatten_settings_from_form ( settings , field , form_values [ field . _name ] ) ) elif field . _name in settings : setting_values [ field . _name ] = form_values [ field . _name ] return setting_values","if isinstance ( field , _ContainerMixin ) :",if field . _name in form_values :,87.13725413,FALSE,92.74
526,"def add_include_dirs ( self , args ) : ids = [ ] for a in args : # FIXME same hack, forcibly unpack from holder. if hasattr ( a , "" includedirs "" ) : a = a . includedirs <MASK> raise InvalidArguments ( "" Include directory to be added is not an include directory object. "" ) ids . append ( a ) self . include_dirs + = ids","if not isinstance ( a , IncludeDirs ) :","if not isinstance ( a , ( list , tuple ) ) :",97.17419002,FALSE,92.71
527,"def _clip_array ( array , config ) : if "" threshold "" in config . keys ( ) : threshold = config [ "" threshold "" ] else : abs_array = np . max ( np . abs ( array ) ) <MASK> return array threshold = np . percentile ( np . abs ( array ) , 99.99 ) return np . clip ( array , - threshold , threshold )",if abs_array < 1.0 :,if abs_array == 0 :,92.31879051,FALSE,94.52
528,def dfs ( v : str ) - > Iterator [ Set [ str ] ] : index [ v ] = len ( stack ) stack . append ( v ) boundaries . append ( index [ v ] ) for w in edges [ v ] : <MASK> yield from dfs ( w ) elif w not in identified : while index [ w ] < boundaries [ - 1 ] : boundaries . pop ( ) if boundaries [ - 1 ] == index [ v ] : boundaries . pop ( ) scc = set ( stack [ index [ v ] : ] ) del stack [ index [ v ] : ] identified . update ( scc ) yield scc,if w not in index :,if len ( boundaries ) == 0 :,80.89246203,FALSE,93.62
529,"def create_balancer ( self , name , members , protocol = "" http "" , port = 80 , algorithm = DEFAULT_ALGORITHM ) : balancer = self . ex_create_balancer_nowait ( name , members , protocol , port , algorithm ) timeout = 60 * 20 waittime = 0 interval = 2 * 15 if balancer . id is not None : return balancer else : while waittime < timeout : balancers = self . list_balancers ( ) for i in balancers : <MASK> return i waittime + = interval time . sleep ( interval ) raise Exception ( "" Failed to get id "" )",if i . name == balancer . name and i . id is not None :,if i . id is not None :,91.10052852,FALSE,92.81
530,"def handle ( self , scope : Scope , receive : Receive , send : Send ) - > None : if self . methods and scope [ "" method "" ] not in self . methods : <MASK> raise HTTPException ( status_code = 405 ) else : response = PlainTextResponse ( "" Method Not Allowed "" , status_code = 405 ) await response ( scope , receive , send ) else : await self . app ( scope , receive , send )","if ""app"" in scope :",if self . http_methods :,94.92851526,FALSE,92.95
531,"def convert ( data ) : result = [ ] for d in data : # noinspection PyCompatibility if isinstance ( d , tuple ) and len ( d ) == 2 : result . append ( ( d [ 0 ] , None , d [ 1 ] ) ) <MASK> result . append ( d ) return result","elif isinstance ( d , basestring ) :","elif isinstance ( d , ( list , tuple ) ) and len ( d ) == 3",93.68412522,FALSE,80.26
532,"def register_adapters ( ) : global adapters_registered if adapters_registered is True : return try : import pkg_resources packageDir = pkg_resources . resource_filename ( "" pyamf "" , "" adapters "" ) except : packageDir = os . path . dirname ( __file__ ) for f in glob . glob ( os . path . join ( packageDir , "" *.py "" ) ) : mod = os . path . basename ( f ) . split ( os . path . extsep , 1 ) [ 0 ] <MASK> continue try : register_adapter ( mod [ 1 : ] . replace ( "" _ "" , "" . "" ) , PackageImporter ( mod ) ) except ImportError : pass adapters_registered = True","if mod == ""__init__"" or not mod . startswith ( ""_"" ) :","if mod [ 0 ] == ""__"" :",92.11258842,FALSE,89.97
533,"def load_modules ( to_load , load , attr , modules_dict , excluded_aliases , loading_message = None ) : if loading_message : print ( loading_message ) for name in to_load : module = load ( name ) if module is None or not hasattr ( module , attr ) : continue cls = getattr ( module , attr ) if hasattr ( cls , "" initialize "" ) and not cls . initialize ( ) : continue if hasattr ( module , "" aliases "" ) : for alias in module . aliases ( ) : <MASK> modules_dict [ alias ] = module else : modules_dict [ name ] = module if loading_message : print ( )",if alias not in excluded_aliases :,if alias not in excluded_aliases :,100,TRUE,100
534,"def clean_items ( event , items , variations ) : for item in items : <MASK> raise ValidationError ( _ ( "" One or more items do not belong to this event. "" ) ) if item . has_variations : if not any ( var . item == item for var in variations ) : raise ValidationError ( _ ( "" One or more items has variations but none of these are in the variations list. "" ) )",if event != item . event :,if not any ( item . item == item for item in variations ) :,77.43627573,FALSE,85.61
535,"def __get_file_by_num ( self , num , file_list , idx = 0 ) : for element in file_list : <MASK> return element if element [ 3 ] and element [ 4 ] : i = self . __get_file_by_num ( num , element [ 3 ] , idx + 1 ) if not isinstance ( i , int ) : return i idx = i else : idx + = 1 return idx",if idx == num :,"if not isinstance ( element , dict ) :",85.77206265,FALSE,91.35
536,"def check ( chip , xeddb , chipdb ) : all_inst = [ ] undoc = [ ] for inst in xeddb . recs : <MASK> if inst . undocumented : undoc . append ( inst ) else : all_inst . append ( inst ) return ( all_inst , undoc )",if inst . isa_set in chipdb [ chip ] :,"if inst . name == ""doc"" :",88.95821346,FALSE,85.68
537,"def get_all_topic_src_files ( self ) : """"""Retrieves the file paths of all the topics in directory"""""" topic_full_paths = [ ] topic_names = os . listdir ( self . topic_dir ) for topic_name in topic_names : # Do not try to load hidden files. <MASK> topic_full_path = os . path . join ( self . topic_dir , topic_name ) # Ignore the JSON Index as it is stored with topic files. if topic_full_path != self . index_file : topic_full_paths . append ( topic_full_path ) return topic_full_paths","if not topic_name . startswith ( ""."" ) :","if not topic_name . startswith ( ""topic"" ) :",98.64581151,FALSE,98.33
538,"def _get_element ( dom_msi , tag_name , name = None , id_ = None ) : """"""Get a xml element defined on Product."""""" product = dom_msi . getElementsByTagName ( "" Product "" ) [ 0 ] elements = product . getElementsByTagName ( tag_name ) for element in elements : <MASK> if ( element . getAttribute ( "" Name "" ) == name and element . getAttribute ( "" Id "" ) == id_ ) : return element elif id_ : if element . getAttribute ( "" Id "" ) == id_ : return element",if name and id_ :,if name :,94.87537258,FALSE,96.31
539,"def __init__ ( self , * models ) : super ( ) . __init__ ( ) self . models = ModuleList ( models ) for m in models : <MASK> raise ValueError ( "" IndependentModelList currently only supports models that have a likelihood (e.g. ExactGPs) "" ) self . likelihood = LikelihoodList ( * [ m . likelihood for m in models ] )","if not hasattr ( m , ""likelihood"" ) :","if isinstance ( m , ModuleList ) :",89.5227619,FALSE,90.57
540,"def _sniff ( filename , oxlitype ) : try : with open ( filename , "" rb "" ) as fileobj : header = fileobj . read ( 4 ) <MASK> fileobj . read ( 1 ) # skip the version number ftype = fileobj . read ( 1 ) if binascii . hexlify ( ftype ) == oxlitype : return True return False except OSError : return False","if header == b""OXLI"" :","if header == b""version"" :",98.26705278,FALSE,96.74
541,"def convert_port_bindings ( port_bindings ) : result = { } for k , v in six . iteritems ( port_bindings ) : key = str ( k ) if "" / "" not in key : key + = "" /tcp "" <MASK> result [ key ] = [ _convert_port_binding ( binding ) for binding in v ] else : result [ key ] = [ _convert_port_binding ( v ) ] return result","if isinstance ( v , list ) :","if isinstance ( v , ( list , tuple ) ) :",92.8797803,FALSE,93.81
542,"def input_data ( self ) : gen = self . config . generator # don't try running the generator if we specify an output file explicitly, # otherwise generator may segfault and we end up returning the output file anyway if gen and ( not self . config [ "" out "" ] or not self . config [ "" in "" ] ) : <MASK> self . _run_generator ( gen , args = self . config . generator_args ) if self . _generated [ 0 ] : return self . _generated [ 0 ] # in file is optional return ( self . _normalize ( self . problem . problem_data [ self . config [ "" in "" ] ] ) if self . config [ "" in "" ] else b "" "" )",if self . _generated is None :,if self . _generated :,98.5670311,FALSE,97.73
543,"def __new__ ( cls , * tasks , * * kwargs ) : # This forces `chain(X, Y, Z)` to work the same way as `X | Y | Z` if not kwargs and tasks : <MASK> tasks = tasks [ 0 ] if len ( tasks ) == 1 else tasks return reduce ( operator . or_ , tasks ) return super ( chain , cls ) . __new__ ( cls , * tasks , * * kwargs )",if len ( tasks ) != 1 or is_list ( tasks [ 0 ] ) :,"if isinstance ( tasks , tuple ) :",67.61361756,FALSE,85.15
544,"def get_file_sources ( ) : global _file_sources if _file_sources is None : from galaxy . files import ConfiguredFileSources file_sources = None if os . path . exists ( "" file_sources.json "" ) : file_sources_as_dict = None with open ( "" file_sources.json "" , "" r "" ) as f : file_sources_as_dict = json . load ( f ) if file_sources_as_dict is not None : file_sources = ConfiguredFileSources . from_dict ( file_sources_as_dict ) <MASK> ConfiguredFileSources . from_dict ( [ ] ) _file_sources = file_sources return _file_sources",if file_sources is None :,if file_sources is None :,100,TRUE,100
545,"def InitializeColours ( self ) : """"""Initializes the 16 custom colours in :class:`CustomPanel`."""""" curr = self . _colourData . GetColour ( ) self . _colourSelection = - 1 for i in range ( 16 ) : c = self . _colourData . GetCustomColour ( i ) <MASK> self . _customColours [ i ] = self . _colourData . GetCustomColour ( i ) else : self . _customColours [ i ] = wx . WHITE if c == curr : self . _colourSelection = i",if c . IsOk ( ) :,if c == curr :,91.43968912,FALSE,94.92
546,"def convert_obj_into_marshallable ( self , obj ) : if isinstance ( obj , self . marshalable_types ) : return obj if isinstance ( obj , array . array ) : if obj . typecode == "" c "" : return obj . tostring ( ) <MASK> return obj . tounicode ( ) return obj . tolist ( ) return self . class_to_dict ( obj )","if obj . typecode == ""u"" :","if obj . typecode == ""u"" :",100,TRUE,100
547,"def run ( self ) : self . run_command ( "" egg_info "" ) from glob import glob for pattern in self . match : pattern = self . distribution . get_name ( ) + "" * "" + pattern files = glob ( os . path . join ( self . dist_dir , pattern ) ) files = [ ( os . path . getmtime ( f ) , f ) for f in files ] files . sort ( ) files . reverse ( ) log . info ( "" %d  file(s) matching  %s "" , len ( files ) , pattern ) files = files [ self . keep : ] for ( t , f ) in files : log . info ( "" Deleting  %s "" , f ) <MASK> os . unlink ( f )",if not self . dry_run :,if os . path . exists ( f ) :,95.75038308,FALSE,94.3
548,"def render_token_list ( self , tokens ) : result = [ ] vars = [ ] for token in tokens : if token . token_type == TOKEN_TEXT : result . append ( token . contents . replace ( "" % "" , "" %% "" ) ) <MASK> result . append ( "" %% ( %s )s "" % token . contents ) vars . append ( token . contents ) return "" "" . join ( result ) , vars",elif token . token_type == TOKEN_VAR :,elif token . token_type == TOKEN_VAR :,100,TRUE,100
549,"def _handle_raise ( self , values , is_NAs , origins ) : for is_NA , origin in zip ( is_NAs , origins ) : <MASK> msg = ( "" Missing values detected. If you want rows with missing  "" "" values to be automatically deleted in a list-wise  "" "" manner (not recommended), please set dropna=True in  "" "" the Bambi Model initialization. "" ) raise PatsyError ( msg , origin ) return values",if np . any ( is_NA ) :,if not values or not is_NA :,86.93974363,FALSE,91.75
550,"def add_node_data ( node_array , ntwk ) : node_ntwk = nx . Graph ( ) newdata = { } for idx , data in ntwk . nodes ( data = True ) : <MASK> newdata [ "" value "" ] = node_array [ int ( idx ) - 1 ] data . update ( newdata ) node_ntwk . add_node ( int ( idx ) , * * data ) return node_ntwk",if not int ( idx ) == 0 :,if int ( idx ) > 0 :,96.54634675,FALSE,93.6
551,"def safe_parse_date ( date_hdr ) : """"""Parse a Date: or Received: header into a unix timestamp."""""" try : if "" ; "" in date_hdr : date_hdr = date_hdr . split ( "" ; "" ) [ - 1 ] . strip ( ) msg_ts = long ( rfc822 . mktime_tz ( rfc822 . parsedate_tz ( date_hdr ) ) ) <MASK> return None else : return msg_ts except ( ValueError , TypeError , OverflowError ) : return None",if ( msg_ts > ( time . time ( ) + 24 * 3600 ) ) or ( msg_ts < 1 ) :,if msg_ts < 0 :,79.80785766,FALSE,80.89
552,"def _route_db ( self , model , * * hints ) : chosen_db = None for router in self . routers : try : method = getattr ( router , action ) except AttributeError : # If the router doesn't have a method, skip to the next one. pass else : chosen_db = method ( model , * * hints ) <MASK> return chosen_db try : return hints [ "" instance "" ] . _state . db or DEFAULT_DB_ALIAS except KeyError : return DEFAULT_DB_ALIAS",if chosen_db :,if chosen_db is not None :,98.25705881,FALSE,95.87
553,"def get_keys ( struct , ignore_first_level = False ) : res = [ ] if isinstance ( struct , dict ) : if not ignore_first_level : keys = [ x . split ( "" ( "" ) [ 0 ] for x in struct . keys ( ) ] res . extend ( keys ) for key in struct : <MASK> logging . debug ( "" Ignored:  %s :  %s "" , key , struct [ key ] ) continue res . extend ( get_keys ( struct [ key ] , key in IGNORED_FIRST_LEVEL ) ) elif isinstance ( struct , list ) : for item in struct : res . extend ( get_keys ( item ) ) return res",if key in IGNORED_KEYS :,if key in IGNORED_FIRST_LEVEL :,98.99073422,FALSE,96.94
554,"def launch_app ( self , fs_id ) : if fs_id in self . app_infos : row = self . get_row_by_fsid ( fs_id ) <MASK> return app_info = self . app_infos [ fs_id ] filepath = os . path . join ( row [ SAVEDIR_COL ] , row [ SAVENAME_COL ] ) gfile = Gio . File . new_for_path ( filepath ) app_info . launch ( [ gfile , ] , None , ) self . app_infos . pop ( fs_id , None )",if not row :,if not row :,100,TRUE,100
555,"def create_skipfile ( files_changed , skipfile ) : # File is likely to contain some garbage values at start, # only the corresponding json should be parsed. json_pattern = re . compile ( r "" ^ \ { .* \ } "" ) for line in files_changed . readlines ( ) : <MASK> for filename in json . loads ( line ) : if "" /COMMIT_MSG "" in filename : continue skipfile . write ( "" +*/ %s \n "" % filename ) skipfile . write ( "" -* \n "" )","if re . match ( json_pattern , line ) :",if json_pattern . search ( line ) :,71.63393633,FALSE,93.78
556,"def zscore ( self , client , request , N ) : check_input ( request , N != 2 ) key = request [ 1 ] db = client . db value = db . get ( key ) if value is None : client . reply_bulk ( None ) elif not isinstance ( value , self . zset_type ) : client . reply_wrongtype ( ) else : score = value . score ( request [ 2 ] , None ) <MASK> score = str ( score ) . encode ( "" utf-8 "" ) client . reply_bulk ( score )",if score is not None :,if six . PY2 :,78.87742355,FALSE,95.23
557,"def _list_cases ( suite ) : for test in suite : if isinstance ( test , unittest . TestSuite ) : _list_cases ( test ) elif isinstance ( test , unittest . TestCase ) : <MASK> print ( test . id ( ) )",if support . match_test ( test ) :,"if test . name ( ) != ""test"" :",89.70236946,FALSE,82.51
558,"def Run ( self ) : """"""The main run method of the client."""""" for thread in self . _threads . values ( ) : thread . start ( ) logging . info ( START_STRING ) while True : dead_threads = [ tn for ( tn , t ) in self . _threads . items ( ) if not t . isAlive ( ) ] <MASK> raise FatalError ( "" These threads are dead:  %r . Shutting down... "" % dead_threads ) time . sleep ( 10 )",if dead_threads :,if dead_threads :,100,TRUE,100
559,"def _slice_queryset ( queryset , order_by , per_page , start ) : page_len = int ( per_page ) + 1 if start : <MASK> filter_name = "" %s __lte "" % order_by [ 1 : ] else : filter_name = "" %s __gte "" % order_by return queryset . filter ( * * { filter_name : start } ) [ : page_len ] return queryset [ : page_len ]","if order_by . startswith ( ""-"" ) :","if order_by . startswith ( ""LIMIT"" ) :",98.32357001,FALSE,97.7
560,"def compute_timer_precision ( timer ) : precision = None points = 0 timeout = timeout_timer ( ) + 1.0 previous = timer ( ) while timeout_timer ( ) < timeout or points < 5 : for _ in XRANGE ( 10 ) : t1 = timer ( ) t2 = timer ( ) dt = t2 - t1 <MASK> break else : dt = t2 - previous if dt < = 0.0 : continue if precision is not None : precision = min ( precision , dt ) else : precision = dt points + = 1 previous = timer ( ) return precision",if 0 < dt :,if dt <= 0.0 :,96.11257978,FALSE,95.64
561,"def findWorkingDir ( ) : frozen = getattr ( sys , "" frozen "" , "" "" ) if not frozen : path = os . path . dirname ( __file__ ) elif frozen in ( "" dll "" , "" console_exe "" , "" windows_exe "" , "" macosx_app "" ) : path = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( __file__ ) ) ) ) elif frozen : # needed for PyInstaller <MASK> path = getattr ( sys , "" _MEIPASS "" , "" "" ) # --onefile else : path = os . path . dirname ( sys . executable ) # --onedir else : path = "" "" return path","if getattr ( sys , ""_MEIPASS"" , """" ) is not None :","if hasattr ( sys , ""_MEIPASS"" ) :",96.23177938,FALSE,93.4
562,"def CreateDataType ( vmodlName , wsdlName , parent , version , props ) : with _lazyLock : dic = [ vmodlName , wsdlName , parent , version , props ] names = vmodlName . split ( "" . "" ) <MASK> vmodlName = "" . "" . join ( name [ 0 ] . lower ( ) + name [ 1 : ] for name in names ) _AddToDependencyMap ( names ) typeNs = GetWsdlNamespace ( version ) _dataDefMap [ vmodlName ] = dic _wsdlDefMap [ ( typeNs , wsdlName ) ] = dic _wsdlTypeMapNSs . add ( typeNs )",if _allowCapitalizedNames :,if len ( names ) > 1 :,83.75058528,FALSE,93.2
563,"def ParseResponses ( self , knowledge_base : rdf_client . KnowledgeBase , responses : Iterable [ rdfvalue . RDFValue ] , ) - > Iterator [ rdf_client . User ] : for response in responses : if not isinstance ( response , rdf_client_fs . StatEntry ) : raise TypeError ( f "" Unexpected response type: ` { type ( response ) } ` "" ) # TODO: `st_mode` has to be an `int`, not `StatMode`. if stat . S_ISDIR ( int ( response . st_mode ) ) : homedir = response . pathspec . path username = os . path . basename ( homedir ) <MASK> yield rdf_client . User ( username = username , homedir = homedir )",if username not in self . _ignore_users :,if username :,72.13400611,FALSE,93.76
564,"def process_question ( qtxt ) : question = "" "" skip = False for letter in qtxt : if letter == "" < "" : skip = True if letter == "" > "" : skip = False if skip : continue <MASK> if letter == ""   "" : letter = "" _ "" question + = letter . lower ( ) return question","if letter . isalnum ( ) or letter == "" "" :",if not skip :,76.54182673,FALSE,83.83
565,"def process_all ( self , lines , times = 1 ) : gap = False for _ in range ( times ) : for line in lines : if gap : self . write ( "" "" ) self . process ( line ) <MASK> gap = True return 0",if not is_command ( line ) :,if len ( line ) > times :,92.55950088,FALSE,87.89
566,"def _get ( self , domain ) : with self . lock : try : record = self . cache [ domain ] time_now = time . time ( ) if time_now - record [ "" update "" ] > self . ttl : record = None except KeyError : record = None <MASK> record = { "" r "" : "" unknown "" , "" dns "" : { } , "" g "" : 1 , "" query_count "" : 0 } # self.cache[domain] = record return record",if not record :,elif record is None :,78.44639844,FALSE,94.95
567,"def gen_constant_folding ( cw ) : types = [ "" Int32 "" , "" Double "" , "" BigInteger "" , "" Complex "" ] for cur_type in types : cw . enter_block ( "" if (constLeft.Value.GetType() == typeof( %s )) "" % ( cur_type , ) ) cw . enter_block ( "" switch (_op) "" ) for op in ops : gen = getattr ( op , "" genConstantFolding "" , None ) <MASK> gen ( cw , cur_type ) cw . exit_block ( ) cw . exit_block ( )",if gen is not None :,if gen is not None :,100,TRUE,100
568,"def unreferenced_dummy ( self ) : for g , base in zip ( self . evgroups , self . evbases ) : for ind , j in enumerate ( g ) : <MASK> debug_print ( "" replacing unreferenced  %d   %s  with dummy "" % ( ( base + ind ) , g [ ind ] ) ) g [ ind ] = "" dummy "" self . evnum [ base + ind ] = "" dummy """,if not self . indexobj [ base + ind ] :,if j == base + ind :,86.75967772,FALSE,90.1
569,"def handle_signature ( self , sig : str , signode : desc_signature ) - > Tuple [ str , str ] : for cls in self . __class__ . __mro__ : <MASK> warnings . warn ( "" PyDecoratorMixin is deprecated.  "" "" Please check the implementation of  %s "" % cls , RemovedInSphinx50Warning , stacklevel = 2 , ) break else : warnings . warn ( "" PyDecoratorMixin is deprecated "" , RemovedInSphinx50Warning , stacklevel = 2 ) ret = super ( ) . handle_signature ( sig , signode ) # type: ignore signode . insert ( 0 , addnodes . desc_addname ( "" @ "" , "" @ "" ) ) return ret","if cls . __name__ != ""DirectiveAdapter"" :",if cls . __name__ == sig :,93.38030289,FALSE,95.67
570,"def _iter_lines ( path = path , response = response , max_next = options . http_max_next ) : path . responses = [ ] n = 0 while response : path . responses . append ( response ) yield from response . iter_lines ( decode_unicode = True ) src = response . links . get ( "" next "" , { } ) . get ( "" url "" , None ) <MASK> break n + = 1 if n > max_next : vd . warning ( f "" stopping at max  { max_next }  pages "" ) break vd . status ( f "" fetching next page from  { src } "" ) response = requests . get ( src , stream = True )",if not src :,if not src :,100,TRUE,100
571,"def ordered_indices ( self ) : with data_utils . numpy_seed ( self . seed , self . epoch ) : # Used to store the order of indices of each dataset to use indices = [ np . random . permutation ( len ( dataset ) ) for dataset in self . datasets . values ( ) ] # Keep track of which samples we've  used for each dataset counters = [ 0 for _ in self . datasets ] sampled_indices = [ self . _sample ( indices , counters ) for _ in range ( self . total_num_instances ) ] <MASK> sampled_indices . sort ( key = lambda i : self . num_tokens ( i ) ) return np . array ( sampled_indices , dtype = np . int64 )",if self . sort_indices :,if self . num_tokens :,99.07023682,FALSE,97.24
572,"def _build_columns ( self ) : self . columns = [ Column ( ) for col in self . keys ] for row in self : for ( col_idx , col_val ) in enumerate ( row ) : col = self . columns [ col_idx ] col . append ( col_val ) <MASK> col . is_quantity = False for ( idx , key_name ) in enumerate ( self . keys ) : self . columns [ idx ] . name = key_name self . x = Column ( ) self . ys = [ ]",if ( col_val is not None ) and ( not is_quantity ( col_val ) ) :,if col . is_quantity :,71.7874108,FALSE,84.43
573,"def tearDown ( self ) : subprocess_list = self . subprocess_list processes = subprocess_list . processes self . schedule . reset ( ) del self . schedule for proc in processes : <MASK> terminate_process ( proc . pid , kill_children = True , slow_stop = True ) subprocess_list . cleanup ( ) processes = subprocess_list . processes if processes : for proc in processes : <MASK> terminate_process ( proc . pid , kill_children = True , slow_stop = False ) subprocess_list . cleanup ( ) processes = subprocess_list . processes if processes : log . warning ( "" Processes left running:  %s "" , processes )",if proc . is_alive ( ) :,if proc . is_alive ( ) :,100,TRUE,100
574,"def colorNetwork ( cls , network , nodesInNetwork , nodeByID = None ) : for node in nodesInNetwork : node . use_custom_color = True neededCopies = sum ( socket . execution . neededCopies for socket in node . outputs ) <MASK> color = ( 0.7 , 0.9 , 0.7 ) else : color = ( 1.0 , 0.3 , 0.3 ) node . color = color",if neededCopies == 0 :,if neededCopies == 0 :,100,TRUE,100
575,"def _init_warmup_scheduler ( self , optimizer , states ) : updates_so_far = states . get ( "" number_training_updates "" , 0 ) if self . warmup_updates > 0 and ( updates_so_far < = self . warmup_updates or self . hard_reset ) : self . warmup_scheduler = optim . lr_scheduler . LambdaLR ( optimizer , self . _warmup_lr ) <MASK> self . warmup_scheduler . load_state_dict ( states [ "" warmup_scheduler "" ] ) else : self . warmup_scheduler = None","if states . get ( ""warmup_scheduler"" ) :","if states . get ( ""warmup_scheduler"" ) :",100,TRUE,100
576,"def inner ( self , * iargs , * * ikwargs ) : try : return getattr ( super ( VEXResilienceMixin , self ) , func ) ( * iargs , * * ikwargs ) except excs as e : for exc , handler in zip ( excs , handlers ) : if isinstance ( e , exc ) : v = getattr ( self , handler ) ( * iargs , * * ikwargs ) <MASK> raise return v assert False , "" this should be unreachable if Python is working correctly """,if v is raiseme :,if v is None :,93.45574152,FALSE,97.39
577,"def unwrap_envelope ( self , data , many ) : if many : if data [ "" items "" ] : <MASK> self . context [ "" total "" ] = len ( data ) return data else : self . context [ "" total "" ] = data [ "" total "" ] else : self . context [ "" total "" ] = 0 data = { "" items "" : [ ] } return data [ "" items "" ] return data","if isinstance ( data , InstrumentedList ) or isinstance ( data , list ) :","if data [ ""items"" ] :",75.78042702,FALSE,85.52
578,"def __subclasscheck__ ( self , cls ) : if self . __origin__ is not None : <MASK> raise TypeError ( "" Parameterized generics cannot be used with class  "" "" or instance checks "" ) return False if self is Generic : raise TypeError ( "" Class  %r  cannot be used with class  "" "" or instance checks "" % self ) return super ( ) . __subclasscheck__ ( cls )","if sys . _getframe ( 1 ) . f_globals [ ""__name__"" ] not in [ ""abc"" , ""functools"" ] :",if self is Parameterized :,83.43539629,FALSE,68.94
579,"def __init__ ( self , pyversions , coverage_service ) : build_matrix = "" "" for version in pyversions : build_matrix + = "" \n      {} , "" . format ( version <MASK> else "" py {} "" . format ( "" "" . join ( version . split ( "" . "" ) ) ) ) coverage_package = "" "" if coverage_service : coverage_package + = "" \n      {} "" . format ( coverage_service . package ) coverage_package + = "" \n "" super ( Tox , self ) . __init__ ( "" tox.ini "" , TEMPLATE . format ( build_matrix = build_matrix , coverage_package = coverage_package ) , )","if version . startswith ( ""pypy"" )","if version . startswith ( ""py"" )",91.76712847,FALSE,98.39
580,"def _get_app ( self , body = None ) : app = self . _app if app is None : try : tasks = self . tasks . tasks # is a group except AttributeError : tasks = self . tasks if len ( tasks ) : app = tasks [ 0 ] . _app <MASK> app = body . _app return app if app is not None else current_app",if app is None and body is not None :,elif body :,92.5799331,FALSE,87.79
581,"def logic ( ) : for v in [ True , False , None , 0 , True , None , None , 1 ] : yield clk . posedge xd . next = v <MASK> yd . next = zd . next = None elif v : yd . next = zd . next = 11 else : yd . next = zd . next = 0",if v is None :,if v is None :,100,TRUE,100
582,"def run ( self ) : eid = self . start_episode ( ) obs = self . env . reset ( ) while True : <MASK> action = self . env . action_space . sample ( ) self . log_action ( eid , obs , action ) else : action = self . get_action ( eid , obs ) obs , reward , done , info = self . env . step ( action ) self . log_returns ( eid , reward , info = info ) if done : self . end_episode ( eid , obs ) obs = self . env . reset ( ) eid = self . start_episode ( )",if random . random ( ) < self . off_pol_frac :,if self . env . action_space :,89.40785718,FALSE,90.49
583,"def tearDown ( self ) : os . chdir ( self . orig_working_dir ) sys . argv = self . orig_argv sys . stdout = self . orig_stdout sys . stderr = self . orig_stderr for dirname in [ "" lv_LV "" , "" ja_JP "" ] : locale_dir = os . path . join ( self . datadir , "" project "" , "" i18n "" , dirname ) <MASK> shutil . rmtree ( locale_dir )",if os . path . isdir ( locale_dir ) :,if os . path . exists ( locale_dir ) :,97.32761429,FALSE,97.59
584,"def sentry_set_scope ( process_context , entity , project , email = None , url = None ) : # Using GLOBAL_HUB means these tags will persist between threads. # Normally there is one hub per thread. with sentry_sdk . hub . GLOBAL_HUB . configure_scope ( ) as scope : scope . set_tag ( "" process_context "" , process_context ) scope . set_tag ( "" entity "" , entity ) scope . set_tag ( "" project "" , project ) <MASK> scope . user = { "" email "" : email } if url : scope . set_tag ( "" url "" , url )",if email :,if email :,100,TRUE,100
585,"def getDataMax ( self ) : result = - Double . MAX_VALUE nCurves = self . chart . getNCurves ( ) for i in range ( nCurves ) : c = self . getSystemCurve ( i ) <MASK> continue if c . getYAxis ( ) == Y_AXIS : nPoints = c . getNPoints ( ) for j in range ( nPoints ) : result = self . maxIgnoreNaNAndMaxValue ( result , c . getPoint ( j ) . getY ( ) ) if result == - Double . MAX_VALUE : return Double . NaN return result",if not c . isVisible ( ) :,if c . getXAxis ( ) == Y_AXIS :,95.34155359,FALSE,91.13
586,"def handle_starttag ( self , tag , attrs ) : if tag == "" link "" and ( "" rel "" , "" icon "" ) in attrs or ( "" rel "" , "" shortcut icon "" ) in attrs : href = None icon_type = None for attr , value in attrs : if attr == "" href "" : href = value elif attr == "" type "" : icon_type = value <MASK> try : mimetype = extension_to_mimetype ( href . rpartition ( "" . "" ) [ 2 ] ) except KeyError : pass else : icon_type = mimetype if icon_type : self . icons . append ( ( href , icon_type ) )",if href :,"elif attr == ""type"" :",83.95281451,FALSE,94.02
587,"def get_version ( version_file = STATIC_VERSION_FILE ) : version_info = get_static_version_info ( version_file ) version = version_info [ "" version "" ] if version == "" __use_git__ "" : version = get_version_from_git ( ) <MASK> version = get_version_from_git_archive ( version_info ) <MASK> version = Version ( "" unknown "" , None , None ) return pep440_format ( version ) else : return version",if not version :,"if version == ""__version__"" :",75.13413429,FALSE,82.43
588,"def _Sleep ( self , seconds ) : if threading . current_thread ( ) is not self . _worker_thread : return self . _original_sleep ( seconds ) self . _time + = seconds self . _budget - = seconds while self . _budget < 0 : self . _worker_thread_turn . clear ( ) self . _owner_thread_turn . set ( ) self . _worker_thread_turn . wait ( ) <MASK> raise FakeTimeline . _WorkerThreadExit ( )",if self . _worker_thread_done :,if self . _owner_thread_turn . is_set ( ) :,95.63981604,FALSE,90.43
589,"def validate_attributes ( self ) : if not ( self . has_variants or self . variant_of ) : return if not self . variant_based_on : self . variant_based_on = "" Item Attribute "" if self . variant_based_on == "" Item Attribute "" : attributes = [ ] <MASK> frappe . throw ( _ ( "" Attribute table is mandatory "" ) ) for d in self . attributes : if d . attribute in attributes : frappe . throw ( _ ( "" Attribute  {0}  selected multiple times in Attributes Table "" ) . format ( d . attribute ) ) else : attributes . append ( d . attribute )",if not self . attributes :,if not self . attributes :,100,TRUE,100
590,"def check_digest_auth ( user , passwd ) : """"""Check user authentication using HTTP Digest auth"""""" if request . headers . get ( "" Authorization "" ) : credentails = parse_authorization_header ( request . headers . get ( "" Authorization "" ) ) if not credentails : return response_hash = response ( credentails , passwd , dict ( uri = request . script_root + request . path , body = request . data , method = request . method , ) , ) <MASK> return True return False","if credentails . get ( ""response"" ) == response_hash :",if response_hash == user . hash :,84.53328284,FALSE,90.33
591,"def _get_index_type ( return_index_type , ctx ) : if return_index_type is None : # pragma: no cover if ctx . running_mode == RunningMode . local : return_index_type = "" object "" <MASK> return_index_type = "" filename "" else : return_index_type = "" bytes "" return return_index_type",elif ctx . running_mode == RunningMode . local_cluster :,elif ctx . running_mode == RunningMode . file :,97.62381414,FALSE,95.01
592,"def iter_event_handlers ( self , resource : resources_ . Resource , event : bodies . RawEvent , ) - > Iterator [ handlers . ResourceWatchingHandler ] : warnings . warn ( "" SimpleRegistry.iter_event_handlers() is deprecated; use  "" "" ResourceWatchingRegistry.iter_handlers(). "" , DeprecationWarning , ) cause = _create_watching_cause ( resource , event ) for handler in self . _handlers : if not isinstance ( handler , handlers . ResourceWatchingHandler ) : pass <MASK> yield handler","elif registries . match ( handler = handler , cause = cause , ignore_fields = True ) :","if handler . event_handler ( resource , event ) :",84.82499294,FALSE,84.6
593,"def subprocess_post_check ( completed_process : subprocess . CompletedProcess , raise_error : bool = True ) - > None : if completed_process . returncode : <MASK> print ( completed_process . stdout , file = sys . stdout , end = "" "" ) if completed_process . stderr is not None : print ( completed_process . stderr , file = sys . stderr , end = "" "" ) if raise_error : raise PipxError ( f "" { '   ' . join ( [ str ( x ) for x in completed_process . args ] ) !r}  failed "" ) else : logger . info ( f "" { '   ' . join ( completed_process . args ) !r}  failed "" )",if completed_process . stdout is not None :,if completed_process . stdout is not None :,100,TRUE,100
594,"def __pow__ ( self , power ) : if power == 1 : return self if power == - 1 : # HACK: break cycle from cirq . devices import line_qubit decomposed = protocols . decompose_once_with_qubits ( self , qubits = line_qubit . LineQid . for_gate ( self ) , default = None ) <MASK> return NotImplemented inverse_decomposed = protocols . inverse ( decomposed , None ) if inverse_decomposed is None : return NotImplemented return _InverseCompositeGate ( self ) return NotImplemented",if decomposed is None :,if decomposed is None :,100,TRUE,100
595,"def tearDown ( self ) : """"""Close the application after tests"""""" # set it back to it's old position so not to annoy users :-) self . old_pos = self . dlg . rectangle # close the application self . dlg . menu_select ( "" File->Exit "" ) try : <MASK> self . app . UntitledNotepad [ "" Do&n ' t Save "" ] . click ( ) self . app . UntitledNotepad . wait_not ( "" visible "" ) except Exception : pass finally : self . app . kill ( )","if self . app . UntitledNotepad [ ""Do&n't Save"" ] . exists ( ) :","if self . app . UntitledNotepad . get_value ( ) == """,70.54184179,FALSE,89.55
596,"def terminate_subprocess ( proc , timeout = 0.1 , log = None ) : <MASK> if log : log . info ( "" Sending SIGTERM to  %r "" , proc ) proc . terminate ( ) timeout_time = time . time ( ) + timeout while proc . poll ( ) is None and time . time ( ) < timeout_time : time . sleep ( 0.02 ) <MASK> if log : log . info ( "" Sending SIGKILL to  %r "" , proc ) proc . kill ( ) return proc . returncode",if proc . poll ( ) is None :,if proc . is_alive ( ) :,89.09025483,FALSE,90.43
597,"def validate ( self , detection , expectation ) : config = SigmaConfiguration ( ) self . basic_rule [ "" detection "" ] = detection with patch ( "" yaml.safe_load_all "" , return_value = [ self . basic_rule ] ) : parser = SigmaCollectionParser ( "" any sigma io "" , config , None ) backend = SQLiteBackend ( config , self . table ) assert len ( parser . parsers ) == 1 for p in parser . parsers : <MASK> self . assertEqual ( expectation , backend . generate ( p ) ) elif isinstance ( expectation , Exception ) : self . assertRaises ( type ( expectation ) , backend . generate , p )","if isinstance ( expectation , str ) :","if isinstance ( expectation , ( list , tuple ) ) :",95.79260004,FALSE,95.36
598,"def makelist ( d ) : """"""Convert d into a list if all the keys of d are integers."""""" if isinstance ( d , dict ) : <MASK> return [ makelist ( d [ k ] ) for k in sorted ( d , key = int ) ] else : return web . storage ( ( k , makelist ( v ) ) for k , v in d . items ( ) ) else : return d",if all ( isint ( k ) for k in d ) :,"if isinstance ( d [ ""items"" ] , list ) :",61.74426283,FALSE,88.55
599,"def __share_local_dir ( self , lpath , rpath , fast ) : result = const . ENoError for walk in self . __walk_normal_file ( lpath ) : ( dirpath , dirnames , filenames ) = walk for filename in filenames : rpart = os . path . relpath ( dirpath , lpath ) if rpart == "" . "" : rpart = "" "" subr = self . __share_local_file ( joinpath ( dirpath , filename ) , posixpath . join ( rpath , rpart , filename ) , fast , ) <MASK> result = subr return result",if subr != const . ENoError :,if subr is not const . ENoError :,98.53526219,FALSE,97.11
600,"def _targets ( self , sigmaparser ) : # build list of matching target mappings targets = set ( ) for condfield in self . conditions : <MASK> rulefieldvalues = sigmaparser . values [ condfield ] for condvalue in self . conditions [ condfield ] : if condvalue in rulefieldvalues : targets . update ( self . conditions [ condfield ] [ condvalue ] ) return targets",if condfield in sigmaparser . values :,if condfield in sigmaparser . values :,75,TRUE,100
601,"def _wrapped_view ( request , * args , * * kwargs ) : # based on authority/decorators.py user = request . user if user . is_authenticated ( ) : obj = _resolve_lookup ( obj_lookup , kwargs ) perm_obj = _resolve_lookup ( perm_obj_lookup , kwargs ) granted = access . has_perm_or_owns ( user , perm , obj , perm_obj , owner_attr ) <MASK> return view_func ( request , * args , * * kwargs ) # In all other cases, permission denied return HttpResponseForbidden ( )",if granted or user . has_perm ( perm ) :,if granted :,70.23698246,FALSE,91.78
602,"def assert_parts_cleaned ( self , earlier_parts , current_parts , expected_parts , hint ) : cleaned_parts = [ ] for earlier in earlier_parts : earlier_part = earlier [ "" part "" ] earlier_step = earlier [ "" step "" ] found = False for current in current_parts : <MASK> found = True break if not found : cleaned_parts . append ( dict ( part = earlier_part , step = earlier_step ) ) self . assertThat ( cleaned_parts , HasLength ( len ( expected_parts ) ) , hint ) for expected in expected_parts : self . assertThat ( cleaned_parts , Contains ( expected ) , hint )","if earlier_part == current [ ""part"" ] and earlier_step == current [ ""step"" ] :","if current [ ""part"" ] == earlier_part :",82.90865112,FALSE,88.75
603,"def show_image ( self , wnd_name , img ) : if wnd_name in self . named_windows : if self . named_windows [ wnd_name ] == 0 : self . named_windows [ wnd_name ] = 1 self . on_create_window ( wnd_name ) <MASK> self . capture_mouse ( wnd_name ) self . on_show_image ( wnd_name , img ) else : print ( "" show_image: named_window  "" , wnd_name , ""  not found. "" )",if wnd_name in self . capture_mouse_windows :,elif self . named_windows [ wnd_name ] == 1 :,63.8411081,FALSE,89.96
604,"def readlines ( self , hint = None ) : # Again, allow hint but ignore body = self . _get_body ( ) rest = body [ self . position : ] self . position = len ( body ) result = [ ] while 1 : next = rest . find ( "" \r \n "" ) <MASK> result . append ( rest ) break result . append ( rest [ : next + 2 ] ) rest = rest [ next + 2 : ] return result",if next == - 1 :,if next < 0 :,97.59825839,FALSE,94.46
605,"def __lt__ ( self , other ) : olen = len ( other ) for i in range ( olen ) : try : c = self [ i ] < other [ i ] except IndexError : # self must be shorter return True if c : return c <MASK> return False return len ( self ) < olen",elif other [ i ] < self [ i ] :,if self [ i ] != other [ i ] :,96.16802073,FALSE,91.73
606,"def social_user ( backend , uid , user = None , * args , * * kwargs ) : provider = backend . name social = backend . strategy . storage . user . get_social_auth ( provider , uid ) if social : if user and social . user != user : msg = "" This account is already in use. "" raise AuthAlreadyAssociated ( backend , msg ) <MASK> user = social . user return { "" social "" : social , "" user "" : user , "" is_new "" : user is None , "" new_association "" : social is None , }",elif not user :,if not user :,96.7283688,FALSE,97.89
607,"def markUVs ( self , indices = None ) : if isinstance ( indices , tuple ) : indices = indices [ 0 ] ntexco = len ( self . texco ) if indices is None : self . utexc = True else : if self . utexc is False : self . utexc = np . zeros ( ntexco , dtype = bool ) <MASK> self . utexc [ indices ] = True",if self . utexc is not True :,if indices :,87.92621043,FALSE,90.46
608,"def destination ( self , type , name , arglist ) : classname = "" ResFunction "" listname = "" functions "" if arglist : t , n , m = arglist [ 0 ] <MASK> classname = "" ResMethod "" listname = "" resmethods "" return classname , listname","if t == ""Handle"" and m == ""InMode"" :","if t == ""ResMethod"" and m == ""InMode"" :",97.78276054,FALSE,95.95
609,"def select ( self , regions , register ) : self . view . sel ( ) . clear ( ) to_store = [ ] for r in regions : self . view . sel ( ) . add ( r ) if register : to_store . append ( self . view . substr ( self . view . full_line ( r ) ) ) if register : text = "" "" . join ( to_store ) <MASK> text = text + "" \n "" state = State ( self . view ) state . registers [ register ] = [ text ]","if not text . endswith ( ""\n"" ) :","if self . view . full_line ( r ) . endswith ( ""\n"" ) :",92.39672329,FALSE,91.11
610,"def _skip_start ( self ) : start , stop = self . start , self . stop for chunk in self . app_iter : self . _pos + = len ( chunk ) if self . _pos < start : continue <MASK> return b "" "" else : chunk = chunk [ start - self . _pos : ] if stop is not None and self . _pos > stop : chunk = chunk [ : stop - self . _pos ] assert len ( chunk ) == stop - start return chunk else : raise StopIteration ( )",elif self . _pos == start :,if start is None and self . _pos >= start :,75.32049742,FALSE,92.49
611,"def start ( self ) : self . on_config_change ( ) self . start_config_watch ( ) try : if self . config [ "" MITMf "" ] [ "" DNS "" ] [ "" tcp "" ] . lower ( ) == "" on "" : self . startTCP ( ) else : self . startUDP ( ) except socket . error as e : <MASK> shutdown ( "" \n [DNS] Unable to start DNS server on port  {} : port already in use "" . format ( self . config [ "" MITMf "" ] [ "" DNS "" ] [ "" port "" ] ) )","if ""Address already in use"" in e :",if e . errno == errno . EEXIST :,94.6443252,FALSE,92.78
612,"def ignore ( self , other ) : if isinstance ( other , Suppress ) : if other not in self . ignoreExprs : super ( ParseElementEnhance , self ) . ignore ( other ) <MASK> self . expr . ignore ( self . ignoreExprs [ - 1 ] ) else : super ( ParseElementEnhance , self ) . ignore ( other ) <MASK> self . expr . ignore ( self . ignoreExprs [ - 1 ] ) return self",if self . expr is not None :,if self . expr :,90.98228661,FALSE,90.1
613,"def test_relative_deploy_path_override ( ) : s = Site ( TEST_SITE_ROOT ) s . load ( ) res = s . content . resource_from_relative_path ( "" blog/2010/december/merry-christmas.html "" ) res . relative_deploy_path = "" blog/2010/december/happy-holidays.html "" for page in s . content . walk_resources ( ) : <MASK> assert page . relative_deploy_path == "" blog/2010/december/happy-holidays.html "" else : assert page . relative_deploy_path == Folder ( page . relative_path )",if res . source_file == page . source_file :,if page . is_dir ( ) :,92.63519818,FALSE,90.99
614,"def _parser ( cls , buf ) : tlvs = [ ] while buf : tlv_type = LLDPBasicTLV . get_type ( buf ) tlv = cls . _tlv_parsers [ tlv_type ] ( buf ) tlvs . append ( tlv ) offset = LLDP_TLV_SIZE + tlv . len buf = buf [ offset : ] <MASK> break assert len ( buf ) > 0 lldp_pkt = cls ( tlvs ) assert lldp_pkt . _tlvs_len_valid ( ) assert lldp_pkt . _tlvs_valid ( ) return lldp_pkt , None , buf",if tlv . tlv_type == LLDP_TLV_END :,if not tlvs :,93.12521732,FALSE,89.27
615,"def _do_pull ( self , repo , pull_kwargs , silent , ignore_pull_failures ) : try : output = self . client . pull ( repo , * * pull_kwargs ) if silent : with open ( os . devnull , "" w "" ) as devnull : yield from stream_output ( output , devnull ) else : yield from stream_output ( output , sys . stdout ) except ( StreamOutputError , NotFound ) as e : <MASK> raise else : log . error ( str ( e ) )",if not ignore_pull_failures :,if ignore_pull_failures :,70.42985499,FALSE,97.71
616,def _collect_bytecode ( ordered_code ) : bytecode_blocks = [ ] stack = [ ordered_code ] while stack : code = stack . pop ( ) bytecode_blocks . append ( code . co_code ) for const in code . co_consts : <MASK> stack . append ( const ) return bytecode_blocks,"if isinstance ( const , blocks . OrderedCode ) :",if not _is_bytecode ( const ) :,88.21501144,FALSE,89.37
617,"def displayhook ( value ) : if value is None : return builtins = modules [ "" builtins "" ] # Set '_' to None to avoid recursion builtins . _ = None text = repr ( value ) try : local_stdout = stdout except NameError as e : raise RuntimeError ( "" lost sys.stdout "" ) from e try : local_stdout . write ( text ) except UnicodeEncodeError : bytes = text . encode ( local_stdout . encoding , "" backslashreplace "" ) <MASK> local_stdout . buffer . write ( bytes ) else : text = bytes . decode ( local_stdout . encoding , "" strict "" ) local_stdout . write ( text ) local_stdout . write ( "" \n "" ) builtins . _ = value","if hasattr ( local_stdout , ""buffer"" ) :",if PY3 :,97.26621966,FALSE,92.53
618,"def _analyze ( self ) : lines = open ( self . log_path , "" r "" ) . readlines ( ) prev_line = None for line in lines : if line . startswith ( "" ERROR: "" ) and prev_line and prev_line . startswith ( "" = "" ) : self . errors . append ( line [ len ( "" ERROR: "" ) : ] . strip ( ) ) <MASK> self . failures . append ( line [ len ( "" FAIL: "" ) : ] . strip ( ) ) prev_line = line","elif line . startswith ( ""FAIL:"" ) and prev_line and prev_line . startswith ( ""="" ) :","if line . startswith ( ""FAIL:"" ) and line . startswith ( ""="" ) :",88.5626453,FALSE,92.49
619,"def _flush ( self ) : if self . _data : if self . _last is not None : text = "" "" . join ( self . _data ) <MASK> assert self . _last . tail is None , "" internal error (tail) "" self . _last . tail = text else : assert self . _last . text is None , "" internal error (text) "" self . _last . text = text self . _data = [ ]",if self . _tail :,if self . _last . tail is not None :,76.26837084,FALSE,92.9
620,"def write ( self , chunk ) : consumer = self . _current_consumer server_side = consumer . server_side if server_side : server_side . data_received ( chunk ) else : consumer . message + = chunk assert consumer . in_parser . execute ( chunk , len ( chunk ) ) == len ( chunk ) <MASK> consumer . finished ( )",if consumer . in_parser . is_message_complete ( ) :,if not consumer . in_parser . execute ( chunk ) :,93.31399455,FALSE,90.04
621,"def _api_change_cat ( name , output , kwargs ) : """"""API: accepts output, value(=nzo_id), value2(=category)"""""" value = kwargs . get ( "" value "" ) value2 = kwargs . get ( "" value2 "" ) if value and value2 : nzo_id = value cat = value2 <MASK> cat = None result = sabnzbd . NzbQueue . change_cat ( nzo_id , cat ) return report ( output , keyword = "" status "" , data = bool ( result > 0 ) ) else : return report ( output , _MSG_NO_VALUE )","if cat == ""None"" :","if nzo_id == ""category"" :",87.32631724,FALSE,94.73
622,"def get_allocated_address ( self , config : ActorPoolConfig , allocated : allocated_type ) - > str : addresses = config . get_external_addresses ( label = self . label ) for addr in addresses : occupied = False for strategy , _ in allocated . get ( addr , dict ( ) ) . values ( ) : <MASK> occupied = True break if not occupied : return addr raise NoIdleSlot ( f "" No idle slot for creating actor  "" f "" with label  { self . label } , mark  { self . mark } "" )",if strategy == self :,"if strategy == ""idle"" :",95.75161248,FALSE,96.02
623,"def schedule_logger ( job_id = None , delete = False ) : if not job_id : return getLogger ( "" fate_flow_schedule "" ) else : <MASK> with LoggerFactory . lock : try : for key in LoggerFactory . schedule_logger_dict . keys ( ) : if job_id in key : del LoggerFactory . schedule_logger_dict [ key ] except : pass return True key = job_id + "" schedule "" if key in LoggerFactory . schedule_logger_dict : return LoggerFactory . schedule_logger_dict [ key ] return LoggerFactory . get_schedule_logger ( job_id )",if delete :,if delete :,100,TRUE,100
624,"def quick_load ( tool_file , async_load = True ) : try : tool = self . load_tool ( tool_file , tool_cache_data_dir ) self . __add_tool ( tool , load_panel_dict , elems ) # Always load the tool into the integrated_panel_dict, or it will not be included in the integrated_tool_panel.xml file. key = "" tool_ %s "" % str ( tool . id ) integrated_elems [ key ] = tool <MASK> self . _load_tool_panel ( ) self . _save_integrated_tool_panel ( ) return tool . id except Exception : log . exception ( "" Failed to load potential tool  %s . "" , tool_file ) return None",if async_load :,if async_load :,100,TRUE,100
625,"def _get_default_ordering ( self ) : try : ordering = super ( DocumentChangeList , self ) . _get_default_ordering ( ) except AttributeError : ordering = [ ] if self . model_admin . ordering : ordering = self . model_admin . ordering <MASK> ordering = self . lookup_opts . ordering return ordering",elif self . lookup_opts . ordering :,elif self . lookup_opts :,77.4396081,FALSE,95.22
626,"def names ( self , persistent = None ) : u = set ( ) result = [ ] for s in [ self . __storage ( None ) , self . __storage ( self . __category ) , ] : for b in s : <MASK> continue if b . name . startswith ( "" __ "" ) : continue if b . name not in u : result . append ( b . name ) u . add ( b . name ) return result",if persistent is not None and b . persistent != persistent :,if b . persistent and b . name in persistent :,92.31005459,FALSE,91.09
627,"def common_check_get_messages_query ( self , query_params : Dict [ str , object ] , expected : str ) - > None : user_profile = self . example_user ( "" hamlet "" ) request = POSTRequestMock ( query_params , user_profile ) with queries_captured ( ) as queries : get_messages_backend ( request , user_profile ) for query in queries : <MASK> sql = str ( query [ "" sql "" ] ) . replace ( ""  /* get_messages */ "" , "" "" ) self . assertEqual ( sql , expected ) return raise AssertionError ( "" get_messages query not found "" )","if ""/* get_messages */"" in query [ ""sql"" ] :","if query [ ""sql"" ] :",93.85748535,FALSE,92.09
628,"def _activate_only_current_top_active ( ) : for i in range ( 0 , len ( current_sequence ( ) . tracks ) - 1 ) : <MASK> current_sequence ( ) . tracks [ i ] . active = True else : current_sequence ( ) . tracks [ i ] . active = False gui . tline_column . widget . queue_draw ( )",if i == current_sequence ( ) . get_first_active_track ( ) . id :,if current_sequence ( ) . tracks [ i ] . active :,84.78931162,FALSE,83.2
629,"def http_wrapper ( self , url , postdata = { } ) : try : if postdata != { } : f = urllib . urlopen ( url , postdata ) else : f = urllib . urlopen ( url ) response = f . read ( ) except : import traceback import logging , sys cla , exc , tb = sys . exc_info ( ) logging . error ( url ) <MASK> logging . error ( "" with post data "" ) else : logging . error ( "" without post data "" ) logging . error ( exc . args ) logging . error ( traceback . format_tb ( tb ) ) response = "" "" return response",if postdata :,"if cla . args [ 0 ] == ""post"" :",93.05206587,FALSE,90.96
630,"def frequent_thread_switches ( ) : """"""Make concurrency bugs more likely to manifest."""""" interval = None <MASK> if hasattr ( sys , "" getswitchinterval "" ) : interval = sys . getswitchinterval ( ) sys . setswitchinterval ( 1e-6 ) else : interval = sys . getcheckinterval ( ) sys . setcheckinterval ( 1 ) try : yield finally : <MASK> if hasattr ( sys , "" setswitchinterval "" ) : sys . setswitchinterval ( interval ) else : sys . setcheckinterval ( interval )","if not sys . platform . startswith ( ""java"" ) :",if interval is not None :,71.92568402,FALSE,78.57
631,"def iter_filters ( filters , block_end = False ) : queue = deque ( filters ) while queue : f = queue . popleft ( ) <MASK> if block_end : queue . appendleft ( None ) for gf in f . filters : queue . appendleft ( gf ) yield f","if f is not None and f . type in ( ""or"" , ""and"" , ""not"" ) :","if isinstance ( f , Filter ) :",79.60578112,FALSE,70.61
632,"def smartsplit ( code ) : """"""Split `code` at "" symbol, only if it is not escaped."""""" strings = [ ] pos = 0 while pos < len ( code ) : <MASK> word = "" "" # new word pos + = 1 while pos < len ( code ) : <MASK> break if code [ pos ] == "" \\ "" : word + = "" \\ "" pos + = 1 word + = code [ pos ] pos + = 1 strings . append ( ' "" %s "" ' % word ) pos + = 1 return strings","if code [ pos ] == '""' :","if code [ pos ] == '""' :",100,TRUE,100
633,"def get_folder_content ( cls , name ) : """"""Return (folders, files) for the given folder in the root dir."""""" folders = set ( ) files = set ( ) for path in cls . LAYOUT : <MASK> continue parts = path . split ( "" / "" ) if len ( parts ) == 2 : files . add ( parts [ 1 ] ) else : folders . add ( parts [ 1 ] ) folders = list ( folders ) folders . sort ( ) files = list ( files ) files . sort ( ) return ( folders , files )","if not path . startswith ( name + ""/"" ) :",if not path . startswith ( name ) :,92.60871826,FALSE,95.73
634,"def array_for ( self , i ) : if 0 < = i < self . _cnt : <MASK> return self . _tail node = self . _root level = self . _shift while level > 0 : assert isinstance ( node , Node ) node = node . _array [ ( i >> level ) & 0x01F ] level - = 5 assert isinstance ( node , Node ) return node . _array affirm ( False , u "" Index out of Range "" )",if i >= self . tailoff ( ) :,if self . _array [ ( i >> 0x01F ) & 0x01F,60.82438192,FALSE,88.16
635,"def __or__ ( self , other ) - > "" MultiVector "" : r """"""``self | other``, the inner product :math:`M \cdot N`"""""" other , mv = self . _checkOther ( other ) if mv : newValue = self . layout . imt_func ( self . value , other . value ) else : <MASK> obj = self . __array__ ( ) return obj | other # l * M = M * l = 0 for scalar l return self . _newMV ( dtype = np . result_type ( self . value . dtype , other ) ) return self . _newMV ( newValue )","if isinstance ( other , np . ndarray ) :",if self . layout . imt_func is not None :,89.08904869,FALSE,92.36
636,"def parse_bzr_stats ( status ) : stats = RepoStats ( ) statustype = "" changed "" for statusline in status : if statusline [ : 2 ] == ""    "" : setattr ( stats , statustype , getattr ( stats , statustype ) + 1 ) <MASK> statustype = "" staged "" elif statusline == "" unknown: "" : statustype = "" new "" else : # removed, missing, renamed, modified or kind changed statustype = "" changed "" return stats","elif statusline == ""added:"" :","elif statusline [ : 2 ] == ""staged:"" :",94.63093076,FALSE,92.06
637,"def write ( self , timestamps , actualValues , predictedValues , predictionStep = 1 ) : assert len ( timestamps ) == len ( actualValues ) == len ( predictedValues ) for index in range ( len ( self . names ) ) : timestamp = timestamps [ index ] actual = actualValues [ index ] prediction = predictedValues [ index ] writer = self . outputWriters [ index ] <MASK> outputRow = [ timestamp , actual , prediction ] writer . writerow ( outputRow ) self . lineCounts [ index ] + = 1",if timestamp is not None :,if predictionStep :,89.1245601,FALSE,94.51
638,"def clean ( self ) : """"""Delete old files in ""tmp""."""""" now = time . time ( ) for entry in os . listdir ( os . path . join ( self . _path , "" tmp "" ) ) : path = os . path . join ( self . _path , "" tmp "" , entry ) <MASK> # 60 * 60 * 36 os . remove ( path )",if now - os . path . getatime ( path ) > 129600 :,if time . time ( ) - now > 60 * 60 * 36 :,85.14049286,FALSE,86.27
639,"def _get_info ( self , path ) : info = OrderedDict ( ) if not self . _is_mac ( ) or self . _has_xcode_tools ( ) : stdout = None try : stdout , stderr = Popen ( [ self . _find_binary ( ) , "" info "" , os . path . realpath ( path ) ] , stdout = PIPE , stderr = PIPE , ) . communicate ( ) except OSError : pass else : <MASK> for line in stdout . splitlines ( ) : line = u ( line ) . split ( "" :  "" , 1 ) if len ( line ) == 2 : info [ line [ 0 ] ] = line [ 1 ] return info",if stdout :,if stdout :,100,TRUE,100
640,"def add ( meta_list , info_list = None ) : if not info_list : info_list = meta_list if not isinstance ( meta_list , ( list , tuple ) ) : meta_list = ( meta_list , ) if not isinstance ( info_list , ( list , tuple ) ) : info_list = ( info_list , ) for info_f in info_list : <MASK> for meta_f in meta_list : metadata [ meta_f ] = info [ info_f ] break",if info . get ( info_f ) is not None :,if info_f not in metadata :,89.03785427,FALSE,91.63
641,"def _compute_log_r ( model_trace , guide_trace ) : log_r = MultiFrameTensor ( ) stacks = get_plate_stacks ( model_trace ) for name , model_site in model_trace . nodes . items ( ) : <MASK> log_r_term = model_site [ "" log_prob "" ] if not model_site [ "" is_observed "" ] : log_r_term = log_r_term - guide_trace . nodes [ name ] [ "" log_prob "" ] log_r . add ( ( stacks [ name ] , log_r_term . detach ( ) ) ) return log_r","if model_site [ ""type"" ] == ""sample"" :","if ""log_prob"" in guide_trace . nodes [ name ] :",90.80421861,FALSE,90.49
642,"def pickline ( file , key , casefold = 1 ) : try : f = open ( file , "" r "" ) except IOError : return None pat = re . escape ( key ) + "" : "" prog = re . compile ( pat , casefold and re . IGNORECASE ) while 1 : line = f . readline ( ) if not line : break if prog . match ( line ) : text = line [ len ( key ) + 1 : ] while 1 : line = f . readline ( ) <MASK> break text = text + line return text . strip ( ) return None",if not line or not line [ 0 ] . isspace ( ) :,if not line :,91.05148491,FALSE,90.68
643,"def build_iterator ( data , infinite = True ) : """"""Build the iterator for inputs."""""" index = 0 size = len ( data [ 0 ] ) while True : if index + batch_size > size : <MASK> index = 0 else : return yield data [ 0 ] [ index : index + batch_size ] , data [ 1 ] [ index : index + batch_size ] index + = batch_size",if infinite :,if infinite :,100,TRUE,100
644,"def checkall ( g , bg , dst_nodes , include_dst_in_src = True ) : for etype in g . etypes : ntype = g . to_canonical_etype ( etype ) [ 2 ] <MASK> check ( g , bg , ntype , etype , dst_nodes [ ntype ] , include_dst_in_src ) else : check ( g , bg , ntype , etype , None , include_dst_in_src )",if dst_nodes is not None and ntype in dst_nodes :,if ntype in dst_nodes :,79.4926992,FALSE,91.64
645,"def minimalBases ( classes ) : """"""Reduce a list of base classes to its ordered minimum equivalent"""""" if not __python3 : # pragma: no cover classes = [ c for c in classes if c is not ClassType ] candidates = [ ] for m in classes : for n in classes : if issubclass ( n , m ) and m is not n : break else : # m has no subclasses in 'classes' <MASK> candidates . remove ( m ) # ensure that we're later in the list candidates . append ( m ) return candidates",if m in candidates :,if m in candidates :,75,TRUE,100
646,"def __keep_songs_enable ( self , enabled ) : config . set ( "" memory "" , "" queue_keep_songs "" , enabled ) if enabled : self . queue . set_first_column_type ( CurrentColumn ) else : for col in self . queue . get_columns ( ) : # Remove the CurrentColum if it exists <MASK> self . queue . set_first_column_type ( None ) break","if isinstance ( col , CurrentColumn ) :",if col . name == CurrentColum . name :,94.4973614,FALSE,90.18
647,"def outlineView_heightOfRowByItem_ ( self , tree , item ) - > float : default_row_height = self . rowHeight if item is self : return default_row_height heights = [ default_row_height ] for column in self . tableColumns : value = getattr ( item . attrs [ "" node "" ] , str ( column . identifier ) ) <MASK> # if the cell value is a widget, use its height heights . append ( value . _impl . native . intrinsicContentSize ( ) . height ) return max ( heights )","if isinstance ( value , toga . Widget ) :","if isinstance ( value , Widget ) :",93.88515268,FALSE,96.94
648,"def condition ( self ) : if self . __condition is None : <MASK> # Avoid an extra indirection in the common case of only one condition. self . __condition = self . flat_conditions [ 0 ] elif len ( self . flat_conditions ) == 0 : # Possible, if unlikely, due to filter predicate rewriting self . __condition = lambda _ : True else : self . __condition = lambda x : all ( cond ( x ) for cond in self . flat_conditions ) return self . __condition",if len ( self . flat_conditions ) == 1 :,if len ( self . flat_conditions ) == 1 :,100,TRUE,100
649,"def _find_delimiter ( f , block_size = 2 * * 16 ) : delimiter = b "" \n "" if f . tell ( ) == 0 : return 0 while True : b = f . read ( block_size ) if not b : return f . tell ( ) <MASK> return f . tell ( ) - len ( b ) + b . index ( delimiter ) + 1",elif delimiter in b :,if b [ 0 ] == delimiter :,92.57879311,FALSE,89.66
650,"def serialize ( self , name = None ) : data = super ( SimpleText , self ) . serialize ( name ) data [ "" contentType "" ] = self . contentType data [ "" content "" ] = self . content if self . width : <MASK> raise InvalidWidthException ( self . width ) data [ "" inputOptions "" ] = { } data [ "" width "" ] = self . width return data","if self . width not in [ 100 , 50 , 33 , 25 ] :","if self . width not in ( ""0x"" , ""0x7E"" )",89.61670498,FALSE,87.54
651,"def inference ( self ) : self . attention_weight_dim = self . input_dims [ 0 ] [ - 1 ] if self . keep_dim : self . output_dim = copy . deepcopy ( self . input_dims [ 0 ] ) else : self . output_dim = [ ] for idx , dim in enumerate ( self . input_dims [ 0 ] ) : <MASK> self . output_dim . append ( dim ) super ( LinearAttentionConf , self ) . inference ( ) # PUT THIS LINE AT THE END OF inference()",if idx != len ( self . input_dims [ 0 ] ) - 2 :,if dim . is_constant ( ) and dim . is_constant ( ) :,90.10439506,FALSE,86.92
652,"def __delete_hook ( self , rpc ) : try : rpc . check_success ( ) except apiproxy_errors . Error : return None result = [ ] for status in rpc . response . delete_status_list ( ) : if status == MemcacheDeleteResponse . DELETED : result . append ( DELETE_SUCCESSFUL ) <MASK> result . append ( DELETE_ITEM_MISSING ) else : result . append ( DELETE_NETWORK_FAILURE ) return result",elif status == MemcacheDeleteResponse . NOT_FOUND :,elif status == MemcacheDeleteResponse . NOT_FOUND :,100,TRUE,100
653,def identify_page_at_cursor ( self ) : for region in self . view . sel ( ) : text_on_cursor = None pos = region . begin ( ) scope_region = self . view . extract_scope ( pos ) <MASK> text_on_cursor = self . view . substr ( scope_region ) return text_on_cursor . strip ( string . punctuation ) return None,if not scope_region . empty ( ) :,if scope_region :,82.35380289,FALSE,91.03
654,"def from_elem ( cls , parent , when_elem ) : """"""Loads the proper when by attributes of elem"""""" when_value = when_elem . get ( "" value "" , None ) <MASK> return ValueToolOutputActionConditionalWhen ( parent , when_elem , when_value ) else : when_value = when_elem . get ( "" datatype_isinstance "" , None ) <MASK> return DatatypeIsInstanceToolOutputActionConditionalWhen ( parent , when_elem , when_value ) raise TypeError ( "" When type not implemented "" )",if when_value is not None :,if when_value is not None :,100,TRUE,100
655,"def test_insert_entity_empty_string_rk ( self , tables_cosmos_account_name , tables_primary_cosmos_account_key ) : # Arrange await self . _set_up ( tables_cosmos_account_name , tables_primary_cosmos_account_key ) try : entity = { "" PartitionKey "" : "" pk "" , "" RowKey "" : "" "" } # Act with pytest . raises ( HttpResponseError ) : await self . table . create_entity ( entity = entity ) # Assert #  assert resp is None finally : await self . _tear_down ( ) <MASK> sleep ( SLEEP_DELAY )",if self . is_live :,if self . is_live :,75,TRUE,100
656,"def provider_uris ( self ) : login_urls = { } continue_url = self . request . get ( "" continue_url "" ) for provider in self . provider_info : <MASK> login_url = self . uri_for ( "" social-login "" , provider_name = provider , continue_url = continue_url ) else : login_url = self . uri_for ( "" social-login "" , provider_name = provider ) login_urls [ provider ] = login_url return login_urls",if continue_url :,if continue_url :,100,TRUE,100
657,"def expand_extensions ( existing ) : for name in extension_names : ext = ( im ( "" lizard_ext.lizard "" + name . lower ( ) ) . LizardExtension ( ) <MASK> else name ) existing . insert ( len ( existing ) if not hasattr ( ext , "" ordering_index "" ) else ext . ordering_index , ext , ) return existing","if isinstance ( name , str )","if not hasattr ( ext , ""ordering_index"" )",80.98092552,FALSE,86.76
658,"def wrapper ( self , * args , * * kwargs ) : if not self . request . path . endswith ( "" / "" ) : if self . request . method in ( "" GET "" , "" HEAD "" ) : uri = self . request . path + "" / "" <MASK> uri + = "" ? "" + self . request . query self . redirect ( uri , permanent = True ) return raise HTTPError ( 404 ) return method ( self , * args , * * kwargs )",if self . request . query :,if self . request . query :,100,TRUE,100
659,"def subword_map_by_joiner ( subwords , marker = SubwordMarker . JOINER ) : """"""Return word id for each subword token (annotate by joiner)."""""" flags = [ 0 ] * len ( subwords ) for i , tok in enumerate ( subwords ) : <MASK> flags [ i ] = 1 if tok . startswith ( marker ) : assert i > = 1 and flags [ i - 1 ] != 1 , "" Sentence ` {} ` not correct! "" . format ( ""   "" . join ( subwords ) ) flags [ i - 1 ] = 1 marker_acc = list ( accumulate ( [ 0 ] + flags [ : - 1 ] ) ) word_group = [ ( i - maker_sofar ) for i , maker_sofar in enumerate ( marker_acc ) ] return word_group",if tok . endswith ( marker ) :,"if tok . startswith ( ""word"" ) :",93.67477629,FALSE,96.41
660,"def next_item ( self , direction ) : """"""Selects next menu item, based on self._direction"""""" start , i = - 1 , 0 try : start = self . items . index ( self . _selected ) i = start + direction except : pass while True : if i == start : # Cannot find valid menu item self . select ( start ) break if i > = len ( self . items ) : i = 0 continue if i < 0 : i = len ( self . items ) - 1 continue <MASK> break i + = direction if start < 0 : start = 0",if self . select ( i ) :,if i == 0 :,96.96448606,FALSE,94.43
661,"def get_config ( cls ) : # FIXME: Replace this as soon as we have a config module config = { } # Try to get iflytek_yuyin config from config profile_path = dingdangpath . config ( "" profile.yml "" ) if os . path . exists ( profile_path ) : with open ( profile_path , "" r "" ) as f : profile = yaml . safe_load ( f ) <MASK> if "" vid "" in profile [ "" iflytek_yuyin "" ] : config [ "" vid "" ] = profile [ "" iflytek_yuyin "" ] [ "" vid "" ] return config","if ""iflytek_yuyin"" in profile :","if ""iflytek_yuyin"" in profile :",75,TRUE,100
662,"def get_signed_in_user ( test_case ) : playback = not ( test_case . is_live or test_case . in_recording ) if playback : return MOCKED_USER_NAME else : account_info = test_case . cmd ( "" account show "" ) . get_output_in_json ( ) <MASK> return account_info [ "" user "" ] [ "" name "" ] return None","if account_info [ ""user"" ] [ ""type"" ] != ""servicePrincipal"" :","if account_info [ ""user"" ] :",91.77541486,FALSE,88.66
663,"def rename_project ( self , project , new_name ) : """"""Rename project, update the related projects if necessary"""""" old_name = project . name for proj in self . projects : relproj = proj . get_related_projects ( ) <MASK> relproj [ relproj . index ( old_name ) ] = new_name proj . set_related_projects ( relproj ) project . rename ( new_name ) self . save ( )",if old_name in relproj :,if old_name in relproj :,100,TRUE,100
664,"def test_call_extern_c_fn ( self ) : global memcmp memcmp = cffi_support . ExternCFunction ( "" memcmp "" , ( "" int memcmp ( const uint8_t * ptr1,  "" "" const uint8_t * ptr2, size_t num ) "" ) , ) @udf ( BooleanVal ( FunctionContext , StringVal , StringVal ) ) def fn ( context , a , b ) : if a . is_null != b . is_null : return False if a is None : return True if len ( a ) != b . len : return False <MASK> return True return memcmp ( a . ptr , b . ptr , a . len ) == 0",if a . ptr == b . ptr :,"if memcmp ( a . ptr , b . ptr ) != 0 :",94.54304204,FALSE,92.42
665,"def parse_variable ( self ) : begin = self . _pos while True : ch = self . read ( ) <MASK> return ScriptVariable ( self . _text [ begin : self . _pos - 1 ] ) elif ch is None : self . __raise_eof ( ) elif not isidentif ( ch ) and ch != "" : "" : self . __raise_char ( ch )","if ch == ""%"" :","if isidentif ( ch ) and ch == "":"" :",93.67721699,FALSE,90.1
666,"def h_file ( self ) : filename = self . abspath ( ) st = os . stat ( filename ) cache = self . ctx . hashes_md5_tstamp if filename in cache and cache [ filename ] [ 0 ] == st . st_mtime : return cache [ filename ] [ 1 ] if STRONGEST : ret = Utils . h_file ( filename ) else : <MASK> raise IOError ( "" Not a file "" ) ret = Utils . md5 ( str ( ( st . st_mtime , st . st_size ) ) . encode ( ) ) . digest ( ) cache [ filename ] = ( st . st_mtime , ret ) return ret",if stat . S_ISDIR ( st [ stat . ST_MODE ] ) :,if not os . path . exists ( filename ) :,90.8345317,FALSE,90
667,"def add_widgets ( self , * widgets_or_spacings ) : """"""Add widgets/spacing to dialog vertical layout"""""" layout = self . layout ( ) for widget_or_spacing in widgets_or_spacings : <MASK> layout . addSpacing ( widget_or_spacing ) else : layout . addWidget ( widget_or_spacing )","if isinstance ( widget_or_spacing , int ) :",if widget_or_spacing . is_fixed ( ) :,87.98517536,FALSE,90.5
668,"def _str_index ( self ) : idx = self [ "" index "" ] out = [ ] if len ( idx ) == 0 : return out out + = [ "" .. index::  %s "" % idx . get ( "" default "" , "" "" ) ] for section , references in idx . iteritems ( ) : <MASK> continue elif section == "" refguide "" : out + = [ ""    single:  %s "" % ( "" ,  "" . join ( references ) ) ] else : out + = [ ""     %s :  %s "" % ( section , "" , "" . join ( references ) ) ] return out","if section == ""default"" :","if section == ""default"" :",100,TRUE,100
669,"def dictify_CPPDEFINES ( env ) : cppdefines = env . get ( "" CPPDEFINES "" , { } ) if cppdefines is None : return { } if SCons . Util . is_Sequence ( cppdefines ) : result = { } for c in cppdefines : <MASK> result [ c [ 0 ] ] = c [ 1 ] else : result [ c ] = None return result if not SCons . Util . is_Dict ( cppdefines ) : return { cppdefines : None } return cppdefines",if SCons . Util . is_Sequence ( c ) :,if SCons . Util . is_Dict ( c ) :,95.82182224,FALSE,97.57
670,"def decoder ( s ) : r = [ ] decode = [ ] for c in s : if c == "" & "" and not decode : decode . append ( "" & "" ) elif c == "" - "" and decode : if len ( decode ) == 1 : r . append ( "" & "" ) else : r . append ( modified_unbase64 ( "" "" . join ( decode [ 1 : ] ) ) ) decode = [ ] <MASK> decode . append ( c ) else : r . append ( c ) if decode : r . append ( modified_unbase64 ( "" "" . join ( decode [ 1 : ] ) ) ) bin_str = "" "" . join ( r ) return ( bin_str , len ( s ) )",elif decode :,"elif c in ( ""+"" , ""-"" ) :",90.48350786,FALSE,92.35
671,"def optimize ( self , graph : Graph ) : MAX_TEXTURE_SIZE = config . WEBGL_MAX_TEXTURE_SIZE flag_changed = False for v in traverse . listup_variables ( graph ) : <MASK> continue height , width = TextureShape . get ( v ) if height < = MAX_TEXTURE_SIZE and width < = MAX_TEXTURE_SIZE : continue if not v . has_attribute ( SplitTarget ) : flag_changed = True v . attributes . add ( SplitTarget ( ) ) return graph , flag_changed",if not Placeholder . check_resolved ( v . size ) :,if v . has_attribute ( ImageShape ) :,91.24087391,FALSE,91.15
672,"def one_gpr_reg_one_mem_scalable ( ii ) : n , r = 0 , 0 for op in _gen_opnds ( ii ) : if op_agen ( op ) or ( op_mem ( op ) and op . oc2 in [ "" v "" ] ) : n + = 1 <MASK> r + = 1 else : return False return n == 1 and r == 1",elif op_gprv ( op ) :,"elif op_reg ( op ) and op . oc2 in [ ""gpr""",86.56465236,FALSE,86.6
673,"def get_genome_dir ( gid , galaxy_dir , data ) : """"""Return standard location of genome directories."""""" if galaxy_dir : refs = genome . get_refs ( gid , None , galaxy_dir , data ) seq_file = tz . get_in ( [ "" fasta "" , "" base "" ] , refs ) if seq_file and os . path . exists ( seq_file ) : return os . path . dirname ( os . path . dirname ( seq_file ) ) else : gdirs = glob . glob ( os . path . join ( _get_data_dir ( ) , "" genomes "" , "" * "" , gid ) ) <MASK> return gdirs [ 0 ]",if len ( gdirs ) == 1 and os . path . exists ( gdirs [ 0 ] ) :,if gdirs :,84.97339693,FALSE,87.34
674,"def __modules ( self ) : raw_output = self . __module_avail_output ( ) . decode ( "" utf-8 "" ) for line in StringIO ( raw_output ) : line = line and line . strip ( ) if not line or line . startswith ( "" - "" ) : continue line_modules = line . split ( ) for module in line_modules : <MASK> module = module [ 0 : - len ( self . default_indicator ) ] . strip ( ) module_parts = module . split ( "" / "" ) module_version = None if len ( module_parts ) == 2 : module_version = module_parts [ 1 ] module_name = module_parts [ 0 ] yield module_name , module_version",if module . endswith ( self . default_indicator ) :,if module . endswith ( self . default_indicator ) :,100,TRUE,100
675,"def save ( self ) : updates = self . cinder_obj_get_changes ( ) if updates : <MASK> metadata = updates . pop ( "" metadata "" , None ) self . metadata = db . backup_metadata_update ( self . _context , self . id , metadata , True ) updates . pop ( "" parent "" , None ) db . backup_update ( self . _context , self . id , updates ) self . obj_reset_changes ( )","if ""metadata"" in updates :",if self . metadata :,92.99137967,FALSE,93.76
676,"def test_set_tag ( association_obj , sagemaker_session ) : tag = { "" Key "" : "" foo "" , "" Value "" : "" bar "" } association_obj . set_tag ( tag ) while True : actual_tags = sagemaker_session . sagemaker_client . list_tags ( ResourceArn = association_obj . source_arn ) [ "" Tags "" ] <MASK> break time . sleep ( 5 ) # When sagemaker-client-config endpoint-url is passed as argument to hit some endpoints, # length of actual tags will be greater than 1 assert len ( actual_tags ) > 0 assert actual_tags [ 0 ] == tag",if actual_tags :,if actual_tags :,100,TRUE,100
677,"def test_error_stream ( environ , start_response ) : writer = start_response ( "" 200 OK "" , [ ] ) wsgi_errors = environ [ "" wsgi.errors "" ] error_msg = None for method in [ "" flush "" , "" write "" , "" writelines "" , ] : if not hasattr ( wsgi_errors , method ) : error_msg = "" wsgi.errors has no  ' %s '  attr "" % method <MASK> error_msg = "" wsgi.errors. %s  attr is not callable "" % method if error_msg : break return_msg = error_msg or "" success "" writer ( return_msg ) return [ ]","if not error_msg and not callable ( getattr ( wsgi_errors , method ) ) :",elif not callable ( wsgi_errors [ method ] ) :,89.57703418,FALSE,90.45
678,"def current_dict ( cursor_offset , line ) : """"""If in dictionary completion, return the dict that should be used"""""" for m in current_dict_re . finditer ( line ) : <MASK> return LinePart ( m . start ( 1 ) , m . end ( 1 ) , m . group ( 1 ) ) return None",if m . start ( 2 ) <= cursor_offset and m . end ( 2 ) >= cursor_offset :,if m . start ( 1 ) <= cursor_offset and m . end ( 1 ) >=,83.85039428,FALSE,88.63
679,"def show_file_browser ( self ) : """"""Show/hide the file browser."""""" if self . show_file_browser_action . isChecked ( ) : sizes = self . panel . sizes ( ) <MASK> sizes [ 0 ] = sum ( sizes ) / / 4 self . panel . setSizes ( sizes ) self . file_browser . show ( ) else : self . file_browser . hide ( )",if sizes [ 0 ] == 0 :,if sizes :,84.72648987,FALSE,91.94
680,"def run ( self , paths = [ ] ) : items = [ ] for item in SideBarSelection ( paths ) . getSelectedItems ( ) : items . append ( item . nameEncoded ( ) ) if len ( items ) > 0 : sublime . set_clipboard ( "" \n "" . join ( items ) ) <MASK> sublime . status_message ( "" Items copied "" ) else : sublime . status_message ( "" Item copied "" )",if len ( items ) > 1 :,if len ( items ) > 1 :,100,TRUE,100
681,"def prepend ( self , value ) : """"""prepend value to nodes"""""" root , root_text = self . _get_root ( value ) for i , tag in enumerate ( self ) : <MASK> tag . text = "" "" if len ( root ) > 0 : root [ - 1 ] . tail = tag . text tag . text = root_text else : tag . text = root_text + tag . text if i > 0 : root = deepcopy ( list ( root ) ) tag [ : 0 ] = root root = tag [ : len ( root ) ] return self",if not tag . text :,"if tag . text == """" :",91.7130767,FALSE,94.52
682,"def getLabel ( self , address = None ) : if address is None : address = self . address label = address if shared . config . has_section ( address ) : label = shared . config . get ( address , "" label "" ) queryreturn = sqlQuery ( """""" select label from addressbook where address=? """""" , address ) <MASK> for row in queryreturn : ( label , ) = row else : queryreturn = sqlQuery ( """"""select label from subscriptions where address=?"""""" , address ) <MASK> for row in queryreturn : ( label , ) = row return label",if queryreturn != [ ] :,if label is None :,88.47570518,FALSE,89.92
683,"def _parse ( self , engine ) : """"""Parse the layer."""""" if isinstance ( self . args , dict ) : if "" axis "" in self . args : self . axis = engine . evaluate ( self . args [ "" axis "" ] , recursive = True ) if not isinstance ( self . axis , int ) : raise ParsingError ( ' "" axis ""  must be an integer. ' ) <MASK> self . momentum = engine . evaluate ( self . args [ "" momentum "" ] , recursive = True ) if not isinstance ( self . momentum , ( int , float ) ) : raise ParsingError ( ' "" momentum ""  must be numeric. ' )","if ""momentum"" in self . args :","if ""momentum"" in self . args :",100,TRUE,100
684,"def urlquote ( * args , * * kwargs ) : new_kwargs = dict ( kwargs ) if not PY3 : new_kwargs = dict ( kwargs ) if "" encoding "" in new_kwargs : del new_kwargs [ "" encoding "" ] <MASK> del new_kwargs [ "" errors "" ] return quote ( * args , * * new_kwargs )","if ""errors"" in kwargs :","if ""errors"" in new_kwargs :",96.19265927,FALSE,95.45
685,"def setNextFormPrevious ( self , backup = STARTING_FORM ) : try : if self . _THISFORM . FORM_NAME == self . _FORM_VISIT_LIST [ - 1 ] : self . _FORM_VISIT_LIST . pop ( ) # Remove the current form. if it is at the end of the list <MASK> # take no action if it looks as if someone has already set the next form. self . setNextForm ( self . _FORM_VISIT_LIST . pop ( ) ) # Switch to the previous form if one exists except IndexError : self . setNextForm ( backup )",if self . _THISFORM . FORM_NAME == self . NEXT_ACTIVE_FORM :,if self . _THISFORM .FORM_NAME in self . _FORM_VISIT,96.32276102,FALSE,92.7
686,"def iter_chars_to_words ( self , chars ) : current_word = [ ] for char in chars : if not self . keep_blank_chars and char [ "" text "" ] . isspace ( ) : if current_word : yield current_word current_word = [ ] <MASK> yield current_word current_word = [ char ] else : current_word . append ( char ) if current_word : yield current_word","elif current_word and self . char_begins_new_word ( current_word , char ) :","elif char [ ""text"" ] . isspace ( ) :",72.59126795,FALSE,81.98
687,"def get ( self ) : """"""return a secret by name"""""" results = self . _get ( "" secrets "" , self . name ) results [ "" decoded "" ] = { } results [ "" exists "" ] = False if results [ "" returncode "" ] == 0 and results [ "" results "" ] [ 0 ] : results [ "" exists "" ] = True <MASK> if "" data "" in results [ "" results "" ] [ 0 ] : for sname , value in results [ "" results "" ] [ 0 ] [ "" data "" ] . items ( ) : results [ "" decoded "" ] [ sname ] = base64 . b64decode ( value ) if results [ "" returncode "" ] != 0 and ' "" %s ""  not found ' % self . name in results [ "" stderr "" ] : results [ "" returncode "" ] = 0 return results",if self . decode :,"if results [ ""returncode"" ] == 0 :",96.8391139,FALSE,94.27
688,"def insert_use ( self , edit ) : if self . is_first_use ( ) : for location in [ r "" ^ \ s*namespace \ s+[ \ w \\ ]+[; { ] "" , r "" < \ ?php "" ] : inserted = self . insert_first_use ( location , edit ) <MASK> break else : self . insert_use_among_others ( edit )",if inserted :,if not inserted :,92.35633549,FALSE,97.15
689,"def _new_rsa_key ( spec ) : if "" name "" not in spec : <MASK> ( head , tail ) = os . path . split ( spec [ "" key "" ] ) spec [ "" path "" ] = head spec [ "" name "" ] = tail else : spec [ "" name "" ] = spec [ "" key "" ] return rsa_init ( spec )","if ""/"" in spec [ ""key"" ] :","if spec [ ""key"" ] :",93.46732211,FALSE,93.59
690,"def mimeData ( self , indexes ) : if len ( indexes ) == 1 : index = indexes [ 0 ] model = song = index . data ( Qt . UserRole ) <MASK> try : model = song . album except ( ProviderIOError , Exception ) : model = None return ModelMimeData ( model )",if index . column ( ) == Column . album :,if model is None :,61.88901193,FALSE,82.63
691,"def get ( self , url , * * kwargs ) : app , url = self . _prepare_call ( url , kwargs ) if app : if url . endswith ( "" ping "" ) and self . _first_ping : self . _first_ping = False return EmptyCapabilitiesResponse ( ) <MASK> return ErrorApiResponse ( ) else : response = app . get ( url , * * kwargs ) return TestingResponse ( response ) else : return requests . get ( url , * * kwargs )","elif ""Hello0"" in url and ""1.2.1"" in url and ""v1"" in url :","elif url . endswith ( ""error"" ) :",60.64750846,FALSE,83.98
692,"def handle_noargs ( self , * * options ) : self . style = color_style ( ) print ( "" Running Django ' s own validation: "" ) self . validate ( display_num_errors = True ) for model in loading . get_models ( ) : if hasattr ( model , "" _create_content_base "" ) : self . validate_base_model ( model ) <MASK> self . validate_content_type ( model )","if hasattr ( model , ""_feincms_content_models"" ) :","if hasattr ( model , ""_create_content_type"" ) :",98.30068623,FALSE,95.17
693,"def test_rules_widget ( self ) : subreddit = self . reddit . subreddit ( pytest . placeholders . test_subreddit ) widgets = subreddit . widgets with self . use_cassette ( "" TestSubredditWidgets.fetch_widgets "" ) : rules = None for widget in widgets . sidebar : <MASK> rules = widget break assert isinstance ( rules , RulesWidget ) assert rules == rules assert rules . id == rules assert rules . display assert len ( rules ) > 0 assert subreddit == rules . subreddit","if isinstance ( widget , RulesWidget ) :","if isinstance ( widget , RulesWidget ) :",100,TRUE,100
694,"def __init__ ( self , exception ) : message = str ( exception ) with contextlib . suppress ( IndexError ) : underlying_exception = exception . args [ 0 ] <MASK> message = ( "" maximum retries exceeded trying to reach the store. \n "" "" Check your network connection, and check the store  "" "" status at  {} "" . format ( _STORE_STATUS_URL ) ) super ( ) . __init__ ( message = message )","if isinstance ( underlying_exception , urllib3 . exceptions . MaxRetryError ) :","if underlying_exception . args [ 1 ] == ""max_retries"" :",79.73635557,FALSE,86.47
695,"def wrapped ( self , request ) : try : return self . _finished except AttributeError : if self . node_ids : <MASK> log . debug ( "" %s  is still going to be used, not terminating it.  "" "" Still in use on: \n %s "" , self , pprint . pformat ( list ( self . node_ids ) ) , ) return log . debug ( "" Finish called on  %s "" , self ) try : return func ( request ) finally : self . _finished = True",if not request . session . shouldfail and not request . session . shouldstop :,if self . _finished :,82.5544354,FALSE,87.55
696,"def get_min_vertical_scroll ( ) - > int : # Make sure that the cursor line is not below the bottom. # (Calculate how many lines can be shown between the cursor and the .) used_height = 0 prev_lineno = ui_content . cursor_position . y for lineno in range ( ui_content . cursor_position . y , - 1 , - 1 ) : used_height + = get_line_height ( lineno ) <MASK> return prev_lineno else : prev_lineno = lineno return 0",if used_height > height - scroll_offsets_bottom :,if used_height < ui_content . scroll_height :,71.78326005,FALSE,93.11
697,"def cookies ( self ) : # strip cookie_suffix from all cookies in the request, return result cookies = flask . Request . cookies . __get__ ( self ) result = { } desuffixed = { } for key , value in cookies . items ( ) : <MASK> desuffixed [ key [ : - len ( self . cookie_suffix ) ] ] = value else : result [ key ] = value result . update ( desuffixed ) return result",if key . endswith ( self . cookie_suffix ) :,if key . endswith ( self . cookie_suffix ) :,75,TRUE,100
698,"def update_vars ( state1 , state2 ) : ops = [ ] for name in state1 . _fields : state1_vs = getattr ( state1 , name ) <MASK> ops + = [ tf . assign ( _v1 , _v2 ) for _v1 , _v2 in zip ( state1_vs , getattr ( state2 , name ) ) ] else : ops + = [ tf . assign ( state1_vs , getattr ( state2 , name ) ) ] return tf . group ( * ops )","if isinstance ( state1_vs , list ) :","if isinstance ( state1_vs , ( list , tuple ) ) :",75.95693813,FALSE,94.27
699,"def manifest ( self ) : """"""The current manifest dictionary."""""" if self . reload : <MASK> return { } mtime = self . getmtime ( self . manifest_path ) if self . _mtime is None or mtime > self . _mtime : self . _manifest = self . get_manifest ( ) self . _mtime = mtime return self . _manifest",if not self . exists ( self . manifest_path ) :,if not self . manifest_path :,90.58544391,FALSE,92.09
700,"def csvtitle ( self ) : if isinstance ( self . name , six . string_types ) : return ' "" ' + self . name + ' "" ' + char [ "" sep "" ] * ( len ( self . nick ) - 1 ) else : ret = "" "" for i , name in enumerate ( self . name ) : ret = ret + ' "" ' + name + ' "" ' + char [ "" sep "" ] * ( len ( self . nick ) - 1 ) <MASK> ret = ret + char [ "" sep "" ] return ret",if i + 1 != len ( self . name ) :,if i == 0 :,90.41460575,FALSE,90.71
701,"def cache_dst ( self ) : final_dst = None final_linenb = None for linenb , assignblk in enumerate ( self ) : for dst , src in viewitems ( assignblk ) : <MASK> if final_dst is not None : raise ValueError ( "" Multiple destinations! "" ) final_dst = src final_linenb = linenb self . _dst = final_dst self . _dst_linenb = final_linenb return final_dst","if dst . is_id ( ""IRDst"" ) :",if dst is not None and linenb == len ( self . destinations ) - 1 :,91.06937557,FALSE,84.95
702,"def _ProcessName ( self , name , dependencies ) : """"""Retrieve a module name from a node name."""""" module_name , dot , base_name = name . rpartition ( "" . "" ) if dot : <MASK> if module_name in dependencies : dependencies [ module_name ] . add ( base_name ) else : dependencies [ module_name ] = { base_name } else : # If we have a relative import that did not get qualified (usually due # to an empty package_name), don't insert module_name='' into the # dependencies; we get a better error message if we filter it out here # and fail later on. logging . warning ( "" Empty package name:  %s "" , name )",if module_name :,if self . _IsModule ( module_name ) :,87.95175543,FALSE,94.41
703,"def get_aa_from_codonre ( re_aa ) : aas = [ ] m = 0 for i in re_aa : if i == "" [ "" : m = - 1 aas . append ( "" "" ) elif i == "" ] "" : m = 0 continue elif m == - 1 : aas [ - 1 ] = aas [ - 1 ] + i <MASK> aas . append ( i ) return aas",elif m == 0 :,elif m == 1 :,85.89484421,FALSE,97.24
704,"def logic ( ) : count = intbv ( 0 , min = 0 , max = MAXVAL + 1 ) while True : yield clock . posedge , reset . posedge if reset == 1 : count [ : ] = 0 else : flag . next = 0 <MASK> flag . next = 1 count [ : ] = 0 else : count + = 1",if count == MAXVAL :,if reset == 0 :,93.20320239,FALSE,93.37
705,"def _history_define_metric ( self , hkey : str ) - > Optional [ wandb_internal_pb2 . MetricRecord ] : """"""check for hkey match in glob metrics, return defined metric."""""" # Dont define metric for internal metrics if hkey . startswith ( "" _ "" ) : return None for k , mglob in six . iteritems ( self . _metric_globs ) : if k . endswith ( "" * "" ) : <MASK> m = wandb_internal_pb2 . MetricRecord ( ) m . CopyFrom ( mglob ) m . ClearField ( "" glob_name "" ) m . name = hkey return m return None",if hkey . startswith ( k [ : - 1 ] ) :,if hkey in mglob :,70.02577642,FALSE,91.58
706,"def optimize_models ( args , use_cuda , models ) : """"""Optimize ensemble for generation"""""" for model in models : model . make_generation_fast_ ( beamable_mm_beam_size = None if args . no_beamable_mm else args . beam , need_attn = args . print_alignment , ) <MASK> model . half ( ) if use_cuda : model . cuda ( )",if args . fp16 :,if use_half :,91.54206574,FALSE,94.95
707,"def _Dynamic_Rollback ( self , transaction , transaction_response ) : txid = transaction . handle ( ) self . __local_tx_lock . acquire ( ) try : <MASK> raise apiproxy_errors . ApplicationError ( datastore_pb . Error . BAD_REQUEST , "" Transaction  %d  not found. "" % ( txid , ) ) txdata = self . __transactions [ txid ] assert ( txdata . thread_id == thread . get_ident ( ) ) , "" Transactions are single-threaded. "" del self . __transactions [ txid ] finally : self . __local_tx_lock . release ( )",if txid not in self . __transactions :,if txid not in self . __transactions :,100,TRUE,100
708,"def get_job_dirs ( path ) : regex = re . compile ( "" [1-9][0-9]*- "" ) jobdirs = [ ] for d in os . listdir ( path ) : # skip directories not matching the job result dir pattern <MASK> continue d = os . path . join ( options . resultsdir , d ) if os . path . isdir ( d ) and not os . path . exists ( os . path . join ( d , PUBLISH_FLAGFILE ) ) : jobdirs . append ( d ) return jobdirs",if not regex . match ( d ) :,if not regex . search ( d ) :,98.75230434,FALSE,97.84
709,"def traverse ( node , functions = [ ] ) : if hasattr ( node , "" grad_fn "" ) : node = node . grad_fn if hasattr ( node , "" variable "" ) : node = graph . nodes_by_id . get ( id ( node . variable ) ) if node : node . functions = list ( functions ) del functions [ : ] if hasattr ( node , "" next_functions "" ) : functions . append ( type ( node ) . __name__ ) for f in node . next_functions : <MASK> functions . append ( type ( f [ 0 ] ) . __name__ ) traverse ( f [ 0 ] , functions ) if hasattr ( node , "" saved_tensors "" ) : for t in node . saved_tensors : traverse ( t )",if f [ 0 ] :,"if hasattr ( f [ 0 ] , ""__name__"" ) :",97.03128773,FALSE,92.14
710,"def get_all_snap_points ( self , forts ) : points = [ ] radius = Constants . MAX_DISTANCE_FORT_IS_REACHABLE for i in range ( 0 , len ( forts ) ) : for j in range ( i + 1 , len ( forts ) ) : c1 , c2 = self . get_enclosing_circles ( forts [ i ] , forts [ j ] , radius ) <MASK> points . append ( ( c1 , c2 , forts [ i ] , forts [ j ] ) ) return points",if c1 and c2 :,if c1 and c2 :,100,TRUE,100
711,"def doDir ( elem ) : for child in elem . childNodes : if not isinstance ( child , minidom . Element ) : continue if child . tagName == "" Directory "" : doDir ( child ) elif child . tagName == "" Component "" : for grandchild in child . childNodes : <MASK> continue if grandchild . tagName != "" File "" : continue files . add ( grandchild . getAttribute ( "" Source "" ) . replace ( os . sep , "" / "" ) )","if not isinstance ( grandchild , minidom . Element ) :","if not isinstance ( grandchild , minidom . Element ) :",100,TRUE,100
712,"def computeLeadingWhitespaceWidth ( s , tab_width ) : w = 0 for ch in s : if ch == ""   "" : w + = 1 <MASK> w + = abs ( tab_width ) - ( w % abs ( tab_width ) ) else : break return w","elif ch == ""\t"" :","elif ch == "" "" :",91.44273791,FALSE,94.28
713,"def test_avg_group_by ( self ) : ret = ( await Book . annotate ( avg = Avg ( "" rating "" ) ) . group_by ( "" author_id "" ) . values ( "" author_id "" , "" avg "" ) ) for item in ret : author_id = item . get ( "" author_id "" ) avg = item . get ( "" avg "" ) <MASK> self . assertEqual ( avg , 4.5 ) elif author_id == self . a2 . pk : self . assertEqual ( avg , 2.0 )",if author_id == self . a1 . pk :,if author_id == self . a1 . pk :,100,TRUE,100
714,"def open_session ( self , app , request ) : sid = request . cookies . get ( app . session_cookie_name ) if sid : stored_session = self . cls . objects ( sid = sid ) . first ( ) <MASK> expiration = stored_session . expiration if not expiration . tzinfo : expiration = expiration . replace ( tzinfo = utc ) if expiration > datetime . datetime . utcnow ( ) . replace ( tzinfo = utc ) : return MongoEngineSession ( initial = stored_session . data , sid = stored_session . sid ) return MongoEngineSession ( sid = str ( uuid . uuid4 ( ) ) )",if stored_session :,if stored_session :,100,TRUE,100
715,"def one_line_description ( self ) : MAX_LINE_LENGTH = 120 desc = util . remove_html_tags ( self . description or "" "" ) desc = re . sub ( "" \ s+ "" , ""   "" , desc ) . strip ( ) if not desc : return _ ( "" No description available "" ) else : # Decode the description to avoid gPodder bug 1277 desc = util . convert_bytes ( desc ) . strip ( ) <MASK> return desc [ : MAX_LINE_LENGTH ] + "" ... "" else : return desc",if len ( desc ) > MAX_LINE_LENGTH :,if len ( desc ) > MAX_LINE_LENGTH :,100,TRUE,100
716,"def setInnerHTML ( self , html ) : log . HTMLClassifier . classify ( log . ThugLogging . url if log . ThugOpts . local else log . last_url , html ) self . tag . clear ( ) for node in bs4 . BeautifulSoup ( html , "" html.parser "" ) . contents : self . tag . append ( node ) name = getattr ( node , "" name "" , None ) if name is None : continue handler = getattr ( log . DFT , "" handle_ %s "" % ( name , ) , None ) <MASK> handler ( node )",if handler :,if handler is not None :,97.57040298,FALSE,96.09
717,def get_supported_period_type_map ( cls ) : if cls . supported_period_map is None : cls . supported_period_map = { } cls . supported_period_map . update ( cls . period_type_map ) try : from dateutil import relativedelta <MASK> cls . supported_period_map . update ( cls . optional_period_type_map ) except Exception : pass return cls . supported_period_map,if relativedelta is not None :,if relativedelta ( cls . date_time ) < relativedelta ( cls . date,85.50055638,FALSE,86.1
718,"def _compare_single_run ( self , compares_done ) : try : compare_id , redo = self . in_queue . get ( timeout = float ( self . config [ "" ExpertSettings "" ] [ "" block_delay "" ] ) ) except Empty : pass else : <MASK> if redo : self . db_interface . delete_old_compare_result ( compare_id ) compares_done . add ( compare_id ) self . _process_compare ( compare_id ) if self . callback : self . callback ( )","if self . _decide_whether_to_process ( compare_id , redo , compares_done ) :",if compare_id not in compares_done :,91.3693682,FALSE,84.75
719,"def _get_field_actual ( cant_be_number , raw_string , field_names ) : for line in raw_string . splitlines ( ) : for field_name in field_names : field_name = field_name . lower ( ) if "" : "" in line : left , right = line . split ( "" : "" , 1 ) left = left . strip ( ) . lower ( ) right = right . strip ( ) if left == field_name and len ( right ) > 0 : if cant_be_number : <MASK> return right else : return right return None",if not right . isdigit ( ) :,if _is_number_of_fields ( right ) :,95.22918042,FALSE,91.8
720,"def _p_basicstr_content ( s , content = _basicstr_re ) : res = [ ] while True : res . append ( s . expect_re ( content ) . group ( 0 ) ) if not s . consume ( "" \\ "" ) : break <MASK> pass elif s . consume_re ( _short_uni_re ) or s . consume_re ( _long_uni_re ) : res . append ( _chr ( int ( s . last ( ) . group ( 1 ) , 16 ) ) ) else : s . expect_re ( _escapes_re ) res . append ( _escapes [ s . last ( ) . group ( 0 ) ] ) return "" "" . join ( res )",if s . consume_re ( _newline_esc_re ) :,if s . consume_re ( _char_re ) :,99.00323141,FALSE,97.24
721,"def removedir ( self , path ) : # type: (Text) -> None _path = self . validatepath ( path ) if _path == "" / "" : raise errors . RemoveRootError ( ) with ftp_errors ( self , path ) : try : self . ftp . rmd ( _encode ( _path , self . ftp . encoding ) ) except error_perm as error : code , _ = _parse_ftp_error ( error ) if code == "" 550 "" : if self . isfile ( path ) : raise errors . DirectoryExpected ( path ) <MASK> raise errors . DirectoryNotEmpty ( path ) raise # pragma: no cover",if not self . isempty ( path ) :,elif self . isdir ( path ) :,72.27115407,FALSE,95.74
722,"def _normalize_store_path ( self , resource_store ) : if resource_store [ "" type "" ] == "" filesystem "" : <MASK> resource_store [ "" base_directory "" ] = os . path . join ( self . root_directory , resource_store [ "" base_directory "" ] ) return resource_store","if not os . path . isabs ( resource_store [ ""base_directory"" ] ) :","if not resource_store [ ""base_directory"" ] . startswith ( self . root_",84.16711477,FALSE,88.02
723,"def _apply_nested ( name , val , nested ) : parts = name . split ( "" . "" ) cur = nested for i in range ( 0 , len ( parts ) - 1 ) : cur = cur . setdefault ( parts [ i ] , { } ) <MASK> conflicts_with = "" . "" . join ( parts [ 0 : i + 1 ] ) raise ValueError ( "" %r  cannot be nested: conflicts with  { %r :  %s } "" % ( name , conflicts_with , cur ) ) cur [ parts [ - 1 ] ] = val","if not isinstance ( cur , dict ) :",if cur . get ( parts [ - 1 ] ) != val :,86.27705585,FALSE,89.27
724,"def build_packages ( targeted_packages , distribution_directory , is_dev_build = False ) : # run the build and distribution for package_root in targeted_packages : service_hierarchy = os . path . join ( os . path . basename ( package_root ) ) <MASK> verify_update_package_requirement ( package_root ) print ( "" Generating Package Using Python  {} "" . format ( sys . version ) ) run_check_call ( [ sys . executable , build_packing_script_location , "" --dest "" , os . path . join ( distribution_directory , service_hierarchy ) , package_root , ] , root_dir , )",if is_dev_build :,if is_dev_build and os . path . exists ( service_hierarchy ) :,97.02714967,FALSE,91.85
725,"def resolve_root_node_address ( self , root_node ) : if "" [ "" in root_node : name , numbers = root_node . split ( "" [ "" , maxsplit = 1 ) number = numbers . split ( "" , "" , maxsplit = 1 ) [ 0 ] <MASK> number = number . split ( "" - "" ) [ 0 ] number = re . sub ( "" [^0-9] "" , "" "" , number ) root_node = name + number return root_node","if ""-"" in number :","if ""-"" in number :",100,TRUE,100
726,"def _map_args ( maps : dict , * * kwargs ) : # maps: key=old name, value= new name output = { } for name , val in kwargs . items ( ) : if name in maps : assert isinstance ( maps [ name ] , str ) output . update ( { maps [ name ] : val } ) else : output . update ( { name : val } ) for keys in maps . keys ( ) : <MASK> pass return output",if keys not in output . keys ( ) :,if key in keys :,70.18156546,FALSE,91.14
727,"def next_item ( self , direction ) : """"""Selects next menu item, based on self._direction"""""" start , i = - 1 , 0 try : start = self . items . index ( self . _selected ) i = start + direction except : pass while True : if i == start : # Cannot find valid menu item self . select ( start ) break <MASK> i = 0 continue if i < 0 : i = len ( self . items ) - 1 continue if self . select ( i ) : break i + = direction if start < 0 : start = 0",if i >= len ( self . items ) :,if i == - 1 :,96.43283389,FALSE,92.88
728,"def detect_reentrancy ( self , contract ) : for function in contract . functions_and_modifiers_declared : <MASK> if self . KEY in function . context : continue self . _explore ( function . entry_point , [ ] ) function . context [ self . KEY ] = True",if function . is_implemented :,if self . _is_function_reentrancy ( function ) :,91.66730525,FALSE,83.41
729,"def load_model ( self ) : if not os . path . exists ( self . get_filename ( absolute = True ) ) : <MASK> return { } , { } error ( "" Model file with pre-trained convolution layers not found. Download it here... "" , "" https://github.com/alexjc/neural-enhance/releases/download/v %s / %s "" % ( __version__ , self . get_filename ( ) ) , ) print ( ""   - Loaded file ` {} ` with trained model. "" . format ( self . get_filename ( ) ) ) return pickle . load ( bz2 . open ( self . get_filename ( ) , "" rb "" ) )",if args . train :,if not bz2 . exists ( self . get_filename ( ) ) :,91.64672537,FALSE,91
730,"def get_nonexisting_check_definition_extends ( definition , indexed_oval_defs ) : # TODO: handle multiple levels of referrals. # OVAL checks that go beyond one level of extend_definition won't be properly identified for extdefinition in definition . findall ( "" .// { %s }extend_definition "" % oval_ns ) : # Verify each extend_definition in the definition extdefinitionref = extdefinition . get ( "" definition_ref "" ) # Search the OVAL tree for a definition with the referred ID referreddefinition = indexed_oval_defs . get ( extdefinitionref ) <MASK> # There is no oval satisfying the extend_definition referal return extdefinitionref return None",if referreddefinition is None :,if referreddefinition is not None :,73.99152839,FALSE,98.13
731,"def pause ( self ) : if self . is_playing : self . state = MusicPlayerState . PAUSED <MASK> self . _current_player . pause ( ) self . emit ( "" pause "" , player = self , entry = self . current_entry ) return elif self . is_paused : return raise ValueError ( "" Cannot pause a MusicPlayer in state  %s "" % self . state )",if self . _current_player :,if self . _current_player :,100,TRUE,100
732,"def setNextFormPrevious ( self , backup = STARTING_FORM ) : try : <MASK> self . _FORM_VISIT_LIST . pop ( ) # Remove the current form. if it is at the end of the list if self . _THISFORM . FORM_NAME == self . NEXT_ACTIVE_FORM : # take no action if it looks as if someone has already set the next form. self . setNextForm ( self . _FORM_VISIT_LIST . pop ( ) ) # Switch to the previous form if one exists except IndexError : self . setNextForm ( backup )",if self . _THISFORM . FORM_NAME == self . _FORM_VISIT_LIST [ - 1 ] :,if self . _THISFORM .FORM_NAME == self . NEXT_ACTIVE_FORM,79.55691094,FALSE,91.37
733,"def get_expr_referrers ( schema : s_schema . Schema , obj : so . Object ) - > Dict [ so . Object , str ] : """"""Return schema referrers with refs in expressions."""""" refs = schema . get_referrers_ex ( obj ) result = { } for ( mcls , fn ) , referrers in refs . items ( ) : field = mcls . get_field ( fn ) <MASK> result . update ( { ref : fn for ref in referrers } ) return result","if issubclass ( field . type , ( Expression , ExpressionList ) ) :",if field . is_ref :,90.12930173,FALSE,88.83
734,"def _fields_to_index ( cls ) : fields = [ ] for field in cls . _meta . sorted_fields : <MASK> continue requires_index = any ( ( field . index , field . unique , isinstance ( field , ForeignKeyField ) ) ) if requires_index : fields . append ( field ) return fields",if field . primary_key :,"if field . name . startswith ( ""_"" ) :",93.27750654,FALSE,87.53
735,"def ident_values ( self ) : value = self . _ident_values if value is False : value = None # XXX: how will this interact with orig_prefix ? #      not exposing attrs for now if orig_prefix is set. if not self . orig_prefix : wrapped = self . wrapped idents = getattr ( wrapped , "" ident_values "" , None ) <MASK> value = [ self . _wrap_hash ( ident ) for ident in idents ] ##else: ##    ident = self.ident ##    if ident is not None: ##        value = [ident] self . _ident_values = value return value",if idents :,if idents is not None :,98.5010565,FALSE,96.62
736,"def apply_incpaths_ml ( self ) : inc_lst = self . includes . split ( ) lst = self . incpaths_lst for dir in inc_lst : node = self . path . find_dir ( dir ) if not node : error ( "" node not found:  "" + str ( dir ) ) continue <MASK> lst . append ( node ) self . bld_incpaths_lst . append ( node )",if not node in lst :,if not self . bld_incpaths_lst . is_set ( node ) :,90.12637071,FALSE,84.88
737,"def application_openFiles_ ( self , nsapp , filenames ) : # logging.info('[osx] file open') # logging.info('[osx] file : %s' % (filenames)) for filename in filenames : logging . info ( "" [osx] receiving from macOS :  %s "" , filename ) if os . path . exists ( filename ) : <MASK> sabnzbd . add_nzbfile ( filename , keep = True )",if sabnzbd . filesystem . get_ext ( filename ) in VALID_ARCHIVES + VALID_NZB_FILES :,"if nsapp == ""nzbd"" :",65.73981572,FALSE,79.55
738,"def check ( self , xp , nout ) : input = xp . asarray ( self . x ) . astype ( numpy . float32 ) with warnings . catch_warnings ( ) : if self . ignore_warning : warnings . simplefilter ( "" ignore "" , self . ignore_warning ) <MASK> self . check_positive ( xp , self . func , input , self . eps , nout ) else : self . check_negative ( xp , self . func , input , self . eps , nout )",if self . result :,if self . positive :,98.64864216,FALSE,97.54
739,"def _set_scheme ( url , newscheme ) : scheme = _get_scheme ( url ) newscheme = newscheme or "" "" newseparator = "" : "" if newscheme in COLON_SEPARATED_SCHEMES else "" :// "" if scheme == "" "" : # Protocol relative URL. url = "" %s : %s "" % ( newscheme , url ) elif scheme is None and url : # No scheme. url = "" "" . join ( [ newscheme , newseparator , url ] ) elif scheme : # Existing scheme. remainder = url [ len ( scheme ) : ] <MASK> remainder = remainder [ 3 : ] elif remainder . startswith ( "" : "" ) : remainder = remainder [ 1 : ] url = "" "" . join ( [ newscheme , newseparator , remainder ] ) return url","if remainder . startswith ( ""://"" ) :","if remainder . startswith ( "":"" ) :",99.14205919,FALSE,97.89
740,"def parquet ( tables , data_directory , ignore_missing_dependency , * * params ) : try : import pyarrow as pa # noqa: F401 import pyarrow . parquet as pq # noqa: F401 except ImportError : msg = "" PyArrow dependency is missing "" <MASK> logger . warning ( "" Ignored:  %s "" , msg ) return 0 else : raise click . ClickException ( msg ) data_directory = Path ( data_directory ) for table , df in read_tables ( tables , data_directory ) : arrow_table = pa . Table . from_pandas ( df ) target_path = data_directory / "" {} .parquet "" . format ( table ) pq . write_table ( arrow_table , str ( target_path ) )",if ignore_missing_dependency :,if ignore_missing_dependency :,75,TRUE,100
741,"def h2i ( self , pkt , s ) : t = ( ) if type ( s ) is str : t = time . strptime ( s ) t = t [ : 2 ] + t [ 2 : - 3 ] else : <MASK> y , m , d , h , min , sec , rest , rest , rest = time . gmtime ( time . time ( ) ) t = ( y , m , d , h , min , sec ) else : t = s return t",if not s :,if type ( s ) is tuple :,66.51801767,FALSE,92.97
742,"def filter_episodes ( self , batch , cross_entropy ) : """"""Filter the episodes for the cross_entropy method"""""" accumulated_reward = [ sum ( rewards ) for rewards in batch [ "" rewards "" ] ] percentile = cross_entropy * 100 reward_bound = np . percentile ( accumulated_reward , percentile ) # we save the batch with reward above the bound result = { k : [ ] for k in self . data_keys } episode_kept = 0 for i in range ( len ( accumulated_reward ) ) : <MASK> for k in self . data_keys : result [ k ] . append ( batch [ k ] [ i ] ) episode_kept + = 1 return result",if accumulated_reward [ i ] >= reward_bound :,if reward_bound < episode_kept :,97.08016362,FALSE,93.1
743,"def _readenv ( var , msg ) : match = _ENV_VAR_PAT . match ( var ) if match and match . groups ( ) : envvar = match . groups ( ) [ 0 ] if envvar in os . environ : value = os . environ [ envvar ] <MASK> value = value . decode ( "" utf8 "" ) return value else : raise InvalidConfigException ( "" {}  - environment variable  ' {} '  not set "" . format ( msg , var ) ) else : raise InvalidConfigException ( "" {}  - environment variable name  ' {} '  does not match pattern  ' {} ' "" . format ( msg , var , _ENV_VAR_PAT_STR ) )",if six . PY2 :,"if isinstance ( value , bytes ) :",76.08804886,FALSE,94.73
744,"def _allocate_nbd ( self ) : if not os . path . exists ( "" /sys/block/nbd0 "" ) : self . error = _ ( "" nbd unavailable: module not loaded "" ) return None while True : if not self . _DEVICES : # really want to log this info, not raise self . error = _ ( "" No free nbd devices "" ) return None device = self . _DEVICES . pop ( ) <MASK> break return device","if not os . path . exists ( ""/sys/block/%s/pid"" % os . path . basename ( device ) ) :",if not device . is_available ( ) :,91.64984685,FALSE,76.63
745,"def _expand_deps_java_generation ( self ) : """"""Ensure that all multilingual dependencies such as proto_library generate java code."""""" queue = collections . deque ( self . deps ) keys = set ( ) while queue : k = queue . popleft ( ) <MASK> keys . add ( k ) dep = self . target_database [ k ] if "" generate_java "" in dep . attr : # Has this attribute dep . attr [ "" generate_java "" ] = True queue . extend ( dep . deps )",if k not in keys :,if k not in keys :,100,TRUE,100
746,"def load_syntax ( syntax ) : context = _create_scheme ( ) or { } partition_scanner = PartitionScanner ( syntax . get ( "" partitions "" , [ ] ) ) scanners = { } for part_name , part_scanner in list ( syntax . get ( "" scanner "" , { } ) . items ( ) ) : scanners [ part_name ] = Scanner ( part_scanner ) formats = [ ] for fname , fstyle in list ( syntax . get ( "" formats "" , { } ) . items ( ) ) : if isinstance ( fstyle , basestring ) : <MASK> key = fstyle [ 2 : - 2 ] fstyle = context [ key ] else : fstyle = fstyle % context formats . append ( ( fname , fstyle ) ) return partition_scanner , scanners , formats","if fstyle . startswith ( ""%("" ) and fstyle . endswith ( "")s"" ) :","if fstyle . startswith ( ""key"" ) :",94.0665714,FALSE,92.25
747,"def rollback ( self ) : for operation , values in self . current_transaction_state [ : : - 1 ] : <MASK> values . remove ( ) elif operation == "" update "" : old_value , new_value = values if new_value . full_filename != old_value . full_filename : os . unlink ( new_value . full_filename ) old_value . write ( ) self . _post_xact_cleanup ( )","if operation == ""insert"" :","if operation == ""delete"" :",98.17617264,FALSE,97.49
748,"def _buildOffsets ( offsetDict , localeData , indexStart ) : o = indexStart for key in localeData : <MASK> for k in key . split ( "" | "" ) : offsetDict [ k ] = o else : offsetDict [ key ] = o o + = 1","if ""|"" in key :","if ""|"" in key :",100,TRUE,100
749,"def _check_start_pipeline_execution_errors ( graphene_info , execution_params , execution_plan ) : if execution_params . step_keys : for step_key in execution_params . step_keys : <MASK> raise UserFacingGraphQLError ( graphene_info . schema . type_named ( "" InvalidStepError "" ) ( invalid_step_key = step_key ) )",if not execution_plan . has_step ( step_key ) :,if not execution_plan . is_valid_step ( step_key ) :,72.4224375,FALSE,95.06
750,"def __setattr__ ( self , option_name , option_value ) : if option_name in self . _options : # type checking sort = self . OPTIONS [ self . arch . name ] [ option_name ] [ 0 ] <MASK> self . _options [ option_name ] = option_value else : raise ValueError ( ' Value for option  "" %s ""  must be of type  %s ' % ( option_name , sort ) ) else : super ( CFGArchOptions , self ) . __setattr__ ( option_name , option_value )","if sort is None or isinstance ( option_value , sort ) :","if sort == ""sort"" :",94.15979543,FALSE,90.4
751,"def value ( self ) : quote = False if self . defects : quote = True else : for x in self : <MASK> quote = True if quote : pre = post = "" "" if self [ 0 ] . token_type == "" cfws "" or self [ 0 ] [ 0 ] . token_type == "" cfws "" : pre = ""   "" if self [ - 1 ] . token_type == "" cfws "" or self [ - 1 ] [ - 1 ] . token_type == "" cfws "" : post = ""   "" return pre + quote_string ( self . display_name ) + post else : return super ( DisplayName , self ) . value","if x . token_type == ""quoted-string"" :","if x . token_type == ""cfws"" or x [ 0 ] . token_",89.85346098,FALSE,92.71
752,"def __init__ ( self , patch_files , patch_directories ) : files = [ ] files_data = { } for filename_data in patch_files : <MASK> filename , data = filename_data else : filename = filename_data data = None if not filename . startswith ( os . sep ) : filename = "" {0} {1} "" . format ( FakeState . deploy_dir , filename ) files . append ( filename ) if data : files_data [ filename ] = data self . files = files self . files_data = files_data self . directories = patch_directories","if isinstance ( filename_data , list ) :","if isinstance ( filename_data , tuple ) :",98.73714741,FALSE,98.1
753,"def _evaluateStack ( s ) : op = s . pop ( ) if op in "" +-*/@^ "" : op2 = _evaluateStack ( s ) op1 = _evaluateStack ( s ) result = opn [ op ] ( op1 , op2 ) <MASK> print ( result ) return result else : return op",if debug_flag :,if result :,95.4356081,FALSE,93
754,"def reconnect_user ( self , user_id , host_id , server_id ) : if host_id == settings . local . host_id : return if server_id and self . server . id != server_id : return for client in self . clients . find ( { "" user_id "" : user_id } ) : self . clients . update_id ( client [ "" id "" ] , { "" ignore_routes "" : True , } , ) <MASK> self . instance . disconnect_wg ( client [ "" id "" ] ) else : self . instance_com . client_kill ( client [ "" id "" ] )","if len ( client [ ""id"" ] ) > 32 :","if self . server . id == client [ ""id"" ] :",92.72062053,FALSE,93.24
755,"def _get_library ( self , name , args ) : library_database = self . _library_manager . get_new_connection_to_library_database ( ) try : last_updated = library_database . get_library_last_updated ( name , args ) if last_updated : <MASK> self . _library_manager . fetch_keywords ( name , args , self . _libraries_need_refresh_listener ) return library_database . fetch_library_keywords ( name , args ) return self . _library_manager . get_and_insert_keywords ( name , args ) finally : library_database . close ( )",if time . time ( ) - last_updated > 10.0 :,if last_updated < self . _library_last_updated :,88.84100715,FALSE,91.95
756,"def get_paths ( self , path , commit ) : """"""Return a generator of all filepaths under path at commit."""""" _check_path_is_repo_relative ( path ) git_path = _get_git_path ( path ) tree = self . gl_repo . git_repo [ commit . tree [ git_path ] . id ] assert tree . type == pygit2 . GIT_OBJ_TREE for tree_entry in tree : tree_entry_path = os . path . join ( path , tree_entry . name ) <MASK> for fp in self . get_paths ( tree_entry_path , commit ) : yield fp else : yield tree_entry_path","if tree_entry . type == ""tree"" :",if os . path . isdir ( tree_entry_path ) :,89.96390967,FALSE,92.56
757,"def scan_resource_conf ( self , conf ) : if "" properties "" in conf : if "" attributes "" in conf [ "" properties "" ] : <MASK> if conf [ "" properties "" ] [ "" attributes "" ] [ "" exp "" ] : return CheckResult . PASSED return CheckResult . FAILED","if ""exp"" in conf [ ""properties"" ] [ ""attributes"" ] :","if ""exp"" in conf [ ""properties"" ] [ ""attributes"" ] :",75,TRUE,100
758,"def _set_parse_context ( self , tag , tag_attrs ) : # special case: script or style parse context if not self . _wb_parse_context : if tag == "" style "" : self . _wb_parse_context = "" style "" elif tag == "" script "" : <MASK> self . _wb_parse_context = "" script """,if self . _allow_js_type ( tag_attrs ) :,"if tag in ( ""style"" , ""style"" ) :",69.3625789,FALSE,85.59
759,"def modified ( self ) : paths = set ( ) dictionary_list = [ ] for op_list in self . _operations : if not isinstance ( op_list , list ) : op_list = ( op_list , ) for item in chain ( * op_list ) : <MASK> continue dictionary = item . dictionary if dictionary . path in paths : continue paths . add ( dictionary . path ) dictionary_list . append ( dictionary ) return dictionary_list",if item is None :,"if item . name in ( ""dict"" , ""dict_list_list_list_",81.26894577,FALSE,83.24
760,def preorder ( root ) : res = [ ] if not root : return res stack = [ ] stack . append ( root ) while stack : root = stack . pop ( ) res . append ( root . val ) <MASK> stack . append ( root . right ) if root . left : stack . append ( root . left ) return res,if root . right :,if root . right :,100,TRUE,100
761,"def create ( exported_python_target ) : if exported_python_target not in created : self . context . log . info ( "" Creating setup.py project for  {} "" . format ( exported_python_target ) ) subject = self . derived_by_original . get ( exported_python_target , exported_python_target ) setup_dir , dependencies = self . create_setup_py ( subject , dist_dir ) created [ exported_python_target ] = setup_dir if self . _recursive : for dep in dependencies : <MASK> create ( dep )",if is_exported_python_target ( dep ) :,if not self . _recursive_dependency ( dep ) :,95.82775526,FALSE,93.8
762,"def test_array_interface ( self , data ) : result = np . array ( data ) np . testing . assert_array_equal ( result [ 0 ] , data [ 0 ] ) result = np . array ( data , dtype = object ) expected = np . array ( list ( data ) , dtype = object ) for a1 , a2 in zip ( result , expected ) : <MASK> assert np . isnan ( a1 ) and np . isnan ( a2 ) else : tm . assert_numpy_array_equal ( a2 , a1 )",if np . isscalar ( a1 ) :,"if np . allclose ( a1 , a2 ) :",95.37235785,FALSE,95.08
763,"def valueChanged ( plug ) : changed = plug . getInput ( ) is not None if not changed and isinstance ( plug , Gaffer . ValuePlug ) : <MASK> changed = not Gaffer . NodeAlgo . isSetToUserDefault ( plug ) else : changed = not plug . isSetToDefault ( ) return changed",if Gaffer . NodeAlgo . hasUserDefault ( plug ) :,if Gaffer . NodeAlgo . isUserDefault ( plug ) :,97.94913605,FALSE,95.81
764,"def process_tag ( hive_name , company , company_key , tag , default_arch ) : with winreg . OpenKeyEx ( company_key , tag ) as tag_key : version = load_version_data ( hive_name , company , tag , tag_key ) <MASK> # if failed to get version bail major , minor , _ = version arch = load_arch_data ( hive_name , company , tag , tag_key , default_arch ) if arch is not None : exe_data = load_exe ( hive_name , company , company_key , tag ) if exe_data is not None : exe , args = exe_data return company , major , minor , arch , exe , args",if version is not None :,if version is not None :,100,TRUE,100
765,"def __iter__ ( self ) : for name , value in self . __class__ . __dict__ . items ( ) : if isinstance ( value , alias_flag_value ) : continue <MASK> yield ( name , self . _has_flag ( value . flag ) )","if isinstance ( value , flag_value ) :",if value . flag :,86.46254915,FALSE,87.05
766,"def connect ( self ) : self . sock = sockssocket ( ) self . sock . setproxy ( * proxy_args ) if type ( self . timeout ) in ( int , float ) : self . sock . settimeout ( self . timeout ) self . sock . connect ( ( self . host , self . port ) ) if isinstance ( self , compat_http_client . HTTPSConnection ) : <MASK> # Python > 2.6 self . sock = self . _context . wrap_socket ( self . sock , server_hostname = self . host ) else : self . sock = ssl . wrap_socket ( self . sock )","if hasattr ( self , ""_context"" ) :","if sys . version_info < ( 2 , 6 ) :",92.32021578,FALSE,91.98
767,"def frequent_thread_switches ( ) : """"""Make concurrency bugs more likely to manifest."""""" interval = None if not sys . platform . startswith ( "" java "" ) : if hasattr ( sys , "" getswitchinterval "" ) : interval = sys . getswitchinterval ( ) sys . setswitchinterval ( 1e-6 ) else : interval = sys . getcheckinterval ( ) sys . setcheckinterval ( 1 ) try : yield finally : if not sys . platform . startswith ( "" java "" ) : <MASK> sys . setswitchinterval ( interval ) else : sys . setcheckinterval ( interval )","if hasattr ( sys , ""setswitchinterval"" ) :","if hasattr ( sys , ""setswitchinterval"" ) :",100,TRUE,100
768,"def vars ( self ) : ret = [ ] if op . intlist : varlist = op . intlist else : varlist = self . discover for name in varlist : if name in ( "" 0 "" , "" 1 "" , "" 2 "" , "" 8 "" , "" CPU0 "" , "" ERR "" , "" LOC "" , "" MIS "" , "" NMI "" ) : varlist . remove ( name ) if not op . full and len ( varlist ) > 3 : varlist = varlist [ - 3 : ] for name in varlist : if name in self . discover : ret . append ( name ) <MASK> ret . append ( self . intmap [ name . lower ( ) ] ) return ret",elif name . lower ( ) in self . intmap :,elif name . lower ( ) in self . intmap :,100,TRUE,100
769,"def deleteDuplicates ( gadgets , callback = None ) : toReturn = [ ] inst = set ( ) count = 0 added = False len_gadgets = len ( gadgets ) for i , gadget in enumerate ( gadgets ) : inst . add ( gadget . _gadget ) <MASK> count = len ( inst ) toReturn . append ( gadget ) added = True if callback : callback ( gadget , added , float ( i + 1 ) / ( len_gadgets ) ) added = False return toReturn",if len ( inst ) > count :,if count == len_gadgets :,93.09702208,FALSE,92.99
770,"def ident ( self ) : value = self . _ident if value is False : value = None # XXX: how will this interact with orig_prefix ? #      not exposing attrs for now if orig_prefix is set. if not self . orig_prefix : wrapped = self . wrapped ident = getattr ( wrapped , "" ident "" , None ) <MASK> value = self . _wrap_hash ( ident ) self . _ident = value return value",if ident is not None :,if ident is not None :,100,TRUE,100
771,"def _flatten_settings_from_form ( self , settings , form , form_values ) : """"""Take a nested dict and return a flat dict of setting values."""""" setting_values = { } for field in form . c : if isinstance ( field , _ContainerMixin ) : setting_values . update ( self . _flatten_settings_from_form ( settings , field , form_values [ field . _name ] ) ) <MASK> setting_values [ field . _name ] = form_values [ field . _name ] return setting_values",elif field . _name in settings :,"elif isinstance ( field , SettingsField ) :",91.08384173,FALSE,94.11
772,"def _decorator ( cls ) : for name , meth in inspect . getmembers ( cls , inspect . isroutine ) : if name not in cls . __dict__ : continue if name != "" __init__ "" : <MASK> continue if name in butnot : continue setattr ( cls , name , decorator ( meth ) ) return cls","if not private and name . startswith ( ""_"" ) :",if meth is not object . __init__ :,85.46377901,FALSE,85.14
773,"def _do_cmp ( f1 , f2 ) : bufsize = BUFSIZE with open ( f1 , "" rb "" ) as fp1 , open ( f2 , "" rb "" ) as fp2 : while True : b1 = fp1 . read ( bufsize ) b2 = fp2 . read ( bufsize ) <MASK> return False if not b1 : return True",if b1 != b2 :,if b1 != b2 :,100,TRUE,100
774,"def _memoized ( * args ) : now = time . time ( ) try : value , last_update = self . cache [ args ] age = now - last_update if self . _call_count > self . ctl or age > self . ttl : self . _call_count = 0 raise AttributeError <MASK> self . _call_count + = 1 return value except ( KeyError , AttributeError ) : value = func ( * args ) if value : self . cache [ args ] = ( value , now ) return value except TypeError : return func ( * args )",if self . ctl :,if age > self . ttl :,96.18960608,FALSE,95.2
775,"def check ( self , hyperlinks : Dict [ str , Hyperlink ] ) - > Generator [ CheckResult , None , None ] : self . invoke_threads ( ) total_links = 0 for hyperlink in hyperlinks . values ( ) : <MASK> yield CheckResult ( hyperlink . uri , hyperlink . docname , hyperlink . lineno , "" ignored "" , "" "" , 0 ) else : self . wqueue . put ( CheckRequest ( CHECK_IMMEDIATELY , hyperlink ) , False ) total_links + = 1 done = 0 while done < total_links : yield self . rqueue . get ( ) done + = 1 self . shutdown_threads ( )",if self . is_ignored_uri ( hyperlink . uri ) :,if hyperlink . is_linked :,93.79358169,FALSE,91.46
776,"def remove_subscriber ( self , topic , subscriber ) : if subscriber in self . subscribers [ topic ] : <MASK> subscriber . _pyroRelease ( ) if hasattr ( subscriber , "" _pyroUri "" ) : try : proxy = self . proxy_cache [ subscriber . _pyroUri ] proxy . _pyroRelease ( ) del self . proxy_cache [ subscriber . _pyroUri ] except KeyError : pass self . subscribers [ topic ] . discard ( subscriber )","if hasattr ( subscriber , ""_pyroRelease"" ) :","if hasattr ( subscriber , ""_pyroRelease"" ) :",100,TRUE,100
777,"def delete_arc ( collection , document , origin , target , type ) : directory = collection real_dir = real_directory ( directory ) mods = ModificationTracker ( ) projectconf = ProjectConfiguration ( real_dir ) document = path_join ( real_dir , document ) with TextAnnotations ( document ) as ann_obj : # bail as quick as possible if read-only <MASK> raise AnnotationsIsReadOnlyError ( ann_obj . get_document ( ) ) _delete_arc_with_ann ( origin , target , type , mods , ann_obj , projectconf ) mods_json = mods . json_response ( ) mods_json [ "" annotations "" ] = _json_from_ann ( ann_obj ) return mods_json",if ann_obj . _read_only :,"if ann_obj . get_document ( ) . get_type ( ) == ""read",96.82885714,FALSE,90.23
778,"def _select_from ( self , parent_path , is_dir , exists , listdir ) : if not is_dir ( parent_path ) : return with _cached ( listdir ) as listdir : yielded = set ( ) try : successor_select = self . successor . _select_from for starting_point in self . _iterate_directories ( parent_path , is_dir , listdir ) : for p in successor_select ( starting_point , is_dir , exists , listdir ) : <MASK> yield p yielded . add ( p ) finally : yielded . clear ( )",if p not in yielded :,if p not in yielded :,100,TRUE,100
779,"def _fractional_part ( self , n , expr , evaluation ) : n_sympy = n . to_sympy ( ) if n_sympy . is_constant ( ) : <MASK> positive_integer_part = ( Expression ( "" Floor "" , n ) . evaluate ( evaluation ) . to_python ( ) ) result = n - positive_integer_part else : negative_integer_part = ( Expression ( "" Ceiling "" , n ) . evaluate ( evaluation ) . to_python ( ) ) result = n - negative_integer_part else : return expr return from_python ( result )",if n_sympy >= 0 :,if n_sympy . is_integer ( ) :,62.53669835,FALSE,94.28
780,"def check_bounds ( geometry ) : if isinstance ( geometry [ 0 ] , ( list , tuple ) ) : return list ( map ( check_bounds , geometry ) ) else : if geometry [ 0 ] > 180 or geometry [ 0 ] < - 180 : raise ValueError ( "" Longitude is out of bounds, check your JSON format or data "" ) <MASK> raise ValueError ( "" Latitude is out of bounds, check your JSON format or data "" )",if geometry [ 1 ] > 90 or geometry [ 1 ] < - 90 :,if geometry [ 0 ] > 180 or geometry [ 0 ] < - 180 :,95.00718369,FALSE,90.44
781,"def get_absolute_path ( self , root , path ) : # find the first absolute path that exists self . root = self . roots [ 0 ] for root in self . roots : abspath = os . path . abspath ( os . path . join ( root , path ) ) <MASK> self . root = root # make sure all the other methods in the base class know how to find the file break return abspath",if os . path . exists ( abspath ) :,if abspath . startswith ( self . root ) :,70.72149023,FALSE,91.72
782,"def do_setflow ( self , l = "" "" ) : try : <MASK> l = str ( self . flow_slider . GetValue ( ) ) else : l = l . lower ( ) flow = int ( l ) if self . p . online : self . p . send_now ( "" M221 S "" + l ) self . log ( _ ( "" Setting print flow factor to  %d %% . "" ) % flow ) else : self . logError ( _ ( "" Printer is not online. "" ) ) except Exception as x : self . logError ( _ ( "" You must enter a flow. ( %s ) "" ) % ( repr ( x ) , ) )","if not isinstance ( l , str ) or not len ( l ) :",if self . flow_slider :,86.13275448,FALSE,90.34
783,"def sources ( ) : for d in os . listdir ( base ) : #        if d.startswith('talis'): #            continue <MASK> continue if d == "" indcat "" : continue if not os . path . isdir ( base + d ) : continue yield d","if d . endswith ( ""old"" ) :","if d . startswith ( ""talis"" ) :",70.92679022,FALSE,92.03
784,"def create_accumulator ( self ) - > tf_metric_accumulators . TFCompilableMetricsAccumulator : configs = zip ( self . _metric_configs , self . _loss_configs ) padding_options = None if self . _eval_config is not None : model_spec = model_util . get_model_spec ( self . _eval_config , self . _model_name ) <MASK> padding_options = model_spec . padding_options return tf_metric_accumulators . TFCompilableMetricsAccumulator ( padding_options , [ len ( m ) + len ( l ) for m , l in configs ] , desired_batch_size = self . _desired_batch_size , )","if model_spec is not None and model_spec . HasField ( ""padding_options"" ) :",if model_spec is not None :,79.25127785,FALSE,90.46
785,"def parseImpl ( self , instring , loc , doActions = True ) : try : loc , tokens = self . expr . _parse ( instring , loc , doActions , callPreParse = False ) except ( ParseException , IndexError ) : <MASK> if self . expr . resultsName : tokens = ParseResults ( [ self . defaultValue ] ) tokens [ self . expr . resultsName ] = self . defaultValue else : tokens = [ self . defaultValue ] else : tokens = [ ] return loc , tokens",if self . defaultValue is not self . __optionalNotMatched :,if self . expr . resultsName :,94.02485395,FALSE,90.9
786,"def handleConnection ( self ) : # connection handshake try : <MASK> return True self . csock . close ( ) except : ex_t , ex_v , ex_tb = sys . exc_info ( ) tb = util . formatTraceback ( ex_t , ex_v , ex_tb ) log . warning ( "" error during connect/handshake:  %s ;  %s "" , ex_v , "" \n "" . join ( tb ) ) self . csock . close ( ) return False",if self . daemon . _handshake ( self . csock ) :,if self . csock . connect ( ) :,70.94046673,FALSE,92.54
787,"def getProc ( su , innerTarget ) : if len ( su ) == 1 : # have a one element wedge proc = ( "" first "" , "" last "" ) else : <MASK> proc = ( "" first "" , "" last "" ) # same element can be first and last elif su . isFirst ( innerTarget ) : proc = ( "" first "" , ) elif su . isLast ( innerTarget ) : proc = ( "" last "" , ) else : proc = ( ) return proc",if su . isFirst ( innerTarget ) and su . isLast ( innerTarget ) :,if su . isEqual ( innerTarget ) :,95.49865838,FALSE,91.11
788,"def get_color_dtype ( data , column_names ) : has_color = all ( column in data [ "" points "" ] for column in column_names ) if has_color : color_data_types = [ data [ "" points "" ] [ column_name ] . dtype for column_name in column_names ] <MASK> raise TypeError ( f "" Data types of color values are inconsistent: got  { color_data_types } "" ) color_data_type = color_data_types [ 0 ] else : color_data_type = None return color_data_type",if len ( set ( color_data_types ) ) > 1 :,if len ( color_data_types ) != 1 :,63.93700086,FALSE,95.34
789,"def close ( self ) : children = [ ] for children_part , line_offset , last_line_offset_leaf in self . children_groups : <MASK> try : _update_positions ( children_part , line_offset , last_line_offset_leaf ) except _PositionUpdatingFinished : pass children + = children_part self . tree_node . children = children # Reset the parents for node in children : node . parent = self . tree_node",if line_offset != 0 :,if children_part . parent is None :,90.79835133,FALSE,91.84
790,"def get_multi ( self , keys , index = None ) : with self . _lmdb . begin ( ) as txn : result = [ ] for key in keys : packed = txn . get ( key . encode ( ) ) <MASK> result . append ( ( key , cbor . loads ( packed ) ) ) return result",if packed is not None :,if packed :,93.1127658,FALSE,93.39
791,"def get_directory_info ( prefix , pth , recursive ) : res = [ ] directory = os . listdir ( pth ) directory . sort ( ) for p in directory : if p [ 0 ] != "" . "" : subp = os . path . join ( pth , p ) p = os . path . join ( prefix , p ) <MASK> res . append ( [ p , get_directory_info ( prefix , subp , 1 ) ] ) else : res . append ( [ p , None ] ) return res",if recursive and os . path . isdir ( subp ) :,if recursive :,81.02502707,FALSE,90.77
792,"def __schedule ( self , workflow_scheduler_id , workflow_scheduler ) : invocation_ids = self . __active_invocation_ids ( workflow_scheduler_id ) for invocation_id in invocation_ids : log . debug ( "" Attempting to schedule workflow invocation [ %s ] "" , invocation_id ) self . __attempt_schedule ( invocation_id , workflow_scheduler ) <MASK> return",if not self . monitor_running :,if self . __is_scheduled ( invocation_id ) :,88.8374629,FALSE,87.25
793,"def write ( self , data ) : self . size - = len ( data ) passon = None if self . size > 0 : self . data . append ( data ) else : if self . size : data , passon = data [ : self . size ] , data [ self . size : ] else : passon = b "" "" <MASK> self . data . append ( data ) return passon",if data :,if data :,100,TRUE,100
794,"def __getstate__ ( self ) : try : store_func , load_func = self . store_function , self . load_function self . store_function , self . load_function = None , None # ignore analyses. we re-initialize analyses when restoring from pickling so that we do not lose any newly # added analyses classes d = dict ( ( k , v ) for k , v in self . __dict__ . items ( ) <MASK> not in { "" analyses "" , } ) return d finally : self . store_function , self . load_function = store_func , load_func",if k,"if k . startswith ( ""_"" ) and k . startswith ( ""_"" )",96.52332645,FALSE,87.71
795,"def mouse_down ( self , event ) : if event . button == 1 : if self . scrolling : p = event . local <MASK> self . scroll_up ( ) return elif self . scroll_down_rect ( ) . collidepoint ( p ) : self . scroll_down ( ) return if event . button == 4 : self . scroll_up ( ) if event . button == 5 : self . scroll_down ( ) GridView . mouse_down ( self , event )",if self . scroll_up_rect ( ) . collidepoint ( p ) :,if self . scroll_up_rect ( ) . collidepoint ( p ) :,100,TRUE,100
796,"def on_api_command ( self , command , data ) : if command == "" select "" : if not Permissions . PLUGIN_ACTION_COMMAND_PROMPT_INTERACT . can ( ) : return flask . abort ( 403 , "" Insufficient permissions "" ) if self . _prompt is None : return flask . abort ( 409 , "" No active prompt "" ) choice = data [ "" choice "" ] <MASK> return flask . abort ( 400 , "" {!r}  is not a valid value for choice "" . format ( choice ) ) self . _answer_prompt ( choice )","if not isinstance ( choice , int ) or not self . _prompt . validate_choice ( choice ) :",if not self . _prompt . is_valid ( ) :,87.17227284,FALSE,89.02
797,"def register_predictors ( self , model_data_arr ) : for integration in self . _get_integrations ( ) : <MASK> integration . register_predictors ( model_data_arr ) else : logger . warning ( f "" There is no connection to  { integration . name } . predictor wouldn ' t be registred. "" )",if integration . check_connection ( ) :,if integration . connection_to_connect :,67.88805222,FALSE,91.76
798,"def _pack_shears ( shearData ) : shears = list ( ) vidxs = list ( ) for e_idx , entry in enumerate ( shearData ) : # Should be 3 entries <MASK> shears . extend ( [ float ( "" nan "" ) , float ( "" nan "" ) ] ) vidxs . extend ( [ 0 , 0 ] ) else : vidx1 , vidx2 , shear1 , shear2 = entry shears . extend ( [ shear1 , shear2 ] ) vidxs . extend ( [ vidx1 , vidx2 ] ) return ( np . asarray ( shears , dtype = np . float32 ) , np . asarray ( vidxs , dtype = np . uint32 ) )",if entry is None :,if e_idx == 3 :,97.88224079,FALSE,94.28
799,"def aiter_cogs ( cls ) - > AsyncIterator [ Tuple [ str , str ] ] : yield "" Core "" , "" 0 "" for _dir in data_manager . cog_data_path ( ) . iterdir ( ) : fpath = _dir / "" settings.json "" if not fpath . exists ( ) : continue with fpath . open ( ) as f : try : data = json . load ( f ) except json . JSONDecodeError : continue if not isinstance ( data , dict ) : continue cog_name = _dir . stem for cog_id , inner in data . items ( ) : <MASK> continue yield cog_name , cog_id","if not isinstance ( inner , dict ) :",if inner is None :,91.13440672,FALSE,94.02
800,"def subFeaName ( m , newNames , state ) : try : int ( m [ 3 ] , 16 ) except : return m [ 0 ] name = m [ 2 ] if name in newNames : # print('sub %r => %r' % (m[0], m[1] + newNames[name] + m[4])) <MASK> print ( "" sub  %r  =>  %r "" % ( m [ 0 ] , m [ 1 ] + newNames [ name ] + m [ 4 ] ) ) state [ "" didChange "" ] = True return m [ 1 ] + newNames [ name ] + m [ 4 ] return m [ 0 ]","if name == ""uni0402"" :","if state [ ""didChange"" ] :",97.84155025,FALSE,95
801,"def log_graph ( self , model : LightningModule , input_array = None ) : if self . _log_graph : if input_array is None : input_array = model . example_input_array <MASK> input_array = model . _apply_batch_transfer_handler ( input_array ) self . experiment . add_graph ( model , input_array ) else : rank_zero_warn ( "" Could not log computational graph since the "" ""  `model.example_input_array` attribute is not set "" ""  or `input_array` was not given "" , UserWarning , )",if input_array is not None :,"elif isinstance ( input_array , list ) :",82.81132471,FALSE,93.18
802,"def apply ( self , db , person ) : for family_handle in person . get_family_handle_list ( ) : family = db . get_family_from_handle ( family_handle ) if family : for event_ref in family . get_event_ref_list ( ) : if event_ref : event = db . get_event_from_handle ( event_ref . ref ) <MASK> return True if not event . get_date_object ( ) : return True return False",if not event . get_place_handle ( ) :,if not event . get_date_object ( ) :,98.29642918,FALSE,96.27
803,"def format ( m ) : if m > 1000 : <MASK> return ( str ( int ( m / 1000 ) ) , "" km "" ) else : return ( str ( round ( m / 1000 , 1 ) ) , "" km "" ) return ( str ( m ) , "" m "" )",if m % 1000 == 0 :,if m % 1000 == 0 :,75,TRUE,100
804,"def previous ( self ) : try : idx = _jump_list_index next_index = idx + 1 <MASK> next_index = 100 next_index = min ( len ( _jump_list ) - 1 , next_index ) _jump_list_index = next_index return _jump_list [ next_index ] except ( IndexError , KeyError ) as e : return None",if next_index > 100 :,if idx > 100 :,84.37826974,FALSE,94.66
805,"def _validate_and_set_default_hyperparameters ( self ) : """"""Placeholder docstring"""""" # Check if all the required hyperparameters are set. If there is a default value # for one, set it. for name , definition in self . hyperparameter_definitions . items ( ) : if name not in self . hyperparam_dict : spec = definition [ "" spec "" ] <MASK> self . hyperparam_dict [ name ] = spec [ "" DefaultValue "" ] elif "" IsRequired "" in spec and spec [ "" IsRequired "" ] : raise ValueError ( "" Required hyperparameter:  %s  is not set "" % name )","if ""DefaultValue"" in spec :","if ""DefaultValue"" in spec and ""IsRequired"" in spec :",73.92893151,FALSE,95.57
806,"def _actions_read ( self , c ) : self . action_input . handle_read ( c ) if c in [ curses . KEY_ENTER , util . KEY_ENTER2 ] : # take action if self . action_input . selected_index == 0 : # Cancel self . back_to_parent ( ) <MASK> # Apply self . _apply_prefs ( ) client . core . get_config ( ) . addCallback ( self . _update_preferences ) elif self . action_input . selected_index == 2 : # OK self . _apply_prefs ( ) self . back_to_parent ( )",elif self . action_input . selected_index == 1 :,"elif c in [ curses . KEY_ESCAPE , util . KEY_ESCAPE ] :",95.03082809,FALSE,89.31
807,"def _split_anonymous_function ( s ) : # Regex is not sufficient to handle differences between anonymous # functions and YAML encoded lists. We perform a sniff test to see # if it might be an anonymous function and then confirm by # decoding it as YAML and testing the result. if s [ : 1 ] == "" [ "" and s [ - 1 : ] == "" ] "" and "" : "" in s : try : l = yaml_util . decode_yaml ( s ) except Exception : return None , s [ 1 : - 1 ] else : <MASK> return None , s [ 1 : - 1 ] return None","if len ( l ) == 1 and isinstance ( l [ 0 ] , ( six . string_types , int ) ) :","if l [ : 1 ] == ""anonymous"" :",67.04579947,FALSE,83.63
808,"def test_source_address ( self ) : for addr , is_ipv6 in VALID_SOURCE_ADDRESSES : <MASK> warnings . warn ( "" No IPv6 support: skipping. "" , NoIPv6Warning ) continue pool = HTTPConnectionPool ( self . host , self . port , source_address = addr , retries = False ) self . addCleanup ( pool . close ) r = pool . request ( "" GET "" , "" /source_address "" ) self . assertEqual ( r . data , b ( addr [ 0 ] ) )",if is_ipv6 and not HAS_IPV6_AND_DNS :,if not is_ipv6 :,91.4092756,FALSE,89.54
809,"def vim_G ( self ) : """"""Put the cursor on the last character of the file."""""" if self . is_text_wrapper ( self . w ) : <MASK> self . do ( "" end-of-buffer-extend-selection "" ) else : self . do ( "" end-of-buffer "" ) self . done ( ) else : self . quit ( )","if self . state == ""visual"" :",if self . is_visual ( ) :,90.32225851,FALSE,90.61
810,"def backend_supported ( module , manager , * * kwargs ) : if CollectionNodeModule . backend_supported ( module , manager , * * kwargs ) : if "" tid "" not in kwargs : return True conn = manager . connection ( did = kwargs [ "" did "" ] ) template_path = "" partitions/sql/ {0} /# {0} # {1} # "" . format ( manager . server_type , manager . version ) SQL = render_template ( "" / "" . join ( [ template_path , "" backend_support.sql "" ] ) , tid = kwargs [ "" tid "" ] ) status , res = conn . execute_scalar ( SQL ) # check if any errors <MASK> return internal_server_error ( errormsg = res ) return res",if not status :,if status != 200 :,98.40049059,FALSE,96.74
811,"def _get_regex_config ( self , data_asset_name : Optional [ str ] = None ) - > dict : regex_config : dict = copy . deepcopy ( self . _default_regex ) asset : Optional [ Asset ] = None if data_asset_name : asset = self . _get_asset ( data_asset_name = data_asset_name ) if asset is not None : # Override the defaults <MASK> regex_config [ "" pattern "" ] = asset . pattern if asset . group_names : regex_config [ "" group_names "" ] = asset . group_names return regex_config",if asset . pattern :,if asset . pattern :,100,TRUE,100
812,"def resolve ( self , other ) : if other == ANY_TYPE : return self elif isinstance ( other , ComplexType ) : f = self . first . resolve ( other . first ) s = self . second . resolve ( other . second ) <MASK> return ComplexType ( f , s ) else : return None elif self == ANY_TYPE : return other else : return None",if f and s :,if f is not None and s is not None :,67.95636543,FALSE,89.58
813,"def collect_pages ( app ) : new_images = { } for full_path , basename in app . builder . images . iteritems ( ) : base , ext = os . path . splitext ( full_path ) retina_path = base + "" @2x "" + ext <MASK> new_images [ retina_path ] = app . env . images [ retina_path ] [ 1 ] app . builder . images . update ( new_images ) return [ ]",if retina_path in app . env . images :,if retina_path in app . env . images :,100,TRUE,100
814,"def has_bad_headers ( self ) : headers = [ self . sender , self . reply_to ] + self . recipients for header in headers : if _has_newline ( header ) : return True if self . subject : if _has_newline ( self . subject ) : for linenum , line in enumerate ( self . subject . split ( "" \r \n "" ) ) : <MASK> return True if linenum > 0 and line [ 0 ] not in "" \t   "" : return True if _has_newline ( line ) : return True if len ( line . strip ( ) ) == 0 : return True return False",if not line :,"if linenum == 0 and line [ 0 ] in ""\t "" :",95.70260676,FALSE,89.36
815,"def reader ( ) : try : imgs = mp4_loader ( video_path , seg_num , seglen , mode ) <MASK> logger . error ( "" {}  frame length  {}  less than 1. "" . format ( video_path , len ( imgs ) ) ) yield None , None except : logger . error ( "" Error when loading  {} "" . format ( mp4_path ) ) yield None , None imgs_ret = imgs_transform ( imgs , mode , seg_num , seglen , short_size , target_size , img_mean , img_std ) label_ret = video_path yield imgs_ret , label_ret",if len ( imgs ) < 1 :,if len ( imgs ) < 1 :,100,TRUE,100
816,"def translate_from_sortname ( name , sortname ) : """"""'Translate' the artist name by reversing the sortname."""""" for c in name : ctg = unicodedata . category ( c ) <MASK> for separator in ( ""  &  "" , "" ;  "" , ""  and  "" , ""  vs.  "" , ""  with  "" , ""  y  "" ) : if separator in sortname : parts = sortname . split ( separator ) break else : parts = [ sortname ] separator = "" "" return separator . join ( map ( _reverse_sortname , parts ) ) return name","if ctg [ 0 ] == ""L"" and unicodedata . name ( c ) . find ( ""LATIN"" ) == - 1 :","if ctg . startswith ( ""artist"" ) :",78.70648142,FALSE,80.73
817,"def _to_local_path ( path ) : """"""Convert local path to SFTP path"""""" if sys . platform == "" win32 "" : # pragma: no cover path = os . fsdecode ( path ) <MASK> path = path [ 1 : ] path = path . replace ( "" / "" , "" \\ "" ) return path","if path [ : 1 ] == ""/"" and path [ 2 : 3 ] == "":"" :","if path [ 0 ] == ""/"" :",65.06197424,FALSE,81.63
818,"def __call__ ( self , text : str ) - > str : for t in self . cleaner_types : if t == "" tacotron "" : text = tacotron_cleaner . cleaners . custom_english_cleaners ( text ) <MASK> text = jaconv . normalize ( text ) elif t == "" vietnamese "" : if vietnamese_cleaners is None : raise RuntimeError ( "" Please install underthesea "" ) text = vietnamese_cleaners . vietnamese_cleaner ( text ) else : raise RuntimeError ( f "" Not supported: type= { t } "" ) return text","elif t == ""jaconv"" :","elif t == ""jaconv"" :",100,TRUE,100
819,"def cb_syncthing_system_data ( self , daemon , mem , cpu , d_failed , d_total ) : if self . daemon . get_my_id ( ) in self . devices : # Update my device display device = self . devices [ self . daemon . get_my_id ( ) ] device [ "" ram "" ] = sizeof_fmt ( mem ) device [ "" cpu "" ] = "" %3.2f %% "" % ( cpu ) <MASK> device [ "" announce "" ] = _ ( "" disabled "" ) else : device [ "" announce "" ] = "" %s / %s "" % ( d_total - d_failed , d_total )",if d_total == 0 :,if d_failed == 0 :,98.88926599,FALSE,98.27
820,"def update_kls ( self , sampled_kls ) : for i , kl in enumerate ( sampled_kls ) : <MASK> self . kl_coeff_val [ i ] * = 0.5 elif kl > 1.5 * self . kl_target : self . kl_coeff_val [ i ] * = 2.0 return self . kl_coeff_val",if kl < self . kl_target / 1.5 :,if kl < 0.5 * self . kl_target :,89.10704552,FALSE,93.54
821,"def DeleteEmptyCols ( self ) : cols2delete = [ ] for c in range ( 0 , self . GetCols ( ) ) : f = True for r in range ( 0 , self . GetRows ( ) ) : if self . FindItemAtPosition ( ( r , c ) ) is not None : f = False <MASK> cols2delete . append ( c ) for i in range ( 0 , len ( cols2delete ) ) : self . ShiftColsLeft ( cols2delete [ i ] + 1 ) cols2delete = [ x - 1 for x in cols2delete ]",if f :,if f :,100,TRUE,100
822,"def get_session ( self ) : if self . _session is None : session = super ( ChildResourceManager , self ) . get_session ( ) <MASK> session = session . get_session_for_resource ( self . resource_type . resource ) self . _session = session return self . _session",if self . resource_type . resource != constants . RESOURCE_ACTIVE_DIRECTORY :,if self . resource_type :,64.38737601,FALSE,83.11
823,"def _get_master_authorized_networks_config ( self , raw_cluster ) : if raw_cluster . get ( "" masterAuthorizedNetworksConfig "" ) : config = raw_cluster . get ( "" masterAuthorizedNetworksConfig "" ) config [ "" includes_public_cidr "" ] = False for block in config [ "" cidrBlocks "" ] : <MASK> config [ "" includes_public_cidr "" ] = True return config else : return { "" enabled "" : False , "" cidrBlocks "" : [ ] , "" includes_public_cidr "" : False }","if block [ ""cidrBlock"" ] == ""0.0.0.0/0"" :","if block . get ( ""enabled"" ) :",91.71283223,FALSE,89.06
824,"def scan_folder ( folder ) : scanned_files = [ ] for root , dirs , files in os . walk ( folder ) : dirs [ : ] = [ d for d in dirs if d != "" __pycache__ "" ] relative_path = os . path . relpath ( root , folder ) for f in files : <MASK> continue relative_name = os . path . normpath ( os . path . join ( relative_path , f ) ) . replace ( "" \\ "" , "" / "" ) scanned_files . append ( relative_name ) return sorted ( scanned_files )","if f . endswith ( "".pyc"" ) :","if f . startswith ( ""__"" ) :",98.04303992,FALSE,95.64
825,"def read_progress ( self ) : while True : processed_file = self . queue . get ( ) self . threading_completed . append ( processed_file ) total_number = len ( self . file_list ) completed_number = len ( self . threading_completed ) # Just for the record, this slows down book searching by about 20% if _progress_emitter : # Skip update in reading mode _progress_emitter . update_progress ( completed_number * 100 / / total_number ) <MASK> break",if total_number == completed_number :,if completed_number == total_number :,97.76492797,FALSE,96.88
826,"def next_instruction_is_function_or_class ( lines ) : """"""Is the first non-empty, non-commented line of the cell either a function or a class?"""""" parser = StringParser ( "" python "" ) for i , line in enumerate ( lines ) : if parser . is_quoted ( ) : parser . read_line ( line ) continue parser . read_line ( line ) if not line . strip ( ) : # empty line <MASK> return False continue if line . startswith ( "" def  "" ) or line . startswith ( "" class  "" ) : return True if line . startswith ( ( "" # "" , "" @ "" , ""   "" , "" ) "" ) ) : continue return False return False",if i > 0 and not lines [ i - 1 ] . strip ( ) :,"if line . startswith ( ""function "" ) :",94.53355962,FALSE,90.65
827,def __next__ ( self ) : try : data = next ( self . iter_loader ) except StopIteration : self . _epoch + = 1 <MASK> self . _dataloader . sampler . set_epoch ( self . _epoch ) self . iter_loader = iter ( self . _dataloader ) data = next ( self . iter_loader ) return data,"if hasattr ( self . _dataloader . sampler , ""set_epoch"" ) :",if self . _epoch % self . _dataloader . sampler . get_epoch,87.91399445,FALSE,86.05
828,"def dgl_mp_batchify_fn ( data ) : if isinstance ( data [ 0 ] , tuple ) : data = zip ( * data ) return [ dgl_mp_batchify_fn ( i ) for i in data ] for dt in data : if dt is not None : <MASK> return [ d for d in data if isinstance ( d , dgl . DGLGraph ) ] elif isinstance ( dt , nd . NDArray ) : pad = Pad ( axis = ( 1 , 2 ) , num_shards = 1 , ret_length = False ) data_list = [ dt for dt in data if dt is not None ] return pad ( data_list )","if isinstance ( dt , dgl . DGLGraph ) :","if isinstance ( dt , ( dgl . DGLBatchify , dgl . Batchify",85.09152864,FALSE,94.23
829,"def f ( self , info ) : for k in keys : <MASK> for k2 in list ( info . keys ( ) ) : if k ( k2 ) : info . pop ( k2 ) else : info . pop ( k , None )",if callable ( k ) :,"if isinstance ( info [ k ] , dict ) :",67.26026947,FALSE,83.93
830,"def create ( path , binary = False ) : for i in range ( 10 ) : try : os . makedirs ( os . path . dirname ( path ) , exist_ok = True ) <MASK> return open ( path , "" wb "" ) else : return open ( path , "" w "" , encoding = "" utf-8 "" ) if i > 0 : log ( True , f "" Created  { path }  at attempt  { i + 1 } "" ) except : time . sleep ( 0.5 ) else : raise Error ( f "" Failed to create  { path } "" )",if binary :,if binary :,100,TRUE,100
831,"def validate_update ( self , update_query ) : structure = DotCollapsedDict ( self . doc_class . structure ) for op , fields in update_query . iteritems ( ) : for field in fields : if op != "" $unset "" and op != "" $rename "" : <MASK> raise UpdateQueryError ( "" ' %s '  not found in  %s ' s structure "" % ( field , self . doc_class . __name__ ) )",if field not in structure :,if field not in structure :,100,TRUE,100
832,"def check_enums_ATLAS_ISAEXT ( lines ) : for i , isaext in enumerate ( ATLAS_ISAEXT ) : got = lines . pop ( 0 ) . strip ( ) <MASK> expect = "" none: 1 "" else : expect = "" {0} :  {1} "" . format ( isaext , 1 << i ) if got != expect : raise RuntimeError ( "" ATLAS_ISAEXT mismatch at position  "" + str ( i ) + "" : got >> "" + got + "" <<, expected >> "" + expect + "" << "" )",if i == 0 :,"if isaext == ""none"" :",71.48328408,FALSE,94.37
833,"def _test_export_session_csv ( self , test_session = None ) : with self . app . test_request_context ( ) : <MASK> test_session = SessionFactory ( ) field_data = export_sessions_csv ( [ test_session ] ) session_row = field_data [ 1 ] self . assertEqual ( session_row [ 0 ] , "" example (accepted) "" ) self . assertEqual ( session_row [ 9 ] , "" accepted "" )",if not test_session :,if test_session is None :,77.47744136,FALSE,95.12
834,"def get_report_to_platform ( self , args , scan_reports ) : if self . bc_api_key : <MASK> repo_id = self . get_repository ( args ) self . setup_bridgecrew_credentials ( bc_api_key = self . bc_api_key , repo_id = repo_id ) if self . is_integration_configured ( ) : self . _upload_run ( args , scan_reports )",if args . directory :,if not self . is_running ( ) :,64.98489442,FALSE,90.86
835,"def test_fvalue ( self ) : if not getattr ( self , "" skip_f "" , False ) : rtol = getattr ( self , "" rtol "" , 1e-10 ) assert_allclose ( self . res1 . fvalue , self . res2 . F , rtol = rtol ) <MASK> # only available with ivreg2 assert_allclose ( self . res1 . f_pvalue , self . res2 . Fp , rtol = rtol ) else : raise pytest . skip ( "" TODO: document why this test is skipped "" )","if hasattr ( self . res2 , ""Fp"" ) :","if hasattr ( self . res1 , ""f_pvalue"" ) :",97.83813,FALSE,93.97
836,"def fix_repeating_arguments ( self ) : """"""Fix elements that should accumulate/increment values."""""" either = [ list ( child . children ) for child in transform ( self ) . children ] for case in either : for e in [ child for child in case if case . count ( child ) > 1 ] : if type ( e ) is Argument or type ( e ) is Option and e . argcount : if e . value is None : e . value = [ ] <MASK> e . value = e . value . split ( ) if type ( e ) is Command or type ( e ) is Option and e . argcount == 0 : e . value = 0 return self",elif type ( e . value ) is not list :,elif type ( e . value ) is str :,86.69611016,FALSE,97.67
837,"def touch ( self ) : if not self . exists ( ) : try : self . parent ( ) . touch ( ) except ValueError : pass node = self . _fs . touch ( self . pathnames , { } ) if not node . isdir : raise AssertionError ( "" Not a folder:  %s "" % self . path ) <MASK> self . watcher . emit ( "" created "" , self )",if self . watcher :,if self . _fs . exists ( self . path ) :,94.64852042,FALSE,88.37
838,"def __init__ ( self , _inf = None , _tzinfos = None ) : if _inf : self . _tzinfos = _tzinfos self . _utcoffset , self . _dst , self . _tzname = _inf else : _tzinfos = { } self . _tzinfos = _tzinfos self . _utcoffset , self . _dst , self . _tzname = self . _transition_info [ 0 ] _tzinfos [ self . _transition_info [ 0 ] ] = self for inf in self . _transition_info [ 1 : ] : <MASK> _tzinfos [ inf ] = self . __class__ ( inf , _tzinfos )",if not _tzinfos . has_key ( inf ) :,if inf not in _tzinfos :,78.17943296,FALSE,92.85
839,"def test_sample_output ( ) : comment = "" SAMPLE OUTPUT "" skip_files = [ "" __init__.py "" ] errors = [ ] for _file in sorted ( MODULE_PATH . iterdir ( ) ) : <MASK> with _file . open ( ) as f : if comment not in f . read ( ) : errors . append ( ( comment , _file ) ) if errors : line = "" Missing sample error(s) detected! \n \n "" for error in errors : line + = "" ` {} ` is not in module ` {} ` \n "" . format ( * error ) print ( line [ : - 1 ] ) assert False","if _file . suffix == "".py"" and _file . name not in skip_files :",if _file . name not in skip_files :,84.00538385,FALSE,92.97
840,"def http_get ( url , target ) : req = requests . get ( url , stream = True ) content_length = req . headers . get ( "" Content-Length "" ) total = int ( content_length ) if content_length is not None else None progress = tqdm ( unit = "" B "" , total = total ) with open ( target , "" wb "" ) as target_file : for chunk in req . iter_content ( chunk_size = 1024 ) : <MASK> # filter out keep-alive new chunks progress . update ( len ( chunk ) ) target_file . write ( chunk ) progress . close ( )",if chunk :,if len ( chunk ) > 0 :,97.00132004,FALSE,94.47
841,"def _elements_to_datasets ( self , elements , level = 0 ) : for element in elements : extra_kwds = { "" identifier_ %d "" % level : element [ "" name "" ] } <MASK> for inner_element in self . _elements_to_datasets ( element [ "" elements "" ] , level = level + 1 ) : dataset = extra_kwds . copy ( ) dataset . update ( inner_element ) yield dataset else : dataset = extra_kwds extra_kwds . update ( element ) yield extra_kwds","if ""elements"" in element :","if ""elements"" in element :",100,TRUE,100
842,"def update_dict ( a , b ) : for key , value in b . items ( ) : if value is None : continue <MASK> a [ key ] = value elif isinstance ( a [ key ] , dict ) and isinstance ( value , dict ) : update_dict ( a [ key ] , value ) elif isinstance ( a [ key ] , list ) : a [ key ] . append ( value ) else : a [ key ] = [ a [ key ] , value ]",if key not in a :,elif key not in a :,81.41905111,FALSE,97.52
843,"def scan ( self , targets ) : for target in targets : target . print_infos ( ) if self . is_interesting ( target ) : self . target [ "" other "" ] . append ( target ) <MASK> return target return None",if self . match ( target ) :,"if self . target [ ""other"" ] [ 0 ] == target :",90.44483349,FALSE,78.36
844,"def printConnections ( switches ) : "" Compactly print connected nodes to each switch "" for sw in switches : output ( "" %s :  "" % sw ) for intf in sw . intfList ( ) : link = intf . link <MASK> intf1 , intf2 = link . intf1 , link . intf2 remote = intf1 if intf1 . node != sw else intf2 output ( "" %s ( %s )  "" % ( remote . node , sw . ports [ intf ] ) ) output ( "" \n "" )",if link :,if link :,100,TRUE,100
845,"def __cut ( sentence ) : global emit_P prob , pos_list = viterbi ( sentence , "" BMES "" , start_P , trans_P , emit_P ) begin , nexti = 0 , 0 # print pos_list, sentence for i , char in enumerate ( sentence ) : pos = pos_list [ i ] if pos == "" B "" : begin = i <MASK> yield sentence [ begin : i + 1 ] nexti = i + 1 elif pos == "" S "" : yield char nexti = i + 1 if nexti < len ( sentence ) : yield sentence [ nexti : ]","elif pos == ""E"" :","elif pos == ""A"" :",98.85563479,FALSE,98.07
846,"def check_files ( self , paths = None ) : """"""Run all checks on the paths."""""" if paths is None : paths = self . paths report = self . options . report runner = self . runner report . start ( ) try : for path in paths : <MASK> self . input_dir ( path ) elif not self . excluded ( path ) : runner ( path ) except KeyboardInterrupt : print ( "" ... stopped "" ) report . stop ( ) return report",if os . path . isdir ( path ) :,if self . is_dir ( path ) :,94.75166664,FALSE,94.16
847,"def verts_of_loop ( edge_loop ) : verts = [ ] for e0 , e1 in iter_pairs ( edge_loop , False ) : <MASK> v0 = e0 . shared_vert ( e1 ) verts + = [ e0 . other_vert ( v0 ) , v0 ] verts + = [ e1 . other_vert ( verts [ - 1 ] ) ] if len ( verts ) > 1 and verts [ 0 ] == verts [ - 1 ] : return verts [ : - 1 ] return verts",if not verts :,if e0 . is_bounding_point ( e1 ) :,95.180839,FALSE,89.85
848,"def generator ( self , data ) : for task in data : # Do we scan everything or just /bin/bash instances? <MASK> continue for bucket in task . bash_hash_entries ( ) : yield ( 0 , [ int ( task . p_pid ) , str ( task . p_comm ) , int ( bucket . times_found ) , str ( bucket . key ) , str ( bucket . data . path ) , ] , )","if not ( self . _config . SCAN_ALL or str ( task . p_comm ) == ""bash"" ) :",if not task . is_bash :,91.03625119,FALSE,78.03
849,"def __get_ratio ( self ) : """"""Return splitter ratio of the main splitter."""""" c = self . c free_layout = c . free_layout if free_layout : w = free_layout . get_main_splitter ( ) if w : aList = w . sizes ( ) <MASK> n1 , n2 = aList # 2017/06/07: guard against division by zero. ratio = 0.5 if n1 + n2 == 0 else float ( n1 ) / float ( n1 + n2 ) return ratio return 0.5",if len ( aList ) == 2 :,if len ( aList ) == 2 :,100,TRUE,100
850,"def geterrors ( self ) : """"""Get all error messages."""""" notes = self . getnotes ( origin = "" translator "" ) . split ( "" \n "" ) errordict = { } for note in notes : <MASK> error = note . replace ( "" (pofilter)  "" , "" "" ) errorname , errortext = error . split ( "" :  "" , 1 ) errordict [ errorname ] = errortext return errordict","if ""(pofilter) "" in note :","if note . startswith ( ""pofilter:"" ) :",83.02102961,FALSE,90.15
851,"def rename_path ( self , path , new_path ) : logger . debug ( "" rename_path  ' %s '  ->  ' %s ' "" % ( path , new_path ) ) dirs = self . readdir ( path ) for d in dirs : if d in [ "" . "" , "" .. "" ] : continue d_path = "" "" . join ( [ path , "" / "" , d ] ) d_new_path = "" "" . join ( [ new_path , "" / "" , d ] ) attr = self . getattr ( d_path ) <MASK> self . rename_path ( d_path , d_new_path ) else : self . rename_item ( d_path , d_new_path ) self . rename_item ( path , new_path , dir = True )","if stat . S_ISDIR ( attr [ ""st_mode"" ] ) :","if isinstance ( attr , dict ) :",89.39926945,FALSE,92.05
852,"def index ( self , url_id : int ) - > FlaskResponse : # pylint: disable=no-self-use url = db . session . query ( models . Url ) . get ( url_id ) if url and url . url : explore_url = "" //superset/explore/? "" <MASK> explore_url + = f "" r= { url_id } "" return redirect ( explore_url [ 1 : ] ) return redirect ( url . url [ 1 : ] ) flash ( "" URL to nowhere... "" , "" danger "" ) return redirect ( "" / "" )",if url . url . startswith ( explore_url ) :,if url_id :,70.42282934,FALSE,91.99
853,"def testShortCircuit ( self ) : """"""Test that creation short-circuits to reuse existing references"""""" sd = { } for s in self . ss : sd [ s ] = 1 for t in self . ts : <MASK> self . assertTrue ( sd . has_key ( safeRef ( t . x ) ) ) self . assertTrue ( safeRef ( t . x ) in sd ) else : self . assertTrue ( sd . has_key ( safeRef ( t ) ) ) self . assertTrue ( safeRef ( t ) in sd )","if hasattr ( t , ""x"" ) :","if isinstance ( t , ( int , long ) ) :",93.75088788,FALSE,92.1
854,"def wrapped ( request , * args , * * kwargs ) : if not request . user . is_authenticated ( ) : request . session [ "" _next "" ] = request . get_full_path ( ) <MASK> redirect_uri = reverse ( "" sentry-auth-organization "" , args = [ kwargs [ "" organization_slug "" ] ] ) else : redirect_uri = get_login_url ( ) return HttpResponseRedirect ( redirect_uri ) return func ( request , * args , * * kwargs )","if ""organization_slug"" in kwargs :","if kwargs . get ( ""is_active"" ) :",80.97274252,FALSE,90.35
855,"def read_info ( reader , dump = None ) : line_number_table_length = reader . read_u2 ( ) <MASK> reader . debug ( ""      "" * dump , "" Line numbers ( %s  total): "" % line_number_table_length ) line_numbers = [ ] for i in range ( 0 , line_number_table_length ) : start_pc = reader . read_u2 ( ) line_number = reader . read_u2 ( ) <MASK> reader . debug ( ""      "" * ( dump + 1 ) , "" %s :  %s "" % ( start_pc , line_number ) ) line_numbers . append ( ( start_pc , line_number ) ) return LineNumberTable ( line_numbers )",if dump is not None :,if dump :,91.8870525,FALSE,94.44
856,"def compute_timer_precision ( timer ) : precision = None points = 0 timeout = timeout_timer ( ) + 1.0 previous = timer ( ) while timeout_timer ( ) < timeout or points < 5 : for _ in XRANGE ( 10 ) : t1 = timer ( ) t2 = timer ( ) dt = t2 - t1 if 0 < dt : break else : dt = t2 - previous if dt < = 0.0 : continue <MASK> precision = min ( precision , dt ) else : precision = dt points + = 1 previous = timer ( ) return precision",if precision is not None :,if precision is not None :,100,TRUE,100
857,def get_hi_lineno ( self ) : lineno = Node . get_hi_lineno ( self ) if self . expr1 is None : pass else : lineno = self . expr1 . get_hi_lineno ( ) <MASK> pass else : lineno = self . expr2 . get_hi_lineno ( ) if self . expr3 is None : pass else : lineno = self . expr3 . get_hi_lineno ( ) return lineno,if self . expr2 is None :,if self . expr2 is None :,100,TRUE,100
858,"def validate_cluster_resource_group ( cmd , namespace ) : if namespace . cluster_resource_group is not None : client = get_mgmt_service_client ( cmd . cli_ctx , ResourceType . MGMT_RESOURCE_RESOURCES ) <MASK> raise InvalidArgumentValueError ( "" Invalid --cluster-resource-group  ' %s ' : resource group must not exist. "" % namespace . cluster_resource_group )",if client . resource_groups . check_existence ( namespace . cluster_resource_group ) :,if client . is_resource_group_exists ( namespace . cluster_resource_group ),87.64583961,FALSE,89.42
859,"def find_word_bounds ( self , text , index , allowed_chars ) : right = left = index done = False while not done : <MASK> done = True elif not self . word_boundary_char ( text [ left - 1 ] ) : left - = 1 else : done = True done = False while not done : if right == len ( text ) : done = True elif not self . word_boundary_char ( text [ right ] ) : right + = 1 else : done = True return left , right",if left == 0 :,if left == 0 and allowed_chars in text :,95.18772789,FALSE,93.66
860,"def _check_good_input ( self , X , y = None ) : if isinstance ( X , dict ) : lengths = [ len ( X1 ) for X1 in X . values ( ) ] if len ( set ( lengths ) ) > 1 : raise ValueError ( "" Not all values of X are of equal length. "" ) x_len = lengths [ 0 ] else : x_len = len ( X ) if y is not None : <MASK> raise ValueError ( "" X and y are not of equal length. "" ) if self . regression and y is not None and y . ndim == 1 : y = y . reshape ( - 1 , 1 ) return X , y",if len ( y ) != x_len :,if x_len != len ( X ) :,95.47043599,FALSE,95.05
861,"def _get_text_nodes ( nodes , html_body ) : text = [ ] open_tags = 0 for node in nodes : if isinstance ( node , HtmlTag ) : if node . tag_type == OPEN_TAG : open_tags + = 1 <MASK> open_tags - = 1 elif ( isinstance ( node , HtmlDataFragment ) and node . is_text_content and open_tags == 0 ) : text . append ( html_body [ node . start : node . end ] ) return text",elif node . tag_type == CLOSE_TAG :,elif node . tag_type == CLOSE_TAG :,100,TRUE,100
862,"def _get_spyne_type ( cls_name , k , v ) : try : v = NATIVE_MAP . get ( v , v ) except TypeError : return try : subc = issubclass ( v , ModelBase ) or issubclass ( v , SelfReference ) except : subc = False if subc : if issubclass ( v , Array ) and len ( v . _type_info ) != 1 : raise Exception ( "" Invalid Array definition in  %s . %s . "" % ( cls_name , k ) ) <MASK> raise Exception ( "" Please specify the number of dimensions "" ) return v","elif issubclass ( v , Point ) and v . Attributes . dim is None :",if len ( v . _type_info ) != 2 :,87.83931364,FALSE,88.73
863,"def customize ( cls , * * kwargs ) : """"""return a class with some existing attributes customized"""""" for name , value in kwargs . iteritems ( ) : <MASK> raise TransportError ( "" you cannot customize the protected attribute  %s "" % name ) if not hasattr ( cls , name ) : raise TransportError ( "" Transport has no attribute  %s "" % name ) NewSubClass = type ( "" Customized_ {} "" . format ( cls . __name__ ) , ( cls , ) , kwargs ) return NewSubClass","if name in [ ""cookie"" , ""circuit"" , ""upstream"" , ""downstream"" , ""stream"" ] :","if value is not None and name in [ ""protected"" , ""public"" ] :",91.60250377,FALSE,85.46
864,"def test_UNrelativize ( self ) : import URIlib relative = self . relative + self . full_relativize for base , rel , fullpath , common in relative : URI = uriparse . UnRelativizeURL ( base , rel ) fullURI = URIlib . URIParser ( URI ) # We need to canonicalize the result from unrelativize # compared to the original full path we expect to see. <MASK> fullpath = fullpath [ : - 1 ] self . failUnlessSamePath ( os . path . normcase ( fullURI . path ) , os . path . normcase ( fullpath ) )","if fullpath [ - 1 ] in ( ""/"" , ""\\"" ) :",if common :,94.15696984,FALSE,84.71
865,"def get_release_info ( file_path = RELEASE_FILE ) : RELEASE_TYPE_REGEX = re . compile ( r "" ^[Rr]elease [Tt]ype: (major|minor|patch)$ "" ) with open ( file_path , "" r "" ) as f : line = f . readline ( ) match = RELEASE_TYPE_REGEX . match ( line ) <MASK> print ( "" The file RELEASE.md should start with `Release type`  "" "" and specify one of the following values: major, minor or patch. "" ) sys . exit ( 1 ) type_ = match . group ( 1 ) changelog = "" "" . join ( [ line for line in f . readlines ( ) ] ) . strip ( ) return type_ , changelog",if not match :,if not match :,100,TRUE,100
866,"def _get_next_history_entry ( self ) : if self . _history : hist_len = len ( self . _history ) - 1 self . history_index = min ( hist_len , self . history_index + 1 ) index = self . history_index <MASK> self . history_index + = 1 return self . _history [ index ] return "" """,if self . history_index == hist_len :,if hist_len > 0 :,90.19448987,FALSE,88.57
867,"def star_op ( self ) : """"""Put a '*' op, with special cases for *args."""""" val = "" * "" if self . paren_level : i = len ( self . code_list ) - 1 if self . code_list [ i ] . kind == "" blank "" : i - = 1 token = self . code_list [ i ] <MASK> self . op_no_blanks ( val ) elif token . value == "" , "" : self . blank ( ) self . add_token ( "" op-no-blanks "" , val ) else : self . op ( val ) else : self . op ( val )","if token . kind == ""lt"" :","if token . value == "","" :",94.27937914,FALSE,96.51
868,"def get_safe_settings ( ) : "" Returns a dictionary of the settings module, with sensitive settings blurred out. "" settings_dict = { } for k in dir ( settings ) : <MASK> if HIDDEN_SETTINGS . search ( k ) : settings_dict [ k ] = "" ******************** "" else : settings_dict [ k ] = getattr ( settings , k ) return settings_dict",if k . isupper ( ) :,"if k . startswith ( ""_"" ) :",92.69572644,FALSE,94.05
869,"def nextEditable ( self ) : """"""Moves focus of the cursor to the next editable window"""""" if self . currentEditable is None : if len ( self . _editableChildren ) : self . _currentEditableRef = self . _editableChildren [ 0 ] else : for ref in weakref . getweakrefs ( self . currentEditable ) : <MASK> cei = self . _editableChildren . index ( ref ) nei = cei + 1 if nei > = len ( self . _editableChildren ) : nei = 0 self . _currentEditableRef = self . _editableChildren [ nei ] return self . currentEditable",if ref in self . _editableChildren :,if ref in self . _editableChildren :,100,TRUE,100
870,"def _handle_dependents_type ( types , type_str , type_name , rel_name , row ) : if types [ type_str [ 0 ] ] is None : <MASK> type_name = "" index "" rel_name = row [ "" indname "" ] + ""  ON  "" + rel_name elif type_str [ 0 ] == "" o "" : type_name = "" operator "" rel_name = row [ "" relname "" ] else : type_name = types [ type_str [ 0 ] ] return type_name , rel_name","if type_str [ 0 ] == ""i"" :","if type_str [ 0 ] == ""i"" :",100,TRUE,100
871,"def streamErrorHandler ( self , conn , error ) : name , text = "" error "" , error . getData ( ) for tag in error . getChildren ( ) : <MASK> if tag . getName ( ) == "" text "" : text = tag . getData ( ) else : name = tag . getName ( ) if name in stream_exceptions . keys ( ) : exc = stream_exceptions [ name ] else : exc = StreamError raise exc ( ( name , text ) )",if tag . getNamespace ( ) == NS_XMPP_STREAMS :,"if isinstance ( tag , StreamError ) :",93.51124251,FALSE,88.03
872,"def _validate_names ( self , settings : _SettingsType ) - > None : """"""Make sure all settings exist."""""" unknown = [ ] for name in settings : <MASK> unknown . append ( name ) if unknown : errors = [ configexc . ConfigErrorDesc ( "" While loading options "" , "" Unknown option  {} "" . format ( e ) ) for e in sorted ( unknown ) ] raise configexc . ConfigFileErrors ( "" autoconfig.yml "" , errors )",if name not in configdata . DATA :,if name not in self . _names :,92.54150439,FALSE,94.75
873,"def can_haz ( self , target , credentials ) : """"""Check whether key-values in target are present in credentials."""""" # TODO(termie): handle ANDs, probably by providing a tuple instead of a #               string for requirement in target : key , match = requirement . split ( "" : "" , 1 ) check = credentials . get ( key ) <MASK> check = [ check ] if match in check : return True","if check is None or isinstance ( check , basestring ) :","if not isinstance ( check , list ) :",69.70684397,FALSE,91.65
874,"def _recursive_fx_apply ( input : dict , fx ) : for k , v in input . items ( ) : <MASK> v = torch . tensor ( v ) if isinstance ( v , torch . Tensor ) : v = fx ( v . float ( ) ) input [ k ] = v else : _recursive_fx_apply ( v , fx )","if isinstance ( v , list ) :","if isinstance ( v , ( list , tuple ) ) :",94.90830004,FALSE,92.47
875,"def get ( self , url , * * kwargs ) : app , url = self . _prepare_call ( url , kwargs ) if app : <MASK> self . _first_ping = False return EmptyCapabilitiesResponse ( ) elif "" Hello0 "" in url and "" 1.2.1 "" in url and "" v1 "" in url : return ErrorApiResponse ( ) else : response = app . get ( url , * * kwargs ) return TestingResponse ( response ) else : return requests . get ( url , * * kwargs )","if url . endswith ( ""ping"" ) and self . _first_ping :","if ""AllCapabilities"" in url :",62.87551564,FALSE,85.69
876,"def server_thread_fn ( ) : server_ctx = ssl . create_default_context ( ssl . Purpose . CLIENT_AUTH ) server_ctx . load_cert_chain ( "" trio-test-1.pem "" ) server = server_ctx . wrap_socket ( server_sock , server_side = True , suppress_ragged_eofs = False , ) while True : data = server . recv ( 4096 ) print ( "" server got: "" , data ) <MASK> print ( "" server waiting for client to finish everything "" ) client_done . wait ( ) print ( "" server attempting to send back close-notify "" ) server . unwrap ( ) print ( "" server ok "" ) break server . sendall ( data )",if not data :,if client_done . wait ( timeout = 0.1 ) :,93.33703433,FALSE,92.55
877,"def find_hostnames ( data ) : # sends back an array of hostnames hostnames = [ ] for i in re . finditer ( hostname_regex , data ) : h = string . lower ( i . group ( 1 ) ) tld = h . split ( "" . "" ) [ - 1 : ] [ 0 ] <MASK> hostnames . append ( h ) return hostnames",if tld in tlds :,"if tld . startswith ( ""host"" ) :",71.36192283,FALSE,89.41
878,"def Validate ( self , win ) : textCtrl = self . GetWindow ( ) text = textCtrl . GetValue ( ) . strip ( ) sChar = Character . getInstance ( ) try : <MASK> raise ValueError ( _t ( "" You must supply a name for the Character! "" ) ) elif text in [ x . name for x in sChar . getCharacterList ( ) ] : raise ValueError ( _t ( "" Character name already in use, please choose another. "" ) ) return True except ValueError as e : pyfalog . error ( e ) wx . MessageBox ( "" {} "" . format ( e ) , _t ( "" Error "" ) ) textCtrl . SetFocus ( ) return False",if len ( text ) == 0 :,if not sChar . isValid ( ) :,84.6135592,FALSE,94.29
879,def get_random_user_agent ( agent_list = UA_CACHE ) : if not len ( agent_list ) : ua_file = file ( UA_FILE ) for line in ua_file : line = line . strip ( ) <MASK> agent_list . append ( line ) ua = random . choice ( UA_CACHE ) return ua,if line :,if line :,100,TRUE,100
880,"def _validate_action_like_for_prefixes ( self , key ) : for statement in self . _statements : <MASK> if isinstance ( statement [ key ] , string_types ) : self . _validate_action_prefix ( statement [ key ] ) else : for action in statement [ key ] : self . _validate_action_prefix ( action )",if key in statement :,if key in statement :,75,TRUE,100
881,"def predict ( self , X ) : if self . regression : return self . predict_proba ( X ) else : y_pred = np . argmax ( self . predict_proba ( X ) , axis = 1 ) <MASK> y_pred = self . enc_ . inverse_transform ( y_pred ) return y_pred",if self . use_label_encoder :,if self . enc_ :,81.63331078,FALSE,91.44
882,"def _threaded_request_tracker ( self , builder ) : while True : event_type = self . _read_q . get ( ) <MASK> return payload = { "" body "" : b "" "" } request_id = builder . build_record ( event_type , payload , "" "" ) self . _write_q . put_nowait ( request_id )",if event_type is False :,if event_type is None :,92.50355414,FALSE,96.9
883,"def __call__ ( self , value ) : try : super ( EmailValidator , self ) . __call__ ( value ) except ValidationError as e : # Trivial case failed. Try for possible IDN domain-part <MASK> parts = value . split ( "" @ "" ) try : parts [ - 1 ] = parts [ - 1 ] . encode ( "" idna "" ) . decode ( "" ascii "" ) except UnicodeError : raise e super ( EmailValidator , self ) . __call__ ( "" @ "" . join ( parts ) ) else : raise","if value and ""@"" in value :","if ""@"" in value :",73.1834208,FALSE,97.04
884,"def PreprocessConditionalStatement ( self , IfList , ReplacedLine ) : while self : if self . __Token : x = 1 <MASK> if self < = 2 : continue RegionSizeGuid = 3 if not RegionSizeGuid : RegionLayoutLine = 5 continue RegionLayoutLine = self . CurrentLineNumber return 1",elif not IfList :,if self . __Token [ 0 ] == IfList [ 0 ] :,60.15386235,FALSE,73.9
885,"def _arg_with_type ( self ) : for t in self . d [ "" Args "" ] : m = re . search ( "" ([A-Za-z0-9_-]+) \ s { 0,4}( \ (.+ \ )) \ s { 0,4}: "" , t ) <MASK> self . args [ m . group ( 1 ) ] = m . group ( 2 ) return self . args",if m :,if m :,100,TRUE,100
886,"def get_palette_for_custom_classes ( self , class_names , palette = None ) : if self . label_map is not None : # return subset of palette palette = [ ] for old_id , new_id in sorted ( self . label_map . items ( ) , key = lambda x : x [ 1 ] ) : if new_id != - 1 : palette . append ( self . PALETTE [ old_id ] ) palette = type ( self . PALETTE ) ( palette ) elif palette is None : <MASK> palette = np . random . randint ( 0 , 255 , size = ( len ( class_names ) , 3 ) ) else : palette = self . PALETTE return palette",if self . PALETTE is None :,if class_names :,97.24722145,FALSE,95.68
887,"def Visit_star_expr ( self , node ) : # pylint: disable=invalid-name # star_expr ::= '*' expr for child in node . children : self . Visit ( child ) <MASK> _AppendTokenSubtype ( child , format_token . Subtype . UNARY_OPERATOR ) _AppendTokenSubtype ( child , format_token . Subtype . VARARGS_STAR )","if isinstance ( child , pytree . Leaf ) and child . value == ""*"" :","if isinstance ( child , pytree . Leaf ) and child . value == ""*"" :",75,TRUE,100
888,"def create_if_compatible ( cls , typ : Type , * , root : "" RootNode "" ) - > Optional [ "" Node "" ] : if cls . compatible_types : target_type : Type = typ <MASK> target_type = getattr ( typ , "" __origin__ "" , None ) or typ if cls . _issubclass ( target_type , cls . compatible_types ) : return cls ( typ , root = root ) return None",if cls . use_origin :,"if hasattr ( typ , ""__origin__"" ) :",92.89398637,FALSE,87.76
889,"def grep_full_py_identifiers ( tokens ) : global pykeywords tokens = list ( tokens ) i = 0 while i < len ( tokens ) : tokentype , token = tokens [ i ] i + = 1 if tokentype != "" id "" : continue while ( i + 1 < len ( tokens ) and tokens [ i ] == ( "" op "" , "" . "" ) and tokens [ i + 1 ] [ 0 ] == "" id "" ) : token + = "" . "" + tokens [ i + 1 ] [ 1 ] i + = 2 <MASK> continue if token in pykeywords : continue if token [ 0 ] in "" .0123456789 "" : continue yield token","if token == """" :","if token in ( ""op"" , ""dot"" ) :",97.52243639,FALSE,92.52
890,"def create_config_filepath ( cls , visibility = None ) : if cls . is_local ( visibility ) : # Local to this directory base_path = os . path . join ( "" . "" ) <MASK> # Add it to the current ""./.polyaxon"" base_path = os . path . join ( base_path , "" .polyaxon "" ) cls . _create_dir ( base_path ) elif cls . CONFIG_PATH : # Custom path pass else : # Handle both global and all cases base_path = polyaxon_user_path ( ) cls . _create_dir ( base_path )",if cls . IS_POLYAXON_DIR :,if os . path . exists ( base_path ) :,96.57016033,FALSE,91.98
891,"def test_len ( self ) : eq = self . assertEqual eq ( base64MIME . base64_len ( "" hello "" ) , len ( base64MIME . encode ( "" hello "" , eol = "" "" ) ) ) for size in range ( 15 ) : if size == 0 : bsize = 0 <MASK> bsize = 4 elif size < = 6 : bsize = 8 elif size < = 9 : bsize = 12 elif size < = 12 : bsize = 16 else : bsize = 20 eq ( base64MIME . base64_len ( "" x "" * size ) , bsize )",elif size <= 3 :,elif size <= 3 :,100,TRUE,100
892,"def as_dict ( path = "" "" , version = "" latest "" , section = "" meta-data "" ) : result = { } dirs = dir ( path , version , section ) if not dirs : return None for item in dirs : <MASK> records = as_dict ( path + item , version , section ) if records : result [ item [ : - 1 ] ] = records elif is_dict . match ( item ) : idx , name = is_dict . match ( item ) . groups ( ) records = as_dict ( path + idx + "" / "" , version , section ) if records : result [ name ] = records else : result [ item ] = valueconv ( get ( path + item , version , section ) ) return result","if item . endswith ( ""/"" ) :",if is_dict . match ( item ) :,94.35734422,FALSE,95.12
893,"def api_read ( self ) : result = { } files = [ "" my.cnf "" , "" debian.cnf "" ] directory_list = self . exec_payload ( "" mysql_config_directory "" ) [ "" directory "" ] for _file in files : for directory in directory_list : mysql_conf = directory + _file content = self . shell . read ( mysql_conf ) <MASK> result [ mysql_conf ] = content return result",if content :,if content :,100,TRUE,100
894,"def generate ( self , count = 100 ) : self . pre_generate ( ) counter = iter ( range ( count ) ) created = 0 while True : batch = list ( islice ( counter , self . batch_size ) ) <MASK> break self . do_generate ( batch , self . batch_size ) from_size = created created + = len ( batch ) print ( "" Generate  %s :  %s - %s "" % ( self . resource , from_size , created ) ) self . after_generate ( )",if not batch :,if len ( batch ) == 0 :,94.90355441,FALSE,92.77
895,"def _normalize_fields ( self , document , loader ) : # type: (Dict[Text, Text], Loader) -> None # Normalize fields which are prefixed or full URIn to vocabulary terms for d in list ( document . keys ( ) ) : d2 = loader . expand_url ( d , u "" "" , scoped_id = False , vocab_term = True ) <MASK> document [ d2 ] = document [ d ] del document [ d ]",if d != d2 :,if d2 not in document :,72.01877442,FALSE,94.58
896,"def load_cache ( filename , get_key = mangle_key ) : cache = { } if not os . path . exists ( filename ) : return cache f = open ( filename , "" rb "" ) l = 0 for line in f . readlines ( ) : l + = 1 fields = line . split ( b ""   "" ) <MASK> sys . stderr . write ( "" Invalid file format in [ %s ], line  %d \n "" % ( filename , l ) ) continue # put key:value in cache, key without ^: cache [ get_key ( fields [ 0 ] [ 1 : ] ) ] = fields [ 1 ] . split ( b "" \n "" ) [ 0 ] f . close ( ) return cache","if fields == None or not len ( fields ) == 2 or fields [ 0 ] [ 0 : 1 ] != b"":"" :",if len ( fields ) != 2 :,75.06061525,FALSE,84.51
897,"def __lshift__ ( self , other ) : if not self . symbolic and type ( other ) is int : return RegisterOffset ( self . _bits , self . reg , self . _to_signed ( self . offset << other ) ) else : <MASK> return RegisterOffset ( self . _bits , self . reg , self . offset << other ) else : return RegisterOffset ( self . _bits , self . reg , ArithmeticExpression ( ArithmeticExpression . LShift , ( self . offset , other , ) , ) , )",if self . symbolic :,if self . symbolic :,75,TRUE,100
898,"def SaveSettings ( self , force = False ) : if self . config is not None : frame . ShellFrameMixin . SaveSettings ( self ) <MASK> frame . Frame . SaveSettings ( self , self . config ) self . shell . SaveSettings ( self . config )",if self . autoSaveSettings or force :,if force :,89.438423,FALSE,89.58
899,"def _parse_gene ( element ) : for genename_element in element : if "" type "" in genename_element . attrib : ann_key = "" gene_ %s _ %s "" % ( genename_element . tag . replace ( NS , "" "" ) , genename_element . attrib [ "" type "" ] , ) <MASK> self . ParsedSeqRecord . annotations [ ann_key ] = genename_element . text else : append_to_annotations ( ann_key , genename_element . text )","if genename_element . attrib [ ""type"" ] == ""primary"" :",if ann_key not in self . ParsedSeqRecord . annotations :,66.37047839,FALSE,86.32
900,"def _write_pkg_file ( self , file ) : with TemporaryFile ( mode = "" w+ "" ) as tmpfd : _write_pkg_file_orig ( self , tmpfd ) tmpfd . seek ( 0 ) for line in tmpfd : <MASK> file . write ( "" Metadata-Version: 2.1 \n "" ) elif line . startswith ( "" Description:  "" ) : file . write ( "" Description-Content-Type:  %s ; charset=UTF-8 \n "" % long_description_content_type ) file . write ( line ) else : file . write ( line )","if line . startswith ( ""Metadata-Version: "" ) :","if line . startswith ( ""Metadata-Version: 2.1"" ) :",98.65376697,FALSE,97.95
901,"def get ( self ) : """"""If a value/an exception is stored, return/raise it. Otherwise until switch() or throw() is called."""""" if self . _exception is not _NONE : <MASK> return self . value getcurrent ( ) . throw ( * self . _exception ) # pylint:disable=undefined-variable else : if self . greenlet is not None : raise ConcurrentObjectUseError ( "" This Waiter is already used by  %r "" % ( self . greenlet , ) ) self . greenlet = getcurrent ( ) # pylint:disable=undefined-variable try : return self . hub . switch ( ) finally : self . greenlet = None",if self . _exception is None :,if self . value is not None :,95.15273817,FALSE,96.89
902,"def connect ( self , * args ) : """"""connects to the dropbox. args[0] is the username."""""" if len ( args ) != 1 : return "" expected one argument! "" try : dbci = get_dropbox_client ( args [ 0 ] , False , None , None ) except Exception as e : return e . message else : <MASK> return "" No Dropbox configured for  ' {u} ' . "" . format ( u = args [ 0 ] ) else : self . client = dbci return True",if dbci is None :,if dbci is None :,100,TRUE,100
903,"def escape ( text , newline = False ) : """"""Escape special html characters."""""" if isinstance ( text , str ) : if "" & "" in text : text = text . replace ( "" & "" , "" &amp; "" ) if "" > "" in text : text = text . replace ( "" > "" , "" &gt; "" ) if "" < "" in text : text = text . replace ( "" < "" , "" &lt; "" ) if ' "" ' in text : text = text . replace ( ' "" ' , "" &quot; "" ) if "" ' "" in text : text = text . replace ( "" ' "" , "" &quot; "" ) if newline : <MASK> text = text . replace ( "" \n "" , "" <br> "" ) return text","if ""\n"" in text :","if ""\n"" in text :",100,TRUE,100
904,def t ( ret ) : with IPDB ( ) as ipdb : with ipdb . eventqueue ( ) as evq : for msg in evq : <MASK> ret . append ( msg ) return,"if msg . get_attr ( ""IFLA_IFNAME"" ) == ""test1984"" :","if msg . get ( ""type"" ) == ""t"" :",92.35428058,FALSE,79.65
905,"def check_stmt ( self , stmt ) : if is_future ( stmt ) : for name , asname in stmt . names : <MASK> self . found [ name ] = 1 else : raise SyntaxError ( "" future feature  %s  is not defined "" % name ) stmt . valid_future = 1 return 1 return 0",if name in self . features :,if name in self . found :,97.98261337,FALSE,96.31
906,"def process_pypi_option ( option , option_str , option_value , parser ) : if option_str . startswith ( "" --no "" ) : setattr ( parser . values , option . dest , [ ] ) else : indexes = getattr ( parser . values , option . dest , [ ] ) <MASK> indexes . append ( _PYPI ) setattr ( parser . values , option . dest , indexes )",if _PYPI not in indexes :,"if option_str . startswith ( ""--no"" ) :",89.27932582,FALSE,87.67
907,"def modify_address ( self , name , address , domain ) : if not self . get_entries_by_name ( name , domain ) : raise exception . NotFound infile = open ( self . filename , "" r "" ) outfile = tempfile . NamedTemporaryFile ( "" w "" , delete = False ) for line in infile : entry = self . parse_line ( line ) <MASK> outfile . write ( "" %s     %s     %s \n "" % ( address , self . qualify ( name , domain ) , entry [ "" type "" ] ) ) else : outfile . write ( line ) infile . close ( ) outfile . close ( ) shutil . move ( outfile . name , self . filename )","if entry and entry [ ""name"" ] . lower ( ) == self . qualify ( name , domain ) . lower ( ) :","if entry [ ""type"" ] :",81.91088322,FALSE,84.74
908,"def tms_to_quadkey ( self , tms , google = False ) : quadKey = "" "" x , y , z = tms # this algorithm works with google tiles, rather than tms, so convert # to those first. if not google : y = ( 2 * * z - 1 ) - y for i in range ( z , 0 , - 1 ) : digit = 0 mask = 1 << ( i - 1 ) if ( x & mask ) != 0 : digit + = 1 <MASK> digit + = 2 quadKey + = str ( digit ) return quadKey",if ( y & mask ) != 0 :,if ( y & mask ) != 0 :,100,TRUE,100
909,"def add_if_unique ( self , issuer , use , keys ) : if use in self . issuer_keys [ issuer ] and self . issuer_keys [ issuer ] [ use ] : for typ , key in keys : flag = 1 for _typ , _key in self . issuer_keys [ issuer ] [ use ] : if _typ == typ and key is _key : flag = 0 break <MASK> self . issuer_keys [ issuer ] [ use ] . append ( ( typ , key ) ) else : self . issuer_keys [ issuer ] [ use ] = keys",if flag :,if not flag :,98.1439577,FALSE,97.95
910,"def scan_error ( self ) : "" A string describing why the last scan failed, or None if it didn ' t. "" self . acquire_lock ( ) try : <MASK> try : self . _load_buf_data_once ( ) except NotFoundInDatabase : pass return self . _scan_error_cache finally : self . release_lock ( )",if self . _scan_error_cache is None :,if self . _scan_error_cache is None :,75,TRUE,100
911,"def _query ( self ) : if self . _mongo_query is None : self . _mongo_query = self . _query_obj . to_query ( self . _document ) <MASK> if "" _cls "" in self . _mongo_query : self . _mongo_query = { "" $and "" : [ self . _cls_query , self . _mongo_query ] } else : self . _mongo_query . update ( self . _cls_query ) return self . _mongo_query",if self . _cls_query :,if self . _mongo_query is not None :,94.16517364,FALSE,94.17
912,"def CountButtons ( self ) : """"""Returns the number of visible buttons in the docked pane."""""" n = 0 if self . HasCaption ( ) or self . HasCaptionLeft ( ) : if isinstance ( wx . GetTopLevelParent ( self . window ) , AuiFloatingFrame ) : return 1 if self . HasCloseButton ( ) : n + = 1 if self . HasMaximizeButton ( ) : n + = 1 if self . HasMinimizeButton ( ) : n + = 1 <MASK> n + = 1 return n",if self . HasPinButton ( ) :,if self . HasCloseButton ( ) :,98.67233756,FALSE,97.49
913,"def testBind ( self ) : try : with socket . socket ( socket . PF_CAN , socket . SOCK_DGRAM , socket . CAN_J1939 ) as s : addr = ( self . interface , socket . J1939_NO_NAME , socket . J1939_NO_PGN , socket . J1939_NO_ADDR , ) s . bind ( addr ) self . assertEqual ( s . getsockname ( ) , addr ) except OSError as e : <MASK> self . skipTest ( "" network interface ` %s ` does not exist "" % self . interface ) else : raise",if e . errno == errno . ENODEV :,if e . errno == errno . ENOTSUP :,98.76351671,FALSE,97.91
914,"def createFields ( self ) : while self . current_size < self . size : pos = self . stream . searchBytes ( "" \0 \0 \1 "" , self . current_size , self . current_size + 1024 * 1024 * 8 ) # seek forward by at most 1MB if pos is not None : padsize = pos - self . current_size <MASK> yield PaddingBytes ( self , "" pad[] "" , padsize / / 8 ) chunk = Chunk ( self , "" chunk[] "" ) try : # force chunk to be processed, so that CustomFragments are complete chunk [ "" content/data "" ] except : pass yield chunk",if padsize :,if padsize > 1 :,98.8949229,FALSE,97.35
915,"def index_modulemd_files ( repo_path ) : merger = Modulemd . ModuleIndexMerger ( ) for fn in sorted ( os . listdir ( repo_path ) ) : <MASK> continue yaml_path = os . path . join ( repo_path , fn ) mmd = Modulemd . ModuleIndex ( ) mmd . update_from_file ( yaml_path , strict = True ) merger . associate_index ( mmd , 0 ) return merger . resolve ( )","if not fn . endswith ( "".yaml"" ) :","if fn . startswith ( ""__"" ) :",80.82103129,FALSE,91.98
916,"def set_visible ( self , visible = True ) : self . _visible = visible if self . _nswindow is not None : <MASK> # Not really sure why on_resize needs to be here, # but it's what pyglet wants. self . dispatch_event ( "" on_resize "" , self . _width , self . _height ) self . dispatch_event ( "" on_show "" ) self . dispatch_event ( "" on_expose "" ) self . _nswindow . makeKeyAndOrderFront_ ( None ) else : self . _nswindow . orderOut_ ( None )",if visible :,if self . _nswindow . makeKeyAndOrderFront_ ( ) is not None :,87.85698287,FALSE,89.4
917,"def __repr__ ( self ) : if self . _in_repr : return "" <recursion> "" try : self . _in_repr = True if self . is_computed ( ) : status = "" computed,  "" if self . error ( ) is None : <MASK> status + = "" = self "" else : status + = "" =  "" + repr ( self . value ( ) ) else : status + = "" error =  "" + repr ( self . error ( ) ) else : status = "" isn ' t computed "" return "" %s  ( %s ) "" % ( type ( self ) , status ) finally : self . _in_repr = False",if self . value ( ) is self :,if self . value ( ) is None :,99.01476251,FALSE,98.27
918,"def _individual_get ( self , segment , index_type , index , strictdoc ) : if index_type == "" val "" : for key , value in segment . items ( ) : if key == index [ 0 ] : return value <MASK> if key . text == index [ 0 ] : return value raise Exception ( "" Invalid state "" ) elif index_type == "" index "" : return segment [ index ] elif index_type == "" textslice "" : return segment [ index [ 0 ] : index [ 1 ] ] elif index_type == "" key "" : return index [ 1 ] if strictdoc else index [ 0 ] else : raise Exception ( "" Invalid state "" )","if hasattr ( key , ""text"" ) :","elif index_type == ""text"" :",69.48128819,FALSE,94
919,"def _makeSafeAbsoluteURI ( base , rel = None ) : # bail if ACCEPTABLE_URI_SCHEMES is empty if not ACCEPTABLE_URI_SCHEMES : return _urljoin ( base , rel or u "" "" ) if not base : return rel or u "" "" if not rel : try : scheme = urlparse . urlparse ( base ) [ 0 ] except ValueError : return u "" "" <MASK> return base return u "" "" uri = _urljoin ( base , rel ) if uri . strip ( ) . split ( "" : "" , 1 ) [ 0 ] not in ACCEPTABLE_URI_SCHEMES : return u "" "" return uri",if not scheme or scheme in ACCEPTABLE_URI_SCHEMES :,if scheme in ACCEPTABLE_URI_SCHEMES :,97.8708924,FALSE,96.59
920,"def _write_packet ( self , packet ) : # Immediately writes the given packet to the network. The caller must # have the write lock acquired before calling this method. try : for listener in self . early_outgoing_packet_listeners : listener . call_packet ( packet ) <MASK> packet . write ( self . socket , self . options . compression_threshold ) else : packet . write ( self . socket ) for listener in self . outgoing_packet_listeners : listener . call_packet ( packet ) except IgnorePacket : pass",if self . options . compression_enabled :,if self . options . compression_threshold is not None :,72.87010873,FALSE,95.3
921,"def rangelist_to_set ( rangelist ) : result = set ( ) if not rangelist : return result for x in rangelist . split ( "" , "" ) : <MASK> result . add ( int ( x ) ) continue m = re . match ( r "" ^( \ d+)-( \ d+)$ "" , x ) if m : start = int ( m . group ( 1 ) ) end = int ( m . group ( 2 ) ) result . update ( set ( range ( start , end + 1 ) ) ) continue msg = "" Cannot understand data input:  %s   %s "" % ( x , rangelist ) raise ValueError ( msg ) return result","if re . match ( r""^(\d+)$"" , x ) :","if x . startswith ( ""data:"" ) :",89.72800516,FALSE,89.3
922,"def test_device_property_logfile_isinstance ( self ) : mock = MagicMock ( ) with patch ( builtin_string + "" .open "" , mock ) : <MASK> builtin_file = "" io.TextIOWrapper "" else : builtin_file = builtin_string + "" .file "" with patch ( builtin_file , MagicMock ) : handle = open ( "" filename "" , "" r "" ) self . dev . logfile = handle self . assertEqual ( self . dev . logfile , handle )","if sys . version > ""3"" :","if sys . platform == ""win32"" :",97.26664541,FALSE,94.11
923,"def _line_ranges ( statements , lines ) : """"""Produce a list of ranges for `format_lines`."""""" statements = sorted ( statements ) lines = sorted ( lines ) pairs = [ ] start = None lidx = 0 for stmt in statements : if lidx > = len ( lines ) : break if stmt == lines [ lidx ] : lidx + = 1 if not start : start = stmt end = stmt <MASK> pairs . append ( ( start , end ) ) start = None if start : pairs . append ( ( start , end ) ) return pairs",elif start :,if start and end :,96.40999837,FALSE,95.62
924,"def reset_parameters ( self ) : initialize = layers . get_initializer ( self . _hparams . initializer ) if initialize is not None : # Do not re-initialize LayerNorm modules. for name , param in self . named_parameters ( ) : <MASK> initialize ( param )","if name . split ( ""."" ) [ - 1 ] == ""weight"" and ""layer_norm"" not in name :","if name . startswith ( ""LayerNorm"" ) :",86.75769032,FALSE,67.67
925,"def billing_invoice_show_validator ( namespace ) : from azure . cli . core . azclierror import ( RequiredArgumentMissingError , MutuallyExclusiveArgumentError , ) valid_combs = ( "" only --account-name, --name / --name / --name, --by-subscription is valid "" ) if namespace . account_name is not None : if namespace . by_subscription is not None : raise MutuallyExclusiveArgumentError ( valid_combs ) <MASK> raise RequiredArgumentMissingError ( "" --name is also required "" ) if namespace . by_subscription is not None : <MASK> raise RequiredArgumentMissingError ( "" --name is also required "" )",if namespace . name is None :,if namespace . name is not None :,97.96820547,FALSE,95.74
926,"def DeleteDocuments ( self , document_ids , response ) : """"""Deletes documents for the given document_ids."""""" for document_id in document_ids : <MASK> document = self . _documents [ document_id ] self . _inverted_index . RemoveDocument ( document ) del self . _documents [ document_id ] delete_status = response . add_status ( ) delete_status . set_code ( search_service_pb . SearchServiceError . OK )",if document_id in self . _documents :,if document_id in self . _documents :,100,TRUE,100
927,"def generate_new_element ( items , prefix , numeric = False ) : """"""Creates a random string with prefix, that is not in 'items' list."""""" while True : <MASK> candidate = prefix + generate_random_numeric ( 8 ) else : candidate = prefix + generate_random_alphanumeric ( 8 ) if not candidate in items : return candidate LOG . debug ( "" Random collision on  %s "" % candidate )",if numeric :,if numeric :,100,TRUE,100
928,"def generate_text_for_vocab ( self , data_dir , tmp_dir ) : for i , sample in enumerate ( self . generate_samples ( data_dir , tmp_dir , problem . DatasetSplit . TRAIN ) ) : if self . has_inputs : yield sample [ "" inputs "" ] yield sample [ "" targets "" ] <MASK> break",if self . max_samples_for_vocab and ( i + 1 ) >= self . max_samples_for_vocab :,if i == self . num_samples - 1 :,70.86013409,FALSE,73.73
929,"def _get_ccp ( config = None , config_path = None , saltenv = "" base "" ) : """""" """""" if config_path : config = __salt__ [ "" cp.get_file_str "" ] ( config_path , saltenv = saltenv ) <MASK> raise SaltException ( "" {}  is not available "" . format ( config_path ) ) if isinstance ( config , six . string_types ) : config = config . splitlines ( ) ccp = ciscoconfparse . CiscoConfParse ( config ) return ccp",if config is False :,if not config :,94.17637504,FALSE,96.17
930,"def rpush ( key , * vals , * * kwargs ) : ttl = kwargs . get ( "" ttl "" ) cap = kwargs . get ( "" cap "" ) if not ttl and not cap : _client . rpush ( key , * vals ) else : pipe = _client . pipeline ( ) pipe . rpush ( key , * vals ) <MASK> pipe . ltrim ( key , 0 , cap ) if ttl : pipe . expire ( key , ttl ) pipe . execute ( )",if cap :,if cap :,100,TRUE,100
931,"def check_apns_certificate ( ss ) : mode = "" start "" for s in ss . split ( "" \n "" ) : <MASK> if "" BEGIN RSA PRIVATE KEY "" in s or "" BEGIN PRIVATE KEY "" in s : mode = "" key "" elif mode == "" key "" : if "" END RSA PRIVATE KEY "" in s or "" END PRIVATE KEY "" in s : mode = "" end "" break elif s . startswith ( "" Proc-Type "" ) and "" ENCRYPTED "" in s : raise ImproperlyConfigured ( "" Encrypted APNS private keys are not supported "" ) if mode != "" end "" : raise ImproperlyConfigured ( "" The APNS certificate doesn ' t contain a private key "" )","if mode == ""start"" :","if s . startswith ( ""APNS-Type"" ) :",95.60876706,FALSE,93.75
932,"def _add_communication_type ( apps , schema_editor , communication_type ) : Worker = apps . get_model ( "" orchestra "" , "" Worker "" ) CommunicationPreference = apps . get_model ( "" orchestra "" , "" CommunicationPreference "" ) for worker in Worker . objects . all ( ) : ( communication_preference , created , ) = CommunicationPreference . objects . get_or_create ( worker = worker , communication_type = communication_type ) # By default set both Slack and Email notifications to True <MASK> communication_preference . methods . slack = True communication_preference . methods . email = True communication_preference . save ( )",if created :,if created :,100,TRUE,100
933,"def get_postgresql_driver_name ( ) : # pylint: disable=unused-variable try : driver = os . getenv ( "" CODECHECKER_DB_DRIVER "" ) <MASK> return driver try : # pylint: disable=W0611 import psycopg2 return "" psycopg2 "" except Exception : # pylint: disable=W0611 import pg8000 return "" pg8000 "" except Exception as ex : LOG . error ( str ( ex ) ) LOG . error ( "" Failed to import psycopg2 or pg8000 module. "" ) raise",if driver :,if driver :,75,TRUE,100
934,"def env_purge_doc ( app : Sphinx , env : BuildEnvironment , docname : str ) - > None : modules = getattr ( env , "" _viewcode_modules "" , { } ) for modname , entry in list ( modules . items ( ) ) : if entry is False : continue code , tags , used , refname = entry for fullname in list ( used ) : if used [ fullname ] == docname : used . pop ( fullname ) <MASK> modules . pop ( modname )",if len ( used ) == 0 :,if modname in modules :,93.56994179,FALSE,91.83
935,"def do_query ( data , q ) : ret = [ ] if not q : return ret qkey = q [ 0 ] for key , value in iterate ( data ) : if len ( q ) == 1 : if key == qkey : ret . append ( value ) elif is_iterable ( value ) : ret . extend ( do_query ( value , q ) ) else : <MASK> continue if key == qkey : ret . extend ( do_query ( value , q [ 1 : ] ) ) else : ret . extend ( do_query ( value , q ) ) return ret",if not is_iterable ( value ) :,if not is_iterable ( value ) :,100,TRUE,100
936,"def _get_bucket_for_key ( self , key : bytes ) - > Optional [ _DBValueTuple ] : dbs : Iterable [ PartitionDB ] try : partition = self . _key_index [ key ] dbs = [ PartitionDB ( partition , self . _dbs [ partition ] ) ] except KeyError : dbs = cast ( Iterable [ PartitionDB ] , self . _dbs . items ( ) ) for partition , db in dbs : if db . key_may_exist ( key ) [ 0 ] : value = db . get ( key ) <MASK> self . _key_index [ key ] = partition return _DBValueTuple ( db , value ) return None",if value is not None :,if value is not None :,100,TRUE,100
937,"def _clean ( self ) : logger . info ( "" Cleaning up... "" ) if self . _process is not None : if self . _process . poll ( ) is None : for _ in range ( 3 ) : self . _process . terminate ( ) time . sleep ( 0.5 ) <MASK> break else : self . _process . kill ( ) self . _process . wait ( ) logger . error ( "" KILLED "" ) if os . path . exists ( self . _tmp_dir ) : shutil . rmtree ( self . _tmp_dir ) self . _process = None self . _ws = None logger . info ( "" Cleanup complete "" )",if self . _process . poll ( ) is not None :,if self . _process . poll ( ) is not None :,100,TRUE,100
938,"def _calculate_runtimes ( states ) : results = { "" runtime "" : 0.00 , "" num_failed_states "" : 0 , "" num_passed_states "" : 0 } for state , resultset in states . items ( ) : if isinstance ( resultset , dict ) and "" duration "" in resultset : # Count the pass vs failures <MASK> results [ "" num_passed_states "" ] + = 1 else : results [ "" num_failed_states "" ] + = 1 # Count durations results [ "" runtime "" ] + = resultset [ "" duration "" ] log . debug ( "" Parsed state metrics:  {} "" . format ( results ) ) return results","if resultset [ ""result"" ] :","if resultset [ ""pass"" ] :",98.94216164,FALSE,98.21
939,"def spaces_after ( token , prev , next , min = - 1 , max = - 1 , min_desc = None , max_desc = None ) : if next is not None and token . end_mark . line == next . start_mark . line : spaces = next . start_mark . pointer - token . end_mark . pointer if max != - 1 and spaces > max : return LintProblem ( token . start_mark . line + 1 , next . start_mark . column , max_desc ) <MASK> return LintProblem ( token . start_mark . line + 1 , next . start_mark . column + 1 , min_desc )",elif min != - 1 and spaces < min :,if min != - 1 and spaces < min :,97.24127633,FALSE,98.28
940,"def getfileinfo ( name ) : finfo = FInfo ( ) with io . open ( name , "" rb "" ) as fp : # Quick check for textfile data = fp . read ( 512 ) <MASK> finfo . Type = "" TEXT "" fp . seek ( 0 , 2 ) dsize = fp . tell ( ) dir , file = os . path . split ( name ) file = file . replace ( "" : "" , "" - "" , 1 ) return file , finfo , dsize , 0",if 0 not in data :,"if data == b"""" :",97.04782108,FALSE,92.9
941,"def dict_to_XML ( tag , dictionary , * * kwargs ) : """"""Return XML element converting dicts recursively."""""" elem = Element ( tag , * * kwargs ) for key , val in dictionary . items ( ) : if tag == "" layers "" : child = dict_to_XML ( "" layer "" , val , name = key ) <MASK> child = dict_to_XML ( key , val ) else : if tag == "" config "" : child = Element ( "" variable "" , name = key ) else : child = Element ( key ) child . text = str ( val ) elem . append ( child ) return elem","elif isinstance ( val , MutableMapping ) :","elif tag == ""dicts"" :",83.14669732,FALSE,94.65
942,"def _read_bytes ( self , length ) : buffer = b "" "" while length : chunk = self . request . recv ( length ) <MASK> log . debug ( "" Connection closed "" ) return False length - = len ( chunk ) buffer + = chunk return buffer","if chunk == b"""" :",if not chunk :,92.60062697,FALSE,87.68
943,"def rec_deps ( services , container_by_name , cnt , init_service ) : deps = cnt [ "" _deps "" ] for dep in deps . copy ( ) : dep_cnts = services . get ( dep ) <MASK> continue dep_cnt = container_by_name . get ( dep_cnts [ 0 ] ) if dep_cnt : # TODO: avoid creating loops, A->B->A if init_service and init_service in dep_cnt [ "" _deps "" ] : continue new_deps = rec_deps ( services , container_by_name , dep_cnt , init_service ) deps . update ( new_deps ) return deps",if not dep_cnts :,if not dep_cnts :,100,TRUE,100
944,"def fix_repeating_arguments ( self ) : """"""Fix elements that should accumulate/increment values."""""" either = [ list ( child . children ) for child in transform ( self ) . children ] for case in either : for e in [ child for child in case if case . count ( child ) > 1 ] : <MASK> if e . value is None : e . value = [ ] elif type ( e . value ) is not list : e . value = e . value . split ( ) if type ( e ) is Command or type ( e ) is Option and e . argcount == 0 : e . value = 0 return self",if type ( e ) is Argument or type ( e ) is Option and e . argcount :,if type ( e ) is Argument and e . argcount == 1 :,94.56774332,FALSE,93.32
945,"def do_cli ( manager , options ) : header = [ "" Name "" , "" Description "" ] table_data = [ header ] for filter_name , filter in get_filters ( ) : <MASK> continue filter_doc = inspect . getdoc ( filter ) or "" "" table_data . append ( [ filter_name , filter_doc ] ) try : table = TerminalTable ( options . table_type , table_data ) except TerminalTableError as e : console ( "" ERROR:  %s "" % str ( e ) ) else : console ( table . output )",if options . name and not options . name in filter_name :,"if filter_name . startswith ( ""_"" ) :",87.36334059,FALSE,90.67
946,"def _do_cmp ( f1 , f2 ) : bufsize = BUFSIZE with open ( f1 , "" rb "" ) as fp1 , open ( f2 , "" rb "" ) as fp2 : while True : b1 = fp1 . read ( bufsize ) b2 = fp2 . read ( bufsize ) if b1 != b2 : return False <MASK> return True",if not b1 :,if b1 == b2 :,94.77487725,FALSE,92.49
947,"def apply ( self , db , person ) : families = person . get_parent_family_handle_list ( ) if families == [ ] : return True for family_handle in person . get_parent_family_handle_list ( ) : family = db . get_family_from_handle ( family_handle ) if family : father_handle = family . get_father_handle ( ) mother_handle = family . get_mother_handle ( ) <MASK> return True if not mother_handle : return True return False",if not father_handle :,if father_handle and mother_handle :,95.38547549,FALSE,95.09
948,"def caesar_cipher ( s , k ) : result = "" "" for char in s : n = ord ( char ) if 64 < n < 91 : n = ( ( n - 65 + k ) % 26 ) + 65 <MASK> n = ( ( n - 97 + k ) % 26 ) + 97 result = result + chr ( n ) return result",if 96 < n < 123 :,elif 32 < n < 97 :,74.23622537,FALSE,92.17
949,"def title_by_index ( self , trans , index , context ) : d_type = self . get_datatype ( trans , context ) for i , ( composite_name , composite_file ) in enumerate ( d_type . writable_files . items ( ) ) : if i == index : rval = composite_name if composite_file . description : rval = "" {}  ( {} ) "" . format ( rval , composite_file . description ) <MASK> rval = "" %s  [optional] "" % rval return rval if index < self . get_file_count ( trans , context ) : return "" Extra primary file "" return None",if composite_file . optional :,if composite_file . optional :,100,TRUE,100
950,"def __str__ ( self ) : t = ""      "" if self . _name != "" root "" : r = f "" { t * ( self . _level - 1 ) } { self . _name } : \n "" else : r = "" "" level = self . _level for i , ( k , v ) in enumerate ( self . _pointer . items ( ) ) : <MASK> r + = f "" { t * ( self . _level ) } { v } \n "" self . _level + = 1 else : r + = f "" { t * ( self . _level ) } { k } :  { v }  ( { type ( v ) . __name__ } ) \n "" self . _level = level return r [ : - 1 ]","if isinstance ( v , Config ) :",if i == level :,92.43231493,FALSE,95.65
951,"def __get_securitygroups ( vm_ ) : vm_securitygroups = config . get_cloud_config_value ( "" securitygroups "" , vm_ , __opts__ , search_global = False ) if not vm_securitygroups : return [ ] securitygroups = list_securitygroups ( ) for i in range ( len ( vm_securitygroups ) ) : vm_securitygroups [ i ] = six . text_type ( vm_securitygroups [ i ] ) <MASK> raise SaltCloudNotFound ( "" The specified securitygroups  ' {0} '  could not be found. "" . format ( vm_securitygroups [ i ] ) ) return vm_securitygroups",if vm_securitygroups [ i ] not in securitygroups :,if not vm_securitygroups :,85.38376925,FALSE,94.04
952,"def assert_walk_snapshot ( self , field , filespecs_or_globs , paths , ignore_patterns = None , prepare = None ) : with self . mk_project_tree ( ignore_patterns = ignore_patterns ) as project_tree : scheduler = self . mk_scheduler ( rules = create_fs_rules ( ) , project_tree = project_tree ) <MASK> prepare ( project_tree ) result = self . execute ( scheduler , Snapshot , self . specs ( filespecs_or_globs ) ) [ 0 ] self . assertEqual ( sorted ( getattr ( result , field ) ) , sorted ( paths ) )",if prepare :,if prepare :,100,TRUE,100
953,"def _parse_rowids ( self , rowids ) : xploded = [ ] rowids = [ x . strip ( ) for x in rowids . split ( "" , "" ) ] for rowid in rowids : try : <MASK> start = int ( rowid . split ( "" - "" ) [ 0 ] . strip ( ) ) end = int ( rowid . split ( "" - "" ) [ - 1 ] . strip ( ) ) xploded + = range ( start , end + 1 ) else : xploded . append ( int ( rowid ) ) except ValueError : continue return sorted ( list ( set ( xploded ) ) )","if ""-"" in rowid :","if ""-"" in rowid :",100,TRUE,100
954,"def ensemble ( self , pairs , other_preds ) : """"""Ensemble the dict with statistical model predictions."""""" lemmas = [ ] assert len ( pairs ) == len ( other_preds ) for p , pred in zip ( pairs , other_preds ) : w , pos = p <MASK> lemma = self . composite_dict [ ( w , pos ) ] elif w in self . word_dict : lemma = self . word_dict [ w ] else : lemma = pred if lemma is None : lemma = w lemmas . append ( lemma ) return lemmas","if ( w , pos ) in self . composite_dict :","if ( w , pos ) in self . composite_dict :",100,TRUE,100
955,"def selectionToChunks ( self , remove = False , add = False ) : box = self . selectionBox ( ) if box : <MASK> self . selectedChunks = set ( self . level . allChunks ) return selectedChunks = self . selectedChunks boxedChunks = set ( box . chunkPositions ) if boxedChunks . issubset ( selectedChunks ) : remove = True if remove and not add : selectedChunks . difference_update ( boxedChunks ) else : selectedChunks . update ( boxedChunks ) self . selectionTool . selectNone ( )",if box == self . level . bounds :,if self . level . allChunks :,89.04350973,FALSE,92.84
956,"def _ensure_max_size ( cls , image , max_size , interpolation ) : if max_size is not None : size = max ( image . shape [ 0 ] , image . shape [ 1 ] ) <MASK> resize_factor = max_size / size new_height = int ( image . shape [ 0 ] * resize_factor ) new_width = int ( image . shape [ 1 ] * resize_factor ) image = ia . imresize_single_image ( image , ( new_height , new_width ) , interpolation = interpolation ) return image",if size > max_size :,if size is not None :,74.90587272,FALSE,95.47
957,"def _1_0_cloud_ips ( self , method , url , body , headers ) : if method == "" GET "" : return self . test_response ( httplib . OK , self . fixtures . load ( "" list_cloud_ips.json "" ) ) elif method == "" POST "" : <MASK> body = json . loads ( body ) node = json . loads ( self . fixtures . load ( "" create_cloud_ip.json "" ) ) if "" reverse_dns "" in body : node [ "" reverse_dns "" ] = body [ "" reverse_dns "" ] return self . test_response ( httplib . ACCEPTED , json . dumps ( node ) )",if body :,if body :,100,TRUE,100
958,"def get_formatted_stats ( self ) : """"""Get percentage or number of rar's done"""""" if self . cur_setname and self . cur_setname in self . total_volumes : # This won't work on obfuscated posts <MASK> return "" %02d / %02d "" % ( self . cur_volume , self . total_volumes [ self . cur_setname ] ) return self . cur_volume",if self . total_volumes [ self . cur_setname ] >= self . cur_volume and self . cur_volume :,if self . is_obfuscated_posts and self . is_obfuscated_,90.02176942,FALSE,78.48
959,"def wdayset ( self , year , month , day ) : # We need to handle cross-year weeks here. dset = [ None ] * ( self . yearlen + 7 ) i = datetime . date ( year , month , day ) . toordinal ( ) - self . yearordinal start = i for j in range ( 7 ) : dset [ i ] = i i + = 1 # if (not (0 <= i < self.yearlen) or #    self.wdaymask[i] == self.rrule._wkst): # This will cross the year boundary, if necessary. <MASK> break return dset , start , i",if self . wdaymask [ i ] == self . rrule . _wkst :,if dset [ i ] == self . rule . _wkst :,72.0040234,FALSE,94.96
960,"def do_acquire_read_lock ( self , wait = True ) : self . condition . acquire ( ) try : # see if a synchronous operation is waiting to start # or is already running, in which case we wait (or just # give up and return) <MASK> while self . current_sync_operation is not None : self . condition . wait ( ) else : if self . current_sync_operation is not None : return False self . asynch + = 1 finally : self . condition . release ( ) if not wait : return True",if wait :,if self . asynch :,98.21373543,FALSE,96.05
961,"def _blend ( x , y ) : # pylint: disable=invalid-name """"""Implements the ""blend"" strategy for `deep_merge`."""""" if isinstance ( x , ( dict , OrderedDict ) ) : <MASK> return y return _merge ( x , y , recursion_func = _blend ) if isinstance ( x , ( list , tuple ) ) : if not isinstance ( y , ( list , tuple ) ) : return y result = [ _blend ( * i ) for i in zip ( x , y ) ] if len ( x ) > len ( y ) : result + = x [ len ( y ) : ] elif len ( x ) < len ( y ) : result + = y [ len ( x ) : ] return result return y","if not isinstance ( y , ( dict , OrderedDict ) ) :","if not isinstance ( y , dict ) :",72.91010128,FALSE,96.27
962,"def update_forum_nums_topic_post ( modeladmin , request , queryset ) : for forum in queryset : forum . num_topics = forum . count_nums_topic ( ) forum . num_posts = forum . count_nums_post ( ) <MASK> forum . last_post = forum . topic_set . order_by ( "" -last_reply_on "" ) [ 0 ] . last_post else : forum . last_post = "" "" forum . save ( )",if forum . num_topics :,if forum . last_post :,81.9968223,FALSE,95.91
963,"def get_docname_for_node ( self , node : Node ) - > str : while node : if isinstance ( node , nodes . document ) : return self . env . path2doc ( node [ "" source "" ] ) <MASK> return node [ "" docname "" ] else : node = node . parent return None # never reached here. only for type hinting","elif isinstance ( node , addnodes . start_of_file ) :","elif isinstance ( node , dict ) :",93.47953459,FALSE,89.67
964,"def _selected_machines ( self , virtual_machines ) : selected_machines = [ ] for machine in virtual_machines : if self . _args . host and self . _args . host == machine . name : selected_machines . append ( machine ) if self . tags and self . _tags_match ( machine . tags , self . tags ) : selected_machines . append ( machine ) <MASK> selected_machines . append ( machine ) return selected_machines",if self . locations and machine . location in self . locations :,if self . _args . port and self . _args . port == machine . port :,90.74369117,FALSE,86.05
965,"def transform_kwarg ( self , name , value , split_single_char_options ) : if len ( name ) == 1 : if value is True : return [ "" - %s "" % name ] <MASK> if split_single_char_options : return [ "" - %s "" % name , "" %s "" % value ] else : return [ "" - %s %s "" % ( name , value ) ] else : if value is True : return [ "" -- %s "" % dashify ( name ) ] elif value is not False and value is not None : return [ "" -- %s = %s "" % ( dashify ( name ) , value ) ] return [ ]","elif value not in ( False , None ) :","elif isinstance ( value , str ) :",68.90707186,FALSE,94.83
966,"def indent ( elem , level = 0 ) : i = "" \n "" + level * ""    "" if len ( elem ) : if not elem . text or not elem . text . strip ( ) : elem . text = i + ""    "" if not elem . tail or not elem . tail . strip ( ) : elem . tail = i for elem in elem : indent ( elem , level + 1 ) if not elem . tail or not elem . tail . strip ( ) : elem . tail = i else : <MASK> elem . tail = i",if level and ( not elem . tail or not elem . tail . strip ( ) ) :,if level and ( not elem . tail or not elem . tail . strip ( ) ) :,100,TRUE,100
967,"def _run_instances_op ( self , op , instance_ids , * * kwargs ) : while instance_ids : try : return self . manager . retry ( op , InstanceIds = instance_ids , * * kwargs ) except ClientError as e : <MASK> instance_ids . remove ( extract_instance_id ( e ) ) raise","if e . response [ ""Error"" ] [ ""Code"" ] == ""IncorrectInstanceState"" :",if extract_instance_id ( e ) in instance_ids :,82.85390965,FALSE,77.6
968,"def runTest ( self ) : self . poco ( text = "" wait UI "" ) . click ( ) bomb_count = 0 while True : blue_fish = self . poco ( "" fish_emitter "" ) . child ( "" blue "" ) yellow_fish = self . poco ( "" fish_emitter "" ) . child ( "" yellow "" ) bomb = self . poco ( "" fish_emitter "" ) . child ( "" bomb "" ) fish = self . poco . wait_for_any ( [ blue_fish , yellow_fish , bomb ] ) <MASK> bomb_count + = 1 if bomb_count > 3 : return else : fish . click ( ) time . sleep ( 2.5 )",if fish is bomb :,if fish . is_visible ( ) :,95.24232086,FALSE,95.1
969,"def lineWidth ( self , lw = None ) : """"""Set/get width of mesh edges. Same as `lw()`."""""" if lw is not None : <MASK> self . GetProperty ( ) . EdgeVisibilityOff ( ) self . GetProperty ( ) . SetRepresentationToSurface ( ) return self self . GetProperty ( ) . EdgeVisibilityOn ( ) self . GetProperty ( ) . SetLineWidth ( lw ) else : return self . GetProperty ( ) . GetLineWidth ( ) return self",if lw == 0 :,if self . GetProperty ( ) . GetLineWidth ( lw ) == 0 :,93.08896307,FALSE,89.21
970,"def _current_date_updater ( doc , field_name , value ) : if isinstance ( doc , dict ) : <MASK> # TODO(juannyg): get_current_timestamp should also be using helpers utcnow, # as it currently using time.time internally doc [ field_name ] = helpers . get_current_timestamp ( ) else : doc [ field_name ] = mongomock . utcnow ( )","if value == { ""$type"" : ""timestamp"" } :",if helpers :,57.26687558,FALSE,84.21
971,"def fill_members ( self ) : if self . _get_retrieve ( ) : after = self . after . id if self . after else None data = await self . get_members ( self . guild . id , self . retrieve , after ) <MASK> # no data, terminate return if len ( data ) < 1000 : self . limit = 0 # terminate loop self . after = Object ( id = int ( data [ - 1 ] [ "" user "" ] [ "" id "" ] ) ) for element in reversed ( data ) : await self . members . put ( self . create_member ( element ) )",if not data :,if not data :,100,TRUE,100
972,"def extract ( self , page , start_index = 0 , end_index = None ) : items = [ ] for extractor in self . extractors : extracted = extractor . extract ( page , start_index , end_index , self . template . ignored_regions ) for item in arg_to_iter ( extracted ) : if item : <MASK> item [ u "" _template "" ] = self . template . id items . append ( item ) return items","if isinstance ( item , ( ItemProcessor , dict ) ) :","if u""_template"" not in item :",89.0449096,FALSE,89
973,"def _get_node_type_specific_fields ( self , node_id : str , fields_key : str ) - > Any : fields = self . config [ fields_key ] node_tags = self . provider . node_tags ( node_id ) if TAG_RAY_USER_NODE_TYPE in node_tags : node_type = node_tags [ TAG_RAY_USER_NODE_TYPE ] if node_type not in self . available_node_types : raise ValueError ( f "" Unknown node type tag:  { node_type } . "" ) node_specific_config = self . available_node_types [ node_type ] <MASK> fields = node_specific_config [ fields_key ] return fields",if fields_key in node_specific_config :,if fields_key in node_specific_config :,100,TRUE,100
974,"def _write_all ( self , writer ) : """"""Writes messages and insert comments here and there."""""" # Note: we make no assumptions about the length of original_messages and original_comments for msg , comment in zip_longest ( self . original_messages , self . original_comments , fillvalue = None ) : # msg and comment might be None <MASK> print ( "" writing comment:  "" , comment ) writer . log_event ( comment ) # we already know that this method exists if msg is not None : print ( "" writing message:  "" , msg ) writer ( msg )",if comment is not None :,if comment is not None :,75,TRUE,100
975,"def run_tests ( ) : # type: () -> None x = 5 with switch ( x ) as case : <MASK> print ( "" zero "" ) print ( "" zero "" ) elif case ( 1 , 2 ) : print ( "" one or two "" ) elif case ( 3 , 4 ) : print ( "" three or four "" ) else : print ( "" default "" ) print ( "" another "" )",if case ( 0 ) :,"if case ( 0 , 1 ) :",73.51370599,FALSE,96.11
976,"def date_to_format ( value , target_format ) : """"""Convert date to specified format"""""" if target_format == str : if isinstance ( value , datetime . date ) : ret = value . strftime ( "" %d / % m/ % y "" ) <MASK> ret = value . strftime ( "" %d / % m/ % y "" ) elif isinstance ( value , datetime . time ) : ret = value . strftime ( "" % H: % M: % S "" ) else : ret = value return ret","elif isinstance ( value , datetime . datetime ) :","elif isinstance ( value , datetime . date ) :",94.73432554,FALSE,97.89
977,"def database_app ( request ) : if request . param == "" postgres_app "" : if not which ( "" initdb "" ) : pytest . skip ( "" initdb must be on PATH for postgresql fixture "" ) <MASK> pytest . skip ( "" psycopg2 must be installed for postgresql fixture "" ) if request . param == "" sqlite_rabbitmq_app "" : if not os . environ . get ( "" GALAXY_TEST_AMQP_INTERNAL_CONNECTION "" ) : pytest . skip ( "" rabbitmq tests will be skipped if GALAXY_TEST_AMQP_INTERNAL_CONNECTION env var is unset "" ) return request . getfixturevalue ( request . param )",if not psycopg2 :,"if not which ( ""psycopg2"" ) :",94.13248764,FALSE,94.63
978,"def poll_ms ( self , timeout = - 1 ) : s = bytearray ( self . evbuf ) <MASK> deadline = utime . ticks_add ( utime . ticks_ms ( ) , timeout ) while True : n = epoll_wait ( self . epfd , s , 1 , timeout ) if not os . check_error ( n ) : break <MASK> timeout = utime . ticks_diff ( deadline , utime . ticks_ms ( ) ) if timeout < 0 : n = 0 break res = [ ] if n > 0 : vals = struct . unpack ( epoll_event , s ) res . append ( ( vals [ 1 ] , vals [ 0 ] ) ) return res",if timeout >= 0 :,if timeout :,83.61232181,FALSE,93.79
979,"def get_all_active_plugins ( self ) - > List [ BotPlugin ] : """"""This returns the list of plugins in the callback ordered defined from the config."""""" all_plugins = [ ] for name in self . plugins_callback_order : # None is a placeholder for any plugin not having a defined order if name is None : all_plugins + = [ plugin for name , plugin in self . plugins . items ( ) if name not in self . plugins_callback_order and plugin . is_activated ] else : plugin = self . plugins [ name ] <MASK> all_plugins . append ( plugin ) return all_plugins",if plugin . is_activated :,if plugin . is_activated :,100,TRUE,100
980,"def get_expected_sql ( self ) : sql_base_path = path . join ( path . dirname ( path . realpath ( __file__ ) ) , "" sql "" ) # Iterate the version mapping directories. for version_mapping in get_version_mapping_directories ( self . server [ "" type "" ] ) : <MASK> continue complete_path = path . join ( sql_base_path , version_mapping [ "" name "" ] ) if not path . exists ( complete_path ) : continue break data_sql = "" "" with open ( path . join ( complete_path , "" test_sql_output.sql "" ) ) as fp : data_sql = fp . read ( ) return data_sql","if version_mapping [ ""number"" ] > self . server_information [ ""server_version"" ] :","if version_mapping [ ""type"" ] != ""version"" :",95.73593555,FALSE,90.78
981,"def _validate_headers ( self , headers ) : if headers is None : return headers res = { } for key , value in headers . items ( ) : if isinstance ( value , ( int , float ) ) : value = str ( value ) <MASK> raise ScriptError ( { "" message "" : "" headers must be a table "" ""  with strings as keys and values. "" "" Header: ` {!r} : {!r} ` is not valid "" . format ( key , value ) } ) res [ key ] = value return res","if not isinstance ( key , ( bytes , str ) ) or not isinstance ( value , ( bytes , str ) ) :","if not isinstance ( value , str ) :",67.72529929,FALSE,86.28
982,"def _get_literal_value ( self , pyval ) : if pyval == self . vm . lookup_builtin ( "" builtins.True "" ) : return True elif pyval == self . vm . lookup_builtin ( "" builtins.False "" ) : return False elif isinstance ( pyval , str ) : prefix , value = parser_constants . STRING_RE . match ( pyval ) . groups ( ) [ : 2 ] value = value [ 1 : - 1 ] # remove quotation marks <MASK> value = compat . bytestring ( value ) elif "" u "" in prefix and self . vm . PY2 : value = compat . UnicodeType ( value ) return value else : return pyval","if ""b"" in prefix and not self . vm . PY2 :","if ""q"" in prefix and self . vm . PY3 :",96.93577462,FALSE,94.87
983,"def decode_query_ids ( self , trans , conditional ) : if conditional . operator == "" and "" : self . decode_query_ids ( trans , conditional . left ) self . decode_query_ids ( trans , conditional . right ) else : left_base = conditional . left . split ( "" . "" ) [ 0 ] <MASK> field = self . FIELDS [ left_base ] if field . id_decode : conditional . right = trans . security . decode_id ( conditional . right )",if left_base in self . FIELDS :,if left_base in self . FIELDS :,100,TRUE,100
984,"def testLastPhrases ( self ) : for day in ( 11 , 12 , 13 , 14 , 15 , 16 , 17 ) : start = datetime . datetime ( 2012 , 11 , day , 9 , 0 , 0 ) ( yr , mth , dy , _ , _ , _ , wd , yd , isdst ) = start . timetuple ( ) n = 4 - wd <MASK> n - = 7 target = start + datetime . timedelta ( days = n ) self . assertExpectedResult ( self . cal . parse ( "" last friday "" , start . timetuple ( ) ) , ( target . timetuple ( ) , 1 ) , dateOnly = True , )",if n >= 0 :,if isdst :,94.17982869,FALSE,95.77
985,"def _convertNbCharsInNbBits ( self , nbChars ) : nbMinBit = None nbMaxBit = None if nbChars is not None : if isinstance ( nbChars , int ) : nbMinBit = nbChars * 8 nbMaxBit = nbMinBit else : <MASK> nbMinBit = nbChars [ 0 ] * 8 if nbChars [ 1 ] is not None : nbMaxBit = nbChars [ 1 ] * 8 return ( nbMinBit , nbMaxBit )",if nbChars [ 0 ] is not None :,if nbChars [ 0 ] is not None :,100,TRUE,100
986,"def getpystone ( ) : # Start calculation maxpystone = 0 # Start with a short run, find the the pystone, and increase runtime until duration took > 0.1 second for pyseed in [ 1000 , 2000 , 5000 , 10000 , 20000 , 50000 , 100000 , 200000 ] : duration , pystonefloat = pystones ( pyseed ) maxpystone = max ( maxpystone , int ( pystonefloat ) ) # Stop when pystone() has been running for at least 0.1 second <MASK> break return maxpystone",if duration > 0.1 :,if duration > 0.1 :,75,TRUE,100
987,"def _append_to_io_queue ( self , data , stream_name ) : # Make sure ANSI CSI codes and object links are stored as separate events # TODO: try to complete previously submitted incomplete code parts = re . split ( OUTPUT_SPLIT_REGEX , data ) for part in parts : <MASK> # split may produce empty string in the beginning or start # split the data so that very long lines separated for block in re . split ( "" (. { %d ,}) "" % ( self . _get_squeeze_threshold ( ) + 1 ) , part ) : if block : self . _queued_io_events . append ( ( block , stream_name ) )",if part :,if part :,75,TRUE,100
988,"def qtTypeIdent ( conn , * args ) : # We're not using the conn object at the moment, but - we will # modify the # logic to use the server version specific keywords later. res = None value = None for val in args : # DataType doesn't have len function then convert it to string <MASK> val = str ( val ) if len ( val ) == 0 : continue value = val if Driver . needsQuoting ( val , True ) : value = value . replace ( ' "" ' , ' "" "" ' ) value = ' "" ' + value + ' "" ' res = ( ( res and res + "" . "" ) or "" "" ) + value return res","if not hasattr ( val , ""__len__"" ) :","if not hasattr ( val , ""split"" ) :",74.11205368,FALSE,95.61
989,"def SetVerbose ( self , level ) : """"""Sets the verbose level."""""" try : <MASK> level = int ( level ) if ( level > = 0 ) and ( level < = 3 ) : self . _verbose = level return except ValueError : pass self . Error ( "" Verbose level ( %s ) must be between 0 and 3 inclusive. "" % level )",if type ( level ) != types . IntType :,"if isinstance ( level , int ) :",85.34301786,FALSE,88.81
990,"def step ( self ) - > None : """"""Performs a single optimization step."""""" for group in self . param_groups : for p in group [ "" params "" ] : <MASK> continue p . add_ ( p . grad , alpha = ( - group [ "" lr "" ] * self . num_data ) ) return None",if p . grad is None :,if p . grad is None :,100,TRUE,100
991,"def fill ( self , values ) : if lupa . lua_type ( values ) != "" table "" : raise ScriptError ( { "" argument "" : "" values "" , "" message "" : "" element:fill values is not a table "" , "" splash_method "" : "" fill "" , } ) # marking all tables as arrays by default for key , value in values . items ( ) : <MASK> _mark_table_as_array ( self . lua , value ) values = self . lua . lua2python ( values ) return self . element . fill ( values )","if lupa . lua_type ( value ) == ""table"" :","if key == ""array"" :",70.85501473,FALSE,90.55
992,"def _gen_repr ( self , buf ) : print >> buf , ""     def __repr__(self): "" if self . argnames : fmt = COMMA . join ( [ "" %s "" ] * self . nargs ) <MASK> fmt = "" ( %s ) "" % fmt vals = [ "" repr(self. %s ) "" % name for name in self . argnames ] vals = COMMA . join ( vals ) if self . nargs == 1 : vals = vals + "" , "" print >> buf , '         return  "" %s ( %s ) ""   %%  ( %s ) ' % ( self . name , fmt , vals ) else : print >> buf , '         return  "" %s () "" ' % self . name","if ""("" in self . args :",if self . nargs == 2 :,95.61960958,FALSE,95.25
993,"def render_observation ( self ) : x = self . read_head_position label = "" Observation Grid    :  "" x_str = "" "" for j in range ( - 1 , self . rows + 1 ) : <MASK> x_str + = ""   "" * len ( label ) for i in range ( - 2 , self . input_width + 2 ) : if i == x [ 0 ] and j == x [ 1 ] : x_str + = colorize ( self . _get_str_obs ( ( i , j ) ) , "" green "" , highlight = True ) else : x_str + = self . _get_str_obs ( ( i , j ) ) x_str + = "" \n "" x_str = label + x_str return x_str",if j != - 1 :,if j != self . columns - 1 :,88.58996093,FALSE,97.41
994,"def get_module_comment ( self , attrname : str ) - > Optional [ List [ str ] ] : try : analyzer = ModuleAnalyzer . for_module ( self . modname ) analyzer . analyze ( ) key = ( "" "" , attrname ) <MASK> return list ( analyzer . attr_docs [ key ] ) except PycodeError : pass return None",if key in analyzer . attr_docs :,if key in analyzer . attr_docs :,100,TRUE,100
995,"def tms_to_quadkey ( self , tms , google = False ) : quadKey = "" "" x , y , z = tms # this algorithm works with google tiles, rather than tms, so convert # to those first. if not google : y = ( 2 * * z - 1 ) - y for i in range ( z , 0 , - 1 ) : digit = 0 mask = 1 << ( i - 1 ) <MASK> digit + = 1 if ( y & mask ) != 0 : digit + = 2 quadKey + = str ( digit ) return quadKey",if ( x & mask ) != 0 :,if ( x & mask ) != 0 :,100,TRUE,100
996,"def test_enumerate ( app ) : async with new_stream ( app ) as stream : for i in range ( 100 ) : await stream . channel . deliver ( message ( key = i , value = i * 4 ) ) async for i , value in stream . enumerate ( ) : current_event = stream . current_event assert i == current_event . key assert value == i * 4 <MASK> break assert await channel_empty ( stream . channel )",if i >= 99 :,if i == 0 :,97.97553173,FALSE,95.7
997,"def print_messages ( self ) : output_reports = self . config . get_output_report ( ) for report in output_reports : output_format , output_files = report self . summary [ "" formatter "" ] = output_format formatter = FORMATTERS [ output_format ] ( self . summary , self . messages , self . config . profile ) <MASK> self . write_to ( formatter , sys . stdout ) for output_file in output_files : with open ( output_file , "" w+ "" ) as target : self . write_to ( formatter , target )",if not output_files :,if self . config . verbose :,94.37446081,FALSE,94.81
998,"def eval_metrics ( self ) : for task in self . task_list : <MASK> return [ metrics . Metrics . ACC , metrics . Metrics . NEG_LOG_PERPLEXITY , metrics . Metrics . ROUGE_2_F , metrics . Metrics . ROUGE_L_F , ] return [ metrics . Metrics . ACC , metrics . Metrics . NEG_LOG_PERPLEXITY , ]","if ""summarize"" in task . name :","if task . type == ""ROUGE"" :",66.22228318,FALSE,90.02
999,"def _getBuildRequestForBrdict ( self , brdict ) : # Turn a brdict into a BuildRequest into a brdict. This is useful # for API like 'nextBuild', which operate on BuildRequest objects. breq = self . breqCache . get ( brdict [ "" buildrequestid "" ] ) if not breq : breq = yield BuildRequest . fromBrdict ( self . master , brdict ) <MASK> self . breqCache [ brdict [ "" buildrequestid "" ] ] = breq defer . returnValue ( breq )",if breq :,if breq :,75,TRUE,100
1000,"def _stash_splitter ( states ) : keep , split = [ ] , [ ] if state_func is not None : for s in states : ns = state_func ( s ) <MASK> split . append ( ns ) elif isinstance ( ns , ( list , tuple , set ) ) : split . extend ( ns ) else : split . append ( s ) if stash_func is not None : split = stash_func ( states ) if to_stash is not stash : keep = states return keep , split","if isinstance ( ns , SimState ) :","if isinstance ( ns , str ) :",98.7337685,FALSE,97.72
1001,"def sequence_to_text ( sequence ) : """"""Converts a sequence of IDs back to a string"""""" result = "" "" for symbol_id in sequence : <MASK> s = _id_to_symbol [ symbol_id ] # Enclose ARPAbet back in curly braces: if len ( s ) > 1 and s [ 0 ] == "" @ "" : s = "" { %s } "" % s [ 1 : ] result + = s return result . replace ( "" } { "" , ""   "" )",if symbol_id in _id_to_symbol :,if symbol_id in _id_to_symbol :,100,TRUE,100
1002,"def get_code ( self , fullname = None ) : fullname = self . _fix_name ( fullname ) if self . code is None : mod_type = self . etc [ 2 ] <MASK> source = self . get_source ( fullname ) self . code = compile ( source , self . filename , "" exec "" ) elif mod_type == imp . PY_COMPILED : self . _reopen ( ) try : self . code = read_code ( self . file ) finally : self . file . close ( ) elif mod_type == imp . PKG_DIRECTORY : self . code = self . _get_delegate ( ) . get_code ( ) return self . code",if mod_type == imp . PY_SOURCE :,if mod_type == imp . PY_EXTENSION :,98.96076013,FALSE,98.34
1003,"def identwaf ( self , findall = False ) : detected = list ( ) try : self . attackres = self . performCheck ( self . centralAttack ) except RequestBlocked : return detected for wafvendor in self . checklist : self . log . info ( "" Checking for  %s "" % wafvendor ) if self . wafdetections [ wafvendor ] ( self ) : detected . append ( wafvendor ) <MASK> break self . knowledge [ "" wafname "" ] = detected return detected",if not findall :,if findall :,96.28404186,FALSE,97.16
1004,"def SessionId ( self ) : """"""Returns the Session ID of the process"""""" if self . Session . is_valid ( ) : process_space = self . get_process_address_space ( ) <MASK> return obj . Object ( "" _MM_SESSION_SPACE "" , offset = self . Session , vm = process_space ) . SessionId return obj . NoneObject ( "" Cannot find process session "" )",if process_space :,if process_space :,100,TRUE,100
1005,"def _convert_java_pattern_to_python ( pattern ) : """"""Convert a replacement pattern from the Java-style `$5` to the Python-style `\\5`."""""" s = list ( pattern ) i = 0 while i < len ( s ) - 1 : c = s [ i ] if c == "" $ "" and s [ i + 1 ] in "" 0123456789 "" : s [ i ] = "" \\ "" <MASK> s [ i ] = "" "" i + = 1 i + = 1 return pattern [ : 0 ] . join ( s )","elif c == ""\\"" and s [ i + 1 ] == ""$"" :","elif c == ""."" and s [ i + 1 ] in ""0123456789"" :",89.0334187,FALSE,93.7
1006,"def __init__ ( self , coverage ) : self . coverage = coverage self . config = self . coverage . config self . source_paths = set ( ) if self . config . source : for src in self . config . source : <MASK> if not self . config . relative_files : src = files . canonical_filename ( src ) self . source_paths . add ( src ) self . packages = { } self . xml_out = None",if os . path . exists ( src ) :,"if src . endswith ( "".py"" ) :",95.03381948,FALSE,91.72
1007,"def populate_vol_format ( self ) : rhel6_file_whitelist = [ "" raw "" , "" qcow2 "" , "" qed "" ] model = self . widget ( "" vol-format "" ) . get_model ( ) model . clear ( ) formats = self . vol_class . formats if hasattr ( self . vol_class , "" create_formats "" ) : formats = getattr ( self . vol_class , "" create_formats "" ) if self . vol_class == Storage . FileVolume and not self . conn . rhel6_defaults_caps ( ) : newfmts = [ ] for f in rhel6_file_whitelist : <MASK> newfmts . append ( f ) formats = newfmts for f in formats : model . append ( [ f , f ] )",if f in formats :,if f not in formats :,99.20105912,FALSE,98.4
1008,"def get_file_sources ( ) : global _file_sources if _file_sources is None : from galaxy . files import ConfiguredFileSources file_sources = None <MASK> file_sources_as_dict = None with open ( "" file_sources.json "" , "" r "" ) as f : file_sources_as_dict = json . load ( f ) if file_sources_as_dict is not None : file_sources = ConfiguredFileSources . from_dict ( file_sources_as_dict ) if file_sources is None : ConfiguredFileSources . from_dict ( [ ] ) _file_sources = file_sources return _file_sources","if os . path . exists ( ""file_sources.json"" ) :","if os . path . exists ( ""file_sources.json"" ) :",100,TRUE,100
1009,"def _blend ( x , y ) : # pylint: disable=invalid-name """"""Implements the ""blend"" strategy for `deep_merge`."""""" if isinstance ( x , ( dict , OrderedDict ) ) : if not isinstance ( y , ( dict , OrderedDict ) ) : return y return _merge ( x , y , recursion_func = _blend ) if isinstance ( x , ( list , tuple ) ) : if not isinstance ( y , ( list , tuple ) ) : return y result = [ _blend ( * i ) for i in zip ( x , y ) ] <MASK> result + = x [ len ( y ) : ] elif len ( x ) < len ( y ) : result + = y [ len ( x ) : ] return result return y",if len ( x ) > len ( y ) :,if len ( x ) < len ( y ) :,74.1554075,FALSE,98.51
1010,"def copy_dicts ( dct ) : if "" _remote_data "" in dct : dsindex = dct [ "" _remote_data "" ] [ "" _content "" ] . dsindex newdct = dct . copy ( ) newdct [ "" _remote_data "" ] = { "" _content "" : dsindex } return list ( newdct . items ( ) ) elif "" _data "" in dct : newdct = dct . copy ( ) newdata = copy_dicts ( dct [ "" _data "" ] ) <MASK> newdct [ "" _data "" ] = newdata return list ( newdct . items ( ) ) return None",if newdata :,if newdata :,100,TRUE,100
1011,"def _import_epic_activity ( self , project_data , taiga_epic , epic , options ) : offset = 0 while True : activities = self . _client . get ( "" /projects/ {} /epics/ {} /activity "" . format ( project_data [ "" id "" ] , epic [ "" id "" ] , ) , { "" envelope "" : "" true "" , "" limit "" : 300 , "" offset "" : offset } , ) offset + = 300 for activity in activities [ "" data "" ] : self . _import_activity ( taiga_epic , activity , options ) <MASK> break","if len ( activities [ ""data"" ] ) < 300 :","if not activities [ ""data"" ] :",92.53890158,FALSE,94.33
1012,"def __get__ ( self , instance , instance_type = None ) : if instance : <MASK> rel_obj = self . get_obj ( instance ) if rel_obj : instance . _obj_cache [ self . att_name ] = rel_obj return instance . _obj_cache . get ( self . att_name ) return self",if self . att_name not in instance . _obj_cache :,if self . att_name not in instance . _obj_cache :,100,TRUE,100
1013,"def download_main ( download , download_playlist , urls , playlist , output_dir , merge , info_only ) : for url in urls : if url . startswith ( "" https:// "" ) : url = url [ 8 : ] if not url . startswith ( "" http:// "" ) : url = "" http:// "" + url <MASK> download_playlist ( url , output_dir = output_dir , merge = merge , info_only = info_only ) else : download ( url , output_dir = output_dir , merge = merge , info_only = info_only )",if playlist :,if playlist :,100,TRUE,100
1014,"def _mksubs ( self ) : self . _subs = { } commit_dir = CommitDir ( self , "" .commit "" ) self . _subs [ "" .commit "" ] = commit_dir tag_dir = TagDir ( self , "" .tag "" ) self . _subs [ "" .tag "" ] = tag_dir for ( name , sha ) in git . list_refs ( ) : <MASK> name = name [ 11 : ] date = git . rev_get_date ( sha . encode ( "" hex "" ) ) n1 = BranchList ( self , name , sha ) n1 . ctime = n1 . mtime = date self . _subs [ name ] = n1","if name . startswith ( ""refs/heads/"" ) :","if name . startswith ( ""git-"" ) :",98.96183167,FALSE,96.29
1015,"def readAtOffset ( self , offset , size , shortok = False ) : ret = b "" "" self . fd . seek ( offset ) while len ( ret ) != size : rlen = size - len ( ret ) x = self . fd . read ( rlen ) <MASK> if not shortok : return None return ret ret + = x return ret","if x == b"""" :","if x == b"""" :",100,TRUE,100
1016,"def remove_indent ( self ) : """"""Remove one tab-width of blanks from the previous token."""""" w = abs ( self . tab_width ) if self . result : s = self . result [ - 1 ] <MASK> self . result . pop ( ) s = s . replace ( "" \t "" , ""   "" * w ) if s . startswith ( "" \n "" ) : s2 = s [ 1 : ] self . result . append ( "" \n "" + s2 [ : - w ] ) else : self . result . append ( s [ : - w ] )",if s . isspace ( ) :,"if s . startswith ( ""\t"" ) :",96.53626854,FALSE,94.47
1017,"def flush ( self , * args , * * kwargs ) : with self . _lock : self . _last_updated = time . time ( ) try : if kwargs . get ( "" in_place "" , False ) : self . _locked_flush_without_tempfile ( ) else : mailbox . mbox . flush ( self , * args , * * kwargs ) except OSError : <MASK> self . _locked_flush_without_tempfile ( ) else : raise self . _last_updated = time . time ( )","if ""_create_temporary"" in traceback . format_exc ( ) :","if kwargs . get ( ""in_place"" , False ) :",92.84576037,FALSE,89.35
1018,"def _collect_manual_intervention_nodes ( pipeline_tree ) : for act in pipeline_tree [ "" activities "" ] . values ( ) : <MASK> _collect_manual_intervention_nodes ( act [ "" pipeline "" ] ) elif act [ "" component "" ] [ "" code "" ] in MANUAL_INTERVENTION_COMP_CODES : manual_intervention_nodes . add ( act [ "" id "" ] )","if act [ ""type"" ] == ""SubProcess"" :","if act [ ""component"" ] [ ""code"" ] in MANUAL_INTERVENTION",62.22317533,FALSE,87.36
1019,"def banned ( ) : if request . endpoint == "" views.themes "" : return if authed ( ) : user = get_current_user_attrs ( ) team = get_current_team_attrs ( ) <MASK> return ( render_template ( "" errors/403.html "" , error = "" You have been banned from this CTF "" ) , 403 , ) if team and team . banned : return ( render_template ( "" errors/403.html "" , error = "" Your team has been banned from this CTF "" , ) , 403 , )",if user and user . banned :,if user and user . banned :,100,TRUE,100
1020,"def remove ( self , values ) : if not isinstance ( values , ( list , tuple , set ) ) : values = [ values ] for v in values : v = str ( v ) if isinstance ( self . _definition , dict ) : self . _definition . pop ( v , None ) elif self . _definition == "" ANY "" : if v == "" ANY "" : self . _definition = [ ] <MASK> self . _definition . remove ( v ) if ( self . _value is not None and self . _value not in self . _definition and self . _not_any ( ) ) : raise ConanException ( bad_value_msg ( self . _name , self . _value , self . values_range ) )",elif v in self . _definition :,elif v in self . _definition :,100,TRUE,100
1021,"def save ( self , learner , file_name ) : """"""Save the model to location specified in file_name."""""" with open ( file_name , "" wb "" ) as f : <MASK> # don't store the large inference cache! learner . inference_cache_ , tmp = ( None , learner . inference_cache_ ) pickle . dump ( learner , f , - 1 ) learner . inference_cache_ = tmp else : pickle . dump ( learner , f , - 1 )","if hasattr ( learner , ""inference_cache_"" ) :",if len ( learner . inference_cache_ ) > self . max_inference_,90.18167501,FALSE,88.01
1022,"def __init__ ( self , exprs , savelist = False ) : super ( ParseExpression , self ) . __init__ ( savelist ) if isinstance ( exprs , _generatorType ) : exprs = list ( exprs ) if isinstance ( exprs , basestring ) : self . exprs = [ ParserElement . _literalStringClass ( exprs ) ] elif isinstance ( exprs , collections . Iterable ) : exprs = list ( exprs ) # if sequence of strings provided, wrap with Literal <MASK> exprs = map ( ParserElement . _literalStringClass , exprs ) self . exprs = list ( exprs ) else : try : self . exprs = list ( exprs ) except TypeError : self . exprs = [ exprs ] self . callPreparse = False","if all ( isinstance ( expr , basestring ) for expr in exprs ) :","if isinstance ( exprs , ( list , tuple ) ) :",95.61986283,FALSE,92.25
1023,"def find ( self , back = False ) : flags = 0 <MASK> flags = QTextDocument . FindBackward if self . csBox . isChecked ( ) : flags = flags | QTextDocument . FindCaseSensitively text = self . searchEdit . text ( ) if not self . findMain ( text , flags ) : if text in self . editBoxes [ self . ind ] . toPlainText ( ) : cursor = self . editBoxes [ self . ind ] . textCursor ( ) <MASK> cursor . movePosition ( QTextCursor . End ) else : cursor . movePosition ( QTextCursor . Start ) self . editBoxes [ self . ind ] . setTextCursor ( cursor ) self . findMain ( text , flags )",if back :,if back :,100,TRUE,100
1024,"def _load_storage ( self ) : self . _storage = { } for row in self ( "" SELECT object, resource, amount FROM storage "" ) : ownerid = int ( row [ 0 ] ) <MASK> self . _storage [ ownerid ] . append ( row [ 1 : ] ) else : self . _storage [ ownerid ] = [ row [ 1 : ] ]",if ownerid in self . _storage :,if ownerid in self . _storage :,100,TRUE,100
1025,"def parse_chunked ( self , unreader ) : ( size , rest ) = self . parse_chunk_size ( unreader ) while size > 0 : while size > len ( rest ) : size - = len ( rest ) yield rest rest = unreader . read ( ) <MASK> raise NoMoreData ( ) yield rest [ : size ] # Remove \r\n after chunk rest = rest [ size : ] while len ( rest ) < 2 : rest + = unreader . read ( ) if rest [ : 2 ] != b "" \r \n "" : raise ChunkMissingTerminator ( rest [ : 2 ] ) ( size , rest ) = self . parse_chunk_size ( unreader , data = rest [ 2 : ] )",if not rest :,if len ( rest ) == 0 :,95.46911104,FALSE,94.58
1026,"def _augment_batch_ ( self , batch , random_state , parents , hooks ) : for column in batch . columns : <MASK> for i , cbaoi in enumerate ( column . value ) : column . value [ i ] = cbaoi . clip_out_of_image_ ( ) return batch","if column . name in [ ""keypoints"" , ""bounding_boxes"" , ""polygons"" , ""line_strings"" ] :","if isinstance ( column , Captures ) :",81.80241662,FALSE,67.59
1027,"def to_nim ( self ) : if self . is_pointer == 2 : s = "" cstringArray "" if self . type == "" GLchar "" else "" ptr pointer "" else : s = self . type <MASK> default = "" ptr  "" + s s = self . NIM_POINTER_MAP . get ( s , default ) return s",if self . is_pointer == 1 :,if s not in self . NIM_POINTER_MAP :,82.09703836,FALSE,86.7
1028,"def find ( self , path ) : if os . path . isfile ( path ) or os . path . islink ( path ) : self . num_files = self . num_files + 1 if self . match_function ( path ) : self . files . append ( path ) elif os . path . isdir ( path ) : for content in os . listdir ( path ) : file = os . path . join ( path , content ) <MASK> self . num_files = self . num_files + 1 if self . match_function ( file ) : self . files . append ( file ) else : self . find ( file )",if os . path . isfile ( file ) or os . path . islink ( file ) :,if os . path . isfile ( file ) or os . path . islink ( file ),97.51525293,FALSE,98.29
1029,"def remove ( self , event ) : try : self . _events_current_sweep . remove ( event ) <MASK> assert event . in_sweep == True assert event . other . in_sweep == True event . in_sweep = False event . other . in_sweep = False return True except KeyError : <MASK> assert event . in_sweep == False assert event . other . in_sweep == False return False",if USE_DEBUG :,if self . _events_current_sweep :,91.17095329,FALSE,82.21
1030,"def update_metadata ( self ) : for attrname in dir ( self ) : if attrname . startswith ( "" __ "" ) : continue attrvalue = getattr ( self , attrname , None ) if attrvalue == 0 : continue <MASK> attrname = "" version "" if hasattr ( self . metadata , "" set_ {0} "" . format ( attrname ) ) : getattr ( self . metadata , "" set_ {0} "" . format ( attrname ) ) ( attrvalue ) elif hasattr ( self . metadata , attrname ) : try : setattr ( self . metadata , attrname , attrvalue ) except AttributeError : pass","if attrname == ""salt_version"" :","if attrname == ""version"" :",98.89899208,FALSE,97.25
1031,"def _init_auxiliary_head ( self , auxiliary_head ) : """"""Initialize ``auxiliary_head``"""""" if auxiliary_head is not None : <MASK> self . auxiliary_head = nn . ModuleList ( ) for head_cfg in auxiliary_head : self . auxiliary_head . append ( builder . build_head ( head_cfg ) ) else : self . auxiliary_head = builder . build_head ( auxiliary_head )","if isinstance ( auxiliary_head , list ) :","if isinstance ( auxiliary_head , ( list , tuple ) ) :",89.08716508,FALSE,94.11
1032,"def _str_param_list ( self , name ) : out = [ ] if self [ name ] : out + = self . _str_header ( name ) for param in self [ name ] : parts = [ ] if param . name : parts . append ( param . name ) <MASK> parts . append ( param . type ) out + = [ ""  :  "" . join ( parts ) ] if param . desc and "" "" . join ( param . desc ) . strip ( ) : out + = self . _str_indent ( param . desc ) out + = [ "" "" ] return out",if param . type :,if param . type :,100,TRUE,100
1033,"def _set_handler ( self , name , handle = None , obj = None , constructor_args = ( ) , constructor_kwds = { } ) : if handle is None : handle = obj is not None if handle : handler_class = self . handler_classes [ name ] <MASK> newhandler = handler_class ( obj ) else : newhandler = handler_class ( * constructor_args , * * constructor_kwds ) else : newhandler = None self . _replace_handler ( name , newhandler )",if obj is not None :,if obj is not None :,100,TRUE,100
1034,"def _extract_subtitles ( src ) : subtitles = { } for caption in try_get ( src , lambda x : x [ "" captions "" ] , list ) or [ ] : subtitle_url = url_or_none ( caption . get ( "" uri "" ) ) <MASK> lang = caption . get ( "" language "" , "" deu "" ) subtitles . setdefault ( lang , [ ] ) . append ( { "" url "" : subtitle_url , } ) return subtitles",if subtitle_url :,if subtitle_url :,100,TRUE,100
1035,"def get_keys ( struct , ignore_first_level = False ) : res = [ ] if isinstance ( struct , dict ) : <MASK> keys = [ x . split ( "" ( "" ) [ 0 ] for x in struct . keys ( ) ] res . extend ( keys ) for key in struct : if key in IGNORED_KEYS : logging . debug ( "" Ignored:  %s :  %s "" , key , struct [ key ] ) continue res . extend ( get_keys ( struct [ key ] , key in IGNORED_FIRST_LEVEL ) ) elif isinstance ( struct , list ) : for item in struct : res . extend ( get_keys ( item ) ) return res",if not ignore_first_level :,if ignore_first_level :,96.82225359,FALSE,98.28
1036,"def create_dir ( path ) : curr_path = None for p in path : if curr_path is None : curr_path = os . path . abspath ( p ) else : curr_path = os . path . join ( curr_path , p ) <MASK> os . mkdir ( curr_path )",if not os . path . exists ( curr_path ) :,if not os . path . exists ( curr_path ) :,100,TRUE,100
1037,"def dataToDumpFile ( dumpFile , data ) : try : dumpFile . write ( data ) dumpFile . flush ( ) except IOError as ex : if "" No space left "" in getUnicode ( ex ) : errMsg = "" no space left on output device "" logger . error ( errMsg ) <MASK> errMsg = "" permission denied when flushing dump data "" logger . error ( errMsg ) else : errMsg = ( "" error occurred when writing dump data to file ( ' %s ' ) "" % getUnicode ( ex ) ) logger . error ( errMsg )","elif ""Permission denied"" in getUnicode ( ex ) :","elif ""permission denied"" in getUnicode ( ex ) :",98.863906,FALSE,97.76
1038,"def elements ( self , top ) : res = [ ] # try: #     string = ""== %s (%s)"" % (self.name,self.__class__) # except AttributeError: #     string = ""== (%s)"" % (self.__class__,) # print(string) for part in self . parts : <MASK> res . append ( name_or_ref ( part , top ) ) else : if isinstance ( part , Extension ) : res . append ( part . base ) res . extend ( part . elements ( top ) ) return res","if isinstance ( part , Element ) :","if isinstance ( part , NameOrRef ) :",73.60862289,FALSE,98.1
1039,"def _parse_param_value ( name , datatype , default ) : if datatype == "" bool "" : if default . lower ( ) == "" true "" : return True <MASK> return False else : _s = "" {} : Invalid default value  ' {} '  for bool parameter  {} "" raise SyntaxError ( _s . format ( self . name , default , p ) ) elif datatype == "" int "" : if type ( default ) == int : return default else : return int ( default , 0 ) elif datatype == "" real "" : if type ( default ) == float : return default else : return float ( default ) else : return str ( default )","elif default . lower ( ) == ""false"" :","elif default . lower ( ) == ""false"" :",100,TRUE,100
1040,"def dvmethod ( c , dx , doAST = False ) : for m in c . get_methods ( ) : mx = dx . get_method ( m ) ms = DvMethod ( mx ) ms . process ( doAST = doAST ) <MASK> assert ms . get_ast ( ) is not None assert isinstance ( ms . get_ast ( ) , dict ) assert "" body "" in ms . get_ast ( ) else : assert ms . get_source ( ) is not None",if doAST :,if doAST :,100,TRUE,100
1041,"def _repr_pretty_ ( self , p , cycle ) : if cycle : return "" {{ ...} "" with p . group ( 2 , "" { "" , "" } "" ) : p . breakable ( "" "" ) for idx , key in enumerate ( self . _items ) : <MASK> p . text ( "" , "" ) p . breakable ( ) value = self . _items [ key ] p . pretty ( key ) p . text ( "" :  "" ) if isinstance ( value , bytes ) : value = trimmed_repr ( value ) p . pretty ( value ) p . breakable ( "" "" )",if idx :,if idx == 0 :,97.28794083,FALSE,96.57
1042,"def remove_rating ( self , songs , librarian ) : count = len ( songs ) if count > 1 and config . getboolean ( "" browsers "" , "" rating_confirm_multiple "" ) : parent = qltk . get_menu_item_top_parent ( self ) dialog = ConfirmRateMultipleDialog ( parent , _ ( "" _Remove Rating "" ) , count , None ) <MASK> return reset = [ ] for song in songs : if "" ~#rating "" in song : del song [ "" ~#rating "" ] reset . append ( song ) librarian . changed ( reset )",if dialog . run ( ) != Gtk . ResponseType . YES :,if not dialog . run ( ) :,93.47114355,FALSE,92.07
1043,"def get_or_create_place ( self , place_name ) : "" Return the requested place object tuple-packed with a new indicator. "" LOG . debug ( "" get_or_create_place: looking for:  %s "" , place_name ) for place_handle in self . db . iter_place_handles ( ) : place = self . db . get_place_from_handle ( place_handle ) place_title = place_displayer . display ( self . db , place ) <MASK> return ( 0 , place ) place = Place ( ) place . set_title ( place_name ) place . name = PlaceName ( value = place_name ) self . db . add_place ( place , self . trans ) return ( 1 , place )",if place_title == place_name :,if place_title == place_name :,100,TRUE,100
1044,def _skip_trivial ( constraint_data ) : if skip_trivial_constraints : <MASK> if constraint_data . variables is None : return True else : if constraint_data . body . polynomial_degree ( ) == 0 : return True return False,"if isinstance ( constraint_data , LinearCanonicalRepn ) :","if isinstance ( constraint_data . body , ( Base , Base ) ) :",87.16685067,FALSE,85.8
1045,"def get_other ( self , data , items ) : is_tuple = False if type ( data ) == tuple : data = list ( data ) is_tuple = True if type ( data ) == list : m_items = items . copy ( ) for idx , item in enumerate ( items ) : if item < 0 : m_items [ idx ] = len ( data ) - abs ( item ) for i in sorted ( set ( m_items ) , reverse = True ) : <MASK> del data [ i ] if is_tuple : return tuple ( data ) else : return data else : return None",if i < len ( data ) and i > - 1 :,if i != idx :,92.26223883,FALSE,91.55
1046,"def test_case_insensitivity ( self ) : with support . EnvironmentVarGuard ( ) as env : env . set ( "" PYTHONCASEOK "" , "" 1 "" ) <MASK> self . skipTest ( "" os.environ changes not reflected in  "" "" _os.environ "" ) loader = self . find_module ( ) self . assertTrue ( hasattr ( loader , "" load_module "" ) )","if b""PYTHONCASEOK"" not in _bootstrap . _os . environ :","if ""PYTHONCASEOK"" not in _os . environ :",92.171061,FALSE,92.86
1047,def field_spec ( self ) : <MASK> self . lazy_init_lock_ . acquire ( ) try : <MASK> self . field_spec_ = FieldSpec ( ) finally : self . lazy_init_lock_ . release ( ) return self . field_spec_,if self . field_spec_ is None :,if self . field_spec_ is None :,100,TRUE,100
1048,"def reduce ( self , f , init ) : for x in range ( self . _idx , rt . count ( self . _w_array ) ) : <MASK> return rt . deref ( init ) init = f . invoke ( [ init , rt . nth ( self . _w_array , rt . wrap ( x ) ) ] ) return init",if rt . reduced_QMARK_ ( init ) :,if rt . reduced_QMARK_ ( init ) :,100,TRUE,100
1049,"def _find ( event : E ) - > None : # We first check values after the selected value, then all values. values = list ( self . values ) for value in values [ self . _selected_index + 1 : ] + values : text = fragment_list_to_text ( to_formatted_text ( value [ 1 ] ) ) . lower ( ) <MASK> self . _selected_index = self . values . index ( value ) return",if text . startswith ( event . data . lower ( ) ) :,if text in self . _text_re . search ( text ) :,68.92138009,FALSE,89.02
1050,"def check_permissions ( ) : if platform_os ( ) != "" Windows "" : <MASK> print ( localization . lang_check_permissions [ "" permissions_granted "" ] ) else : print ( localization . lang_check_permissions [ "" permissions_denied "" ] ) exit ( ) else : print ( localization . lang_check_permissions [ "" windows_warning "" ] ) exit ( )",if getuid ( ) == 0 :,"if localization . lang_check_permissions [ ""permissions_granted"" ] :",66.97983421,FALSE,83.57
1051,"def _ProcessName ( self , name , dependencies ) : """"""Retrieve a module name from a node name."""""" module_name , dot , base_name = name . rpartition ( "" . "" ) if dot : if module_name : <MASK> dependencies [ module_name ] . add ( base_name ) else : dependencies [ module_name ] = { base_name } else : # If we have a relative import that did not get qualified (usually due # to an empty package_name), don't insert module_name='' into the # dependencies; we get a better error message if we filter it out here # and fail later on. logging . warning ( "" Empty package name:  %s "" , name )",if module_name in dependencies :,if base_name in dependencies :,97.39040462,FALSE,98.38
1052,"def _load_db ( self ) : try : with open ( self . db ) as db : content = db . read ( 8 ) db . seek ( 0 ) if content == ( "" Salted__ "" ) : data = StringIO ( ) <MASK> self . encryptor . decrypt ( db , data ) else : raise EncryptionError ( "" Encrpyted credential storage:  {} "" . format ( self . db ) ) return json . loads ( data . getvalue ( ) ) else : return json . load ( db ) except : return { "" creds "" : [ ] }",if self . encryptor :,if self . encryptor :,100,TRUE,100
1053,"def _parse ( self , stream , context ) : obj = [ ] try : context_for_subcon = context if self . subcon . conflags & self . FLAG_COPY_CONTEXT : context_for_subcon = context . __copy__ ( ) while True : subobj = self . subcon . _parse ( stream , context_for_subcon ) <MASK> break obj . append ( subobj ) except ConstructError as ex : raise ArrayError ( "" missing terminator "" , ex ) return obj","if self . predicate ( subobj , context ) :",if not subobj :,89.63970432,FALSE,91.21
1054,"def is_active_for_user ( self , user ) : is_active = super ( AbstractUserFlag , self ) . is_active_for_user ( user ) if is_active : return is_active user_ids = self . _get_user_ids ( ) if hasattr ( user , "" pk "" ) and user . pk in user_ids : return True if hasattr ( user , "" groups "" ) : group_ids = self . _get_group_ids ( ) if group_ids : user_groups = set ( user . groups . all ( ) . values_list ( "" pk "" , flat = True ) ) <MASK> return True return None",if group_ids . intersection ( user_groups ) :,if user_groups and user_groups . issubset ( group_ids ) :,96.31926311,FALSE,92.73
1055,"def lookup_member ( self , member_name ) : document_choices = self . choices or [ ] for document_choice in document_choices : doc_and_subclasses = [ document_choice ] + document_choice . __subclasses__ ( ) for doc_type in doc_and_subclasses : field = doc_type . _fields . get ( member_name ) <MASK> return field",if field :,if field is not None :,95.55390339,FALSE,94.77
1056,"def apply ( self , db , person ) : families = person . get_parent_family_handle_list ( ) if families == [ ] : return True for family_handle in person . get_parent_family_handle_list ( ) : family = db . get_family_from_handle ( family_handle ) if family : father_handle = family . get_father_handle ( ) mother_handle = family . get_mother_handle ( ) if not father_handle : return True <MASK> return True return False",if not mother_handle :,if not mother_handle :,100,TRUE,100
1057,"def init_weights ( self ) : for m in self . modules ( ) : if isinstance ( m , nn . Linear ) : normal_init ( m , std = 0.01 ) if isinstance ( m , nn . Conv3d ) : xavier_init ( m , distribution = "" uniform "" ) <MASK> constant_init ( m , 1 )","if isinstance ( m , nn . BatchNorm3d ) :","if isinstance ( m , nn . BatchNorm3d ) :",100,TRUE,100
1058,"def _update_learning_params ( self ) : model = self . model hparams = self . hparams fd = self . runner . feed_dict step_num = self . step_num if hparams . model_type == "" resnet_tf "" : <MASK> lrn_rate = hparams . mom_lrn elif step_num < 30000 : lrn_rate = hparams . mom_lrn / 10 elif step_num < 35000 : lrn_rate = hparams . mom_lrn / 100 else : lrn_rate = hparams . mom_lrn / 1000 fd [ model . lrn_rate ] = lrn_rate",if step_num < hparams . lrn_step :,if step_num < 20000 :,93.30038079,FALSE,95.07
1059,"def token_producer ( source ) : token = source . read_uint8 ( ) while token is not None : if is_push_data_token ( token ) : yield DataToken ( read_data ( token , source ) ) <MASK> yield SmallIntegerToken ( read_small_integer ( token ) ) else : yield Token ( token ) token = source . read_uint8 ( )",elif is_small_integer ( token ) :,elif is_small_integer_token ( token ) :,97.98057299,FALSE,95.88
1060,"def user_info ( oicsrv , userdb , sub , client_id = "" "" , user_info_claims = None ) : identity = userdb [ sub ] if user_info_claims : result = { } for key , restr in user_info_claims [ "" claims "" ] . items ( ) : try : result [ key ] = identity [ key ] except KeyError : <MASK> raise Exception ( "" Missing property  ' %s ' "" % key ) else : result = identity return OpenIDSchema ( * * result )","if restr == { ""essential"" : True } :","if restr [ ""type"" ] == ""identity"" :",93.16936338,FALSE,91.29
1061,"def _helpSlot ( self , * args ) : help_text = "" Filters are applied to packets in both direction. \n \n "" filter_nb = 0 for filter in self . _filters : help_text + = "" {} :  {} "" . format ( filter [ "" name "" ] , filter [ "" description "" ] ) filter_nb + = 1 <MASK> help_text + = "" \n \n "" QtWidgets . QMessageBox . information ( self , "" Help for filters "" , help_text )",if len ( self . _filters ) != filter_nb :,if filter_nb == len ( self . _filters ) - 1 :,94.85493549,FALSE,93.07
1062,"def find_user_theme ( self , name : str ) - > Theme : """"""Find a theme named as *name* from latex_theme_path."""""" for theme_path in self . theme_paths : config_path = path . join ( theme_path , name , "" theme.conf "" ) <MASK> try : return UserTheme ( name , config_path ) except ThemeError as exc : logger . warning ( exc ) return None",if path . isfile ( config_path ) :,if path . exists ( config_path ) :,98.24079986,FALSE,97.52
1063,"def decompress ( self , value ) : if value : <MASK> if value . country_code and value . national_number : return [ "" + %d "" % value . country_code , national_significant_number ( value ) , ] else : return value . split ( "" . "" ) return [ None , "" "" ]",if type ( value ) == PhoneNumber :,"if ""."" in value :",66.19713906,FALSE,88.73
1064,"def update_prevdoc_status ( self , flag ) : for quotation in list ( set ( [ d . prevdoc_docname for d in self . get ( "" items "" ) ] ) ) : <MASK> doc = frappe . get_doc ( "" Quotation "" , quotation ) if doc . docstatus == 2 : frappe . throw ( _ ( "" Quotation  {0}  is cancelled "" ) . format ( quotation ) ) doc . set_status ( update = True ) doc . update_opportunity ( )",if quotation :,if flag :,95.78420322,FALSE,97.52
1065,"def map ( item ) : if item . deleted : return exploration = exp_fetchers . get_exploration_from_model ( item ) for state_name , state in exploration . states . items ( ) : hints_length = len ( state . interaction . hints ) <MASK> exp_and_state_key = "" %s   %s "" % ( item . id , state_name . encode ( "" utf-8 "" ) ) yield ( python_utils . UNICODE ( hints_length ) , exp_and_state_key )",if hints_length > 0 :,if hints_length > 0 :,100,TRUE,100
1066,"def _selected_machines ( self , virtual_machines ) : selected_machines = [ ] for machine in virtual_machines : if self . _args . host and self . _args . host == machine . name : selected_machines . append ( machine ) <MASK> selected_machines . append ( machine ) if self . locations and machine . location in self . locations : selected_machines . append ( machine ) return selected_machines","if self . tags and self . _tags_match ( machine . tags , self . tags ) :",if self . _args . port and self . _args . port == machine . port :,88.17700765,FALSE,86.04
1067,"def _ripple_trim_compositors_move ( self , delta ) : comp_ids = self . multi_data . moved_compositors_destroy_ids tracks_compositors = _get_tracks_compositors_list ( ) track_moved = self . multi_data . track_affected for i in range ( 1 , len ( current_sequence ( ) . tracks ) - 1 ) : if not track_moved [ i - 1 ] : continue track_comps = tracks_compositors [ i - 1 ] for comp in track_comps : <MASK> comp . move ( delta )",if comp . destroy_id in comp_ids :,if comp . id in comp_ids :,98.45642766,FALSE,97.23
1068,"def stream_docker_log ( log_stream ) : async for line in log_stream : if "" stream "" in line and line [ "" stream "" ] . strip ( ) : logger . debug ( line [ "" stream "" ] . strip ( ) ) <MASK> logger . debug ( line [ "" status "" ] . strip ( ) ) elif "" error "" in line : logger . error ( line [ "" error "" ] . strip ( ) ) raise DockerBuildError","elif ""status"" in line :","elif ""status"" in line and line [ ""status"" ] . strip ( ) :",94.0086904,FALSE,88.46
1069,"def create_keyfile ( self , keyfile , size = 64 , force = False ) : if force or not os . path . exists ( keyfile ) : keypath = os . path . dirname ( keyfile ) <MASK> os . makedirs ( keypath ) subprocess . run ( [ "" dd "" , "" if=/dev/random "" , f "" of= { keyfile } "" , f "" bs= { size } "" , "" count=1 "" ] , check = True , stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL , )",if not os . path . exists ( keypath ) :,if not os . path . exists ( keypath ) :,100,TRUE,100
1070,"def calc ( self , arg ) : op = arg [ "" op "" ] if op == "" C "" : self . clear ( ) return str ( self . current ) num = decimal . Decimal ( arg [ "" num "" ] ) if self . op : if self . op == "" + "" : self . current + = num elif self . op == "" - "" : self . current - = num <MASK> self . current * = num elif self . op == "" / "" : self . current / = num self . op = op else : self . op = op self . current = num res = str ( self . current ) if op == "" = "" : self . clear ( ) return res","elif self . op == ""*"" :","elif self . op == ""*"" :",100,TRUE,100
1071,"def chop ( expr , delta = 10.0 * * ( - 10.0 ) ) : if isinstance ( expr , Real ) : if - delta < expr . get_float_value ( ) < delta : return Integer ( 0 ) elif isinstance ( expr , Complex ) and expr . is_inexact ( ) : real , imag = expr . real , expr . imag <MASK> real = Integer ( 0 ) if - delta < imag . get_float_value ( ) < delta : imag = Integer ( 0 ) return Complex ( real , imag ) elif isinstance ( expr , Expression ) : return Expression ( chop ( expr . head ) , * [ chop ( leaf ) for leaf in expr . leaves ] ) return expr",if - delta < real . get_float_value ( ) < delta :,if - delta < real . get_float_value ( ) < delta :,100,TRUE,100
1072,"def get_file_sources ( ) : global _file_sources if _file_sources is None : from galaxy . files import ConfiguredFileSources file_sources = None if os . path . exists ( "" file_sources.json "" ) : file_sources_as_dict = None with open ( "" file_sources.json "" , "" r "" ) as f : file_sources_as_dict = json . load ( f ) <MASK> file_sources = ConfiguredFileSources . from_dict ( file_sources_as_dict ) if file_sources is None : ConfiguredFileSources . from_dict ( [ ] ) _file_sources = file_sources return _file_sources",if file_sources_as_dict is not None :,if file_sources_as_dict is not None :,100,TRUE,100
1073,"def _get_sort_map ( tags ) : """"""See TAG_TO_SORT"""""" tts = { } for name , tag in tags . items ( ) : <MASK> if tag . user : tts [ name ] = "" %s sort "" % name if tag . internal : tts [ "" ~ %s "" % name ] = "" ~ %s sort "" % name return tts",if tag . has_sort :,"if name . startswith ( ""sort_"" ) :",93.22787736,FALSE,89.45
1074,"def __init__ ( self , * * kwargs ) : if self . name is None : raise RuntimeError ( "" RenderPrimitive cannot be used directly "" ) self . option_values = { } for key , val in kwargs . items ( ) : <MASK> raise ValueError ( "" primitive ` {0} '  has no option ` {1} ' "" . format ( self . name , key ) ) self . option_values [ key ] = val # set up defaults for name , ( description , default ) in self . options . items ( ) : if not name in self . option_values : self . option_values [ name ] = default",if not key in self . options :,if not key in self . options :,100,TRUE,100
1075,"def modify_bottle_params ( self , output_stride = None ) : if output_stride is not None and output_stride % 2 != 0 : raise Exception ( "" output stride must to be even number "" ) if output_stride is None : return else : stride = 2 for i , _cfg in enumerate ( self . cfg ) : stride = stride * _cfg [ - 1 ] <MASK> s = 1 self . cfg [ i ] [ - 1 ] = s",if stride > output_stride :,if stride == output_stride :,98.55437752,FALSE,96.6
1076,"def do_query ( data , q ) : ret = [ ] if not q : return ret qkey = q [ 0 ] for key , value in iterate ( data ) : if len ( q ) == 1 : if key == qkey : ret . append ( value ) <MASK> ret . extend ( do_query ( value , q ) ) else : if not is_iterable ( value ) : continue if key == qkey : ret . extend ( do_query ( value , q [ 1 : ] ) ) else : ret . extend ( do_query ( value , q ) ) return ret",elif is_iterable ( value ) :,elif key == qkey :,95.88675075,FALSE,94.16
1077,"def make_shares ( self , plaintext ) : share_arrays = [ ] for i , p in enumerate ( plaintext ) : share_array = self . make_byte_shares ( p ) for sa in share_array : <MASK> share_arrays . append ( array . array ( "" H "" ) ) current_share_array = sa current_share_array . append ( sa ) return share_arrays",if i == 0 :,if i == len ( sa ) - 1 :,92.65447284,FALSE,91.87
1078,"def populate ( self , item ) : # log.message('populate: %s', item) path = self . getItemPath ( item ) # log.message('populate: path=%s', path) value = self . getValue ( path ) for name in sorted ( value . __dict__ . keys ( ) ) : <MASK> continue child = getattr ( value , name , None ) if hasattr ( child , "" __dict__ "" ) : item . addChild ( name , True ) else : item . addChild ( name , False )","if name [ : 2 ] == ""__"" and name [ - 2 : ] == ""__"" :","if name . startswith ( ""_"" ) :",66.88032867,FALSE,82.26
1079,"def __repr__ ( self ) : try : if self . _semlock . _is_mine ( ) : name = current_process ( ) . name <MASK> name + = "" | "" + threading . current_thread ( ) . name elif self . _semlock . _get_value ( ) == 1 : name = "" None "" elif self . _semlock . _count ( ) > 0 : name = "" SomeOtherThread "" else : name = "" SomeOtherProcess "" except Exception : name = "" unknown "" return "" <Lock(owner= %s )> "" % name","if threading . current_thread ( ) . name != ""MainThread"" :",if threading . current_thread ( ) . name :,71.70554104,FALSE,95.06
1080,"def buffer ( self , lines , scroll_end = True , scroll_if_editing = False ) : "" Add data to be displayed in the buffer. "" self . values . extend ( lines ) if scroll_end : <MASK> self . start_display_at = len ( self . values ) - len ( self . _my_widgets ) elif scroll_if_editing : self . start_display_at = len ( self . values ) - len ( self . _my_widgets )",if not self . editing :,if scroll_if_editing :,92.3299786,FALSE,94.82
1081,"def warehouses ( self ) - > tuple : from . . repositories import WarehouseBaseRepo repos = dict ( ) for dep in chain ( self . dependencies , [ self ] ) : if dep . repo is None : continue <MASK> continue for repo in dep . repo . repos : if repo . from_config : continue repos [ repo . name ] = repo return tuple ( repos . values ( ) )","if not isinstance ( dep . repo , WarehouseBaseRepo ) :","if not isinstance ( dep . repo , WarehouseBaseRepo ) :",100,TRUE,100
1082,"def _apply_flag_attrs ( src_flag , dest_flag ) : # Use a baseline flag def to get default values for empty data. baseline_flag = FlagDef ( "" "" , { } , None ) for name in dir ( src_flag ) : <MASK> continue dest_val = getattr ( dest_flag , name , None ) baseline_val = getattr ( baseline_flag , name , None ) if dest_val == baseline_val : setattr ( dest_flag , name , getattr ( src_flag , name ) )","if name [ : 1 ] == ""_"" :","if name . startswith ( ""_"" ) :",70.95223022,FALSE,92.56
1083,"def out ( parent , attr , indent = 0 ) : val = getattr ( parent , attr ) prefix = "" %s %s : "" % ( ""   "" * indent , attr . replace ( "" _ "" , "" - "" ) ) if val is None : cli . out ( prefix ) else : <MASK> val = [ flag_util . encode_flag_val ( c . value ) for c in val ] cli . out ( "" %s   %s "" % ( prefix , flag_util . encode_flag_val ( val ) ) )","if attr == ""choices"" :","if isinstance ( val , list ) :",94.12834765,FALSE,93.66
1084,"def add_cand_to_check ( cands ) : for cand in cands : x = cand . creator if x is None : continue <MASK> # `len(fan_out)` is in order to avoid comparing `x` heapq . heappush ( cand_funcs , ( - x . rank , len ( fan_out ) , x ) ) fan_out [ x ] + = 1",if x not in fan_out :,if x . rank != 0 :,91.52224483,FALSE,92.37
1085,"def task_tree_lines ( task = None ) : if task is None : task = current_root_task ( ) rendered_children = [ ] nurseries = list ( task . child_nurseries ) while nurseries : nursery = nurseries . pop ( ) nursery_children = _rendered_nursery_children ( nursery ) <MASK> nested = _render_subtree ( "" (nested nursery) "" , rendered_children ) nursery_children . append ( nested ) rendered_children = nursery_children return _render_subtree ( task . name , rendered_children )",if rendered_children :,if nursery_children :,90.67957794,FALSE,97.82
1086,"def lock_workspace ( build_dir ) : _BUILDING_LOCK_FILE = "" .blade.building.lock "" lock_file_fd , ret_code = lock_file ( os . path . join ( build_dir , _BUILDING_LOCK_FILE ) ) if lock_file_fd == - 1 : <MASK> console . fatal ( "" There is already an active building in current workspace. "" ) else : console . fatal ( "" Lock exception, please try it later. "" ) return lock_file_fd",if ret_code == errno . EAGAIN :,if ret_code == 1 :,93.16802239,FALSE,96.13
1087,"def test_list ( self ) : self . _create_locations ( ) response = self . client . get ( self . geojson_boxedlocation_list_url ) self . assertEqual ( response . status_code , 200 ) self . assertEqual ( len ( response . data [ "" features "" ] ) , 2 ) for feature in response . data [ "" features "" ] : self . assertIn ( "" bbox "" , feature ) fid = feature [ "" id "" ] if fid == 1 : self . assertEqual ( feature [ "" bbox "" ] , self . bl1 . bbox_geometry . extent ) <MASK> self . assertEqual ( feature [ "" bbox "" ] , self . bl2 . bbox_geometry . extent ) else : self . fail ( "" Unexpected id:  {0} "" . format ( fid ) ) BoxedLocation . objects . all ( ) . delete ( )",elif fid == 2 :,elif fid == 2 :,100,TRUE,100
1088,"def result ( ) : # ""global"" does not work here... R , V = rays , virtual_rays if V is not None : if normalize : V = normalize_rays ( V , lattice ) if check : R = PointCollection ( V , lattice ) V = PointCollection ( V , lattice ) d = lattice . dimension ( ) <MASK> raise ValueError ( "" virtual rays must be linearly  "" "" independent and with other rays span the ambient space. "" ) return RationalPolyhedralFan ( cones , R , lattice , is_complete , V )",if len ( V ) != d - R . dim ( ) or ( R + V ) . dim ( ) != d :,if d < 0 :,64.40899621,FALSE,78.46
1089,"def search_host ( self , search_string ) : results = [ ] for host_entry in self . config_data : if host_entry . get ( "" type "" ) != "" entry "" : continue if host_entry . get ( "" host "" ) == "" * "" : continue searchable_information = host_entry . get ( "" host "" ) for key , value in six . iteritems ( host_entry . get ( "" options "" ) ) : <MASK> value = ""   "" . join ( value ) if isinstance ( value , int ) : value = str ( value ) searchable_information + = ""   "" + value if search_string in searchable_information : results . append ( host_entry ) return results","if isinstance ( value , list ) :","if isinstance ( value , list ) :",100,TRUE,100
1090,"def test_async_iterator ( app ) : async with new_stream ( app ) as stream : for i in range ( 100 ) : await stream . channel . deliver ( message ( key = i , value = i ) ) received = 0 async for value in stream : assert value == received received + = 1 <MASK> break assert await channel_empty ( stream . channel )",if received >= 100 :,if received >= 10 :,98.24540842,FALSE,96.86
1091,"def has_google_credentials ( ) : global _HAS_GOOGLE_CREDENTIALS if _HAS_GOOGLE_CREDENTIALS is None : provider = Provider ( "" google "" ) <MASK> _HAS_GOOGLE_CREDENTIALS = False else : _HAS_GOOGLE_CREDENTIALS = True return _HAS_GOOGLE_CREDENTIALS",if provider . get_access_key ( ) is None or provider . get_secret_key ( ) is None :,if provider . has_credentials ( ) :,61.06386769,FALSE,72.4
1092,"def __cmp__ ( self , other ) : if isinstance ( other , date ) or isinstance ( other , datetime ) : a = self . _d . getTime ( ) b = other . _d . getTime ( ) if a < b : return - 1 <MASK> return 0 else : raise TypeError ( "" expected date or datetime object "" ) return 1",elif a == b :,elif a > b :,98.21366407,FALSE,95.41
1093,"def validate_weight ( self , weight ) : try : add_acl_to_obj ( self . context [ "" user_acl "" ] , self . category ) except AttributeError : return weight # don't validate weight further if category failed if weight > self . category . acl . get ( "" can_pin_threads "" , 0 ) : <MASK> raise ValidationError ( _ ( "" You don ' t have permission to pin threads globally  "" "" in this category. "" ) ) else : raise ValidationError ( _ ( "" You don ' t have permission to pin threads in this category. "" ) ) return weight",if weight == 2 :,"if self . category . acl . get ( ""can_pin_threads"" , 1 )",95.89872654,FALSE,85.91
1094,"def effective ( line ) : for b in line : if not b . cond : return else : try : val = 5 <MASK> if b . ignore : b . ignore - = 1 else : return ( b , True ) except : return ( b , False ) return",if val :,if b . cond . value == val :,93.05543319,FALSE,86.12
1095,"def wheelEvent ( self , event ) : """"""Handle a wheel event."""""" if QtCore . Qt . ControlModifier & event . modifiers ( ) : d = { "" c "" : self . leo_c } if isQt5 : point = event . angleDelta ( ) delta = point . y ( ) or point . x ( ) else : delta = event . delta ( ) <MASK> zoom_out ( d ) else : zoom_in ( d ) event . accept ( ) return QtWidgets . QTextBrowser . wheelEvent ( self , event )",if delta < 0 :,if delta < 0 :,100,TRUE,100
1096,"def test_evname_in_mp_events_testcases ( ) : ok = True for evname in ins . mp_events : if evname == "" version "" : continue for i , args in enumerate ( ins . mp_events [ evname ] [ "" test_cases "" ] ) : <MASK> msg = "" Error, for evname  %s  the testase # %d  does not match evname "" print ( msg % ( evname , i ) ) ok = False if ok : print ( "" test_evname_in_mp_events_testcases: passed "" )",if evname != args [ 0 ] :,"if args [ 0 ] != ""test"" :",91.55770059,FALSE,94.02
1097,"def check_database ( ) : if len ( EmailAddress . objects . all ( ) ) > 0 : print ( "" Are you sure you want to wipe the existing development database and reseed it? (Y/N) "" ) <MASK> destroy_database ( ) else : return False else : return True","if raw_input ( ) . lower ( ) == ""y"" :","if EmailAddress . objects . filter ( name = ""y"" ) . exists ( ) :",58.51022007,FALSE,82.16
1098,"def _get_requested_databases ( self ) : """"""Returns a list of databases requested, not including ignored dbs"""""" requested_databases = [ ] if ( self . _requested_namespaces is not None ) and ( self . _requested_namespaces != [ ] ) : for requested_namespace in self . _requested_namespaces : if requested_namespace [ 0 ] is "" * "" : return [ ] <MASK> requested_databases . append ( requested_namespace [ 0 ] ) return requested_databases",elif requested_namespace [ 0 ] not in IGNORE_DBS :,"if requested_namespace [ 1 ] not in [ ""ignored"" , ""ignored"" ] :",90.94822382,FALSE,87.17
1099,"def decorated ( self , * args , * * kwargs ) : start_time = time . perf_counter ( ) stderr = "" "" saved_exception = None try : yield from fn ( self , * args , * * kwargs ) except GitSavvyError as e : stderr = e . stderr saved_exception = e finally : end_time = time . perf_counter ( ) util . debug . log_git ( args , None , "" <SNIP> "" , stderr , end_time - start_time ) <MASK> raise saved_exception from None",if saved_exception :,if saved_exception is not None :,97.10282464,FALSE,96.12
1100,"def is_suppressed_warning ( type : str , subtype : str , suppress_warnings : List [ str ] ) - > bool : """"""Check the warning is suppressed or not."""""" if type is None : return False for warning_type in suppress_warnings : if "" . "" in warning_type : target , subtarget = warning_type . split ( "" . "" , 1 ) else : target , subtarget = warning_type , None <MASK> if ( subtype is None or subtarget is None or subtarget == subtype or subtarget == "" * "" ) : return True return False",if target == type :,"if target in ( type , ""*"" ) :",96.06032218,FALSE,92.89
1101,"def talk ( self , words ) : if self . writeSentence ( words ) == 0 : return r = [ ] while 1 : i = self . readSentence ( ) if len ( i ) == 0 : continue reply = i [ 0 ] attrs = { } for w in i [ 1 : ] : j = w . find ( "" = "" , 1 ) if j == - 1 : attrs [ w ] = "" "" else : attrs [ w [ : j ] ] = w [ j + 1 : ] r . append ( ( reply , attrs ) ) <MASK> return r","if reply == ""!done"" :",if len ( r ) == self . max_sentences :,92.15349917,FALSE,90.97
1102,"def encrypt ( self , plaintext ) : encrypted = [ ] for p in _string_to_bytes ( plaintext ) : <MASK> self . _remaining_block = self . _aes . encrypt ( self . _last_precipherblock ) self . _last_precipherblock = [ ] precipherbyte = self . _remaining_block . pop ( 0 ) self . _last_precipherblock . append ( precipherbyte ) cipherbyte = p ^ precipherbyte encrypted . append ( cipherbyte ) return _bytes_to_string ( encrypted )",if len ( self . _remaining_block ) == 0 :,if self . _last_precipherblock :,86.14695983,FALSE,89.43
1103,"def find_symbol ( self , r , globally = False ) : query = self . view . substr ( self . view . word ( r ) ) fname = self . view . file_name ( ) . replace ( "" \\ "" , "" / "" ) locations = self . view . window ( ) . lookup_symbol_in_index ( query ) if not locations : return try : <MASK> location = [ hit [ 2 ] for hit in locations if fname . endswith ( hit [ 1 ] ) ] [ 0 ] return location [ 0 ] - 1 , location [ 1 ] - 1 else : # TODO: There might be many symbols with the same name. return locations [ 0 ] except IndexError : return",if not globally :,if globally :,97.65942424,FALSE,98.29
1104,"def __getslice__ ( self , i , j ) : try : <MASK> # handle the case where the right bound is unspecified j = len ( self ) if i < 0 or j < 0 : raise dns . exception . FormError # If it's not an empty slice, access left and right bounds # to make sure they're valid if i != j : super ( WireData , self ) . __getitem__ ( i ) super ( WireData , self ) . __getitem__ ( j - 1 ) return WireData ( super ( WireData , self ) . __getslice__ ( i , j ) ) except IndexError : raise dns . exception . FormError",if j == sys . maxint :,if j == len ( self ) :,87.57279154,FALSE,96.1
1105,"def main ( ) : r = redis . StrictRedis ( ) curr_memory = prev_memory = r . info ( ) [ "" used_memory "" ] while True : <MASK> print ( "" Delta Memory :  %d , Total Memory :  %d "" % ( ( curr_memory - prev_memory ) , curr_memory ) ) time . sleep ( 1 ) prev_memory = curr_memory curr_memory = r . info ( ) [ "" used_memory "" ]",if prev_memory != curr_memory :,if curr_memory > prev_memory :,97.39907038,FALSE,94.92
1106,"def _visit ( self , func ) : fname = func [ 0 ] if fname in self . _flags : if self . _flags [ fname ] == 1 : logger . critical ( "" Fatal error! network ins not Dag. "" ) import sys sys . exit ( - 1 ) else : return else : <MASK> self . _flags [ fname ] = 1 for output in func [ 3 ] : for f in self . _orig : for input in f [ 2 ] : if output == input : self . _visit ( f ) self . _flags [ fname ] = 2 self . _sorted . insert ( 0 , func )",if fname not in self . _flags :,if fname not in self . _flags :,100,TRUE,100
1107,"def urls ( self , version = None ) : """"""Returns all URLS that are mapped to this interface"""""" urls = [ ] for _base_url , routes in self . api . http . routes . items ( ) : for url , methods in routes . items ( ) : for _method , versions in methods . items ( ) : for interface_version , interface in versions . items ( ) : <MASK> if not url in urls : urls . append ( ( "" /v {0} "" . format ( version ) if version else "" "" ) + url ) return urls",if interface_version == version and interface == self :,if _base_url == interface_version :,94.57991074,FALSE,92.1
1108,"def _handle_data ( self , text ) : if self . _translate : <MASK> self . _data . append ( text ) else : self . _translate = False self . _data = [ ] self . _comments = [ ]","if not text . startswith ( ""gtk-"" ) :",if text :,84.63298125,FALSE,81.5
1109,"def set_dir_modes ( self , dirname , mode ) : if not self . is_chmod_supported ( ) : return for dirpath , dirnames , fnames in os . walk ( dirname ) : <MASK> continue log . info ( "" changing mode of  %s  to  %o "" , dirpath , mode ) if not self . dry_run : os . chmod ( dirpath , mode )",if os . path . islink ( dirpath ) :,if not self . is_dir_supported ( dirpath ) :,90.70150474,FALSE,89.51
1110,"def language ( self ) : if self . lang_data : lang_data = [ s if s != "" None "" else None for s in self . lang_data ] <MASK> return Language ( lang_data [ 0 ] , country = lang_data [ 1 ] , script = lang_data [ 2 ] )",if lang_data [ 0 ] :,if lang_data :,92.8509901,FALSE,93.67
1111,"def _addItemToLayout ( self , sample , label ) : col = self . layout . columnCount ( ) row = self . layout . rowCount ( ) if row : row - = 1 nCol = self . columnCount * 2 # FIRST ROW FULL if col == nCol : for col in range ( 0 , nCol , 2 ) : # FIND RIGHT COLUMN <MASK> break if col + 2 == nCol : # MAKE NEW ROW col = 0 row + = 1 self . layout . addItem ( sample , row , col ) self . layout . addItem ( label , row , col + 1 )","if not self . layout . itemAt ( row , col ) :","if self . _addItemToLayout ( sample , col + 1 ) :",95.92896561,FALSE,92.24
1112,"def align_comments ( tlist ) : tidx , token = tlist . token_next_by ( i = sql . Comment ) while token : pidx , prev_ = tlist . token_prev ( tidx ) <MASK> tlist . group_tokens ( sql . TokenList , pidx , tidx , extend = True ) tidx = pidx tidx , token = tlist . token_next_by ( i = sql . Comment , idx = tidx )","if isinstance ( prev_ , sql . TokenList ) :",if prev_ :,87.90812539,FALSE,89.48
1113,"def hook_GetVariable ( ql , address , params ) : if params [ "" VariableName "" ] in ql . env : var = ql . env [ params [ "" VariableName "" ] ] read_len = read_int64 ( ql , params [ "" DataSize "" ] ) if params [ "" Attributes "" ] != 0 : write_int64 ( ql , params [ "" Attributes "" ] , 0 ) write_int64 ( ql , params [ "" DataSize "" ] , len ( var ) ) <MASK> return EFI_BUFFER_TOO_SMALL if params [ "" Data "" ] != 0 : ql . mem . write ( params [ "" Data "" ] , var ) return EFI_SUCCESS return EFI_NOT_FOUND",if read_len < len ( var ) :,if read_len > 0 :,93.40872223,FALSE,95.67
1114,"def _PromptMySQL ( self , config ) : """"""Prompts the MySQL configuration, retrying if the configuration is invalid."""""" while True : self . _PromptMySQLOnce ( config ) if self . _CheckMySQLConnection ( ) : print ( "" Successfully connected to MySQL with the given configuration. "" ) return else : print ( "" Error: Could not connect to MySQL with the given configuration. "" ) retry = RetryBoolQuestion ( "" Do you want to retry MySQL configuration? "" , True ) <MASK> raise ConfigInitError ( )",if not retry :,if not retry :,100,TRUE,100
1115,"def split_long_line_with_indent ( line , max_per_line , indent ) : """"""Split the `line` so that it doesn't go over `max_per_line` and adds `indent` to new lines."""""" words = line . split ( ""   "" ) lines = [ ] current_line = words [ 0 ] for word in words [ 1 : ] : <MASK> lines . append ( current_line ) current_line = ""   "" * indent + word else : current_line = f "" { current_line }   { word } "" lines . append ( current_line ) return "" \n "" . join ( lines )","if len ( f""{current_line} {word}"" ) > max_per_line :",if len ( word ) > max_per_line :,93.71145851,FALSE,92.12
1116,"def gen_cli ( docs_dir ) : with open ( os . path . join ( docs_dir , "" CLI_template.md "" ) , "" r "" ) as cli_temp_file : temp_lines = cli_temp_file . readlines ( ) lines = [ ] for line in temp_lines : matched = re . match ( r "" { onnx-tf.*} "" , line ) <MASK> command = matched . string . strip ( ) [ 1 : - 1 ] output = subprocess . check_output ( command . split ( ""   "" ) ) . decode ( "" UTF-8 "" ) lines . append ( output ) else : lines . append ( line ) with open ( os . path . join ( docs_dir , "" CLI.md "" ) , "" w "" ) as cli_file : cli_file . writelines ( lines )",if matched :,if matched :,100,TRUE,100
1117,"def read ( self , size = None ) : if size == 0 : return "" "" data = list ( ) while size is None or size > 0 : line = self . readline ( size or - 1 ) if not line : break <MASK> size - = len ( line ) data . append ( line ) return "" "" . join ( data )",if size is not None :,if len ( line ) > size :,93.67909443,FALSE,90.47
1118,"def _get_format_and_pattern ( file_path ) : file_path = Path ( file_path ) with file_path . open ( ) as f : first_line = f . readline ( ) . strip ( ) match = re . match ( r "" format *: *(.+) "" , first_line ) <MASK> return "" gztar "" , first_line , 1 return match . group ( 1 ) , f . readline ( ) . strip ( ) , 2",if match is None :,if match is None :,100,TRUE,100
1119,"def remove_old_snapshot ( install_dir ) : logging . info ( "" Removing any old files in  {} "" . format ( install_dir ) ) for file in glob . glob ( "" {} /* "" . format ( install_dir ) ) : try : if os . path . isfile ( file ) : os . unlink ( file ) <MASK> shutil . rmtree ( file ) except Exception as error : logging . error ( "" Error:  {} "" . format ( error ) ) sys . exit ( 1 )",elif os . path . isdir ( file ) :,elif os . path . isdir ( file ) :,100,TRUE,100
1120,"def _test_forever ( self , tests ) : while True : for test_name in tests : yield test_name if self . bad : return <MASK> return",if self . ns . fail_env_changed and self . environment_changed :,if self . _test_forever ( ) :,56.44071891,FALSE,70.35
1121,"def _swig_extract_dependency_files ( self , src ) : dep = [ ] for line in open ( src ) : if line . startswith ( "" #include "" ) or line . startswith ( "" %i nclude "" ) : line = line . split ( ""   "" ) [ 1 ] . strip ( """""" ' "" \r \n """""" ) <MASK> dep . append ( line ) return [ i for i in dep if os . path . exists ( i ) ]","if not ( ""<"" in line or line in dep ) :","if line . startswith ( ""%i dependency"" ) :",90.23643802,FALSE,89.95
1122,"def update_service_key ( kid , name = None , metadata = None ) : try : with db_transaction ( ) : key = db_for_update ( ServiceKey . select ( ) . where ( ServiceKey . kid == kid ) ) . get ( ) <MASK> key . name = name if metadata is not None : key . metadata . update ( metadata ) key . save ( ) except ServiceKey . DoesNotExist : raise ServiceKeyDoesNotExist",if name is not None :,if name is not None :,100,TRUE,100
1123,"def range ( self , dimension , data_range = True , dimension_range = True ) : if self . nodes and dimension in self . nodes . dimensions ( ) : node_range = self . nodes . range ( dimension , data_range , dimension_range ) <MASK> path_range = self . _edgepaths . range ( dimension , data_range , dimension_range ) return max_range ( [ node_range , path_range ] ) return node_range return super ( Graph , self ) . range ( dimension , data_range , dimension_range )",if self . _edgepaths :,if self . _edgepaths and dimension in self . _edgepaths . dimensions ( ) :,88.18794472,FALSE,90.41
1124,"def handler ( chan , host , port ) : sock = socket ( ) try : sock . connect ( ( host , port ) ) except Exception as e : if verbose == True : print ( e ) return while True : r , w , x = select . select ( [ sock , chan ] , [ ] , [ ] ) if sock in r : data = sock . recv ( 1024 ) <MASK> break chan . send ( data ) if chan in r : data = chan . recv ( 1024 ) <MASK> break sock . send ( data ) chan . close ( ) sock . close ( )",if len ( data ) == 0 :,if len ( data ) == 0 :,100,TRUE,100
1125,"def output_layer ( self , features , * * kwargs ) : """"""Project features to the vocabulary size."""""" if self . adaptive_softmax is None : # project back to size of vocabulary <MASK> return F . linear ( features , self . embed_tokens . weight ) else : return F . linear ( features , self . embed_out ) else : return features",if self . share_input_output_embed :,if self . embed_tokens :,98.10225398,FALSE,90.67
1126,"def generate ( self , dest , vars ) : util . ensure_dir ( dest ) for relpath , src , template in self . _file_templates : file_dest = os . path . join ( dest , relpath ) util . ensure_dir ( os . path . dirname ( file_dest ) ) <MASK> shutil . copyfile ( src , file_dest ) else : _render_template ( template , vars , file_dest )",if template is None :,if os . path . isfile ( src ) :,86.05443983,FALSE,90.13
1127,"def _py_matching_callback ( self , context , result , sender , device ) : d = HIDDevice . get_device ( c_void_p ( device ) ) if d not in self . devices : self . devices . add ( d ) for x in self . matching_observers : <MASK> x . device_discovered ( d )","if hasattr ( x , ""device_discovered"" ) :",if x . device_discovered ( d ) :,89.30277476,FALSE,89.19
1128,"def urlquote ( * args , * * kwargs ) : new_kwargs = dict ( kwargs ) if not PY3 : new_kwargs = dict ( kwargs ) <MASK> del new_kwargs [ "" encoding "" ] if "" errors "" in kwargs : del new_kwargs [ "" errors "" ] return quote ( * args , * * new_kwargs )","if ""encoding"" in new_kwargs :","if ""encoding"" in kwargs :",96.19265927,FALSE,95.35
1129,"def Set ( self , attr , value ) : hook = getattr ( self , "" _set_ %s "" % attr , None ) if hook : # If there is a set hook we must use the context manager. <MASK> raise ValueError ( "" Can only update attribute  %s  using the context manager. "" % attr ) if attr not in self . _pending_hooks : self . _pending_hooks . append ( attr ) self . _pending_parameters [ attr ] = value else : super ( Configuration , self ) . Set ( attr , value )",if self . _lock > 0 :,"if not hasattr ( self , ""_set_%s"" % attr ) :",96.12050499,FALSE,87.42
1130,"def on_profiles_loaded ( self , profiles ) : cb = self . builder . get_object ( "" cbProfile "" ) model = cb . get_model ( ) model . clear ( ) for f in profiles : name = f . get_basename ( ) if name . endswith ( "" .mod "" ) : continue <MASK> name = name [ 0 : - 11 ] model . append ( ( name , f , None ) ) cb . set_active ( 0 )","if name . endswith ( "".sccprofile"" ) :","if name . endswith ( "".py"" ) :",98.59118967,FALSE,97.59
1131,"def get_eval_task ( self , worker_id ) : """"""Return next evaluation (task_id, Task) tuple"""""" with self . _lock : <MASK> return - 1 , None self . _task_id + = 1 task = self . _eval_todo . pop ( ) self . _doing [ self . _task_id ] = ( worker_id , task , time . time ( ) ) return self . _task_id , task",if not self . _eval_todo :,if self . _task_id >= self . _eval_todo :,93.29955944,FALSE,91.54
1132,"def queries ( self ) : if DEV : cmd = ShellCommand ( "" docker "" , "" ps "" , "" -qf "" , "" name= %s "" % self . path . k8s ) <MASK> if not cmd . stdout . strip ( ) : log_cmd = ShellCommand ( "" docker "" , "" logs "" , self . path . k8s , stderr = subprocess . STDOUT ) if log_cmd . check ( f "" docker logs for  { self . path . k8s } "" ) : print ( cmd . stdout ) pytest . exit ( f "" container failed to start for  { self . path . k8s } "" ) return ( )","if not cmd . check ( f""docker check for {self.path.k8s}"" ) :",if cmd . check ( ) :,93.48282022,FALSE,87.84
1133,"def disjoined ( data ) : # create marginalized distributions and multiple them together data_disjoined = None dim = len ( data . shape ) for d in range ( dim ) : axes = list ( range ( dim ) ) axes . remove ( d ) data1d = multisum ( data , axes ) shape = [ 1 for k in range ( dim ) ] shape [ d ] = len ( data1d ) data1d = data1d . reshape ( tuple ( shape ) ) <MASK> data_disjoined = data1d else : data_disjoined = data_disjoined * data1d return data_disjoined",if d == 0 :,if data_disjoined is None :,72.8650229,FALSE,94.36
1134,"def safe_repr ( val ) : try : <MASK> # We special case dicts to have a sorted repr. This makes testing # significantly easier val = _obj_with_safe_repr ( val ) ret = repr ( val ) if six . PY2 : ret = ret . decode ( "" utf-8 "" ) except UnicodeEncodeError : ret = red ( "" a  %r  that cannot be represented "" % type ( val ) ) else : ret = green ( ret ) return ret","if isinstance ( val , dict ) :","if isinstance ( val , dict ) :",100,TRUE,100
1135,"def wrapper ( * args , * * kwargs ) : resp = view_func ( * args , * * kwargs ) if isinstance ( resp , dict ) : ctx_params = request . environ . get ( "" webrec.template_params "" ) <MASK> resp . update ( ctx_params ) template = self . jinja_env . jinja_env . get_or_select_template ( template_name ) return template . render ( * * resp ) else : return resp",if ctx_params :,if ctx_params :,100,TRUE,100
1136,"def post ( self , request , * args , * * kwargs ) : contact_id = kwargs . get ( "" pk "" ) self . object = get_object_or_404 ( Contact , id = contact_id ) if ( self . request . user . role != "" ADMIN "" and not self . request . user . is_superuser and self . request . user != self . object . created_by ) or self . object . company != self . request . company : raise PermissionDenied else : if self . object . address_id : self . object . address . delete ( ) self . object . delete ( ) <MASK> return JsonResponse ( { "" error "" : False } ) return redirect ( "" contacts:list "" )",if self . request . is_ajax ( ) :,if not self . object . address :,95.45641767,FALSE,94.06
1137,"def escape ( text , newline = False ) : """"""Escape special html characters."""""" if isinstance ( text , str ) : if "" & "" in text : text = text . replace ( "" & "" , "" &amp; "" ) if "" > "" in text : text = text . replace ( "" > "" , "" &gt; "" ) <MASK> text = text . replace ( "" < "" , "" &lt; "" ) if ' "" ' in text : text = text . replace ( ' "" ' , "" &quot; "" ) if "" ' "" in text : text = text . replace ( "" ' "" , "" &quot; "" ) if newline : if "" \n "" in text : text = text . replace ( "" \n "" , "" <br> "" ) return text","if ""<"" in text :","if ""<"" in text :",100,TRUE,100
1138,"def everythingIsUnicode ( d ) : """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k , v in d . iteritems ( ) : if isinstance ( v , dict ) and k != "" headers "" : <MASK> return False elif isinstance ( v , list ) : for i in v : if isinstance ( i , dict ) and not everythingIsUnicode ( i ) : return False elif isinstance ( i , _bytes ) : return False elif isinstance ( v , _bytes ) : return False return True",if not everythingIsUnicode ( v ) :,if not everythingIsUnicode ( v ) :,100,TRUE,100
1139,"def fill ( self ) : try : while ( not self . stopping . wait ( self . sample_wait ) and len ( self . queue ) < self . queue . maxlen ) : self . queue . append ( self . parent . _read ( ) ) <MASK> self . parent . _fire_events ( ) self . full . set ( ) while not self . stopping . wait ( self . sample_wait ) : self . queue . append ( self . parent . _read ( ) ) if isinstance ( self . parent , EventsMixin ) : self . parent . _fire_events ( ) except ReferenceError : # Parent is dead; time to die! pass","if self . partial and isinstance ( self . parent , EventsMixin ) :","if isinstance ( self . parent , EventsMixin ) :",95.31846714,FALSE,96.2
1140,"def _SetListviewTextItems ( self , items ) : self . listview . DeleteAllItems ( ) index = - 1 for item in items : index = self . listview . InsertItem ( index + 1 , item [ 0 ] ) data = item [ 1 ] <MASK> data = "" "" self . listview . SetItemText ( index , 1 , data )",if data is None :,if not data :,88.70766848,FALSE,93.66
1141,"def process_request ( self , request ) : for old , new in self . names_name : request . uri = request . uri . replace ( old , new ) if is_text_payload ( request ) and request . body : body = six . ensure_str ( request . body ) <MASK> request . body = body . replace ( old , new ) return request",if old in body :,if body :,76.47285735,FALSE,95.53
1142,"def serialize ( cls , value , * args , * * kwargs ) : if value is None : return "" "" value_as_string = six . text_type ( value ) if SHOULD_NOT_USE_LOCALE : return value_as_string else : grouping = kwargs . get ( "" grouping "" , None ) has_decimal_places = value_as_string . find ( "" . "" ) != - 1 <MASK> string_format = "" %d "" else : decimal_places = len ( value_as_string . split ( "" . "" ) [ 1 ] ) string_format = "" % . {} f "" . format ( decimal_places ) return locale . format ( string_format , value , grouping = grouping )",if not has_decimal_places :,if has_decimal_places :,96.71656403,FALSE,98.43
1143,"def review_link ( request , path_obj ) : try : <MASK> if check_permission ( "" translate "" , request ) : text = _ ( "" Review Suggestions "" ) else : text = _ ( "" View Suggestions "" ) return { "" href "" : dispatch . translate ( request , path_obj . pootle_path , matchnames = [ "" hassuggestion "" ] ) , "" text "" : text , } except IOError : pass",if path_obj . has_suggestions ( ) :,if path_obj . pootle_path :,94.22767653,FALSE,93.43
1144,"def _migrate_key ( self , key ) : """"""migrate key from old .dat file"""""" key_path = os . path . join ( self . home_path , "" keys.dat "" ) if os . path . exists ( key_path ) : try : key_data = json . loads ( open ( key_path , "" rb "" ) . read ( ) ) <MASK> self . add_key ( key , key_data . get ( key ) ) except : self . error ( f "" Corrupt key file. Manual migration of  ' { key } '  required. "" )",if key_data . get ( key ) :,if key in key_data :,92.36545847,FALSE,94.69
1145,"def gather_callback_args ( self , obj , callbacks ) : session = sa . orm . object_session ( obj ) for callback in callbacks : backref = callback . backref root_objs = getdotattr ( obj , backref ) if backref else obj if root_objs : <MASK> root_objs = [ root_objs ] with session . no_autoflush : for root_obj in root_objs : if root_obj : args = self . get_callback_args ( root_obj , callback ) if args : yield args","if not isinstance ( root_objs , Iterable ) :","if not isinstance ( root_objs , list ) :",98.55211221,FALSE,97.82
1146,"def GetDefFile ( self , gyp_to_build_path ) : """"""Returns the .def file from sources, if any.  Otherwise returns None."""""" spec = self . spec if spec [ "" type "" ] in ( "" shared_library "" , "" loadable_module "" , "" executable "" ) : def_files = [ s for s in spec . get ( "" sources "" , [ ] ) if s . endswith ( "" .def "" ) ] <MASK> return gyp_to_build_path ( def_files [ 0 ] ) elif len ( def_files ) > 1 : raise Exception ( "" Multiple .def files "" ) return None",if len ( def_files ) == 1 :,if len ( def_files ) == 1 :,100,TRUE,100
1147,"def _validate_gallery ( images ) : for image in images : image_path = image . get ( "" image_path "" , "" "" ) if image_path : if not isfile ( image_path ) : raise TypeError ( f "" { image_path !r}  is not a valid image path. "" ) else : raise TypeError ( "" ' image_path '  is required. "" ) <MASK> raise TypeError ( "" Caption must be 180 characters or less. "" )","if not len ( image . get ( ""caption"" , """" ) ) <= 180 :","if image_path . find ( "" Caption"" ) < 180 :",80.7189527,FALSE,85.86
1148,"def VType ( self ) : if "" DW_AT_type "" in self . attributes : target = self . types [ self . type_id ] target_type = target . VType ( ) <MASK> target_type = [ target_type , None ] return [ "" Pointer "" , dict ( target = target_type [ 0 ] , target_args = target_type [ 1 ] ) ] return [ "" Pointer "" , dict ( target = "" Void "" ) ]","if not isinstance ( target_type , list ) :","if not isinstance ( target_type , list ) :",100,TRUE,100
1149,"def addInPlace ( self , value1 , value2 ) : for group in value2 : for key in value2 [ group ] : <MASK> value1 [ group ] [ key ] = value2 [ group ] [ key ] else : value1 [ group ] [ key ] + = value2 [ group ] [ key ] return value1",if key not in value1 [ group ] :,if key not in value1 [ group ] :,100,TRUE,100
1150,"def _mongo_query_and ( self , queries ) : if len ( queries ) == 1 : return queries [ 0 ] query = { } for q in queries : for k , v in q . items ( ) : <MASK> query [ k ] = { } if isinstance ( v , list ) : # TODO check exists of k in query, may be it should be update query [ k ] = v else : query [ k ] . update ( v ) return query",if k not in query :,if k not in query :,100,TRUE,100
1151,"def _handled_eventtype ( self , eventtype , handler ) : if eventtype not in known_events : log . error ( ' The event  "" %s ""  is not known ' , eventtype ) return False if known_events [ eventtype ] . __module__ . startswith ( "" deluge.event "" ) : <MASK> return True log . error ( "" You cannot register custom notification providers  "" "" for built-in event types. "" ) return False return True",if handler . __self__ is self :,if handler ( known_events [ eventtype ] . __module__ ) :,92.33051305,FALSE,87.36
1152,"def get_ax_arg ( uri ) : if not ax_ns : return u "" "" prefix = "" openid. "" + ax_ns + "" .type. "" ax_name = None for name , values in self . request . arguments . iteritems ( ) : <MASK> part = name [ len ( prefix ) : ] ax_name = "" openid. "" + ax_ns + "" .value. "" + part break if not ax_name : return u "" "" return self . get_argument ( ax_name , u "" "" )",if values [ - 1 ] == uri and name . startswith ( prefix ) :,if uri . startswith ( name ) :,93.70537349,FALSE,89.2
1153,"def handle_starttag ( self , tag , attrs ) : if tag == "" base "" : self . base_url = dict ( attrs ) . get ( "" href "" ) if self . scan_tag ( tag ) : for attr , value in attrs : <MASK> if self . strip : value = strip_html5_whitespace ( value ) url = self . process_attr ( value ) link = Link ( url = url ) self . links . append ( link ) self . current_link = link",if self . scan_attr ( attr ) :,"if attr . startswith ( ""href"" ) :",95.73303268,FALSE,92.94
1154,"def test_long_steadystate_queue_popright ( self ) : for size in ( 0 , 1 , 2 , 100 , 1000 ) : d = deque ( reversed ( range ( size ) ) ) append , pop = d . appendleft , d . pop for i in range ( size , BIG ) : append ( i ) x = pop ( ) <MASK> self . assertEqual ( x , i - size ) self . assertEqual ( list ( reversed ( list ( d ) ) ) , list ( range ( BIG - size , BIG ) ) )",if x != i - size :,if x != i - size :,100,TRUE,100
1155,"def _update_read ( self ) : """"""Update state when there is read event"""""" try : msg = bytes ( self . _sock . recv ( 4096 ) ) <MASK> self . on_message ( msg ) return True # normal close, remote is closed self . close ( ) except socket . error as err : if err . args [ 0 ] in ( errno . EAGAIN , errno . EWOULDBLOCK ) : pass else : self . on_error ( err ) return False",if msg :,if msg :,100,TRUE,100
1156,"def prepend ( self , value ) : """"""prepend value to nodes"""""" root , root_text = self . _get_root ( value ) for i , tag in enumerate ( self ) : if not tag . text : tag . text = "" "" <MASK> root [ - 1 ] . tail = tag . text tag . text = root_text else : tag . text = root_text + tag . text if i > 0 : root = deepcopy ( list ( root ) ) tag [ : 0 ] = root root = tag [ : len ( root ) ] return self",if len ( root ) > 0 :,if i == 0 :,84.69988237,FALSE,94.81
1157,"def cmp ( self , other ) : v_is_ptr = not isinstance ( self , CTypesGenericPrimitive ) w_is_ptr = isinstance ( other , CTypesData ) and not isinstance ( other , CTypesGenericPrimitive ) if v_is_ptr and w_is_ptr : return cmpfunc ( self . _convert_to_address ( None ) , other . _convert_to_address ( None ) ) elif v_is_ptr or w_is_ptr : return NotImplemented else : if isinstance ( self , CTypesGenericPrimitive ) : self = self . _value <MASK> other = other . _value return cmpfunc ( self , other )","if isinstance ( other , CTypesGenericPrimitive ) :","if isinstance ( other , CTypesGenericPrimitive ) :",100,TRUE,100
1158,"def get_external_addresses ( self , label = None ) - > List [ str ] : result = [ ] for c in self . _conf [ "" pools "" ] . values ( ) : <MASK> if label == c [ "" label "" ] : result . append ( c [ "" external_address "" ] [ 0 ] ) else : result . append ( c [ "" external_address "" ] [ 0 ] ) return result",if label is not None :,"if ""external_address"" in c :",92.80040053,FALSE,91.08
1159,"def coerce_text ( v ) : if not isinstance ( v , basestring_ ) : <MASK> attr = "" __unicode__ "" else : attr = "" __str__ "" if hasattr ( v , attr ) : return unicode ( v ) else : return bytes ( v ) return v",if sys . version_info [ 0 ] < 3 :,"if hasattr ( v , ""__unicode__"" ) :",62.42604991,FALSE,81.66
1160,"def check_localhost ( self ) : """"""Warn if any socket_host is 'localhost'. See #711."""""" for k , v in cherrypy . config . items ( ) : <MASK> warnings . warn ( "" The use of  ' localhost '  as a socket host can  "" "" cause problems on newer systems, since  "" "" ' localhost '  can map to either an IPv4 or an  "" "" IPv6 address. You should use  ' 127.0.0.1 '   "" "" or  ' [::1] '  instead. "" )","if k == ""server.socket_host"" and v == ""localhost"" :","if k == ""localhost"" :",91.19429815,FALSE,90.57
1161,"def add_songs ( self , filenames , library ) : changed = [ ] for i in range ( len ( self ) ) : <MASK> song = library [ self . _list [ i ] ] self . _list [ i ] = song changed . append ( song ) if changed : self . _emit_changed ( changed , msg = "" add "" ) return bool ( changed )","if isinstance ( self [ i ] , str ) and self . _list [ i ] in filenames :",if self . _list [ i ] not in filenames :,86.77221721,FALSE,86.19
1162,"def _expand_deps_java_generation ( self ) : """"""Ensure that all multilingual dependencies such as proto_library generate java code."""""" queue = collections . deque ( self . deps ) keys = set ( ) while queue : k = queue . popleft ( ) if k not in keys : keys . add ( k ) dep = self . target_database [ k ] <MASK> # Has this attribute dep . attr [ "" generate_java "" ] = True queue . extend ( dep . deps )","if ""generate_java"" in dep . attr :","if ""generate_java"" not in dep . attr :",98.87838112,FALSE,97.8
1163,"def get ( self ) : name = request . args . get ( "" filename "" ) if name is not None : opts = dict ( ) opts [ "" type "" ] = "" episode "" result = guessit ( name , options = opts ) res = dict ( ) <MASK> res [ "" episode "" ] = result [ "" episode "" ] else : res [ "" episode "" ] = 0 if "" season "" in result : res [ "" season "" ] = result [ "" season "" ] else : res [ "" season "" ] = 0 if "" subtitle_language "" in result : res [ "" subtitle_language "" ] = str ( result [ "" subtitle_language "" ] ) return jsonify ( data = res ) else : return "" "" , 400","if ""episode"" in result :","if ""episode"" in result :",100,TRUE,100
1164,def _get_error_file ( self ) - > Optional [ str ] : error_file = None min_timestamp = sys . maxsize for replicas in self . role_replicas . values ( ) : for replica in replicas : <MASK> continue mtime = os . path . getmtime ( replica . error_file ) if mtime < min_timestamp : min_timestamp = mtime error_file = replica . error_file return error_file,if not os . path . exists ( replica . error_file ) :,if not replica . error_file :,90.63146624,FALSE,89.86
1165,"def findChapterNameForPosition ( self , p ) : """"""Return the name of a chapter containing p or None if p does not exist."""""" cc , c = self , self . c if not p or not c . positionExists ( p ) : return None for name in cc . chaptersDict : <MASK> theChapter = cc . chaptersDict . get ( name ) if theChapter . positionIsInChapter ( p ) : return name return "" main ""","if name != ""main"" :",if name . startswith ( p ) :,93.57066395,FALSE,92.79
1166,"def remove_files ( folder , file_extensions ) : for f in os . listdir ( folder ) : f_path = os . path . join ( folder , f ) if os . path . isfile ( f_path ) : extension = os . path . splitext ( f_path ) [ 1 ] <MASK> os . remove ( f_path )",if extension in file_extensions :,if extension in file_extensions :,100,TRUE,100
1167,"def execute_uncomment ( self , event ) : cursor = self . _editor . GetCurrentPos ( ) line , pos = self . _editor . GetCurLine ( ) spaces = ""   "" * self . _tab_size comment = "" Comment "" + spaces cpos = cursor - len ( comment ) lenline = len ( line ) if lenline > 0 : idx = 0 while idx < lenline and line [ idx ] == ""   "" : idx + = 1 <MASK> self . _editor . DeleteRange ( cursor - pos + idx , len ( comment ) ) self . _editor . SetCurrentPos ( cpos ) self . _editor . SetSelection ( cpos , cpos ) self . store_position ( )",if ( line [ idx : len ( comment ) + idx ] ) . lower ( ) == comment . lower ( ) :,if idx == lenline :,85.33650727,FALSE,83.95
1168,"def test_batch_kwarg_path_relative_dot_slash_is_modified_and_found_in_a_code_cell ( critical_suite_with_citations , empty_data_context ) : obs = SuiteEditNotebookRenderer . from_data_context ( empty_data_context ) . render ( critical_suite_with_citations , { "" path "" : "" ./foo/data "" } ) assert isinstance ( obs , dict ) found_expected = False for cell in obs [ "" cells "" ] : <MASK> source_code = cell [ "" source "" ] if ' batch_kwargs =  { "" path "" :  "" ../.././foo/data "" } ' in source_code : found_expected = True break assert found_expected","if cell [ ""cell_type"" ] == ""code"" :","if cell [ ""type"" ] == ""code"" :",98.59738801,FALSE,97.98
1169,"def _get_file ( self ) : if self . _file is None : self . _file = SpooledTemporaryFile ( max_size = self . _storage . max_memory_size , suffix = "" .S3Boto3StorageFile "" , dir = setting ( "" FILE_UPLOAD_TEMP_DIR "" ) , ) if "" r "" in self . _mode : self . _is_dirty = False self . obj . download_fileobj ( self . _file ) self . _file . seek ( 0 ) <MASK> self . _file = GzipFile ( mode = self . _mode , fileobj = self . _file , mtime = 0.0 ) return self . _file","if self . _storage . gzip and self . obj . content_encoding == ""gzip"" :","if ""gzip"" in self . _mode :",72.2614227,FALSE,88.22
1170,"def _parse_filters ( f_strs ) : filters = [ ] if not f_strs : return filters for f_str in f_strs : <MASK> fname , fopts = f_str . split ( "" : "" , 1 ) filters . append ( ( fname , _parse_options ( [ fopts ] ) ) ) else : filters . append ( ( f_str , { } ) ) return filters","if "":"" in f_str :","if "":"" in f_str :",100,TRUE,100
1171,"def update_completion ( self ) : """"""Update completion model with exist tags"""""" orig_text = self . widget . text ( ) text = "" ,  "" . join ( orig_text . replace ( "" ,  "" , "" , "" ) . split ( "" , "" ) [ : - 1 ] ) tags = [ ] for tag in self . tags_list : <MASK> if orig_text [ - 1 ] not in ( "" , "" , ""   "" ) : tags . append ( "" %s , %s "" % ( text , tag ) ) tags . append ( "" %s ,  %s "" % ( text , tag ) ) else : tags . append ( tag ) if tags != self . completer_model . stringList ( ) : self . completer_model . setStringList ( tags )","if "","" in orig_text :",if tag not in tags :,92.58751606,FALSE,95.24
1172,"def _get_startup_packages ( lib_path : Path , packages ) - > Set [ str ] : names = set ( ) for path in lib_path . iterdir ( ) : name = path . name if name == "" __pycache__ "" : continue <MASK> names . add ( name . split ( "" . "" ) [ 0 ] ) elif path . is_dir ( ) and "" . "" not in name : names . add ( name ) if packages : packages = { package . lower ( ) . replace ( "" - "" , "" _ "" ) for package in packages } if len ( names & packages ) == len ( packages ) : return packages return names","if name . endswith ( "".py"" ) :","elif path . is_file ( ) and ""."" in name :",95.28192359,FALSE,91.15
1173,"def get_cloud_credential ( self ) : """"""Return the credential which is directly tied to the inventory source type."""""" credential = None for cred in self . credentials . all ( ) : <MASK> if cred . kind == self . source . replace ( "" ec2 "" , "" aws "" ) : credential = cred break else : # these need to be returned in the API credential field if cred . credential_type . kind != "" vault "" : credential = cred break return credential",if self . source in CLOUD_PROVIDERS :,"if isinstance ( cred , CloudCredential ) :",91.53833898,FALSE,92.16
1174,"def newickize ( clade ) : """"""Convert a node tree to a Newick tree string, recursively."""""" label = clade . name or "" "" if label : unquoted_label = re . match ( token_dict [ "" unquoted node label "" ] , label ) <MASK> label = "" ' %s ' "" % label . replace ( "" \\ "" , "" \\ \\ "" ) . replace ( "" ' "" , "" \\ ' "" ) if clade . is_terminal ( ) : # terminal return label + make_info_string ( clade , terminal = True ) else : subtrees = ( newickize ( sub ) for sub in clade ) return "" ( %s ) %s "" % ( "" , "" . join ( subtrees ) , label + make_info_string ( clade ) )",if ( not unquoted_label ) or ( unquoted_label . end ( ) < len ( label ) ) :,if unquoted_label :,85.28820188,FALSE,88.34
1175,"def __iter__ ( self ) : for name , value in self . _vars . store . data . items ( ) : source = self . _sources [ name ] prefix = self . _get_prefix ( value ) name = u "" {0} {{ {1} }} "" . format ( prefix , name ) <MASK> yield ArgumentInfo ( name , value ) else : yield VariableInfo ( name , value , source )",if source == self . ARGUMENT_SOURCE :,if source is None :,90.45147289,FALSE,91.12
1176,"def filepath_enumerate ( paths ) : """"""Enumerate the file paths of all subfiles of the list of paths"""""" out = [ ] for path in paths : <MASK> out . append ( path ) else : for root , dirs , files in os . walk ( path ) : for name in files : out . append ( os . path . normpath ( os . path . join ( root , name ) ) ) return out",if os . path . isfile ( path ) :,if os . path . isabs ( path ) :,98.56352373,FALSE,97.33
1177,"def del_ ( self , key ) : hash_ = self . hash ( key ) node_ = self . _table [ hash_ ] pre_node = None while node_ is not None : <MASK> if pre_node is None : self . _table [ hash_ ] = node_ . next else : pre_node . next = node_ . next self . _len - = 1 pre_node = node_ node_ = node_ . next",if node_ . key == key :,if node_ . key == key :,100,TRUE,100
1178,"def _recurse ( self , base_path , rel_source , rel_zip ) : submodules_path = Path ( base_path ) / "" submodules "" if not submodules_path . is_dir ( ) : return for submodule in submodules_path . iterdir ( ) : source_path = submodule / rel_source <MASK> continue output_path = submodule / rel_zip self . _build_lambdas ( source_path , output_path ) self . _recurse ( submodule , rel_source , rel_zip )",if not source_path . is_dir ( ) :,if not source_path . is_dir ( ) :,100,TRUE,100
1179,"def find_test_functions ( collections ) : if not isinstance ( collections , list ) : collections = [ collections ] functions = [ ] for collection in collections : if not isinstance ( collection , dict ) : collection = vars ( collection ) keys = collection . keys ( ) keys . sort ( ) for key in keys : value = collection [ key ] <MASK> functions . append ( value ) return functions","if isinstance ( value , types . FunctionType ) and hasattr ( value , ""unittest"" ) :","if isinstance ( value , ( FunctionType , FunctionType ) ) :",89.56579934,FALSE,87.09
1180,"def __init__ ( self , classifier , layer_name = None , transpose = None , distance = None , copy_weights = True , ) : super ( ) . __init__ ( ) self . copy_weights = copy_weights ### set layer weights ### if layer_name is not None : self . set_weights ( getattr ( classifier , layer_name ) ) else : for x in self . possible_layer_names : layer = getattr ( classifier , x , None ) <MASK> self . set_weights ( layer ) break ### set distance measure ### self . distance = classifier . distance if distance is None else distance self . transpose = transpose",if layer is not None :,if layer is not None :,100,TRUE,100
1181,def multi_dev_generator ( self ) : for data in self . _data_loader ( ) : if len ( self . _tail_data ) < self . _base_number : self . _tail_data + = data <MASK> yield self . _tail_data self . _tail_data = [ ],if len ( self . _tail_data ) == self . _base_number :,if len ( self . _tail_data ) == self . _base_number :,100,TRUE,100
1182,"def Resolve ( self , updater = None ) : if len ( self . Conflicts ) : for setting , edge in self . Conflicts : answer = self . AskUser ( self . Setting , setting ) if answer == Gtk . ResponseType . YES : value = setting . Value . split ( "" | "" ) value . remove ( edge ) setting . Value = "" | "" . join ( value ) if updater : updater . UpdateSetting ( setting ) <MASK> return False return True",if answer == Gtk . ResponseType . NO :,elif answer == Gtk . ResponseType . NO :,97.592842,FALSE,97.49
1183,"def _post_process_ttl ( zone ) : for name in zone : for record_type in zone [ name ] : records = zone [ name ] [ record_type ] if isinstance ( records , list ) : ttl = min ( [ x [ "" ttl "" ] for x in records ] ) for record in records : <MASK> logger . warning ( "" Using lowest TTL  {}  for the record set. Ignoring value  {} "" . format ( ttl , record [ "" ttl "" ] ) ) record [ "" ttl "" ] = ttl","if record [ ""ttl"" ] != ttl :","if ttl < record [ ""ttl"" ] :",94.59979068,FALSE,95.07
1184,"def __init__ ( self , cmds , env , cleanup = [ ] ) : self . handle = None self . cmds = cmds self . env = env if cleanup : <MASK> cleanup = [ cleanup ] else : try : cleanup = [ c for c in cleanup if callable ( c ) ] except : cleanup = [ ] self . cleanup = cleanup",if callable ( cleanup ) :,"if not isinstance ( cleanup , list ) :",88.10160064,FALSE,91.33
1185,"def _parse_data_of_birth ( cls , data_of_birth_string ) : if data_of_birth_string : format = "" % m/ %d / % Y "" try : parsed_date = datetime . datetime . strptime ( data_of_birth_string , format ) return parsed_date except ValueError : # Facebook sometimes provides a partial date format # ie 04/07 (ignore those) <MASK> raise","if data_of_birth_string . count ( ""/"" ) != 1 :","if not isinstance ( data_of_birth_string , str ) :",93.28182558,FALSE,88.42
1186,"def process_lib ( vars_ , coreval ) : for d in vars_ : var = d . upper ( ) if var == "" QTCORE "" : continue value = env [ "" LIBPATH_ "" + var ] if value : core = env [ coreval ] accu = [ ] for lib in value : <MASK> continue accu . append ( lib ) env [ "" LIBPATH_ "" + var ] = accu",if lib in core :,if lib == core :,98.17479095,FALSE,95.83
1187,"def throttle_status ( server = None ) : result = AmonStruct ( ) result . allow = False last_check = server . get ( "" last_check "" ) server_check_period = server . get ( "" check_every "" , 60 ) if last_check : period_since_last_check = unix_utc_now ( ) - last_check # Add 15 seconds buffer, for statsd period_since_last_check = period_since_last_check + 15 <MASK> result . allow = True else : result . allow = True # Never checked return result",if period_since_last_check >= server_check_period :,if period_since_last_check < server_check_period :,98.49386537,FALSE,97.35
1188,"def fetch_scatter_outputs ( self , task ) : scatteroutputs = [ ] for var in task [ "" body "" ] : # TODO variable support if var . startswith ( "" call "" ) : <MASK> for output in self . tasks_dictionary [ task [ "" body "" ] [ var ] [ "" task "" ] ] [ "" outputs "" ] : scatteroutputs . append ( { "" task "" : task [ "" body "" ] [ var ] [ "" alias "" ] , "" output "" : output [ 0 ] } ) return scatteroutputs","if ""outputs"" in self . tasks_dictionary [ task [ ""body"" ] [ var ] [ ""task"" ] ] :","if task [ ""body"" ] [ var ] [ ""task"" ] in self . tasks",95.97503316,FALSE,90.32
1189,"def _add_constant_node ( self , source_node ) : parent_ids = range ( len ( source_node . in_edges ) ) for idx in parent_ids : parent_node = self . tf_graph . get_node ( source_node . in_edges [ idx ] ) <MASK> self . _rename_Const ( parent_node )","if parent_node . type == ""Const"" :","if parent_node . type == ""constant"" :",97.51209869,FALSE,97.05
1190,"def enableCtrls ( self ) : # Check if each ctrl has a requirement or an incompatibility, # look it up, and enable/disable if so for data in self . storySettingsData : name = data [ "" name "" ] if name in self . ctrls : <MASK> set = self . getSetting ( data [ "" requires "" ] ) for i in self . ctrls [ name ] : i . Enable ( set not in [ "" off "" , "" false "" , "" 0 "" ] )","if ""requires"" in data :","if data [ ""requires"" ] :",72.47909315,FALSE,94.27
1191,"def update_realtime ( self , stdout = "" "" , stderr = "" "" , delete = False ) : wooey_cache = wooey_settings . WOOEY_REALTIME_CACHE if delete == False and wooey_cache is None : self . stdout = stdout self . stderr = stderr self . save ( ) elif wooey_cache is not None : cache = django_cache [ wooey_cache ] <MASK> cache . delete ( self . get_realtime_key ( ) ) else : cache . set ( self . get_realtime_key ( ) , json . dumps ( { "" stdout "" : stdout , "" stderr "" : stderr } ) , )",if delete :,if delete :,100,TRUE,100
1192,"def _check_for_batch_clashes ( xs ) : """"""Check that batch names do not overlap with sample names."""""" names = set ( [ x [ "" description "" ] for x in xs ] ) dups = set ( [ ] ) for x in xs : batches = tz . get_in ( ( "" metadata "" , "" batch "" ) , x ) if batches : if not isinstance ( batches , ( list , tuple ) ) : batches = [ batches ] for batch in batches : <MASK> dups . add ( batch ) if len ( dups ) > 0 : raise ValueError ( "" Batch names must be unique from sample descriptions. \n "" "" Clashing batch names:  %s "" % sorted ( list ( dups ) ) )",if batch in names :,if batch not in names :,99.26017624,FALSE,98.39
1193,"def toggle ( self , event = None ) : if self . absolute : if self . save == self . split : self . save = 100 if self . split > 20 : self . save = self . split self . split = 1 else : self . split = self . save else : if self . save == self . split : self . save = 0.3 <MASK> self . split = self . save elif self . split < 0.5 : self . split = self . min else : self . split = self . max self . placeChilds ( )",if self . split <= self . min or self . split >= self . max :,if self . split > 0.5 :,71.84324225,FALSE,88.29
1194,"def can_read ( self ) : if hasattr ( self . file , "" __iter__ "" ) : iterator = iter ( self . file ) head = next ( iterator , None ) <MASK> self . repaired = [ ] return True if isinstance ( head , str ) : self . repaired = itertools . chain ( [ head ] , iterator ) return True else : # We may have mangled a generator at this point, so just abort raise IOSourceError ( "" Could not open source:  %r  (mode:  %r ) "" % ( self . file , self . options [ "" mode "" ] ) ) return False",if head is None :,if head is None :,100,TRUE,100
1195,"def _print_message_content ( self , peer , data ) : inheaders = 1 lines = data . splitlines ( ) for line in lines : # headers first if inheaders and not line : peerheader = "" X-Peer:  "" + peer [ 0 ] <MASK> # decoded_data=false; make header match other binary output peerheader = repr ( peerheader . encode ( "" utf-8 "" ) ) print ( peerheader ) inheaders = 0 <MASK> # Avoid spurious 'str on bytes instance' warning. line = repr ( line ) print ( line )","if not isinstance ( data , str ) :","if not isinstance ( line , bytes ) :",96.03220417,FALSE,92.77
1196,"def connect ( self ) : # Makes connection with MySQL server try : <MASK> connection = pymysql . connect ( read_default_file = "" /etc/mysql/conf.d/my.cnf "" ) else : connection = pymysql . connect ( read_default_file = "" ~/.my.cnf "" ) return connection except ValueError as e : Log . debug ( self , str ( e ) ) raise MySQLConnectionError except pymysql . err . InternalError as e : Log . debug ( self , str ( e ) ) raise MySQLConnectionError","if os . path . exists ( ""/etc/mysql/conf.d/my.cnf"" ) :","if sys . platform == ""win32"" :",70.02720079,FALSE,82.53
1197,"def _copy_package_apps ( local_bin_dir : Path , app_paths : List [ Path ] , suffix : str = "" "" ) - > None : for src_unresolved in app_paths : src = src_unresolved . resolve ( ) app = src . name dest = Path ( local_bin_dir / add_suffix ( app , suffix ) ) if not dest . parent . is_dir ( ) : mkdir ( dest . parent ) if dest . exists ( ) : logger . warning ( f "" { hazard }   Overwriting file  { str ( dest ) }  with  { str ( src ) } "" ) dest . unlink ( ) <MASK> shutil . copy ( src , dest )",if src . exists ( ) :,if not src . is_dir ( ) :,97.37768288,FALSE,95.5
1198,"def update ( self , x , who = None , metadata = None ) : self . _retain_refs ( metadata ) y = self . _get_key ( x ) if self . keep == "" last "" : # remove key if already present so that emitted value # will reflect elements' actual relative ordering self . _buffer . pop ( y , None ) self . _metadata_buffer . pop ( y , None ) self . _buffer [ y ] = x self . _metadata_buffer [ y ] = metadata else : # self.keep == ""first"" <MASK> self . _buffer [ y ] = x self . _metadata_buffer [ y ] = metadata return self . last",if y not in self . _buffer :,"if self . keep == ""first"" :",97.04647381,FALSE,94.17
1199,"def resolve_credential_keys ( m_keys , keys ) : res = [ ] for k in m_keys : if k [ "" c7n:match-type "" ] == "" credential "" : c_date = parse_date ( k [ "" last_rotated "" ] ) for ak in keys : if c_date == ak [ "" CreateDate "" ] : ak = dict ( ak ) ak [ "" c7n:match-type "" ] = "" access "" <MASK> res . append ( ak ) elif k not in res : res . append ( k ) return res",if ak not in res :,"if ak [ ""c7n:match-type"" ] == ""access"" :",94.6396867,FALSE,89.31
1200,"def _apply_flag_attrs ( src_flag , dest_flag ) : # Use a baseline flag def to get default values for empty data. baseline_flag = FlagDef ( "" "" , { } , None ) for name in dir ( src_flag ) : if name [ : 1 ] == "" _ "" : continue dest_val = getattr ( dest_flag , name , None ) baseline_val = getattr ( baseline_flag , name , None ) <MASK> setattr ( dest_flag , name , getattr ( src_flag , name ) )",if dest_val == baseline_val :,if dest_val is None or baseline_val is None :,72.15787456,FALSE,93.54
1201,"def _ws_keep_reading ( self ) : import websockets . exceptions while not self . _reader_stopped : try : data = await self . _ws . recv ( ) <MASK> data = data . encode ( "" UTF-8 "" ) if len ( data ) == 0 : self . _error = "" EOF "" break except websockets . exceptions . ConnectionClosedError : # TODO: try to reconnect in case of Ctrl+D self . _error = "" EOF "" break self . num_bytes_received + = len ( data ) self . _make_output_available ( data , block = False )","if isinstance ( data , str ) :","if isinstance ( data , str ) :",100,TRUE,100
1202,"def to_dict ( self ) - > Dict [ str , Any ] : result = { } for field_name in self . API_FIELDS : <MASK> result [ "" stream_id "" ] = self . id continue elif field_name == "" date_created "" : result [ "" date_created "" ] = datetime_to_timestamp ( self . date_created ) continue result [ field_name ] = getattr ( self , field_name ) result [ "" is_announcement_only "" ] = ( self . stream_post_policy == Stream . STREAM_POST_POLICY_ADMINS ) return result","if field_name == ""id"" :","if field_name == ""stream_id"" :",98.60862289,FALSE,97.43
1203,"def all_masks ( cls , images , run , run_key , step , ) : all_mask_groups = [ ] for image in images : <MASK> mask_group = { } for k in image . _masks : mask = image . _masks [ k ] mask_group [ k ] = mask . to_json ( run ) all_mask_groups . append ( mask_group ) else : all_mask_groups . append ( None ) if all_mask_groups and not all ( x is None for x in all_mask_groups ) : return all_mask_groups else : return False",if image . _masks :,if image . _masks :,100,TRUE,100
1204,"def disconnect_all ( listener ) : """"""Disconnect from all signals"""""" for emitter in listener . _signal_data . emitters : for signal in emitter . _signal_data . listeners : emitter . _signal_data . listeners [ signal ] = [ i for i in emitter . _signal_data . listeners [ signal ] <MASK> ]","if getattr ( i , ""__self__"" , None ) != listener",if i . _signal_type == signal . _signal_type . _DISCONNECTED,71.05567278,FALSE,81.31
1205,"def wait ( self , timeout = None ) : if self . returncode is None : if timeout is None : msecs = _subprocess . INFINITE else : msecs = max ( 0 , int ( timeout * 1000 + 0.5 ) ) res = _subprocess . WaitForSingleObject ( int ( self . _handle ) , msecs ) if res == _subprocess . WAIT_OBJECT_0 : code = _subprocess . GetExitCodeProcess ( self . _handle ) <MASK> code = - signal . SIGTERM self . returncode = code return self . returncode",if code == TERMINATE :,if code == signal . SIGTERM :,82.02729309,FALSE,95.84
1206,"def set_pbar_fraction ( self , frac , progress , stage = None ) : gtk . gdk . threads_enter ( ) try : self . is_pulsing = False self . set_stage_text ( stage or _ ( "" Processing... "" ) ) self . pbar . set_text ( progress ) <MASK> frac = 1.0 if frac < 0 : frac = 0 self . pbar . set_fraction ( frac ) finally : gtk . gdk . threads_leave ( )",if frac > 1 :,if frac > 1.0 :,73.63673579,FALSE,97.52
1207,"def get_aa_from_codonre ( re_aa ) : aas = [ ] m = 0 for i in re_aa : if i == "" [ "" : m = - 1 aas . append ( "" "" ) <MASK> m = 0 continue elif m == - 1 : aas [ - 1 ] = aas [ - 1 ] + i elif m == 0 : aas . append ( i ) return aas","elif i == ""]"" :","elif i == ""]"" :",100,TRUE,100
1208,"def link ( token , base_url ) : """"""Validation for ``link``."""""" if get_keyword ( token ) == "" none "" : return "" none "" parsed_url = get_url ( token , base_url ) if parsed_url : return parsed_url function = parse_function ( token ) if function : name , args = function prototype = ( name , [ a . type for a in args ] ) args = [ getattr ( a , "" value "" , a ) for a in args ] <MASK> return ( "" attr() "" , args [ 0 ] )","if prototype == ( ""attr"" , [ ""ident"" ] ) :",if prototype ( args ) :,92.01021858,FALSE,90.38
1209,"def on_bt_search_clicked ( self , widget ) : if self . current_provider is None : return query = self . en_query . get_text ( ) @self . obtain_podcasts_with def load_data ( ) : if self . current_provider . kind == directory . Provider . PROVIDER_SEARCH : return self . current_provider . on_search ( query ) <MASK> return self . current_provider . on_url ( query ) elif self . current_provider . kind == directory . Provider . PROVIDER_FILE : return self . current_provider . on_file ( query )",elif self . current_provider . kind == directory . Provider . PROVIDER_URL :,elif self . current_provider . kind == directory . Provider . PROVIDER_URL :,100,TRUE,100
1210,"def test_handle_single ( self ) : self . skipTest ( "" Pops up windows and needs user input.. so disabled. "" "" Still worth keeping whilst we don ' t have unit tests  "" "" for all plugins. "" ) # Ignored... for id_ , plugin in self . plugins . items ( ) : <MASK> self . h . plugin_enable ( plugin , None ) self . h . handle ( id_ , self . lib , self . parent , SONGS ) self . h . plugin_disable ( plugin )",if self . h . plugin_handle ( plugin ) :,if self . h . is_windows ( ) :,97.87840566,FALSE,94.92
1211,"def __repr__ ( self ) : attrs = [ ] for k in self . _keydata : <MASK> attrs . append ( "" p( %d ) "" % ( self . size ( ) + 1 , ) ) elif hasattr ( self , k ) : attrs . append ( k ) if self . has_private ( ) : attrs . append ( "" private "" ) # PY3K: This is meant to be text, do not change to bytes (data) return "" < %s  @0x %x   %s > "" % ( self . __class__ . __name__ , id ( self ) , "" , "" . join ( attrs ) )","if k == ""p"" :","if k == ""p"" :",100,TRUE,100
1212,"def apply ( self , node , code , required ) : yield "" try: "" yield from self . iterIndented ( code ) yield ""     pass "" yield "" except  {} : "" . format ( self . exceptionString ) outputVariables = node . getOutputSocketVariables ( ) for i , s in enumerate ( node . outputs ) : <MASK> if hasattr ( s , "" getDefaultValueCode "" ) : yield f ""      { outputVariables [ s . identifier ] }  =  { s . getDefaultValueCode ( ) } "" else : yield f ""      { outputVariables [ s . identifier ] }  = self.outputs[ { i } ].getDefaultValue() "" yield ""     pass """,if s . identifier in required :,if s . identifier in self . outputs :,95.78550421,FALSE,96.54
1213,"def __import__ ( name , globals = None , locals = None , fromlist = ( ) , level = 0 ) : module = orig___import__ ( name , globals , locals , fromlist , level ) if fromlist and module . __name__ in modules : if "" * "" in fromlist : fromlist = list ( fromlist ) fromlist . remove ( "" * "" ) fromlist . extend ( getattr ( module , "" __all__ "" , [ ] ) ) for x in fromlist : <MASK> from_name = "" {} . {} "" . format ( module . __name__ , x ) if from_name in modules : importlib . import_module ( from_name ) return module","if isinstance ( getattr ( module , x , None ) , types . ModuleType ) :","if x . startswith ( ""__"" ) :",88.13578456,FALSE,90.98
1214,"def _consume_msg ( self ) : ws = self . _ws try : while True : r = await ws . recv ( ) <MASK> r = r . decode ( "" utf-8 "" ) msg = json . loads ( r ) stream = msg . get ( "" stream "" ) if stream is not None : await self . _dispatch ( stream , msg ) except websockets . WebSocketException as wse : logging . warn ( wse ) await self . close ( ) asyncio . ensure_future ( self . _ensure_ws ( ) )","if isinstance ( r , bytes ) :","if isinstance ( r , bytes ) :",100,TRUE,100
1215,"def add_source ( self , source , name = None ) : """"""Adds a new data source to an existing provider."""""" if self . randomize : <MASK> raise ValueError ( "" Cannot add a non-shuffleable source to an  "" "" already shuffled provider. "" ) super ( ) . add_source ( source , name = name ) if self . randomize is True : self . _shuffle_len = self . entries",if not source . can_shuffle ( ) :,if source in self . entries :,90.81481924,FALSE,90.33
1216,"def __str__ ( self ) : buf = [ "" "" ] if self . fileName : buf . append ( self . fileName + "" : "" ) if self . line != - 1 : if not self . fileName : buf . append ( "" line  "" ) buf . append ( str ( self . line ) ) <MASK> buf . append ( "" : "" + str ( self . column ) ) buf . append ( "" : "" ) buf . append ( ""   "" ) return str ( "" "" ) . join ( buf )",if self . column != - 1 :,if self . column != - 1 :,100,TRUE,100
1217,"def has_bad_headers ( self ) : headers = [ self . sender , self . reply_to ] + self . recipients for header in headers : if _has_newline ( header ) : return True if self . subject : if _has_newline ( self . subject ) : for linenum , line in enumerate ( self . subject . split ( "" \r \n "" ) ) : if not line : return True if linenum > 0 and line [ 0 ] not in "" \t   "" : return True if _has_newline ( line ) : return True <MASK> return True return False",if len ( line . strip ( ) ) == 0 :,"if linenum == 0 and line [ 0 ] not in ""\t "" :",92.21174745,FALSE,89.08
1218,"def scanHexEscape ( self , prefix ) : code = 0 leng = 4 if ( prefix == "" u "" ) else 2 for i in xrange ( leng ) : <MASK> ch = self . source [ self . index ] self . index + = 1 code = code * 16 + HEX_CONV [ ch ] else : return "" "" return unichr ( code )",if self . index < self . length and isHexDigit ( self . source [ self . index ] ) :,if self . source [ self . index ] . startswith ( prefix ) :,86.09688251,FALSE,87.4
1219,"def _get_table_info ( self , table_name ) : table_addr = self . addr_space . profile . get_symbol ( table_name ) table_size = self . _get_table_info_distorm ( ) <MASK> table_size = self . _get_table_info_other ( table_addr , table_name ) <MASK> debug . error ( "" Unable to get system call table size "" ) return [ table_addr , table_size ]",if table_size == 0 :,if table_size is None :,90.11190074,FALSE,91.96
1220,"def format_file_path ( filepath ) : """"""Formats a path as absolute and with the correct platform separator."""""" try : is_windows_network_mount = WINDOWS_NETWORK_MOUNT_PATTERN . match ( filepath ) filepath = os . path . realpath ( os . path . abspath ( filepath ) ) filepath = re . sub ( BACKSLASH_REPLACE_PATTERN , "" / "" , filepath ) is_windows_drive = WINDOWS_DRIVE_PATTERN . match ( filepath ) <MASK> filepath = filepath . capitalize ( ) if is_windows_network_mount : # Add back a / to the front, since the previous modifications # will have replaced any double slashes with single filepath = "" / "" + filepath except : pass return filepath",if is_windows_drive :,if is_windows_drive :,100,TRUE,100
1221,"def _match ( self , cre , s ) : # Run compiled regular expression match method on 's'. # Save result, return success. self . mo = cre . match ( s ) if __debug__ : <MASK> self . _mesg ( "" \t matched r ' %r '  =>  %r "" % ( cre . pattern , self . mo . groups ( ) ) ) return self . mo is not None",if self . mo is not None and self . debug >= 5 :,if self . mo :,68.23567946,FALSE,88.07
1222,"def reload_sanitize_allowlist ( self , explicit = True ) : self . sanitize_allowlist = [ ] try : with open ( self . sanitize_allowlist_file ) as f : for line in f . readlines ( ) : if not line . startswith ( "" # "" ) : self . sanitize_allowlist . append ( line . strip ( ) ) except OSError : <MASK> log . warning ( "" Sanitize log file explicitly specified as  ' %s '  but does not exist, continuing with no tools allowlisted. "" , self . sanitize_allowlist_file , )",if explicit :,if explicit :,100,TRUE,100
1223,"def conj ( self ) : dtype = self . dtype if issubclass ( self . dtype . type , np . complexfloating ) : if not self . flags . forc : raise RuntimeError ( "" only contiguous arrays may  "" "" be used as arguments to this operation "" ) <MASK> order = "" F "" else : order = "" C "" result = self . _new_like_me ( order = order ) func = elementwise . get_conj_kernel ( dtype ) func . prepared_async_call ( self . _grid , self . _block , None , self . gpudata , result . gpudata , self . mem_size ) return result else : return self",if self . flags . f_contiguous :,if self . flags . f :,98.96164359,FALSE,97.47
1224,"def scan_spec_conf ( self , conf ) : if "" metadata "" in conf : if "" annotations "" in conf [ "" metadata "" ] and conf [ "" metadata "" ] . get ( "" annotations "" ) : for annotation in conf [ "" metadata "" ] [ "" annotations "" ] : for key in annotation : <MASK> if ( "" docker/default "" in annotation [ key ] or "" runtime/default "" in annotation [ key ] ) : return CheckResult . PASSED return CheckResult . FAILED","if ""seccomp.security.alpha.kubernetes.io/defaultProfileName"" in key :","if key . startswith ( ""docker"" ) :",95.26496715,FALSE,86.19
1225,"def test_error_through_destructor ( self ) : # Test that the exception state is not modified by a destructor, # even if close() fails. rawio = self . CloseFailureIO ( ) with support . catch_unraisable_exception ( ) as cm : with self . assertRaises ( AttributeError ) : self . tp ( rawio ) . xyzzy if not IOBASE_EMITS_UNRAISABLE : self . assertIsNone ( cm . unraisable ) <MASK> self . assertEqual ( cm . unraisable . exc_type , OSError )",elif cm . unraisable is not None :,if cm . unraisable :,70.25612159,FALSE,93.32
1226,"def _dumpf ( frame ) : if frame is None : return "" <None> "" else : addn = "" (with trace!) "" <MASK> addn = ""  **No Trace Set ** "" return "" Frame at  %d , file  %s , line:  %d %s "" % ( id ( frame ) , frame . f_code . co_filename , frame . f_lineno , addn , )",if frame . f_trace is None :,if frame . f_lineno == 0 :,71.38617309,FALSE,94.03
1227,"def containsBadbytes ( self , value , bytecount = 4 ) : for b in self . badbytes : tmp = value <MASK> b = ord ( b ) for i in range ( bytecount ) : if ( tmp & 0xFF ) == b : return True tmp >> = 8 return False",if type ( b ) == str :,if b :,69.91420449,FALSE,86.5
1228,"def _set_peer_statuses ( self ) : """"""Set peer statuses."""""" cutoff = time . time ( ) - STALE_SECS for peer in self . peers : <MASK> peer . status = PEER_BAD elif peer . last_good > cutoff : peer . status = PEER_GOOD elif peer . last_good : peer . status = PEER_STALE else : peer . status = PEER_NEVER",if peer . bad :,if peer . last_good < cutoff :,90.79182885,FALSE,92.95
1229,"def afterTest ( self , test ) : try : # If the browser window is still open, close it now. self . driver . quit ( ) except AttributeError : pass except Exception : pass if self . options . headless : <MASK> try : self . display . stop ( ) except AttributeError : pass except Exception : pass",if self . headless_active :,if self . driver . is_open ( ) :,71.62709859,FALSE,88.4
1230,"def _written_variables_in_proxy ( self , contract ) : variables = [ ] if contract . is_upgradeable : variables_name_written_in_proxy = self . _variable_written_in_proxy ( ) <MASK> variables_in_contract = [ contract . get_state_variable_from_name ( v ) for v in variables_name_written_in_proxy ] variables_in_contract = [ v for v in variables_in_contract if v ] variables + = variables_in_contract return list ( set ( variables ) )",if variables_name_written_in_proxy :,if variables_name_written_in_proxy :,100,TRUE,100
1231,"def _available_symbols ( self , scoperef , expr ) : cplns = [ ] found_names = set ( ) while scoperef : elem = self . _elem_from_scoperef ( scoperef ) for child in elem : name = child . get ( "" name "" , "" "" ) if name . startswith ( expr ) : <MASK> found_names . add ( name ) ilk = child . get ( "" ilk "" ) or child . tag cplns . append ( ( ilk , name ) ) scoperef = self . parent_scoperef_from_scoperef ( scoperef ) if not scoperef : break return sorted ( cplns , key = operator . itemgetter ( 1 ) )",if name not in found_names :,if name not in found_names :,100,TRUE,100
1232,"def get_resource_public_actions ( resource_class ) : resource_class_members = inspect . getmembers ( resource_class ) resource_methods = { } for name , member in resource_class_members : if not name . startswith ( "" _ "" ) : <MASK> if not name . startswith ( "" wait_until "" ) : if is_resource_action ( member ) : resource_methods [ name ] = member return resource_methods",if not name [ 0 ] . isupper ( ) :,if is_resource_action ( member ) :,83.05614302,FALSE,90.77
1233,def UpdateControlState ( self ) : active = self . demoModules . GetActiveID ( ) # Update the radio/restore buttons for moduleID in self . radioButtons : btn = self . radioButtons [ moduleID ] <MASK> btn . SetValue ( True ) else : btn . SetValue ( False ) if self . demoModules . Exists ( moduleID ) : btn . Enable ( True ) if moduleID == modModified : self . btnRestore . Enable ( True ) else : btn . Enable ( False ) if moduleID == modModified : self . btnRestore . Enable ( False ),if moduleID == active :,if moduleID == active :,100,TRUE,100
1234,"def test_controlcharacters ( self ) : for i in range ( 128 ) : c = chr ( i ) testString = "" string containing  %s "" % c if i > = 32 or c in "" \r \n \t "" : # \r, \n and \t are the only legal control chars in XML data = plistlib . dumps ( testString , fmt = plistlib . FMT_XML ) <MASK> self . assertEqual ( plistlib . loads ( data ) , testString ) else : with self . assertRaises ( ValueError ) : plistlib . dumps ( testString , fmt = plistlib . FMT_XML ) plistlib . dumps ( testString , fmt = plistlib . FMT_BINARY )","if c != ""\r"" :",if data :,98.16772789,FALSE,93.9
1235,"def remove_usernames ( self , username : SLT [ str ] ) - > None : with self . __lock : <MASK> raise RuntimeError ( f "" Can ' t set  { self . username_name }  in conjunction with (already set)  "" f "" { self . chat_id_name } s. "" ) parsed_username = self . _parse_username ( username ) self . _usernames - = parsed_username",if self . _chat_ids :,if self . _usernames :,98.1788164,FALSE,95.12
1236,"def get_size ( self , shape_info ) : # The size is the data, that have constant size. state = np . random . RandomState ( ) . get_state ( ) size = 0 for elem in state : if isinstance ( elem , str ) : size + = len ( elem ) elif isinstance ( elem , np . ndarray ) : size + = elem . size * elem . itemsize <MASK> size + = np . dtype ( "" int "" ) . itemsize elif isinstance ( elem , float ) : size + = np . dtype ( "" float "" ) . itemsize else : raise NotImplementedError ( ) return size","elif isinstance ( elem , int ) :","elif isinstance ( elem , int ) :",75,TRUE,100
1237,"def before_step ( self , step , feed_dict ) : if step == 0 : for _type , mem in self . memories . items ( ) : <MASK> self . gan . session . run ( tf . assign ( mem [ "" var "" ] , mem [ "" source "" ] ) )","if ""var"" in mem and ""source"" in mem :","if _type == ""var"" :",63.38818851,FALSE,84.68
1238,"def write ( self , * bits ) : for bit in bits : if not self . bytestream : self . bytestream . append ( 0 ) byte = self . bytestream [ self . bytenum ] <MASK> if self . bytenum == len ( self . bytestream ) - 1 : byte = 0 self . bytestream + = bytes ( [ byte ] ) self . bytenum + = 1 self . bitnum = 0 mask = 2 * * self . bitnum if bit : byte | = mask else : byte & = ~ mask self . bytestream [ self . bytenum ] = byte self . bitnum + = 1",if self . bitnum == 8 :,if byte == 0 :,94.88978767,FALSE,94.47
1239,"def _validate_parameter_range ( self , value_hp , parameter_range ) : """"""Placeholder docstring"""""" for ( parameter_range_key , parameter_range_value , ) in parameter_range . __dict__ . items ( ) : if parameter_range_key == "" scaling_type "" : continue # Categorical ranges <MASK> for categorical_value in parameter_range_value : value_hp . validate ( categorical_value ) # Continuous, Integer ranges else : value_hp . validate ( parameter_range_value )","if isinstance ( parameter_range_value , list ) :","if parameter_range_key == ""parallel_range"" :",69.67701998,FALSE,91.34
1240,"def _trackA ( self , tracks ) : try : track , start , end = self . featureA assert track in tracks return track except TypeError : for track in tracks : for feature_set in track . get_sets ( ) : if hasattr ( feature_set , "" features "" ) : <MASK> return track return None",if self . featureA in feature_set . features . values ( ) :,"if feature_set . features . get ( ""A"" ) == self . featureA :",90.00279934,FALSE,84.86
1241,"def walk ( directory , path_so_far ) : for name in sorted ( os . listdir ( directory ) ) : if any ( fnmatch ( name , pattern ) for pattern in basename_ignore ) : continue path = path_so_far + "" / "" + name if path_so_far else name if any ( fnmatch ( path , pattern ) for pattern in path_ignore ) : continue full_name = os . path . join ( directory , name ) <MASK> for file_path in walk ( full_name , path ) : yield file_path elif os . path . isfile ( full_name ) : yield path",if os . path . isdir ( full_name ) :,if os . path . isdir ( full_name ) :,100,TRUE,100
1242,"def _poll_ipc_requests ( self ) - > None : try : <MASK> return while not self . _ipc_requests . empty ( ) : args = self . _ipc_requests . get ( ) try : for filename in args : if os . path . isfile ( filename ) : self . get_editor_notebook ( ) . show_file ( filename ) except Exception as e : logger . exception ( "" Problem processing ipc request "" , exc_info = e ) self . become_active_window ( ) finally : self . after ( 50 , self . _poll_ipc_requests )",if self . _ipc_requests . empty ( ) :,if self . _poll_ipc_requests . empty ( ) :,98.7918803,FALSE,97.96
1243,"def test_read1 ( self ) : self . test_write ( ) blocks = [ ] nread = 0 with gzip . GzipFile ( self . filename , "" r "" ) as f : while True : d = f . read1 ( ) <MASK> break blocks . append ( d ) nread + = len ( d ) # Check that position was updated correctly (see issue10791). self . assertEqual ( f . tell ( ) , nread ) self . assertEqual ( b "" "" . join ( blocks ) , data1 * 50 )",if not d :,if not d :,100,TRUE,100
1244,"def _target_generator ( self ) : if self . _internal_target_generator is None : <MASK> return None from . . . . model_zoo . rcnn . rpn . rpn_target import RPNTargetGenerator self . _internal_target_generator = RPNTargetGenerator ( num_sample = self . _num_sample , pos_iou_thresh = self . _pos_iou_thresh , neg_iou_thresh = self . _neg_iou_thresh , pos_ratio = self . _pos_ratio , stds = self . _box_norm , * * self . _kwargs ) return self . _internal_target_generator else : return self . _internal_target_generator",if self . _net_none :,if self . _num_sample == 0 :,96.55645803,FALSE,95.14
1245,"def time_left ( self ) : """"""Return how many seconds are left until the timeout expires"""""" if self . is_non_blocking : return 0 elif self . is_infinite : return None else : delta = self . target_time - self . TIME ( ) <MASK> # clock jumped, recalculate self . target_time = self . TIME ( ) + self . duration return self . duration else : return max ( 0 , delta )",if delta > self . duration :,if delta < 0 :,89.29979507,FALSE,94.35
1246,"def _decorator ( cls ) : for name , meth in inspect . getmembers ( cls , inspect . isroutine ) : if name not in cls . __dict__ : continue if name != "" __init__ "" : if not private and name . startswith ( "" _ "" ) : continue <MASK> continue setattr ( cls , name , decorator ( meth ) ) return cls",if name in butnot :,"if not private and name . startswith ( ""_"" ) :",92.00737388,FALSE,85.96
1247,"def load_vocab ( vocab_file : str ) - > List : """"""Loads a vocabulary file into a dictionary."""""" vocab = collections . OrderedDict ( ) with io . open ( vocab_file , "" r "" , encoding = "" UTF-8 "" ) as file : for num , line in enumerate ( file ) : items = convert_to_unicode ( line . strip ( ) ) . split ( "" \t "" ) <MASK> break token = items [ 0 ] index = items [ 1 ] if len ( items ) == 2 else num token = token . strip ( ) vocab [ token ] = int ( index ) return vocab",if len ( items ) > 2 :,if len ( items ) != 2 :,98.95107179,FALSE,97.46
1248,"def slice_fill ( self , slice_ ) : "" Fills the slice with zeroes for the dimensions that have single elements and squeeze_dims true "" if isinstance ( self . indexes , int ) : new_slice_ = [ 0 ] offset = 0 else : new_slice_ = [ slice_ [ 0 ] ] offset = 1 for i in range ( 1 , len ( self . nums ) ) : <MASK> new_slice_ . append ( 0 ) elif offset < len ( slice_ ) : new_slice_ . append ( slice_ [ offset ] ) offset + = 1 new_slice_ + = slice_ [ offset : ] return new_slice_",if self . squeeze_dims [ i ] :,if i == 0 :,92.81379528,FALSE,93.53
1249,"def check_update_function ( url , folder , update_setter , version_setter , auto ) : remote_version = urllib . urlopen ( url ) . read ( ) if remote_version . isdigit ( ) : local_version = get_local_timestamp ( folder ) if remote_version > local_version : <MASK> update_setter . set_value ( True ) version_setter . set_value ( remote_version ) return True else : return False else : return False",if auto :,if auto :,100,TRUE,100
1250,"def iter_content ( self , chunk_size_bytes ) : while True : try : data = self . _fp . read ( chunk_size_bytes ) except IOError as e : raise Fetcher . PermanentError ( "" Problem reading chunk from  {} :  {} "" . format ( self . _fp . name , e ) ) <MASK> break yield data",if not data :,if not data :,100,TRUE,100
1251,"def gvariant_args ( args : List [ Any ] ) - > str : """"""Convert args into gvariant."""""" gvariant = "" "" for arg in args : if isinstance ( arg , bool ) : gvariant + = ""   {} "" . format ( str ( arg ) . lower ( ) ) elif isinstance ( arg , ( int , float ) ) : gvariant + = f ""   { arg } "" <MASK> gvariant + = f '   "" { arg } "" ' else : gvariant + = f ""   { arg !s} "" return gvariant . lstrip ( )","elif isinstance ( arg , str ) :","elif isinstance ( arg , str ) :",100,TRUE,100
1252,"def _element_keywords ( cls , backend , elements = None ) : "" Returns a dictionary of element names to allowed keywords "" if backend not in Store . loaded_backends ( ) : return { } mapping = { } backend_options = Store . options ( backend ) elements = elements if elements is not None else backend_options . keys ( ) for element in elements : <MASK> continue element = element if isinstance ( element , tuple ) else ( element , ) element_keywords = [ ] options = backend_options [ "" . "" . join ( element ) ] for group in Options . _option_groups : element_keywords . extend ( options [ group ] . allowed_keywords ) mapping [ element [ 0 ] ] = element_keywords return mapping","if ""."" in element :",if not element :,96.37465845,FALSE,96.48
1253,"def setup_parameter_node ( self , param_node ) : if param_node . bl_idname == "" SvNumberNode "" : if self . use_prop or self . get_prop_name ( ) : value = self . sv_get ( ) [ 0 ] [ 0 ] print ( "" V "" , value ) <MASK> param_node . selected_mode = "" int "" param_node . int_ = value elif isinstance ( value , float ) : param_node . selected_mode = "" float "" param_node . float_ = value","if isinstance ( value , int ) :","if isinstance ( value , int ) :",100,TRUE,100
1254,"def _get_oshape ( indices_shape , depth , axis ) : oshape = [ ] true_axis = len ( indices_shape ) if axis == - 1 else axis ndim = len ( indices_shape ) + 1 indices_index = 0 for i in range ( 0 , ndim ) : <MASK> oshape . append ( depth ) else : oshape . append ( indices_shape [ indices_index ] ) indices_index + = 1 return oshape",if i == true_axis :,if i == true_axis :,100,TRUE,100
1255,"def check ( self , value ) : value = String . check ( self , value ) if isinstance ( value , str ) : value = value . upper ( ) for prefix in ( self . prefix , self . prefix . split ( "" _ "" , 1 ) [ 1 ] ) : # e.g. PANGO_WEIGHT_BOLD --> BOLD but also WEIGHT_BOLD --> BOLD <MASK> value = value [ len ( prefix ) : ] value = value . lstrip ( "" _ "" ) if hasattr ( self . group , value ) : return getattr ( self . group , value ) else : raise ValueError ( "" No such constant:  %s _ %s "" % ( self . prefix , value ) ) else : return value",if value . startswith ( prefix ) :,if value . startswith ( prefix ) :,100,TRUE,100
1256,"def shuffle_unison_inplace ( list_of_lists , random_state = None ) : if list_of_lists : assert all ( len ( l ) == len ( list_of_lists [ 0 ] ) for l in list_of_lists ) <MASK> random_state . permutation ( len ( list_of_lists [ 0 ] ) ) else : p = np . random . permutation ( len ( list_of_lists [ 0 ] ) ) return [ l [ p ] for l in list_of_lists ] return None",if random_state is not None :,if random_state :,91.98622517,FALSE,96.22
1257,"def _load_module ( self ) : spec = self . default_module_spec module_identifier = self . module_identifier if module_identifier : impls = self . get_module_implementation_map ( ) <MASK> raise ModuleNotFound ( "" Invalid module identifier  %r  in  %s "" % ( module_identifier , force_ascii ( repr ( self ) ) ) ) spec = impls [ module_identifier ] cls = load ( spec , context_explanation = "" Loading module for  %s "" % force_ascii ( repr ( self ) ) ) options = getattr ( self , self . module_options_field , None ) or { } return cls ( self , options )",if module_identifier not in impls :,if module_identifier not in impls :,100,TRUE,100
1258,"def get_data ( self , state = None , request = None ) : if self . load_in_memory : data , shapes = self . _in_memory_get_data ( state , request ) else : data , shapes = self . _out_of_memory_get_data ( state , request ) for i in range ( len ( data ) ) : <MASK> if isinstance ( request , numbers . Integral ) : data [ i ] = data [ i ] . reshape ( shapes [ i ] ) else : for j in range ( len ( data [ i ] ) ) : data [ i ] [ j ] = data [ i ] [ j ] . reshape ( shapes [ i ] [ j ] ) return tuple ( data )",if shapes [ i ] is not None :,if data [ i ] is not None :,99.09980227,FALSE,98.43
1259,"def resolve_credential_keys ( m_keys , keys ) : res = [ ] for k in m_keys : if k [ "" c7n:match-type "" ] == "" credential "" : c_date = parse_date ( k [ "" last_rotated "" ] ) for ak in keys : <MASK> ak = dict ( ak ) ak [ "" c7n:match-type "" ] = "" access "" if ak not in res : res . append ( ak ) elif k not in res : res . append ( k ) return res","if c_date == ak [ ""CreateDate"" ] :","if c_date . date ( ) == ak [ ""last_rotated"" ] :",94.94994768,FALSE,91.87
1260,"def _is_legacy_mode ( self , node ) : """"""Checks if the ``ast.Call`` node's keywords signal using legacy mode."""""" script_mode = False py_version = "" py2 "" for kw in node . keywords : <MASK> script_mode = ( bool ( kw . value . value ) if isinstance ( kw . value , ast . NameConstant ) else True ) if kw . arg == "" py_version "" : py_version = kw . value . s if isinstance ( kw . value , ast . Str ) else "" py3 "" return not ( py_version . startswith ( "" py3 "" ) or script_mode )","if kw . arg == ""script_mode"" :","if kw . arg == ""script_mode"" :",100,TRUE,100
1261,"def get_upstream_statuses_events ( self , upstream : Set ) - > Dict [ str , V1Statuses ] : statuses_by_refs = { u : [ ] for u in upstream } events = self . events or [ ] # type: List[V1EventTrigger] for e in events : entity_ref = contexts_refs . get_entity_ref ( e . ref ) if not entity_ref : continue <MASK> continue for kind in e . kinds : status = V1EventKind . events_statuses_mapping . get ( kind ) if status : statuses_by_refs [ entity_ref ] . append ( status ) return statuses_by_refs",if entity_ref not in statuses_by_refs :,if entity_ref not in statuses_by_refs :,100,TRUE,100
1262,"def items ( self ) : dict = { } for userdir in self . XDG_DIRS . keys ( ) : prefix = self . get ( userdir ) . strip ( ' "" ' ) . split ( "" / "" ) [ 0 ] <MASK> path = ( os . getenv ( "" HOME "" ) + "" / "" + "" / "" . join ( self . get ( userdir ) . strip ( ' "" ' ) . split ( "" / "" ) [ 1 : ] ) ) else : path = self . get ( userdir ) . strip ( ' "" ' ) dict [ userdir ] = path return dict . items ( )",if prefix :,"if prefix . startswith ( ""nt"" ) :",81.00561404,FALSE,93.62
1263,"def clean_objects ( string , common_attributes ) : """"""Return object and attribute lists"""""" string = clean_string ( string ) words = string . split ( ) if len ( words ) > 1 : prefix_words_are_adj = True for att in words [ : - 1 ] : <MASK> prefix_words_are_adj = False if prefix_words_are_adj : return words [ - 1 : ] , words [ : - 1 ] else : return [ string ] , [ ] else : return [ string ] , [ ]",if att not in common_attributes :,if att in common_attributes :,98.46247545,FALSE,97.94
1264,"def extract_custom ( extractor , * args , * * kw ) : for match in extractor ( * args , * * kw ) : msg = match [ 2 ] <MASK> unused = ( "" <unused singular (hash= %s )> "" % md5 ( msg [ 1 ] . encode ( "" utf8 "" ) ) . hexdigest ( ) ) msg = ( unused , msg [ 1 ] , msg [ 2 ] ) match = ( match [ 0 ] , match [ 1 ] , msg , match [ 3 ] ) yield match","if isinstance ( msg , tuple ) and msg [ 0 ] == """" :","if isinstance ( msg , tuple ) :",75.65659249,FALSE,91.16
1265,"def test_convex_decomposition ( self ) : mesh = g . get_mesh ( "" quadknot.obj "" ) engines = [ ( "" vhacd "" , g . trimesh . interfaces . vhacd . exists ) ] for engine , exists in engines : <MASK> g . log . warning ( "" skipping convex decomposition engine  %s "" , engine ) continue g . log . info ( "" Testing convex decomposition with engine  %s "" , engine ) meshes = mesh . convex_decomposition ( engine = engine ) self . assertTrue ( len ( meshes ) > 1 ) for m in meshes : self . assertTrue ( m . is_watertight ) g . log . info ( "" convex decomposition succeeded with  %s "" , engine )",if not exists :,if not exists :,100,TRUE,100
1266,"def _to_string_infix ( self , ostream , idx , verbose ) : if verbose : ostream . write ( ""  ,  "" ) else : hasConst = not ( self . _const . __class__ in native_numeric_types and self . _const == 0 ) if hasConst : idx - = 1 _l = self . _coef [ id ( self . _args [ idx ] ) ] _lt = _l . __class__ <MASK> ostream . write ( ""  -  "" ) else : ostream . write ( ""  +  "" )",if _lt is _NegationExpression or ( _lt in native_numeric_types and _l < 0 ) :,if _lt . __class__ == 0 :,84.448969,FALSE,85.57
1267,"def get_other ( self , data , items ) : is_tuple = False if type ( data ) == tuple : data = list ( data ) is_tuple = True if type ( data ) == list : m_items = items . copy ( ) for idx , item in enumerate ( items ) : if item < 0 : m_items [ idx ] = len ( data ) - abs ( item ) for i in sorted ( set ( m_items ) , reverse = True ) : if i < len ( data ) and i > - 1 : del data [ i ] <MASK> return tuple ( data ) else : return data else : return None",if is_tuple :,if is_tuple :,100,TRUE,100
1268,"def process_error ( self , data ) : if data . get ( "" error "" ) : <MASK> raise AuthCanceled ( self , data . get ( "" error_description "" , "" "" ) ) raise AuthFailed ( self , data . get ( "" error_description "" ) or data [ "" error "" ] ) elif "" denied "" in data : raise AuthCanceled ( self , data [ "" denied "" ] )","if ""denied"" in data [ ""error"" ] or ""cancelled"" in data [ ""error"" ] :","if ""error_description"" in data :",88.19575946,FALSE,80.2
1269,"def tamper ( payload , * * kwargs ) : junk_chars = "" !#$ % &()*~+-_.,:;?@[/| \ ]^` "" retval = "" "" for i , char in enumerate ( payload , start = 1 ) : amount = random . randint ( 10 , 15 ) if char == "" > "" : retval + = "" > "" for _ in range ( amount ) : retval + = random . choice ( junk_chars ) elif char == "" < "" : retval + = "" < "" for _ in range ( amount ) : retval + = random . choice ( junk_chars ) <MASK> for _ in range ( amount ) : retval + = random . choice ( junk_chars ) else : retval + = char return retval","elif char == "" "" :","elif char == ""!="" :",94.23982363,FALSE,97.93
1270,"def retry_http_digest_auth ( self , req , auth ) : token , challenge = auth . split ( ""   "" , 1 ) chal = parse_keqv_list ( parse_http_list ( challenge ) ) auth = self . get_authorization ( req , chal ) if auth : auth_val = "" Digest  %s "" % auth <MASK> return None req . add_unredirected_header ( self . auth_header , auth_val ) resp = self . parent . open ( req ) return resp","if req . headers . get ( self . auth_header , None ) == auth_val :",if req . headers . get ( auth_val ) is None :,89.25880318,FALSE,89.74
1271,"def close ( self ) : self . selector . close ( ) if self . sock : sockname = None try : sockname = self . sock . getsockname ( ) except ( socket . error , OSError ) : pass self . sock . close ( ) if type ( sockname ) is str : # it was a Unix domain socket, remove it from the filesystem <MASK> os . remove ( sockname ) self . sock = None",if os . path . exists ( sockname ) :,if os . path . exists ( sockname ) :,100,TRUE,100
1272,"def to_nurbs ( self , curves ) : result = [ ] for i , c in enumerate ( curves ) : nurbs = SvNurbsCurve . to_nurbs ( c ) <MASK> raise Exception ( f "" Curve # { i }  -  { c }  - can not be converted to NURBS! "" ) result . append ( nurbs ) return result",if nurbs is None :,if nurbs is None :,100,TRUE,100
1273,"def handle_1_roomid_raffle ( self , i ) : if i [ 1 ] in [ "" handle_1_room_TV "" , "" handle_1_room_captain "" ] : <MASK> await self . notify ( "" post_watching_history "" , i [ 0 ] ) await self . notify ( i [ 1 ] , i [ 0 ] , i [ 2 ] ) else : print ( "" hhjjkskddrsfvsfdfvdfvvfdvdvdfdfffdfsvh "" , i )","if await self . notify ( ""check_if_normal_room"" , i [ 0 ] , - 1 ) :",if i [ 2 ] :,61.20128902,FALSE,78.09
1274,"def init_ps_var_partition ( self ) : ps_vars = { } for v in self . _non_embed_vars . values ( ) : if v . name not in self . _var_to_ps : self . _var_to_ps [ v . name ] = string_to_id ( v . name , self . _ps_num ) ps_id = self . _var_to_ps [ v . name ] <MASK> ps_vars [ ps_id ] = [ v ] else : ps_vars [ ps_id ] . append ( v ) self . _ps_vars = ps_vars",if ps_id not in ps_vars :,if ps_id not in ps_vars :,100,TRUE,100
1275,"def get_files ( d ) : f = [ ] for root , dirs , files in os . walk ( d ) : for name in files : if "" meta-environment "" in root or "" cross-canadian "" in root : continue if "" qemux86copy- "" in root or "" qemux86- "" in root : continue <MASK> f . append ( os . path . join ( root , name ) ) return f","if ""do_build"" not in name and ""do_populate_sdk"" not in name :","if name . endswith ( "".py"" ) :",90.5392143,FALSE,79.13
1276,"def setSelectedLabelState ( self , p ) : # selected, disabled c = self . c # g.trace(p,c.edit_widget(p)) if p and c . edit_widget ( p ) : <MASK> g . trace ( self . trace_n , c . edit_widget ( p ) , p ) # g.trace(g.callers(6)) self . trace_n + = 1 self . setDisabledHeadlineColors ( p )",if 0 :,if self . trace_n < 4 :,71.44059353,FALSE,91.44
1277,"def filter_tasks ( self , task_types = None , task_states = None , task_text = None ) : tasks = self . api . tasks ( self . id ) . get ( "" tasks "" , { } ) if tasks and tasks . get ( "" task "" ) : return [ Task ( self , task ) for task in tasks . get ( "" task "" , [ ] ) <MASK> and ( not task_states or task [ "" state "" ] . lower ( ) in task_states ) and ( not task_text or task_text . lower ( ) in str ( task ) . lower ( ) ) ] else : return [ ]","if ( not task_types or task [ ""type"" ] . lower ( ) in task_types )","if task [ ""type"" ] . lower ( ) in task_types",89.26327677,FALSE,93.45
1278,"def GenerateVector ( self , hits , vector , level ) : """"""Generate possible hit vectors which match the rules."""""" for item in hits . get ( level , [ ] ) : <MASK> if item < vector [ - 1 ] : continue if item > self . max_separation + vector [ - 1 ] : break new_vector = vector + [ item ] if level + 1 == len ( hits ) : yield new_vector elif level + 1 < len ( hits ) : for result in self . GenerateVector ( hits , new_vector , level + 1 ) : yield result",if vector :,if vector :,100,TRUE,100
1279,def _transmit_from_storage ( self ) - > None : for blob in self . storage . gets ( ) : # give a few more seconds for blob lease operation # to reduce the chance of race (for perf consideration) if blob . lease ( self . _timeout + 5 ) : envelopes = [ TelemetryItem ( * * x ) for x in blob . get ( ) ] result = self . _transmit ( list ( envelopes ) ) <MASK> blob . lease ( 1 ) else : blob . delete ( ),if result == ExportResult . FAILED_RETRYABLE :,if result :,97.28982168,FALSE,92.25
1280,"def load_dictionary ( file ) : oui = { } with open ( file , "" r "" ) as f : for line in f : <MASK> data = line . split ( "" (hex) "" ) key = data [ 0 ] . replace ( "" - "" , "" : "" ) . lower ( ) . strip ( ) company = data [ 1 ] . strip ( ) oui [ key ] = company return oui","if ""(hex)"" in line :","if line . startswith ( ""## "" ) :",85.03936943,FALSE,90.01
1281,"def _yield_minibatches_idx ( self , rgen , n_batches , data_ary , shuffle = True ) : indices = np . arange ( data_ary . shape [ 0 ] ) if shuffle : indices = rgen . permutation ( indices ) if n_batches > 1 : remainder = data_ary . shape [ 0 ] % n_batches <MASK> minis = np . array_split ( indices [ : - remainder ] , n_batches ) minis [ - 1 ] = np . concatenate ( ( minis [ - 1 ] , indices [ - remainder : ] ) , axis = 0 ) else : minis = np . array_split ( indices , n_batches ) else : minis = ( indices , ) for idx_batch in minis : yield idx_batch",if remainder :,if remainder > 0 :,89.58979226,FALSE,97.78
1282,"def canonical_custom_headers ( self , headers ) : hoi = [ ] custom_headers = { } for key in headers : lk = key . lower ( ) if headers [ key ] is not None : <MASK> custom_headers [ lk ] = "" , "" . join ( v . strip ( ) for v in headers . get_all ( key ) ) sorted_header_keys = sorted ( custom_headers . keys ( ) ) for key in sorted_header_keys : hoi . append ( "" %s : %s "" % ( key , custom_headers [ key ] ) ) return "" \n "" . join ( hoi )","if lk . startswith ( ""x-amz-"" ) :",if lk not in custom_headers :,95.03387294,FALSE,93.9
1283,"def validate ( self , data ) : if not data . get ( "" reason "" ) : # If reason is not provided, message is required and can not be # null or blank. message = data . get ( "" message "" ) if not message : if "" message "" not in data : msg = serializers . Field . default_error_messages [ "" required "" ] <MASK> msg = serializers . Field . default_error_messages [ "" null "" ] else : msg = serializers . CharField . default_error_messages [ "" blank "" ] raise serializers . ValidationError ( { "" message "" : [ msg ] } ) return data",elif message is None :,"elif ""null"" in data :",97.87336327,FALSE,95.08
1284,def tearDown ( self ) : try : os . chdir ( self . cwd ) <MASK> os . remove ( self . pythonexe ) test_support . rmtree ( self . parent_dir ) finally : BaseTestCase . tearDown ( self ),if self . pythonexe != sys . executable :,if os . path . exists ( self . pythonexe ) :,87.19826766,FALSE,81.85
1285,"def update ( self , value , label ) : if self . _disabled : return try : self . _progress . value = value self . _label . value = label <MASK> self . _displayed = True display_widget ( self . _widget ) except Exception as e : self . _disabled = True logger . exception ( e ) wandb . termwarn ( "" Unable to render progress bar, see the user log for details "" )",if not self . _displayed :,if self . _widget :,76.65966203,FALSE,94.48
1286,"def GetBinaryOperationBinder ( self , op ) : with self . _lock : <MASK> return self . _binaryOperationBinders [ op ] b = runtime . SymplBinaryOperationBinder ( op ) self . _binaryOperationBinders [ op ] = b return b",if self . _binaryOperationBinders . ContainsKey ( op ) :,if op in self . _binaryOperationBinders :,84.99964575,FALSE,84.36
1287,"def apply ( self , l , b , evaluation ) : "" FromDigits[l_, b_] "" if l . get_head_name ( ) == "" System`List "" : value = Integer ( 0 ) for leaf in l . leaves : value = Expression ( "" Plus "" , Expression ( "" Times "" , value , b ) , leaf ) return value elif isinstance ( l , String ) : value = FromDigits . _parse_string ( l . get_string_value ( ) , b ) <MASK> evaluation . message ( "" FromDigits "" , "" nlst "" ) else : return value else : evaluation . message ( "" FromDigits "" , "" nlst "" )",if value is None :,if value is None :,100,TRUE,100
1288,"def hsconn_sender ( self ) : while not self . stop_event . is_set ( ) : try : # Block, but timeout, so that we can exit the loop gracefully request = self . send_queue . get ( True , 6.0 ) if self . socket is not None : # Socket got closed and set to None in another thread... self . socket . sendall ( request ) <MASK> self . send_queue . task_done ( ) except queue . Empty : pass except OSError : self . stop_event . set ( )",if self . send_queue is not None :,if request . status == 0 :,71.0174369,FALSE,92.24
1289,"def check_expected ( result , expected , contains = False ) : if sys . version_info [ 0 ] > = 3 : if isinstance ( result , str ) : result = result . encode ( "" ascii "" ) if isinstance ( expected , str ) : expected = expected . encode ( "" ascii "" ) resultlines = result . splitlines ( ) expectedlines = expected . splitlines ( ) if len ( resultlines ) != len ( expectedlines ) : return False for rline , eline in zip ( resultlines , expectedlines ) : if contains : <MASK> return False else : if not rline . endswith ( eline ) : return False return True",if eline not in rline :,if rline . startswith ( eline ) :,95.98614248,FALSE,94.57
1290,"def init_weights ( self ) : """"""Initialize model weights."""""" for _ , m in self . multi_deconv_layers . named_modules ( ) : <MASK> normal_init ( m , std = 0.001 ) elif isinstance ( m , nn . BatchNorm2d ) : constant_init ( m , 1 ) for m in self . multi_final_layers . modules ( ) : if isinstance ( m , nn . Conv2d ) : normal_init ( m , std = 0.001 , bias = 0 )","if isinstance ( m , nn . ConvTranspose2d ) :","if isinstance ( m , nn . ConvTranspose2d ) :",100,TRUE,100
1291,"def filter_rel_attrs ( field_name , * * rel_attrs ) : clean_dict = { } for k , v in rel_attrs . items ( ) : <MASK> splitted_key = k . split ( "" __ "" ) key = "" __ "" . join ( splitted_key [ 1 : ] ) clean_dict [ key ] = v else : clean_dict [ k ] = v return clean_dict","if k . startswith ( field_name + ""__"" ) :","if ""__"" in k :",78.51560276,FALSE,88.34
1292,"def cancel ( self ) : with self . _condition : <MASK> self . _squash ( state_root = self . _previous_state_hash , context_ids = [ self . _previous_context_id ] , persist = False , clean_up = True , ) self . _cancelled = True self . _condition . notify_all ( )",if not self . _cancelled and not self . _final and self . _previous_context_id :,if self . _previous_context_id is not None :,78.82349088,FALSE,83.67
1293,"def _get_level ( levels , level_ref ) : if level_ref in levels : return levels . index ( level_ref ) if isinstance ( level_ref , six . integer_types ) : <MASK> level_ref + = len ( levels ) if not ( 0 < = level_ref < len ( levels ) ) : raise PatsyError ( "" specified level  %r  is out of range "" % ( level_ref , ) ) return level_ref raise PatsyError ( "" specified level  %r  not found "" % ( level_ref , ) )",if level_ref < 0 :,if level_ref < 0 :,100,TRUE,100
1294,"def parse_node ( self , node , alias_map = None , conv = None ) : sql , params , unknown = self . _parse ( node , alias_map , conv ) if unknown and conv and params : params = [ conv . db_value ( i ) for i in params ] if isinstance ( node , Node ) : if node . _negated : sql = "" NOT  %s "" % sql <MASK> sql = ""   "" . join ( ( sql , "" AS "" , node . _alias ) ) if node . _ordering : sql = ""   "" . join ( ( sql , node . _ordering ) ) return sql , params",if node . _alias :,if node . _alias :,100,TRUE,100
1295,"def parse_object_id ( _ , values ) : if values : for key in values : <MASK> val = values [ key ] if len ( val ) > 10 : try : values [ key ] = utils . ObjectIdSilent ( val ) except : values [ key ] = None","if key . endswith ( ""_id"" ) :","if key in ( ""id"" , ""id"" ) :",83.95577896,FALSE,88.24
1296,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 10 : self . set_app_id ( d . getPrefixedString ( ) ) continue if tt == 16 : self . set_max_rows ( d . getVarInt32 ( ) ) continue <MASK> raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 0 :,if tt == 0 :,100,TRUE,100
1297,"def has_invalid_cce ( yaml_file , product_yaml = None ) : rule = yaml . open_and_macro_expand ( yaml_file , product_yaml ) if "" identifiers "" in rule and rule [ "" identifiers "" ] is not None : for i_type , i_value in rule [ "" identifiers "" ] . items ( ) : <MASK> if not checks . is_cce_value_valid ( "" CCE- "" + str ( i_value ) ) : return True return False","if i_type [ 0 : 3 ] == ""cce"" :","if i_type == ""product"" :",91.20313852,FALSE,92.2
1298,"def _generate_table ( self , fromdesc , todesc , diffs ) : if fromdesc or todesc : yield ( simple_colorize ( fromdesc , "" description "" ) , simple_colorize ( todesc , "" description "" ) , ) for i , line in enumerate ( diffs ) : if line is None : # mdiff yields None on separator lines; skip the bogus ones # generated for the first line <MASK> yield ( simple_colorize ( "" --- "" , "" separator "" ) , simple_colorize ( "" --- "" , "" separator "" ) , ) else : yield line",if i > 0 :,if i == 0 :,98.79281621,FALSE,96.95
1299,"def _getPatternTemplate ( pattern , key = None ) : if key is None : key = pattern if "" % "" not in pattern : key = pattern . upper ( ) template = DD_patternCache . get ( key ) if not template : if key in ( "" EPOCH "" , "" { ^LN-BEG}EPOCH "" , "" ^EPOCH "" ) : template = DateEpoch ( lineBeginOnly = ( key != "" EPOCH "" ) ) <MASK> template = DateTai64n ( wordBegin = ( "" start "" if key != "" TAI64N "" else False ) ) else : template = DatePatternRegex ( pattern ) DD_patternCache . set ( key , template ) return template","elif key in ( ""TAI64N"" , ""{^LN-BEG}TAI64N"" , ""^TAI64N"" ) :","elif key in ( ""TAI64N"" , ""{^TAI64N}""",95.01245596,FALSE,92.03
1300,"def ref_max_pooling_2d ( x , kernel , stride , ignore_border , pad ) : y = [ ] for xx in x . reshape ( ( - 1 , ) + x . shape [ - 3 : ] ) : <MASK> xx = xx [ np . newaxis ] y + = [ refs . pooling_2d ( xx , "" max "" , kernel , stride , pad , ignore_border ) [ np . newaxis ] ] y = np . vstack ( y ) if x . ndim == 2 : y = np . squeeze ( y , 1 ) return y . reshape ( x . shape [ : - 3 ] + y . shape [ 1 : ] )",if xx . ndim == 2 :,if xx . ndim == 2 :,100,TRUE,100
1301,"def show_topics ( ) : """"""prints all available miscellaneous help topics."""""" print ( _stash . text_color ( "" Miscellaneous Topics: "" , "" yellow "" ) ) for pp in PAGEPATHS : if not os . path . isdir ( pp ) : continue content = os . listdir ( pp ) for pn in content : <MASK> name = pn [ : pn . index ( "" . "" ) ] else : name = pn print ( name )","if ""."" in pn :","if pn . endswith ( "".py"" ) :",93.40052095,FALSE,90.91
1302,"def justify_toggle_auto ( self , event = None ) : c = self if c . editCommands . autojustify == 0 : c . editCommands . autojustify = abs ( c . config . getInt ( "" autojustify "" ) or 0 ) <MASK> g . es ( "" Autojustify on, @int autojustify ==  %s "" % c . editCommands . autojustify ) else : g . es ( "" Set @int autojustify in @settings "" ) else : c . editCommands . autojustify = 0 g . es ( "" Autojustify off "" )",if c . editCommands . autojustify :,if c . editCommands . autojustify :,100,TRUE,100
1303,"def render_token_list ( self , tokens ) : result = [ ] vars = [ ] for token in tokens : <MASK> result . append ( token . contents . replace ( "" % "" , "" %% "" ) ) elif token . token_type == TOKEN_VAR : result . append ( "" %% ( %s )s "" % token . contents ) vars . append ( token . contents ) return "" "" . join ( result ) , vars",if token . token_type == TOKEN_TEXT :,if token . token_type == TOKEN_VAR_RE :,98.53977451,FALSE,95.72
1304,"def get_target_dimensions ( self ) : width , height = self . engine . size for operation in self . operations : if operation [ "" type "" ] == "" crop "" : width = operation [ "" right "" ] - operation [ "" left "" ] height = operation [ "" bottom "" ] - operation [ "" top "" ] <MASK> width = operation [ "" width "" ] height = operation [ "" height "" ] return ( width , height )","if operation [ ""type"" ] == ""resize"" :","elif operation [ ""type"" ] == ""crop"" :",94.9302306,FALSE,95.07
1305,"def get_eval_matcher ( self ) : if isinstance ( self . data [ "" match "" ] , str ) : <MASK> values = [ "" explicitDeny "" , "" implicitDeny "" ] else : values = [ "" allowed "" ] vf = ValueFilter ( { "" type "" : "" value "" , "" key "" : "" EvalDecision "" , "" value "" : values , "" op "" : "" in "" } ) else : vf = ValueFilter ( self . data [ "" match "" ] ) vf . annotate = False return vf","if self . data [ ""match"" ] == ""denied"" :","if self . data [ ""match"" ] == ""implicit"" :",98.84158918,FALSE,97.84
1306,"def test_training ( self ) : if not self . model_tester . is_training : return config , inputs_dict = self . model_tester . prepare_config_and_inputs_for_common ( ) config . return_dict = True for model_class in self . all_model_classes : <MASK> continue model = model_class ( config ) model . to ( torch_device ) model . train ( ) inputs = self . _prepare_for_class ( inputs_dict , model_class , return_labels = True ) loss = model ( * * inputs ) . loss loss . backward ( )",if model_class in MODEL_MAPPING . values ( ) :,"if model_class . __name__ == ""Tensor"" :",93.09413262,FALSE,91.56
1307,"def prehook ( self , emu , op , eip ) : if op in self . badops : emu . stopEmu ( ) raise v_exc . BadOpBytes ( op . va ) if op . mnem in STOS : <MASK> reg = emu . getRegister ( envi . archs . i386 . REG_EDI ) elif self . arch == "" amd64 "" : reg = emu . getRegister ( envi . archs . amd64 . REG_RDI ) if self . vw . isValidPointer ( reg ) and self . vw . getLocation ( reg ) is None : self . vw . makePointer ( reg , follow = True )","if self . arch == ""i386"" :","if self . arch == ""i386"" :",100,TRUE,100
1308,"def test_len ( self ) : eq = self . assertEqual eq ( base64mime . base64_len ( "" hello "" ) , len ( base64mime . encode ( "" hello "" , eol = "" "" ) ) ) for size in range ( 15 ) : <MASK> bsize = 0 elif size < = 3 : bsize = 4 elif size < = 6 : bsize = 8 elif size < = 9 : bsize = 12 elif size < = 12 : bsize = 16 else : bsize = 20 eq ( base64mime . base64_len ( "" x "" * size ) , bsize )",if size == 0 :,if size == 0 :,100,TRUE,100
1309,"def __new__ ( cls , dependencies ) : deps = check . list_param ( dependencies , "" dependencies "" , of_type = DependencyDefinition ) seen = { } for dep in deps : key = dep . solid + "" : "" + dep . output <MASK> raise DagsterInvalidDefinitionError ( ' Duplicate dependencies on solid  "" {dep.solid} ""  output  "" {dep.output} ""   ' "" used in the same MultiDependencyDefinition. "" . format ( dep = dep ) ) seen [ key ] = True return super ( MultiDependencyDefinition , cls ) . __new__ ( cls , deps )",if key in seen :,if key in seen :,100,TRUE,100
1310,"def get_explanation ( self , spec ) : """"""Expand an explanation."""""" if spec : try : a = self . dns_txt ( spec ) if len ( a ) == 1 : return str ( self . expand ( to_ascii ( a [ 0 ] ) , stripdot = False ) ) except PermError : # RFC4408 6.2/4 syntax errors cause exp= to be ignored <MASK> raise # but report in harsh mode for record checking tools pass el<MASK> raise PermError ( "" Empty domain-spec on exp= "" ) # RFC4408 6.2/4 empty domain spec is ignored # (unless you give precedence to the grammar). return None",if self . strict > 1 :,if self . is_harsh_mode ( ) :,96.26081583,FALSE,88.53
1311,"def build ( self ) : if self . args . get ( "" sle_id "" ) : self . process_sle_against_current_voucher ( ) else : entries_to_fix = self . get_future_entries_to_fix ( ) i = 0 while i < len ( entries_to_fix ) : sle = entries_to_fix [ i ] i + = 1 self . process_sle ( sle ) <MASK> self . get_dependent_entries_to_fix ( entries_to_fix , sle ) if self . exceptions : self . raise_exceptions ( ) self . update_bin ( )",if sle . dependant_sle_voucher_detail_no :,"if self . args . get ( ""dependent"" ) :",93.48177086,FALSE,91.33
1312,"def ValidateStopLatitude ( self , problems ) : if self . stop_lat is not None : value = self . stop_lat try : if not isinstance ( value , ( float , int ) ) : self . stop_lat = util . FloatStringToFloat ( value , problems ) except ( ValueError , TypeError ) : problems . InvalidValue ( "" stop_lat "" , value ) del self . stop_lat else : <MASK> problems . InvalidValue ( "" stop_lat "" , value )",if self . stop_lat > 90 or self . stop_lat < - 90 :,if self . stop_lat < 0 :,91.70253538,FALSE,89.26
1313,"def set ( self , obj , * * kwargs ) : """"""Check for missing event functions and substitute these with"""""" """"""the ignore method"""""" ignore = getattr ( self , "" ignore "" ) for k , v in kwargs . iteritems ( ) : setattr ( self , k , getattr ( obj , v ) ) <MASK> for k1 in self . combinations [ k ] : if not hasattr ( self , k1 ) : setattr ( self , k1 , ignore )",if k in self . combinations :,if k in self . combinations :,100,TRUE,100
1314,"def split ( self , duration , include_remainder = True ) : # Convert seconds to timedelta, if appropriate. duration = _seconds_or_timedelta ( duration ) if duration < = timedelta ( seconds = 0 ) : raise ValueError ( "" cannot call split with a non-positive timedelta "" ) start = self . start while start < self . end : if start + duration < = self . end : yield MayaInterval ( start , start + duration ) <MASK> yield MayaInterval ( start , self . end ) start + = duration",elif include_remainder :,if start + duration <= self . end :,96.56337217,FALSE,90.48
1315,"def get_first_field ( layout , clz ) : for layout_object in layout . fields : if issubclass ( layout_object . __class__ , clz ) : return layout_object <MASK> gf = get_first_field ( layout_object , clz ) if gf : return gf","elif hasattr ( layout_object , ""get_field_names"" ) :","if issubclass ( layout_object , BaseField ) :",60.50621418,FALSE,83.64
1316,"def _getPatternTemplate ( pattern , key = None ) : if key is None : key = pattern if "" % "" not in pattern : key = pattern . upper ( ) template = DD_patternCache . get ( key ) if not template : <MASK> template = DateEpoch ( lineBeginOnly = ( key != "" EPOCH "" ) ) elif key in ( "" TAI64N "" , "" { ^LN-BEG}TAI64N "" , "" ^TAI64N "" ) : template = DateTai64n ( wordBegin = ( "" start "" if key != "" TAI64N "" else False ) ) else : template = DatePatternRegex ( pattern ) DD_patternCache . set ( key , template ) return template","if key in ( ""EPOCH"" , ""{^LN-BEG}EPOCH"" , ""^EPOCH"" ) :","if key == ""EPOCH"" :",94.21899905,FALSE,86.65
1317,"def findOwningViewController ( self , object ) : while object : <MASK> description = fb . evaluateExpressionValue ( object ) . GetObjectDescription ( ) print ( "" Found the owning view controller. \n {} "" . format ( description ) ) cmd = ' echo  {}  | tr -d  "" \n ""  | pbcopy ' . format ( object ) os . system ( cmd ) return else : object = self . nextResponder ( object ) print ( "" Could not find an owning view controller "" )",if self . isViewController ( object ) :,if fb . evaluateExpressionValue ( object ) . IsObject ( ) :,77.52499512,FALSE,91.25
1318,"def __get_file_by_num ( self , num , file_list , idx = 0 ) : for element in file_list : if idx == num : return element if element [ 3 ] and element [ 4 ] : i = self . __get_file_by_num ( num , element [ 3 ] , idx + 1 ) <MASK> return i idx = i else : idx + = 1 return idx","if not isinstance ( i , int ) :",if i is not None :,89.27426751,FALSE,91.58
1319,"def promtool ( * * kwargs ) : key = "" prometheus:promtool "" try : path = pathlib . Path ( util . setting ( key ) ) except TypeError : yield checks . Warning ( "" Missing setting for  %s  in  %s   "" % ( key , settings . PROMGEN_CONFIG_FILE ) , id = "" promgen.W001 "" , ) else : <MASK> yield checks . Warning ( "" Unable to execute file  %s "" % path , id = "" promgen.W003 "" )","if not os . access ( path , os . X_OK ) :",if not os . path . exists ( path ) :,93.07616591,FALSE,91.26
1320,"def parse_config ( schema , config ) : schemaparser = ConfigParser ( ) schemaparser . readfp ( StringIO ( schema ) ) cfgparser = ConfigParser ( ) cfgparser . readfp ( StringIO ( config ) ) result = { } for section in cfgparser . sections ( ) : result_section = { } schema = { } <MASK> schema = dict ( schemaparser . items ( section ) ) for key , value in cfgparser . items ( section ) : converter = converters [ schema . get ( key , "" string "" ) ] result_section [ key ] = converter ( value ) result [ section ] = result_section return result",if section in schemaparser . sections ( ) :,if section in schemaparser :,93.83252044,FALSE,95.66
1321,"def validate_arguments ( args ) : if args . num_pss < 1 : print ( "" Value error: must have ore than one parameter servers. "" ) exit ( 1 ) if not GPU_IDS : num_cpus = multiprocessing . cpu_count ( ) <MASK> print ( "" Value error: there are  %s  available CPUs but you are requiring  %s . "" % ( num_cpus , args . cpu_trainers ) ) exit ( 1 ) if not os . path . isfile ( args . file ) : print ( "" Value error: model trainning file does not exist "" ) exit ( 1 )",if args . cpu_trainers > num_cpus :,if num_cpus > args . cpu_trainers :,95.43355763,FALSE,95.94
1322,"def infer_dataset_impl ( path ) : if IndexedRawTextDataset . exists ( path ) : return "" raw "" elif IndexedDataset . exists ( path ) : with open ( index_file_path ( path ) , "" rb "" ) as f : magic = f . read ( 8 ) if magic == IndexedDataset . _HDR_MAGIC : return "" cached "" <MASK> return "" mmap "" else : return None elif FastaDataset . exists ( path ) : return "" fasta "" else : return None",elif magic == MMapIndexedDataset . Index . _HDR_MAGIC [ : 8 ] :,elif magic == IndexedDataset . _HDR_MAGIC :,63.65820577,FALSE,90.92
1323,"def _add_resource_group ( obj ) : if isinstance ( obj , list ) : for array_item in obj : _add_resource_group ( array_item ) elif isinstance ( obj , dict ) : try : if "" resourcegroup "" not in [ x . lower ( ) for x in obj . keys ( ) ] : <MASK> obj [ "" resourceGroup "" ] = _parse_id ( obj [ "" id "" ] ) [ "" resource-group "" ] except ( KeyError , IndexError , TypeError ) : pass for item_key in obj : if item_key != "" sourceVault "" : _add_resource_group ( obj [ item_key ] )","if obj [ ""id"" ] :","if obj [ ""resource-group"" ] :",92.95033099,FALSE,98.21
1324,"def reformatBody ( self , event = None ) : """"""Reformat all paragraphs in the body."""""" c , p = self , self . p undoType = "" reformat-body "" w = c . frame . body . wrapper c . undoer . beforeChangeGroup ( p , undoType ) w . setInsertPoint ( 0 ) while 1 : progress = w . getInsertPoint ( ) c . reformatParagraph ( event , undoType = undoType ) ins = w . getInsertPoint ( ) s = w . getAllText ( ) w . setInsertPoint ( ins ) <MASK> break c . undoer . afterChangeGroup ( p , undoType )",if ins <= progress or ins >= len ( s ) :,if s == s :,91.06347271,FALSE,89.68
1325,"def make_sources ( project : RootDependency ) - > str : content = [ ] if project . readme : content . append ( project . readme . path . name ) <MASK> content . append ( project . readme . to_rst ( ) . path . name ) path = project . package . path for fname in ( "" setup.cfg "" , "" setup.py "" ) : if ( path / fname ) . exists ( ) : content . append ( fname ) for package in chain ( project . package . packages , project . package . data ) : for fpath in package : fpath = fpath . relative_to ( project . package . path ) content . append ( "" / "" . join ( fpath . parts ) ) return "" \n "" . join ( content )","if project . readme . markup != ""rst"" :",if project . readme . to_rst :,94.41284867,FALSE,95.61
1326,"def __init__ ( self , response ) : error = "" {}   {} "" . format ( response . status_code , response . reason ) extra = [ ] try : response_json = response . json ( ) <MASK> error = ""   "" . join ( error [ "" message "" ] for error in response_json [ "" error_list "" ] ) extra = [ error [ "" extra "" ] for error in response_json [ "" error_list "" ] if "" extra "" in error ] except JSONDecodeError : pass super ( ) . __init__ ( response = response , error = error , extra = extra )","if ""error_list"" in response_json :","if ""error_list"" in response_json :",100,TRUE,100
1327,"def handle_event ( self , fileno = None , events = None ) : if self . _state == RUN : <MASK> self . _it = self . _process_result ( 0 ) # non-blocking try : next ( self . _it ) except ( StopIteration , CoroStop ) : self . _it = None",if self . _it is None :,if self . _it is None :,100,TRUE,100
1328,"def find_query ( self , needle , haystack ) : try : import pinyin haystack_py = pinyin . get_initial ( haystack , "" "" ) needle_len = len ( needle ) start = 0 result = [ ] while True : found = haystack_py . find ( needle , start ) <MASK> break result . append ( ( found , needle_len ) ) start = found + needle_len return result except : return None",if found < 0 :,if found == - 1 :,94.94372597,FALSE,94.03
1329,"def decorated_function ( * args , * * kwargs ) : rv = f ( * args , * * kwargs ) if "" Last-Modified "" not in rv . headers : try : result = date if callable ( result ) : result = result ( rv ) if not isinstance ( result , basestring ) : from werkzeug . http import http_date result = http_date ( result ) <MASK> rv . headers [ "" Last-Modified "" ] = result except Exception : logging . getLogger ( __name__ ) . exception ( "" Error while calculating the lastmodified value for response  {!r} "" . format ( rv ) ) return rv",if result :,if result :,100,TRUE,100
1330,"def check_require ( require_modules , require_lines ) : for require_module in require_modules : st = try_import ( require_module ) if st == 0 : continue <MASK> print ( "" installed  {} :  {} \n "" . format ( require_module , require_lines [ require_module ] ) ) elif st == 2 : print ( "" failed installed  {} :  {} \n "" . format ( require_module , require_lines [ require_module ] ) )",elif st == 1 :,if st == 1 :,70.30994516,FALSE,97.7
1331,"def bundle_directory ( self , dirpath ) : """"""Bundle all modules/packages in the given directory."""""" dirpath = os . path . abspath ( dirpath ) for nm in os . listdir ( dirpath ) : nm = _u ( nm ) if nm . startswith ( "" . "" ) : continue itempath = os . path . join ( dirpath , nm ) if os . path . isdir ( itempath ) : if os . path . exists ( os . path . join ( itempath , "" __init__.py "" ) ) : self . bundle_package ( itempath ) <MASK> self . bundle_module ( itempath )","elif nm . endswith ( "".py"" ) :","elif os . path . exists ( os . path . join ( itempath , ""__init",92.91953314,FALSE,87.34
1332,"def _find_root ( ) : test_dirs = [ "" Src "" , "" Build "" , "" Package "" , "" Tests "" , "" Util "" ] root = os . getcwd ( ) test = all ( [ os . path . exists ( os . path . join ( root , x ) ) for x in test_dirs ] ) while not test : last_root = root root = os . path . dirname ( root ) <MASK> raise Exception ( "" Root not found "" ) test = all ( [ os . path . exists ( os . path . join ( root , x ) ) for x in test_dirs ] ) return root",if root == last_root :,if root == last_root :,100,TRUE,100
1333,"def findMarkForUnitTestNodes ( self ) : """"""return the position of *all* non-ignored @mark-for-unit-test nodes."""""" c = self . c p , result , seen = c . rootPosition ( ) , [ ] , [ ] while p : if p . v in seen : p . moveToNodeAfterTree ( ) else : seen . append ( p . v ) if g . match_word ( p . h , 0 , "" @ignore "" ) : p . moveToNodeAfterTree ( ) <MASK> result . append ( p . copy ( ) ) p . moveToNodeAfterTree ( ) else : p . moveToThreadNext ( ) return result","elif p . h . startswith ( ""@mark-for-unit-tests"" ) :","elif g . match_word ( p . h , 0 , ""@mark-for-",91.92282572,FALSE,89.63
1334,"def startTagFrameset ( self , token ) : self . parser . parseError ( "" unexpected-start-tag "" , { "" name "" : "" frameset "" } ) if len ( self . tree . openElements ) == 1 or self . tree . openElements [ 1 ] . name != "" body "" : assert self . parser . innerHTML elif not self . parser . framesetOK : pass else : <MASK> self . tree . openElements [ 1 ] . parent . removeChild ( self . tree . openElements [ 1 ] ) while self . tree . openElements [ - 1 ] . name != "" html "" : self . tree . openElements . pop ( ) self . tree . insertElement ( token ) self . parser . phase = self . parser . phases [ "" inFrameset "" ]",if self . tree . openElements [ 1 ] . parent :,if self . tree . openElements [ 1 ] . parent :,100,TRUE,100
1335,"def try_split ( self , split_text : List [ str ] ) : ret = [ ] for i in split_text : <MASK> continue val = int ( i , 2 ) if val > 255 or val < 0 : return None ret . append ( val ) if len ( ret ) != 0 : ret = bytes ( ret ) logger . debug ( f "" binary successful, returning  { ret . __repr__ ( ) } "" ) return ret",if len ( i ) == 0 :,"if i == """" :",88.43833126,FALSE,92.57
1336,"def generator ( self , data ) : for sock in data : <MASK> offset = sock . obj_offset else : offset = sock . obj_vm . vtop ( sock . obj_offset ) yield ( 0 , [ Address ( offset ) , int ( sock . Pid ) , int ( sock . LocalPort ) , int ( sock . Protocol ) , str ( protos . protos . get ( sock . Protocol . v ( ) , "" - "" ) ) , str ( sock . LocalIpAddress ) , str ( sock . CreateTime ) , ] , )",if not self . _config . PHYSICAL_OFFSET :,"if isinstance ( sock . obj_offset , int ) :",93.42387865,FALSE,90.69
1337,"def __init__ ( self , num_bits = 4 , always_apply = False , p = 0.5 ) : super ( Posterize , self ) . __init__ ( always_apply , p ) if isinstance ( num_bits , ( list , tuple ) ) : <MASK> self . num_bits = [ to_tuple ( i , 0 ) for i in num_bits ] else : self . num_bits = to_tuple ( num_bits , 0 ) else : self . num_bits = to_tuple ( num_bits , num_bits )",if len ( num_bits ) == 3 :,if len ( num_bits ) == 1 :,98.60862289,FALSE,98.04
1338,"def tearDown ( self ) : """"""Just in case yn00 creates some junk files, do a clean-up."""""" del_files = [ self . out_file , "" 2YN.dN "" , "" 2YN.dS "" , "" 2YN.t "" , "" rst "" , "" rst1 "" , "" rub "" ] for filename in del_files : <MASK> os . remove ( filename ) if os . path . exists ( self . working_dir ) : for filename in os . listdir ( self . working_dir ) : filepath = os . path . join ( self . working_dir , filename ) os . remove ( filepath ) os . rmdir ( self . working_dir )",if os . path . exists ( filename ) :,if os . path . exists ( filename ) :,100,TRUE,100
1339,"def reverse_search_history ( self , searchfor , startpos = None ) : if startpos is None : startpos = self . history_cursor if _ignore_leading_spaces : res = [ ( idx , line . lstrip ( ) ) for idx , line in enumerate ( self . history [ startpos : 0 : - 1 ] ) <MASK> ] else : res = [ ( idx , line ) for idx , line in enumerate ( self . history [ startpos : 0 : - 1 ] ) if line . startswith ( searchfor ) ] if res : self . history_cursor - = res [ 0 ] [ 0 ] return res [ 0 ] [ 1 ] . get_line_text ( ) return "" """,if line . lstrip ( ) . startswith ( searchfor . lstrip ( ) ),if line . startswith ( searchfor ),88.45428638,FALSE,93.27
1340,"def ComboBoxDroppedHeightTest ( windows ) : "" Check if each combobox height is the same as the reference "" bugs = [ ] for win in windows : if not win . ref : continue <MASK> continue if win . DroppedRect ( ) . height ( ) != win . ref . DroppedRect ( ) . height ( ) : bugs . append ( ( [ win , ] , { } , testname , 0 , ) ) return bugs","if win . Class ( ) != ""ComboBox"" or win . ref . Class ( ) != ""ComboBox"" :",if win . ComboBoxDroppedHeight ( ) == win . ref . ComboBoxDropped,88.20761737,FALSE,80.52
1341,"def get_changed ( self ) : if self . _is_expression ( ) : result = self . _get_node_text ( self . ast ) if result == self . source : return None return result else : collector = codeanalyze . ChangeCollector ( self . source ) last_end = - 1 for match in self . matches : start , end = match . get_region ( ) <MASK> if not self . _is_expression ( ) : continue last_end = end replacement = self . _get_matched_text ( match ) collector . add_change ( start , end , replacement ) return collector . get_changed ( )",if start < last_end :,if start != last_end and end != last_end :,97.05742097,FALSE,92.72
1342,"def unpickle_from_file ( file_path , gzip = False ) : """"""Unpickle obj from file_path with gzipping."""""" with tf . io . gfile . GFile ( file_path , "" rb "" ) as f : <MASK> obj = pickle . load ( f ) else : with gzip_lib . GzipFile ( fileobj = f , compresslevel = 2 ) as gzipf : obj = pickle . load ( gzipf ) return obj",if not gzip :,if gzip :,98.09187747,FALSE,97.26
1343,"def get_user_context ( request , escape = False ) : if isinstance ( request , HttpRequest ) : user = getattr ( request , "" user "" , None ) result = { "" ip_address "" : request . META [ "" REMOTE_ADDR "" ] } if user and user . is_authenticated ( ) : result . update ( { "" email "" : user . email , "" id "" : user . id , } ) <MASK> result [ "" name "" ] = user . name else : result = { } return mark_safe ( json . dumps ( result ) )",if user . name :,elif escape :,80.33344518,FALSE,95.39
1344,"def get_item_address ( self , item ) : """"""Get an item's address as a collection of names"""""" result = [ ] while True : name = self . tree_ctrl . GetItemPyData ( item ) <MASK> break else : result . insert ( 0 , name ) item = self . tree_ctrl . GetItemParent ( item ) return result",if name is None :,"if name == ""None"" :",90.41855889,FALSE,91.88
1345,"def closest_unseen ( self , row1 , col1 , filter = None ) : # find the closest unseen from this row/col min_dist = maxint closest_unseen = None for row in range ( self . height ) : for col in range ( self . width ) : if filter is None or ( row , col ) not in filter : <MASK> dist = self . distance ( row1 , col1 , row , col ) if dist < min_dist : min_dist = dist closest_unseen = ( row , col ) return closest_unseen",if self . map [ row ] [ col ] == UNSEEN :,if min_dist is None :,94.20996817,FALSE,88.78
1346,"def log_graph ( self , model : LightningModule , input_array = None ) : if self . _log_graph : <MASK> input_array = model . example_input_array if input_array is not None : input_array = model . _apply_batch_transfer_handler ( input_array ) self . experiment . add_graph ( model , input_array ) else : rank_zero_warn ( "" Could not log computational graph since the "" ""  `model.example_input_array` attribute is not set "" ""  or `input_array` was not given "" , UserWarning , )",if input_array is None :,if input_array is None :,100,TRUE,100
1347,"def get_scene_exceptions_by_season ( self , season = - 1 ) : scene_exceptions = [ ] for scene_exception in self . scene_exceptions : if not len ( scene_exception ) == 2 : continue scene_name , scene_season = scene_exception . split ( "" | "" ) <MASK> scene_exceptions . append ( scene_name ) return scene_exceptions",if season == scene_season :,if scene_season == season :,96.53583721,FALSE,95.6
1348,def _clean_temp_files ( ) : for pattern in _temp_files : for path in glob . glob ( pattern ) : <MASK> os . remove ( path ) else : shutil . rmtree ( path ),if os . path . islink ( path ) or os . path . isfile ( path ) :,if os . path . exists ( path ) :,60.63806007,FALSE,79.88
1349,"def wait_for_completion ( self , job_id , offset , max_results , start_time , timeout ) : """"""Wait for job completion and return the first page."""""" while True : result = self . get_query_results ( job_id = job_id , page_token = None , start_index = offset , max_results = max_results ) <MASK> return result if ( time . time ( ) - start_time ) > timeout : raise Exception ( "" Timeout: the query doesn ' t finish within  %d  seconds. "" % timeout ) time . sleep ( 1 )","if result [ ""jobComplete"" ] :",if result :,95.29954902,FALSE,95.09
1350,"def get_data ( self , element , ranges , style ) : <MASK> groups = element . groupby ( element . kdims ) . items ( ) else : groups = [ ( element . label , element ) ] plots = [ ] axis = "" x "" if self . invert_axes else "" y "" for key , group in groups : <MASK> label = "" , "" . join ( [ d . pprint_value ( v ) for d , v in zip ( element . kdims , key ) ] ) else : label = key data = { axis : group . dimension_values ( group . vdims [ 0 ] ) , "" name "" : label } plots . append ( data ) return plots",if element . kdims :,if self . invert_axes :,81.75340076,FALSE,91.6
1351,"def get_files ( self , dirname ) : if not self . _data . has_key ( dirname ) : self . _create ( dirname ) else : new_time = self . _changed ( dirname ) <MASK> self . _update ( dirname , new_time ) dcLog . debug ( "" ==>  "" + "" \t \n "" . join ( self . _data [ dirname ] [ "" flist "" ] ) ) return self . _data [ dirname ] [ "" flist "" ]",if new_time :,if new_time is not None :,96.51287277,FALSE,95.8
1352,"def __init__ ( self , dir ) : self . module_names = set ( ) for name in os . listdir ( dir ) : <MASK> self . module_names . add ( name [ : - 3 ] ) elif "" . "" not in name : self . module_names . add ( name )","if name . endswith ( "".py"" ) :","if name . endswith ( "".py"" ) :",100,TRUE,100
1353,"def logic ( ) : for i in range ( 100 ) : yield clock . posedge , reset . negedge <MASK> count . next = 0 else : if enable : count . next = ( count + 1 ) % n raise StopSimulation",if reset == ACTIVE_LOW :,if reset == ACTIVE_LOW :,100,TRUE,100
1354,"def sortkeypicker ( keynames ) : negate = set ( ) for i , k in enumerate ( keynames ) : if k [ : 1 ] == "" - "" : keynames [ i ] = k [ 1 : ] negate . add ( k [ 1 : ] ) def getit ( adict ) : composite = [ adict [ k ] for k in keynames ] for i , ( k , v ) in enumerate ( zip ( keynames , composite ) ) : <MASK> composite [ i ] = - v return composite return getit",if k in negate :,if negate . add ( k ) :,91.28410419,FALSE,93.45
1355,"def show_image ( self , wnd_name , img ) : if wnd_name in self . named_windows : <MASK> self . named_windows [ wnd_name ] = 1 self . on_create_window ( wnd_name ) if wnd_name in self . capture_mouse_windows : self . capture_mouse ( wnd_name ) self . on_show_image ( wnd_name , img ) else : print ( "" show_image: named_window  "" , wnd_name , ""  not found. "" )",if self . named_windows [ wnd_name ] == 0 :,if not self . named_windows [ wnd_name ] :,89.44246972,FALSE,95.11
1356,"def check_action_permitted ( self ) : if ( self . _action == "" sts:GetCallerIdentity "" ) : # always allowed, even if there's an explicit Deny for it return True policies = self . _access_key . collect_policies ( ) permitted = False for policy in policies : iam_policy = IAMPolicy ( policy ) permission_result = iam_policy . is_action_permitted ( self . _action ) if permission_result == PermissionResult . DENIED : self . _raise_access_denied ( ) <MASK> permitted = True if not permitted : self . _raise_access_denied ( )",elif permission_result == PermissionResult . PERMITTED :,elif permission_result == PermissionResult . NOT_DENIED :,98.6615505,FALSE,96.67
1357,"def _limit_value ( key , value , config ) : if config [ key ] . get ( "" upper_limit "" ) : limit = config [ key ] [ "" upper_limit "" ] # auto handle datetime if isinstance ( value , datetime ) and isinstance ( limit , timedelta ) : if config [ key ] [ "" inverse "" ] is True : <MASK> value = datetime . now ( ) - limit else : if ( datetime . now ( ) + limit ) < value : value = datetime . now ( ) + limit elif value > limit : value = limit return value",if ( datetime . now ( ) - limit ) > value :,if ( datetime . now ( ) - limit ) > limit :,98.97023742,FALSE,98.03
1358,"def replace_dataset_ids ( path , key , value ) : """"""Exchanges dataset_ids (HDA, LDA, HDCA, not Dataset) in input_values with dataset ids used in job."""""" current_case = input_values if key == "" id "" : for i , p in enumerate ( path ) : if isinstance ( current_case , ( list , dict ) ) : current_case = current_case [ p ] <MASK> return key , translate_values . get ( current_case [ "" id "" ] , value ) return key , value","if src == current_case . get ( ""src"" ) :",if i == 0 :,85.54299787,FALSE,89.37
1359,"def load_ext ( name , funcs ) : ExtModule = namedtuple ( "" ExtModule "" , funcs ) ext_list = [ ] lib_root = os . path . dirname ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) for fun in funcs : <MASK> ext_list . append ( extension . load ( fun , name , lib_dir = lib_root ) . op ) else : ext_list . append ( extension . load ( fun , name , lib_dir = lib_root ) . op_ ) return ExtModule ( * ext_list )","if fun in [ ""nms"" , ""softnms"" ] :","if fun == ""ext_module"" :",93.3874248,FALSE,91.74
1360,"def execute_action ( self ) : selected_actions = self . model_action . get_selected_results_with_index ( ) if selected_actions and self . args_for_action : for name , _ , act_idx in selected_actions : try : action = self . actions [ act_idx ] <MASK> action . act ( [ arg for arg , _ , _ in self . args_for_action ] , self ) except Exception as e : debug . log ( "" execute_action "" , e )",if action :,if action . act :,91.26778627,FALSE,96.85
1361,"def __getattr__ ( self , attr ) : proxy = self . __proxy if proxy and hasattr ( proxy , attr ) : return getattr ( proxy , attr ) attrmap = self . __attrmap if attr in attrmap : source = attrmap [ attr ] <MASK> value = source ( ) else : value = _import_object ( source ) setattr ( self , attr , value ) self . __log . debug ( "" loaded lazy attr  %r :  %r "" , attr , value ) return value raise AttributeError ( "" ' module '  object has no attribute  ' %s ' "" % ( attr , ) )",if callable ( source ) :,if callable ( source ) :,100,TRUE,100
1362,"def forward ( self , x ) : # BxT -> BxCxT x = x . unsqueeze ( 1 ) for conv in self . conv_layers : residual = x x = conv ( x ) <MASK> tsz = x . size ( 2 ) r_tsz = residual . size ( 2 ) residual = residual [ . . . , : : r_tsz / / tsz ] [ . . . , : tsz ] x = ( x + residual ) * self . residual_scale if self . log_compression : x = x . abs ( ) x = x + 1 x = x . log ( ) return x",if self . skip_connections and x . size ( 1 ) == residual . size ( 1 ) :,if self . use_tsz :,68.47766114,FALSE,86.14
1363,"def __Prefix_Step2a ( self , token ) : for prefix in self . __prefix_step2a : <MASK> token = token [ len ( prefix ) : ] self . prefix_step2a_success = True break return token",if token . startswith ( prefix ) and len ( token ) > 5 :,if token . startswith ( prefix ) :,61.37199208,FALSE,85.01
1364,"def is_valid ( sample ) : if sample is None : return False if isinstance ( sample , tuple ) : for s in sample : if s is None : return False elif isinstance ( s , np . ndarray ) and s . size == 0 : return False <MASK> return False return True","elif isinstance ( s , collections . abc . Sequence ) and len ( s ) == 0 :","elif isinstance ( s , ( list , tuple ) ) and s . size != 0 :",75.4263049,FALSE,84.46
1365,"def get_all_comments ( self , gallery_id , post_no , comment_cnt ) : comment_page_cnt = ( comment_cnt - 1 ) / / self . options . comments_per_page + 1 comments = [ ] headers = { "" X-Requested-With "" : "" XMLHttpRequest "" } data = { "" ci_t "" : self . _session . cookies [ "" ci_c "" ] , "" id "" : gallery_id , "" no "" : post_no } for i in range ( comment_page_cnt ) : data [ "" comment_page "" ] = i + 1 response = self . request_comment ( headers , data ) batch = self . parse_comments ( response . text ) <MASK> break comments = batch + comments return comments",if not batch :,if len ( batch ) == 0 :,96.31450461,FALSE,95
1366,def run_on_module ( self ) : try : self . module_base . disable ( self . opts . module_spec ) except dnf . exceptions . MarkingErrors as e : <MASK> if e . no_match_group_specs or e . error_group_specs : raise e if ( e . module_depsolv_errors and e . module_depsolv_errors [ 1 ] != libdnf . module . ModulePackageContainer . ModuleErrorType_ERROR_IN_DEFAULTS ) : raise e logger . error ( str ( e ) ),if self . base . conf . strict :,if self . opts . module_spec :,84.13525446,FALSE,94.47
1367,"def find_field_notnull_differ ( self , meta , table_description , table_name ) : if not self . can_detect_notnull_differ : return for field in all_local_fields ( meta ) : attname = field . db_column or field . attname <MASK> continue null = self . get_field_db_nullable ( field , table_name ) if field . null != null : action = field . null and "" DROP "" or "" SET "" self . add_difference ( "" notnull-differ "" , table_name , attname , action )","if ( table_name , attname ) in self . new_db_fields :","if attname == ""NOTNULL"" :",89.18558676,FALSE,87.48
1368,"def _change_moving_module ( self , changes , dest ) : if not self . source . is_folder ( ) : pymodule = self . pycore . resource_to_pyobject ( self . source ) source = self . import_tools . relatives_to_absolutes ( pymodule ) pymodule = self . tools . new_pymodule ( pymodule , source ) source = self . _change_occurrences_in_module ( dest , pymodule ) source = self . tools . new_source ( pymodule , source ) <MASK> changes . add_change ( ChangeContents ( self . source , source ) )",if source != self . source . read ( ) :,if source is not None :,91.95209581,FALSE,91.83
1369,"def get ( quality_name ) : """"""Returns a quality object based on canonical quality name."""""" found_components = { } for part in quality_name . lower ( ) . split ( ) : component = _registry . get ( part ) <MASK> raise ValueError ( "" ` %s ` is not a valid quality string "" % part ) if component . type in found_components : raise ValueError ( "" ` %s ` cannot be defined twice in a quality "" % component . type ) found_components [ component . type ] = component if not found_components : raise ValueError ( "" No quality specified "" ) result = Quality ( ) for type , component in found_components . items ( ) : setattr ( result , type , component ) return result",if not component :,if not component :,100,TRUE,100
1370,def _unselected ( self ) : selected = self . _selected k = 0 z = selected [ k ] k + = 1 for i in range ( self . _n ) : if i == z : <MASK> z = selected [ k ] k + = 1 else : z = - 1 else : yield i,if k < len ( selected ) :,if selected [ k ] :,90.0961553,FALSE,89.86
1371,"def render_headers ( self ) - > bytes : if not hasattr ( self , "" _headers "" ) : parts = [ b "" Content-Disposition: form-data;  "" , format_form_param ( "" name "" , self . name ) , ] <MASK> filename = format_form_param ( "" filename "" , self . filename ) parts . extend ( [ b "" ;  "" , filename ] ) if self . content_type is not None : content_type = self . content_type . encode ( ) parts . extend ( [ b "" \r \n Content-Type:  "" , content_type ] ) parts . append ( b "" \r \n \r \n "" ) self . _headers = b "" "" . join ( parts ) return self . _headers",if self . filename :,if self . filename is not None :,97.46680282,FALSE,97.22
1372,"def app_middleware ( next , root , info , * * kwargs ) : app_auth_header = "" HTTP_AUTHORIZATION "" prefix = "" bearer "" request = info . context if request . path == API_PATH : if not hasattr ( request , "" app "" ) : request . app = None auth = request . META . get ( app_auth_header , "" "" ) . split ( ) <MASK> auth_prefix , auth_token = auth if auth_prefix . lower ( ) == prefix : request . app = SimpleLazyObject ( lambda : get_app ( auth_token ) ) return next ( root , info , * * kwargs )",if len ( auth ) == 2 :,if len ( auth ) == 2 :,100,TRUE,100
1373,"def _shortest_hypernym_paths ( self , simulate_root ) : if self . offset == "" 00000000 "" : return { self : 0 } queue = deque ( [ ( self , 0 ) ] ) path = { } while queue : s , depth = queue . popleft ( ) <MASK> continue path [ s ] = depth depth + = 1 queue . extend ( ( hyp , depth ) for hyp in s . _hypernyms ( ) ) if simulate_root : root = Synset ( self . _wordnet_corpus_reader , None , self . pos ( ) , "" 00000000 "" , "" "" ) path [ root ] = max ( path . values ( ) ) + 1 return path",if s in path :,"if s in ( ""hypernym"" , ""start"" , ""end"" ) :",95.46138274,FALSE,90.61
1374,"def _populate_class_variables ( ) : lookup = { } reverse_lookup = { } characters_for_re = [ ] for codepoint , name in list ( codepoint2name . items ( ) ) : character = chr ( codepoint ) <MASK> # There's no point in turning the quotation mark into # &quot;, unless it happens within an attribute value, which # is handled elsewhere. characters_for_re . append ( character ) lookup [ character ] = name # But we do want to turn &quot; into the quotation mark. reverse_lookup [ name ] = character re_definition = "" [ %s ] "" % "" "" . join ( characters_for_re ) return lookup , reverse_lookup , re . compile ( re_definition )",if codepoint != 34 :,if name not in lookup :,91.28943419,FALSE,96.41
1375,"def prepare_data_status ( self , view : sublime . View , data : Dict [ str , Any ] ) - > Any : """"""Prepare the returned data for status"""""" if ( data [ "" success "" ] and "" No docstring "" not in data [ "" doc "" ] and data [ "" doc "" ] != "" list \n "" ) : self . signature = data [ "" doc "" ] <MASK> return try : self . signature = self . signature . splitlines ( ) [ 2 ] except KeyError : return return self . _show_status ( view )",if self . _signature_excluded ( self . signature ) :,if not self . signature :,84.11975591,FALSE,91.39
1376,"def _setup_once_tables ( cls ) : if cls . run_define_tables == "" once "" : cls . define_tables ( cls . metadata ) <MASK> cls . metadata . create_all ( cls . bind ) cls . tables . update ( cls . metadata . tables )","if cls . run_create_tables == ""once"" :",if not cls . metadata . tables :,91.07264033,FALSE,82.24
1377,"def _send_recursive ( self , files ) : for base in files : <MASK> # filename mixed into the bunch self . _send_files ( [ base ] ) continue last_dir = asbytes ( base ) for root , dirs , fls in os . walk ( base ) : self . _chdir ( last_dir , asbytes ( root ) ) self . _send_files ( [ os . path . join ( root , f ) for f in fls ] ) last_dir = asbytes ( root ) # back out of the directory for i in range ( len ( os . path . split ( last_dir ) ) ) : self . _send_popd ( )",if not os . path . isdir ( base ) :,"if isinstance ( base , str ) :",54.22417224,FALSE,93.97
1378,"def __init__ ( self , * args , * * kwargs ) : super ( ) . __init__ ( * args , * * kwargs ) # Automatically register models if required. if not is_registered ( self . model ) : inline_fields = ( ) for inline in self . inlines : inline_model , follow_field = self . _reversion_introspect_inline_admin ( inline ) if inline_model : self . _reversion_autoregister ( inline_model , ( ) ) <MASK> inline_fields + = ( follow_field , ) self . _reversion_autoregister ( self . model , inline_fields )",if follow_field :,if follow_field :,100,TRUE,100
1379,"def dispatch_hook ( key , hooks , hook_data , * * kwargs ) : """"""Dispatches a hook dictionary on a given piece of data."""""" hooks = hooks or dict ( ) hooks = hooks . get ( key ) if hooks : if hasattr ( hooks , "" __call__ "" ) : hooks = [ hooks ] for hook in hooks : _hook_data = hook ( hook_data , * * kwargs ) <MASK> hook_data = _hook_data return hook_data",if _hook_data is not None :,if _hook_data :,94.02664799,FALSE,95.95
1380,"def __call__ ( self , image , crop = True ) : if isinstance ( image , PTensor ) : return self . crop_to_output ( numpy_to_paddle ( self ( paddle_to_numpy ( image ) , crop = False ) ) ) else : warp = cv . warpAffine ( image , self . transform_matrix , image . shape [ 1 : : - 1 ] , borderMode = cv . BORDER_REPLICATE , ) <MASK> return self . crop_to_output ( warp ) else : return warp",if crop :,if crop :,100,TRUE,100
1381,"def _analyze ( self ) : lines = open ( self . log_path , "" r "" ) . readlines ( ) prev_line = None for line in lines : <MASK> self . errors . append ( line [ len ( "" ERROR: "" ) : ] . strip ( ) ) elif line . startswith ( "" FAIL: "" ) and prev_line and prev_line . startswith ( "" = "" ) : self . failures . append ( line [ len ( "" FAIL: "" ) : ] . strip ( ) ) prev_line = line","if line . startswith ( ""ERROR:"" ) and prev_line and prev_line . startswith ( ""="" ) :","if line . startswith ( ""ERROR:"" ) and line . startswith ( ""="" ) :",95.29272296,FALSE,94.36
1382,"def end ( self , name ) : self . soup . endData ( ) completed_tag = self . soup . tagStack [ - 1 ] namespace , name = self . _getNsTag ( name ) nsprefix = None if namespace is not None : for inverted_nsmap in reversed ( self . nsmaps ) : <MASK> nsprefix = inverted_nsmap [ namespace ] break self . soup . handle_endtag ( name , nsprefix ) if len ( self . nsmaps ) > 1 : # This tag, or one of its parents, introduced a namespace # mapping, so pop it off the stack. self . nsmaps . pop ( )",if inverted_nsmap is not None and namespace in inverted_nsmap :,if inverted_nsmap [ namespace ] == completed_tag :,92.3618251,FALSE,92.49
1383,"def _bind_parameters ( operation , parameters ) : # inspired by MySQL Python Connector (conversion.py) string_parameters = { } for ( name , value ) in parameters . iteritems ( ) : if value is None : string_parameters [ name ] = "" NULL "" <MASK> string_parameters [ name ] = "" ' "" + _escape ( value ) + "" ' "" else : string_parameters [ name ] = str ( value ) return operation % string_parameters","elif isinstance ( value , basestring ) :","elif isinstance ( value , str ) :",73.51029091,FALSE,97.52
1384,"def plugin_on_song_ended ( self , song , skipped ) : if song is not None : rating = song ( "" ~#rating "" ) invrating = 1.0 - rating delta = min ( rating , invrating ) / 2.0 <MASK> rating - = delta else : rating + = delta song [ "" ~#rating "" ] = rating",if skipped :,if skipped :,100,TRUE,100
1385,"def on_activated_async ( self , view ) : if settings [ "" modified_lines_only "" ] : self . freeze_last_version ( view ) if settings [ "" enabled "" ] : match_trailing_spaces ( view ) # continuously watch view for changes to the visible region <MASK> # track active_views [ view . id ( ) ] = view . visible_region ( ) self . update_on_region_change ( view )",if not view . id ( ) in active_views :,if view . id ( ) not in active_views :,97.42391725,FALSE,96.28
1386,"def _notin_text ( term , text , verbose = False ) : index = text . find ( term ) head = text [ : index ] tail = text [ index + len ( term ) : ] correct_text = head + tail diff = _diff_text ( correct_text , text , verbose ) newdiff = [ u ( "" %s  is contained here: "" ) % py . io . saferepr ( term , maxsize = 42 ) ] for line in diff : <MASK> continue if line . startswith ( u ( "" -  "" ) ) : continue if line . startswith ( u ( "" +  "" ) ) : newdiff . append ( u ( ""    "" ) + line [ 2 : ] ) else : newdiff . append ( line ) return newdiff","if line . startswith ( u ( ""Skipping"" ) ) :",if not line :,95.02670417,FALSE,92.35
1387,"def delete_all ( path ) : ppath = os . getcwd ( ) os . chdir ( path ) for fn in glob . glob ( "" * "" ) : fn_full = os . path . join ( path , fn ) if os . path . isdir ( fn ) : delete_all ( fn_full ) elif fn . endswith ( "" .png "" ) : os . remove ( fn_full ) <MASK> os . remove ( fn_full ) elif DELETE_ALL_OLD : os . remove ( fn_full ) os . chdir ( ppath ) os . rmdir ( path )","elif fn . endswith ( "".md"" ) :","elif fn . endswith ( "".png"" ) :",98.82942566,FALSE,98.04
1388,"def reward ( self ) : """"""Returns a tuple of sum of raw and processed rewards."""""" raw_rewards , processed_rewards = 0 , 0 for ts in self . time_steps : # NOTE: raw_reward and processed_reward are None for the first time-step. if ts . raw_reward is not None : raw_rewards + = ts . raw_reward <MASK> processed_rewards + = ts . processed_reward return raw_rewards , processed_rewards",if ts . processed_reward is not None :,if ts . processed_reward is not None :,100,TRUE,100
1389,"def formatmonthname ( self , theyear , themonth , withyear = True ) : with TimeEncoding ( self . locale ) as encoding : s = month_name [ themonth ] <MASK> s = s . decode ( encoding ) if withyear : s = "" %s   %s "" % ( s , theyear ) return ' <tr><th colspan= "" 7 ""  class= "" month "" > %s </th></tr> ' % s",if encoding is not None :,if encoding :,91.704628,FALSE,95.11
1390,"def check_digest_auth ( user , passwd ) : """"""Check user authentication using HTTP Digest auth"""""" if request . headers . get ( "" Authorization "" ) : credentails = parse_authorization_header ( request . headers . get ( "" Authorization "" ) ) <MASK> return response_hash = response ( credentails , passwd , dict ( uri = request . script_root + request . path , body = request . data , method = request . method , ) , ) if credentails . get ( "" response "" ) == response_hash : return True return False",if not credentails :,if not credentails :,100,TRUE,100
1391,"def wrapped ( self , request ) : try : return self . _finished except AttributeError : <MASK> if not request . session . shouldfail and not request . session . shouldstop : log . debug ( "" %s  is still going to be used, not terminating it.  "" "" Still in use on: \n %s "" , self , pprint . pformat ( list ( self . node_ids ) ) , ) return log . debug ( "" Finish called on  %s "" , self ) try : return func ( request ) finally : self . _finished = True",if self . node_ids :,if self . _finished :,96.42521765,FALSE,96.34
1392,"def run_tests ( ) : # type: () -> None x = 5 with switch ( x ) as case : if case ( 0 ) : print ( "" zero "" ) print ( "" zero "" ) elif case ( 1 , 2 ) : print ( "" one or two "" ) <MASK> print ( "" three or four "" ) else : print ( "" default "" ) print ( "" another "" )","elif case ( 3 , 4 ) :","elif case ( 3 , 4 ) :",75,TRUE,100
1393,"def task_done ( self ) : with self . _cond : <MASK> raise ValueError ( "" task_done() called too many times "" ) if self . _unfinished_tasks . _semlock . _is_zero ( ) : self . _cond . notify_all ( )",if not self . _unfinished_tasks . acquire ( False ) :,if self . _unfinished_tasks . _semlock . _is_zero ( ),83.40194142,FALSE,83.96
1394,"def _set_uid ( self , val ) : if val is not None : <MASK> self . bus . log ( "" pwd module not available; ignoring uid. "" , level = 30 ) val = None elif isinstance ( val , text_or_bytes ) : val = pwd . getpwnam ( val ) [ 2 ] self . _uid = val",if pwd is None :,if pwd is None :,100,TRUE,100
1395,"def process_tag ( hive_name , company , company_key , tag , default_arch ) : with winreg . OpenKeyEx ( company_key , tag ) as tag_key : version = load_version_data ( hive_name , company , tag , tag_key ) if version is not None : # if failed to get version bail major , minor , _ = version arch = load_arch_data ( hive_name , company , tag , tag_key , default_arch ) if arch is not None : exe_data = load_exe ( hive_name , company , company_key , tag ) <MASK> exe , args = exe_data return company , major , minor , arch , exe , args",if exe_data is not None :,if exe_data is not None :,100,TRUE,100
1396,"def run ( algs ) : for alg in algs : vcs = alg . get ( "" variantcaller "" ) if vcs : if isinstance ( vcs , dict ) : vcs = reduce ( operator . add , vcs . values ( ) ) <MASK> vcs = [ vcs ] return any ( vc . startswith ( prefix ) for vc in vcs if vc )","if not isinstance ( vcs , ( list , tuple ) ) :","elif isinstance ( vcs , list ) :",87.77096673,FALSE,87.39
1397,"def wrapper ( self , * args , * * kwargs ) : if not self . request . path . endswith ( "" / "" ) : <MASK> uri = self . request . path + "" / "" if self . request . query : uri + = "" ? "" + self . request . query self . redirect ( uri , permanent = True ) return raise HTTPError ( 404 ) return method ( self , * args , * * kwargs )","if self . request . method in ( ""GET"" , ""HEAD"" ) :","if self . request . method == ""GET"" :",93.85928723,FALSE,90.05
1398,"def check_response ( self , response ) : """"""Specialized version of check_response()."""""" for line in response : # Skip blank lines: <MASK> continue if line . startswith ( b "" OK "" ) : return elif line . startswith ( b "" Benutzer/Passwort Fehler "" ) : raise BadLogin ( line ) else : raise FailedPost ( "" Server returned  ' %s ' "" % six . ensure_text ( line ) )",if not line . strip ( ) :,if not line :,96.54251558,FALSE,94.26
1399,"def Walk ( self , hMenu = None ) : if not hMenu : hMenu = self . handle n = user32 . GetMenuItemCount ( hMenu ) mi = MENUITEMINFO ( ) for i in range ( n ) : mi . fMask = 2 #  MIIM_ID user32 . GetMenuItemInfoA ( hMenu , i , 1 , byref ( mi ) ) handle = user32 . GetSubMenu ( hMenu , i ) <MASK> yield handle , self . ListItems ( handle ) for i in self . Walk ( handle ) : yield i",if handle :,if self . IsOpen ( handle ) :,97.56770428,FALSE,92.83
1400,"def setSelection ( self , labels ) : input = self . __validateInput ( labels ) if len ( input ) == 0 and not self . __allowEmptySelection : return if self . __allowMultipleSelection : self . __selectedLabels [ : ] = input self . __selectionChanged ( ) else : <MASK> raise RuntimeError ( "" Parameter must be single item or a list with one element. "" ) else : self . __selectedLabels [ : ] = input self . __selectionChanged ( ) # Remove all selected labels that are not in the menu, emit signals if necessary and update the button. self . __validateState ( )",if len ( input ) > 1 :,"if not isinstance ( input , ( list , tuple ) ) :",88.85785208,FALSE,91.61
1401,"def _parse ( self , engine ) : """"""Parse the layer."""""" if isinstance ( self . args , dict ) : if "" axis "" in self . args : self . axis = engine . evaluate ( self . args [ "" axis "" ] , recursive = True ) if not isinstance ( self . axis , int ) : raise ParsingError ( ' "" axis ""  must be an integer. ' ) if "" momentum "" in self . args : self . momentum = engine . evaluate ( self . args [ "" momentum "" ] , recursive = True ) <MASK> raise ParsingError ( ' "" momentum ""  must be numeric. ' )","if not isinstance ( self . momentum , ( int , float ) ) :","if not isinstance ( self . momentum , int ) :",94.0778155,FALSE,95.53
1402,"def get_order ( self , aBuf ) : if not aBuf : return - 1 , 1 # find out current char's byte length first_char = wrap_ord ( aBuf [ 0 ] ) if ( 0x81 < = first_char < = 0x9F ) or ( 0xE0 < = first_char < = 0xFC ) : charLen = 2 else : charLen = 1 # return its order if it is hiragana if len ( aBuf ) > 1 : second_char = wrap_ord ( aBuf [ 1 ] ) <MASK> return second_char - 0x9F , charLen return - 1 , charLen",if ( first_char == 202 ) and ( 0x9F <= second_char <= 0xF1 ) :,if 0x81 <= second_char and 0xE0 <= second_char <= 0x,68.97806621,FALSE,88.19
1403,"def saveSpecial ( self , * * kwargs ) : for kw in SPECIAL_BOOL_LIST + SPECIAL_VALUE_LIST + SPECIAL_LIST_LIST : item = config . get_config ( "" misc "" , kw ) value = kwargs . get ( kw ) msg = item . set ( value ) <MASK> return badParameterResponse ( msg ) config . save_config ( ) raise Raiser ( self . __root )",if msg :,if msg :,100,TRUE,100
1404,"def sanitize_event_keys ( kwargs , valid_keys ) : # Sanity check: Don't honor keys that we don't recognize. for key in list ( kwargs . keys ( ) ) : <MASK> kwargs . pop ( key ) # Truncate certain values over 1k for key in [ "" play "" , "" role "" , "" task "" , "" playbook "" ] : if isinstance ( kwargs . get ( "" event_data "" , { } ) . get ( key ) , str ) : if len ( kwargs [ "" event_data "" ] [ key ] ) > 1024 : kwargs [ "" event_data "" ] [ key ] = Truncator ( kwargs [ "" event_data "" ] [ key ] ) . chars ( 1024 )",if key not in valid_keys :,if key not in valid_keys :,75,TRUE,100
1405,"def toggleFactorReload ( self , value = None ) : self . serviceFittingOptions [ "" useGlobalForceReload "" ] = ( value if value is not None else not self . serviceFittingOptions [ "" useGlobalForceReload "" ] ) fitIDs = set ( ) for fit in set ( self . _loadedFits ) : <MASK> continue if fit . calculated : fit . factorReload = self . serviceFittingOptions [ "" useGlobalForceReload "" ] fit . clearFactorReloadDependentData ( ) fitIDs . add ( fit . ID ) return fitIDs",if fit is None :,if not fit . enabled :,95.00949542,FALSE,94.16
1406,"def closest_unseen ( self , row1 , col1 , filter = None ) : # find the closest unseen from this row/col min_dist = maxint closest_unseen = None for row in range ( self . height ) : for col in range ( self . width ) : <MASK> if self . map [ row ] [ col ] == UNSEEN : dist = self . distance ( row1 , col1 , row , col ) if dist < min_dist : min_dist = dist closest_unseen = ( row , col ) return closest_unseen","if filter is None or ( row , col ) not in filter :","if filter is not None and filter ( row1 , col ) :",95.43339649,FALSE,92.52
1407,"def getAlphaClone ( lookfor , eager = None ) : if isinstance ( lookfor , int ) : <MASK> item = get_gamedata_session ( ) . query ( AlphaClone ) . get ( lookfor ) else : item = ( get_gamedata_session ( ) . query ( AlphaClone ) . options ( * processEager ( eager ) ) . filter ( AlphaClone . ID == lookfor ) . first ( ) ) else : raise TypeError ( "" Need integer as argument "" ) return item",if eager is None :,if eager is None :,100,TRUE,100
1408,"def _rle_encode ( string ) : new = b "" "" count = 0 for cur in string : if not cur : count + = 1 else : <MASK> new + = b "" \0 "" + bytes ( [ count ] ) count = 0 new + = bytes ( [ cur ] ) return new",if count :,if count :,100,TRUE,100
1409,def result_iterator ( ) : try : for future in fs : <MASK> yield future . result ( ) else : yield future . result ( end_time - time . time ( ) ) finally : for future in fs : future . cancel ( ),if timeout is None :,if end_time is None :,72.44054729,FALSE,91.67
1410,"def _individual_get ( self , segment , index_type , index , strictdoc ) : if index_type == "" val "" : for key , value in segment . items ( ) : <MASK> return value if hasattr ( key , "" text "" ) : if key . text == index [ 0 ] : return value raise Exception ( "" Invalid state "" ) elif index_type == "" index "" : return segment [ index ] elif index_type == "" textslice "" : return segment [ index [ 0 ] : index [ 1 ] ] elif index_type == "" key "" : return index [ 1 ] if strictdoc else index [ 0 ] else : raise Exception ( "" Invalid state "" )",if key == index [ 0 ] :,if key . startswith ( index [ 0 ] ) :,96.08889563,FALSE,95.39
1411,"def _reset_sequences ( self , db_name ) : conn = connections [ db_name ] if conn . features . supports_sequence_reset : sql_list = conn . ops . sequence_reset_by_name_sql ( no_style ( ) , conn . introspection . sequence_list ( ) ) <MASK> try : cursor = conn . cursor ( ) for sql in sql_list : cursor . execute ( sql ) except Exception : transaction . rollback_unless_managed ( using = db_name ) raise transaction . commit_unless_managed ( using = db_name )",if sql_list :,if sql_list :,100,TRUE,100
1412,"def translate_to_statements ( self , statements , conditional_write_vars ) : lines = [ ] for stmt in statements : <MASK> self . temporary_vars . add ( ( stmt . var , stmt . dtype ) ) line = self . translate_statement ( stmt ) if stmt . var in conditional_write_vars : subs = { } condvar = conditional_write_vars [ stmt . var ] lines . append ( "" if  %s : "" % condvar ) lines . append ( indent ( line ) ) else : lines . append ( line ) return lines","if stmt . op == "":="" and not stmt . var in self . variables :",if stmt . var not in self . temporary_vars :,87.87459962,FALSE,88.52
1413,"def _bytecode_filenames ( self , py_filenames ) : bytecode_files = [ ] for py_file in py_filenames : # Since build_py handles package data installation, the # list of outputs can contain more than just .py files. # Make sure we only report bytecode for the .py files. ext = os . path . splitext ( os . path . normcase ( py_file ) ) [ 1 ] if ext != PYTHON_SOURCE_EXTENSION : continue <MASK> bytecode_files . append ( py_file + "" c "" ) if self . optimize > 0 : bytecode_files . append ( py_file + "" o "" ) return bytecode_files",if self . compile :,if self . optimize > 0 :,98.36681416,FALSE,96.83
1414,"def logic ( ) : for i in range ( 100 ) : yield clock . posedge , reset . negedge if reset == ACTIVE_LOW : count . next = 0 else : <MASK> count . next = ( count + 1 ) % n raise StopSimulation",if enable :,if enable :,100,TRUE,100
1415,"def _is_subnet_of ( a , b ) : try : # Always false if one is v4 and the other is v6. <MASK> raise TypeError ( "" %s  and  %s  are not of the same version "" % ( a , b ) ) return ( b . network_address < = a . network_address and b . broadcast_address > = a . broadcast_address ) except AttributeError : raise TypeError ( "" Unable to test subnet containment  "" "" between  %s  and  %s "" % ( a , b ) )",if a . _version != b . _version :,if a . version != b . version :,72.42734716,FALSE,95.71
1416,"def _filter_paths ( basename , path , is_dir , exclude ) : """""".gitignore style file filtering."""""" for item in exclude : # Items ending in '/' apply only to directories. if item . endswith ( "" / "" ) and not is_dir : continue # Items starting with '/' apply to the whole path. # In any other cases just the basename is used. match = path if item . startswith ( "" / "" ) else basename <MASK> return True return False","if fnmatch . fnmatch ( match , item . strip ( ""/"" ) ) :","if match in _filter_paths ( basename , item , match ) :",93.74440156,FALSE,88.03
1417,"def __recv_null ( self ) : """"""Receive a null byte."""""" while 1 : c = self . sock . recv ( 1 ) if c == "" "" : self . close ( ) raise EOFError ( "" Socket Closed "" ) <MASK> return","if c == ""\0"" :","if c == b"""" :",97.59194971,FALSE,93.15
1418,"def onMessage ( self , payload , isBinary ) : if isBinary : self . result = "" Expected text message with payload, but got binary. "" else : <MASK> self . result = ( "" Expected text message with payload of length  %d , but got  %d . "" % ( self . DATALEN , len ( payload ) ) ) else : ## FIXME : check actual content ## self . behavior = Case . OK self . result = "" Received text message of length  %d . "" % len ( payload ) self . p . createWirelog = True self . p . sendClose ( self . p . CLOSE_STATUS_CODE_NORMAL )",if len ( payload ) != self . DATALEN :,if len ( payload ) != self . DATALEN :,100,TRUE,100
1419,"def rename_path ( self , path , new_path ) : logger . debug ( "" rename_path  ' %s '  ->  ' %s ' "" % ( path , new_path ) ) dirs = self . readdir ( path ) for d in dirs : <MASK> continue d_path = "" "" . join ( [ path , "" / "" , d ] ) d_new_path = "" "" . join ( [ new_path , "" / "" , d ] ) attr = self . getattr ( d_path ) if stat . S_ISDIR ( attr [ "" st_mode "" ] ) : self . rename_path ( d_path , d_new_path ) else : self . rename_item ( d_path , d_new_path ) self . rename_item ( path , new_path , dir = True )","if d in [ ""."" , "".."" ] :","if d == ""__init__.py"" :",88.82605248,FALSE,93.72
1420,"def dir_box_click ( self , double ) : if double : name = self . list_box . get_selected_name ( ) path = os . path . join ( self . directory , name ) suffix = os . path . splitext ( name ) [ 1 ] <MASK> self . directory = path else : self . double_click_file ( name ) self . update ( )",if suffix not in self . suffixes and os . path . isdir ( path ) :,"if suffix in ( "".py"" , "".py"" ) :",78.83838059,FALSE,85.64
1421,"def __getattr__ ( self , key ) : try : value = self . __parent . contents [ key ] except KeyError : pass else : if value is not None : <MASK> return value . mod_ns else : assert isinstance ( value , _MultipleClassMarker ) return value . attempt_get ( self . __parent . path , key ) raise AttributeError ( "" Module  %r  has no mapped classes  "" "" registered under the name  %r "" % ( self . __parent . name , key ) )","if isinstance ( value , _ModuleMarker ) :","if isinstance ( value , Module ) :",98.67688914,FALSE,96.85
1422,"def poll_thread ( ) : time . sleep ( 0.5 ) if process . wait ( ) and process_state : time . sleep ( 0.25 ) <MASK> stdout , stderr = process . _communicate ( None ) logger . error ( "" Web server process exited unexpectedly "" , "" app "" , stdout = stdout , stderr = stderr , ) time . sleep ( 1 ) restart_server ( 1 )",if not check_global_interrupt ( ) :,if process . _communicate :,90.601199,FALSE,89.24
1423,"def apply_dateparser_timezone ( utc_datetime , offset_or_timezone_abb ) : for name , info in _tz_offsets : <MASK> tz = StaticTzInfo ( name , info [ "" offset "" ] ) return utc_datetime . astimezone ( tz )","if info [ ""regex"" ] . search ( "" %s"" % offset_or_timezone_abb ) :",if offset_or_timezone_abb and name in offset_or_timezone_abb :,70.65622907,FALSE,75.64
1424,"def _load_wordlist ( filename ) : if filename is None : return { } path = None for dir in ( CONFIG_DIR , ASSETS_DIR ) : path = os . path . realpath ( os . path . join ( dir , filename ) ) <MASK> break words = { } with open ( path , encoding = "" utf-8 "" ) as f : pairs = [ word . strip ( ) . rsplit ( ""   "" , 1 ) for word in f ] pairs . sort ( reverse = True , key = lambda x : int ( x [ 1 ] ) ) words = { p [ 0 ] : int ( p [ 1 ] ) for p in pairs } return words",if os . path . exists ( path ) :,if not os . path . exists ( path ) :,98.09373524,FALSE,98.26
1425,"def terminate_processes_matching_names ( match_strings , kill = False ) : """"""Terminates processes matching particular names (case sensitive)."""""" if isinstance ( match_strings , str ) : match_strings = [ match_strings ] for process in psutil . process_iter ( ) : try : process_info = process . as_dict ( attrs = [ "" name "" , "" pid "" ] ) process_name = process_info [ "" name "" ] except ( psutil . AccessDenied , psutil . NoSuchProcess , OSError ) : continue <MASK> terminate_process ( process_info [ "" pid "" ] , kill )",if any ( x == process_name for x in match_strings ) :,if process_name in match_strings :,89.91525068,FALSE,91.21
1426,"def has_scheme ( self , inp ) : if "" :// "" in inp : return True else : authority = inp . replace ( "" / "" , "" # "" ) . replace ( "" ? "" , "" # "" ) . split ( "" # "" ) [ 0 ] <MASK> _ , host_or_port = authority . split ( "" : "" , 1 ) # Assert it's not a port number if re . match ( r "" ^ \ d+$ "" , host_or_port ) : return False else : return False return True","if "":"" in authority :","if "":"" in authority :",100,TRUE,100
1427,"def close ( self ) : with BrowserContext . _BROWSER_LOCK : BrowserContext . _BROWSER_REFCNT - = 1 <MASK> logger . info ( "" Destroying browser main loop "" ) BrowserContext . _BROWSER_LOOP . destroy ( ) BrowserContext . _BROWSER_LOOP = None",if BrowserContext . _BROWSER_REFCNT == 0 :,if BrowserContext . _BROWSER_REFCNT == 0 :,100,TRUE,100
1428,"def _mock_get_merge_ticks ( self , order_book_id_list , trading_date , last_dt = None ) : for tick in self . _ticks : <MASK> continue if ( self . env . data_proxy . get_future_trading_date ( tick . datetime ) . date ( ) != trading_date . date ( ) ) : continue if last_dt and tick . datetime < = last_dt : continue yield tick",if tick . order_book_id not in order_book_id_list :,if not tick . order_book_id_list :,79.66666081,FALSE,92.51
1429,"def messageSourceStamps ( self , source_stamps ) : text = "" "" for ss in source_stamps : source = "" "" <MASK> source + = "" [branch  %s ]  "" % ss [ "" branch "" ] if ss [ "" revision "" ] : source + = str ( ss [ "" revision "" ] ) else : source + = "" HEAD "" if ss [ "" patch "" ] is not None : source + = ""  (plus patch) "" discriminator = "" "" if ss [ "" codebase "" ] : discriminator = ""   ' %s ' "" % ss [ "" codebase "" ] text + = "" Build Source Stamp %s :  %s \n "" % ( discriminator , source ) return text","if ss [ ""branch"" ] :","if ss [ ""branch"" ] is not None :",98.43487386,FALSE,96.98
1430,"def test_open_read_bytes ( self , sftp ) : """"""Test reading bytes from a file"""""" f = None try : self . _create_file ( "" file "" , "" xxx "" ) f = yield from sftp . open ( "" file "" , "" rb "" ) self . assertEqual ( ( yield from f . read ( ) ) , b "" xxx "" ) finally : <MASK> # pragma: no branch yield from f . close ( ) remove ( "" file "" )",if f :,if f :,100,TRUE,100
1431,"def handler ( chan , host , port ) : sock = socket ( ) try : sock . connect ( ( host , port ) ) except Exception as e : if verbose == True : print ( e ) return while True : r , w , x = select . select ( [ sock , chan ] , [ ] , [ ] ) if sock in r : data = sock . recv ( 1024 ) if len ( data ) == 0 : break chan . send ( data ) <MASK> data = chan . recv ( 1024 ) if len ( data ) == 0 : break sock . send ( data ) chan . close ( ) sock . close ( )",if chan in r :,if chan in r :,100,TRUE,100
1432,"def detect ( get_page ) : retval = False for vector in WAF_ATTACK_VECTORS : page , headers , code = get_page ( get = vector ) retval = re . search ( r "" url \ ( ' /ks-waf-error \ .png ' \ ) "" , page , re . I ) is not None <MASK> break return retval",if retval :,if retval :,100,TRUE,100
1433,"def __init__ ( self , raw ) : ticker_ticks = { } for tick in raw [ "" results "" ] : <MASK> ticker_ticks [ tick [ "" T "" ] ] . append ( tick ) else : ticker_ticks [ tick [ "" T "" ] ] = [ tick ] super ( ) . __init__ ( { ticker : Aggsv2 ( { "" results "" : ticks } ) for ticker , ticks in ticker_ticks . items ( ) } )","if ticker_ticks . get ( tick [ ""T"" ] ) :","if tick [ ""T"" ] in ticker_ticks :",92.335081,FALSE,92.29
1434,"def _makefiles ( self , f ) : if isinstance ( f , dict ) : for k , v in list ( f . items ( ) ) : <MASK> self . makedir ( dirname = k , content = v ) elif isinstance ( v , str ) : self . make_file ( filename = k , content = v ) else : # pragma: nocover raise ValueError ( "" Unexpected: "" , k , v ) elif isinstance ( f , str ) : self . _make_empty_file ( f ) elif isinstance ( f , list ) : self . make_list ( f ) else : # pragma: nocover raise ValueError ( "" Unknown type: "" , f )","if isinstance ( v , list ) :","if isinstance ( v , ( list , tuple ) ) :",71.61795104,FALSE,95.69
1435,"def migrate_command_storage ( apps , schema_editor ) : model = apps . get_model ( "" terminal "" , "" CommandStorage "" ) init_storage_data ( model ) setting = get_setting ( apps , schema_editor , "" TERMINAL_COMMAND_STORAGE "" ) if not setting : return values = get_storage_data ( setting ) for name , meta in values . items ( ) : tp = meta . pop ( "" TYPE "" ) <MASK> continue model . objects . create ( name = name , type = tp , meta = meta )","if not tp or name in [ ""default"" , ""null"" ] :",if tp is None :,89.30079278,FALSE,87.76
1436,"def build_vertices ( self , ulines ) : vertex_idx = 0 vertices = collections . OrderedDict ( ) for line in ulines : for vt in line : <MASK> continue new_vertex = ( vt . u , vt . v , 0.0 ) if new_vertex in vertices : continue vt . index = vertex_idx vertex_idx + = 1 vertices [ new_vertex ] = 1 return vertex_idx , list ( vertices . keys ( ) )",if vt . replacement is not None :,if not vt . index :,88.61628962,FALSE,93.22
1437,"def get_quarantine_count ( self ) : """"""get obj/container/account quarantine counts"""""" qcounts = { "" objects "" : 0 , "" containers "" : 0 , "" accounts "" : 0 } qdir = "" quarantined "" for device in os . listdir ( self . devices ) : for qtype in qcounts : qtgt = os . path . join ( self . devices , device , qdir , qtype ) if os . path . exists ( qtgt ) : linkcount = os . lstat ( qtgt ) . st_nlink <MASK> qcounts [ qtype ] + = linkcount - 2 return qcounts",if linkcount > 2 :,if linkcount > 2 :,100,TRUE,100
1438,"def _format_arg ( self , name , trait_spec , value ) : if name == "" mask_file "" : return "" "" if name == "" op_string "" : <MASK> if isdefined ( self . inputs . mask_file ) : return self . inputs . op_string % self . inputs . mask_file else : raise ValueError ( "" -k  %s  option in op_string requires mask_file "" ) return super ( ImageStats , self ) . _format_arg ( name , trait_spec , value )","if ""-k %s"" in self . inputs . op_string :",if self . inputs :,90.68546709,FALSE,89.12
1439,"def _update_theme_style ( self , * args ) : self . line_color_normal = self . theme_cls . divider_color if not any ( [ self . error , self . _text_len_error ] ) : if not self . focus : self . _current_hint_text_color = self . theme_cls . disabled_hint_text_color self . _current_right_lbl_color = self . theme_cls . disabled_hint_text_color <MASK> self . _current_error_color = self . theme_cls . disabled_hint_text_color","if self . helper_text_mode == ""persistent"" :",if self . error :,91.61722615,FALSE,91.61
1440,"def createFields ( self ) : for item in self . format : <MASK> yield item [ 0 ] ( self , * item [ 1 : - 1 ] , * * item [ - 1 ] ) else : yield item [ 0 ] ( self , * item [ 1 : ] )","if isinstance ( item [ - 1 ] , dict ) :",if len ( item ) > 1 :,63.65532957,FALSE,84.13
1441,"def execute ( self , statement , arguments = None ) : while True : try : <MASK> self . cursor . execute ( statement , arguments ) else : self . cursor . execute ( statement ) except sqlite3 . OperationalError as ex : if "" locked "" not in getSafeExString ( ex ) : raise else : break if statement . lstrip ( ) . upper ( ) . startswith ( "" SELECT "" ) : return self . cursor . fetchall ( )",if arguments :,if arguments :,100,TRUE,100
1442,"def set_income_account_for_fixed_assets ( self ) : disposal_account = depreciation_cost_center = None for d in self . get ( "" items "" ) : <MASK> if not disposal_account : ( disposal_account , depreciation_cost_center , ) = get_disposal_account_and_cost_center ( self . company ) d . income_account = disposal_account if not d . cost_center : d . cost_center = depreciation_cost_center",if d . is_fixed_asset :,if d . income_account :,96.5187623,FALSE,94.28
1443,"def _convertNbCharsInNbBits ( self , nbChars ) : nbMinBit = None nbMaxBit = None if nbChars is not None : if isinstance ( nbChars , int ) : nbMinBit = nbChars * 8 nbMaxBit = nbMinBit else : if nbChars [ 0 ] is not None : nbMinBit = nbChars [ 0 ] * 8 <MASK> nbMaxBit = nbChars [ 1 ] * 8 return ( nbMinBit , nbMaxBit )",if nbChars [ 1 ] is not None :,if nbChars [ 1 ] is not None :,100,TRUE,100
1444,"def _get_service_full_name ( self , name , help_command_table ) : if help_command_table and name not in self . _NON_SERVICE_COMMANDS : <MASK> return self . _HIGH_LEVEL_SERVICE_FULL_NAMES [ name ] service = help_command_table . get ( name ) if service : return service . service_model . metadata [ "" serviceFullName "" ]",if name in self . _HIGH_LEVEL_SERVICE_FULL_NAMES :,if name in self . _HIGH_LEVEL_SERVICE_FULL_NAMES :,100,TRUE,100
1445,"def print_addresses ( self ) : p = 3 tmp_str = "" [ "" if self . get_len ( ) > = 7 : # at least one complete IP address while 1 : <MASK> tmp_str + = "" # "" tmp_str + = self . get_ip_address ( p ) p + = 4 if p > = self . get_len ( ) : break else : tmp_str + = "" ,  "" tmp_str + = "" ]  "" if self . get_ptr ( ) % 4 : # ptr field should be a multiple of 4 tmp_str + = "" nonsense ptr field:  %d   "" % self . get_ptr ( ) return tmp_str",if p + 1 == self . get_ptr ( ) :,if self . get_ptr ( ) % 4 == 0 :,97.2556644,FALSE,95.3
1446,"def run ( self ) : for _ in range ( self . n ) : error = True try : self . collection . insert_one ( { "" test "" : "" insert "" } ) error = False except : if not self . expect_exception : raise <MASK> assert error",if self . expect_exception :,if self . expect_test :,93.55548443,FALSE,95.81
1447,"def create_composite_mounter_by_args ( args ) : """"""Creates a CompositeMounter by the images in given args."""""" logging . info ( "" Mount images... "" ) mounter = composite_mounter . CompositeMounter ( ) for partition in composite_mounter . SUPPORTED_PARTITIONS : image_source = vars ( args ) [ partition ] <MASK> logging . info ( ""    %s = %s "" , partition , image_source ) mounter . add_by_mount_target ( partition , image_source ) if mounter . is_empty ( ) : raise RuntimeError ( "" Must give at least one image source. "" ) return mounter",if image_source :,if image_source :,100,TRUE,100
1448,"def _get_containing_class ( self , pyname ) : if isinstance ( pyname , pynames . DefinedName ) : scope = pyname . get_object ( ) . get_scope ( ) parent = scope . parent <MASK> return parent . pyobject","if parent is not None and parent . get_kind ( ) == ""Class"" :","if parent and isinstance ( parent . pyobject , pynames . ClassType ) :",82.69628243,FALSE,74.84
1449,"def test_chunkcoding ( self ) : tstring_lines = [ ] for b in self . tstring : lines = b . split ( b "" \n "" ) last = lines . pop ( ) assert last == b "" "" lines = [ line + b "" \n "" for line in lines ] tstring_lines . append ( lines ) for native , utf8 in zip ( * tstring_lines ) : u = self . decode ( native ) [ 0 ] self . assertEqual ( u , utf8 . decode ( "" utf-8 "" ) ) <MASK> self . assertEqual ( native , self . encode ( u ) [ 0 ] )",if self . roundtriptest :,if PY3 :,95.9815494,FALSE,96.49
1450,"def set_default_variants ( apps , schema_editor ) : Product = apps . get_model ( "" product "" , "" Product "" ) for product in Product . objects . iterator ( ) : first_variant = product . variants . first ( ) <MASK> product . default_variant = first_variant product . save ( update_fields = [ "" default_variant "" , "" updated_at "" ] )",if first_variant :,if first_variant is not None :,86.65727668,FALSE,94.95
1451,"def json ( self ) : try : if self . is_json ( ) : raw_data = self . raw_data ( ) <MASK> raw_data = raw_data . decode ( "" utf-8 "" ) return json . loads ( raw_data ) except ValueError : pass","if not isinstance ( raw_data , text_type ) :","if isinstance ( raw_data , bytes ) :",90.69761922,FALSE,89.35
1452,"def clear_react ( self , message : discord . Message , emoji : MutableMapping = None ) - > None : try : await message . clear_reactions ( ) except discord . Forbidden : <MASK> return with contextlib . suppress ( discord . HTTPException ) : async for key in AsyncIter ( emoji . values ( ) , delay = 0.2 ) : await message . remove_reaction ( key , self . bot . user ) except discord . HTTPException : return",if not emoji :,if self . bot . user is None :,66.78227611,FALSE,90.98
1453,"def check ( self , value ) : value = String . check ( self , value ) if isinstance ( value , str ) : value = value . upper ( ) for prefix in ( self . prefix , self . prefix . split ( "" _ "" , 1 ) [ 1 ] ) : # e.g. PANGO_WEIGHT_BOLD --> BOLD but also WEIGHT_BOLD --> BOLD if value . startswith ( prefix ) : value = value [ len ( prefix ) : ] value = value . lstrip ( "" _ "" ) <MASK> return getattr ( self . group , value ) else : raise ValueError ( "" No such constant:  %s _ %s "" % ( self . prefix , value ) ) else : return value","if hasattr ( self . group , value ) :","if hasattr ( self . group , value ) :",100,TRUE,100
1454,"def value ( self ) : quote = False if self . defects : quote = True else : for x in self : if x . token_type == "" quoted-string "" : quote = True if quote : pre = post = "" "" if self [ 0 ] . token_type == "" cfws "" or self [ 0 ] [ 0 ] . token_type == "" cfws "" : pre = ""   "" <MASK> post = ""   "" return pre + quote_string ( self . display_name ) + post else : return super ( DisplayName , self ) . value","if self [ - 1 ] . token_type == ""cfws"" or self [ - 1 ] [ - 1 ] . token_type == ""cfws"" :","if self [ 0 ] . token_type == ""cfws"" or self [ 0 ]",66.93973205,FALSE,84.75
1455,"def get_drive ( self , root_path = "" "" , volume_guid_path = "" "" ) : for drive in self . drives : if root_path : config_root_path = drive . get ( "" root_path "" ) if config_root_path and root_path == config_root_path : return drive <MASK> config_volume_guid_path = drive . get ( "" volume_guid_path "" ) if config_volume_guid_path and config_volume_guid_path == volume_guid_path : return drive",elif volume_guid_path :,if volume_guid_path :,95.60970361,FALSE,97.96
1456,"def parse_edges ( self , pcb ) : edges = [ ] drawings = list ( pcb . GetDrawings ( ) ) bbox = None for m in pcb . GetModules ( ) : for g in m . GraphicalItems ( ) : drawings . append ( g ) for d in drawings : if d . GetLayer ( ) == pcbnew . Edge_Cuts : parsed_drawing = self . parse_drawing ( d ) <MASK> edges . append ( parsed_drawing ) if bbox is None : bbox = d . GetBoundingBox ( ) else : bbox . Merge ( d . GetBoundingBox ( ) ) if bbox : bbox . Normalize ( ) return edges , bbox",if parsed_drawing :,if parsed_drawing is not None :,98.38244099,FALSE,96.59
1457,"def to_key ( literal_or_identifier ) : """"""returns string representation of this object"""""" if literal_or_identifier [ "" type "" ] == "" Identifier "" : return literal_or_identifier [ "" name "" ] elif literal_or_identifier [ "" type "" ] == "" Literal "" : k = literal_or_identifier [ "" value "" ] if isinstance ( k , float ) : return unicode ( float_repr ( k ) ) elif "" regex "" in literal_or_identifier : return compose_regex ( k ) elif isinstance ( k , bool ) : return "" true "" if k else "" false "" <MASK> return "" null "" else : return unicode ( k )",elif k is None :,elif k is None :,100,TRUE,100
1458,"def find_multiple_stats ( stats , name , _found = None , _on_found = None ) : if _found is None : _found = [ ] for child_stats in stats : if child_stats . name == name : _found . append ( child_stats ) <MASK> _on_found ( _found ) find_multiple_stats ( child_stats , name , _found ) return _found",if callable ( _on_found ) :,if _on_found is not None :,92.86658851,FALSE,93.59
1459,"def _run_generated_code ( self , code , globs , locs , fails_under_py3k = True , ) : import warnings from zope . interface . _compat import PYTHON3 with warnings . catch_warnings ( record = True ) as log : warnings . resetwarnings ( ) <MASK> exec ( code , globs , locs ) self . assertEqual ( len ( log ) , 0 ) # no longer warn return True else : try : exec ( code , globs , locs ) except TypeError : return False else : if fails_under_py3k : self . fail ( "" Didn ' t raise TypeError "" )",if not PYTHON3 :,if PYTHON3 :,95.12937292,FALSE,97.94
1460,"def _get_node ( self , node_id ) : self . non_terminated_nodes ( { } ) # Side effect: updates cache with self . lock : <MASK> return self . cached_nodes [ node_id ] instance = ( self . compute . instances ( ) . get ( project = self . provider_config [ "" project_id "" ] , zone = self . provider_config [ "" availability_zone "" ] , instance = node_id , ) . execute ( ) ) return instance",if node_id in self . cached_nodes :,if node_id in self . cached_nodes :,100,TRUE,100
1461,"def skip_to_close_match ( self ) : nestedCount = 1 while 1 : tok = self . tokenizer . get_next_token ( ) ttype = tok [ "" style "" ] if ttype == SCE_PL_UNUSED : return elif self . classifier . is_index_op ( tok ) : tval = tok [ "" text "" ] <MASK> if self . opHash [ tval ] [ 1 ] == 1 : nestedCount + = 1 else : nestedCount - = 1 if nestedCount < = 0 : break",if self . opHash . has_key ( tval ) :,if tval in self . opHash :,94.75934566,FALSE,91.32
1462,"def _create_or_get_helper ( self , infer_mode : Optional [ bool ] = None , * * kwargs ) - > Helper : # Prefer creating a new helper when at least one kwarg is specified. prefer_new = len ( kwargs ) > 0 kwargs . update ( infer_mode = infer_mode ) is_training = not infer_mode if infer_mode is not None else self . training helper = self . _train_helper if is_training else self . _infer_helper if prefer_new or helper is None : helper = self . create_helper ( * * kwargs ) if is_training and self . _train_helper is None : self . _train_helper = helper <MASK> self . _infer_helper = helper return helper",elif not is_training and self . _infer_helper is None :,if infer_mode and self . _infer_helper is None :,72.98772845,FALSE,96.35
1463,"def get_ldset ( self , ldsets ) : ldset = None if self . _properties [ "" ldset_name "" ] == "" "" : nldset = len ( ldsets ) if nldset == 0 : msg = _ ( "" Logical Disk Set could not be found. "" ) raise exception . NotFound ( msg ) else : ldset = None else : <MASK> msg = ( _ ( "" Logical Disk Set ` %s ` could not be found. "" ) % self . _properties [ "" ldset_name "" ] ) raise exception . NotFound ( msg ) ldset = ldsets [ self . _properties [ "" ldset_name "" ] ] return ldset","if self . _properties [ ""ldset_name"" ] not in ldsets :","if self . _properties [ ""ldset_name"" ] not in ldsets :",100,TRUE,100
1464,"def calc_fractal_serial ( q , maxiter ) : # calculate z using pure python on a numpy array # note that, unlike the other two implementations, # the number of iterations per point is NOT constant z = np . zeros ( q . shape , complex ) output = np . resize ( np . array ( 0 , ) , q . shape , ) for i in range ( len ( q ) ) : for iter in range ( maxiter ) : z [ i ] = z [ i ] * z [ i ] + q [ i ] <MASK> output [ i ] = iter break return output",if abs ( z [ i ] ) > 2.0 :,if z [ i ] == 0 :,71.79764769,FALSE,93.8
1465,"def _verifySubs ( self ) : for inst in self . subs : if not isinstance ( inst , ( _Block , _Instantiator , Cosimulation ) ) : raise BlockError ( _error . ArgType % ( self . name , ) ) <MASK> if not inst . modctxt : raise BlockError ( _error . InstanceError % ( self . name , inst . callername ) )","if isinstance ( inst , ( _Block , _Instantiator ) ) :","if isinstance ( inst , _Instance ) :",91.63614461,FALSE,90.18
1466,"def walks_generator ( ) : if filelist is not None : bucket = [ ] for filename in filelist : with io . open ( filename ) as inf : for line in inf : walk = [ int ( x ) for x in line . strip ( "" \n "" ) . split ( ""   "" ) ] bucket . append ( walk ) <MASK> yield bucket bucket = [ ] if len ( bucket ) : yield bucket else : for _ in range ( epoch ) : for nodes in graph . node_batch_iter ( batch_size ) : walks = graph . random_walk ( nodes , walk_len ) yield walks",if len ( bucket ) == batch_size :,if len ( bucket ) == batch_size :,100,TRUE,100
1467,def _traverse ( op ) : if op in visited : return visited . add ( op ) if tag . is_injective ( op . tag ) : if op not in s . outputs : s [ op ] . compute_inline ( ) for tensor in op . input_tensors : <MASK> _traverse ( tensor . op ) callback ( op ),"if isinstance ( tensor . op , tvm . te . ComputeOp ) :",if tensor . is_injective :,84.90246197,FALSE,84.09
1468,"def unwatch_run ( self , run_id , handler ) : with self . _dict_lock : <MASK> self . _handlers_dict [ run_id ] = [ ( start_cursor , callback ) for ( start_cursor , callback ) in self . _handlers_dict [ run_id ] if callback != handler ] if not self . _handlers_dict [ run_id ] : del self . _handlers_dict [ run_id ] run_id_dict = self . _run_id_dict del run_id_dict [ run_id ] self . _run_id_dict = run_id_dict",if run_id in self . _run_id_dict :,if run_id not in self . _handlers_dict :,92.58901945,FALSE,95.86
1469,"def _PromptMySQL ( self , config ) : """"""Prompts the MySQL configuration, retrying if the configuration is invalid."""""" while True : self . _PromptMySQLOnce ( config ) <MASK> print ( "" Successfully connected to MySQL with the given configuration. "" ) return else : print ( "" Error: Could not connect to MySQL with the given configuration. "" ) retry = RetryBoolQuestion ( "" Do you want to retry MySQL configuration? "" , True ) if not retry : raise ConfigInitError ( )",if self . _CheckMySQLConnection ( ) :,if self . _Connect ( config ) :,93.19225485,FALSE,95.87
1470,"def get_courses_without_topic ( topic ) : data = [ ] for entry in frappe . db . get_all ( "" Course "" ) : course = frappe . get_doc ( "" Course "" , entry . name ) topics = [ t . topic for t in course . topics ] <MASK> data . append ( course . name ) return data",if not topics or topic not in topics :,if topic not in topics :,92.42158824,FALSE,94.24
1471,"def _error_handler ( action , * * keywords ) : if keywords : file_type = keywords . get ( "" file_type "" , None ) if file_type : raise exceptions . FileTypeNotSupported ( constants . FILE_TYPE_NOT_SUPPORTED_FMT % ( file_type , action ) ) else : <MASK> keywords . pop ( "" on_demand "" ) msg = "" Please check if there were typos in  "" msg + = "" function parameters:  %s . Otherwise  "" msg + = "" unrecognized parameters were given. "" raise exceptions . UnknownParameters ( msg % keywords ) else : raise exceptions . UnknownParameters ( "" No parameters found! "" )","if ""on_demand"" in keywords :","if ""on_demand"" in keywords :",100,TRUE,100
1472,"def select ( self , regions , register ) : self . view . sel ( ) . clear ( ) to_store = [ ] for r in regions : self . view . sel ( ) . add ( r ) <MASK> to_store . append ( self . view . substr ( self . view . full_line ( r ) ) ) <MASK> text = "" "" . join ( to_store ) if not text . endswith ( "" \n "" ) : text = text + "" \n "" state = State ( self . view ) state . registers [ register ] = [ text ]",if register :,if to_store :,96.09120458,FALSE,92.79
1473,"def has_actor ( self , message : HasActorMessage ) - > ResultMessage : actor_ref = message . actor_ref # lookup allocated for address , item in self . _allocated_actors . items ( ) : ref = create_actor_ref ( address , actor_ref . uid ) <MASK> return ResultMessage ( message . message_id , True , protocol = message . protocol ) return ResultMessage ( message . message_id , False , protocol = message . protocol )",if ref in item :,if ref and ref . is_active :,96.95854909,FALSE,92.52
1474,"def toggleMetaButton ( self , event ) : """"""Process clicks on toggle buttons"""""" clickedBtn = event . EventObject if wx . GetMouseState ( ) . GetModifiers ( ) == wx . MOD_CONTROL : activeBtns = [ btn for btn in self . metaButtons if btn . GetValue ( ) ] <MASK> clickedBtn . setUserSelection ( clickedBtn . GetValue ( ) ) self . itemView . filterItemStore ( ) else : # Do 'nothing' if we're trying to turn last active button off # Keep button in the same state clickedBtn . setUserSelection ( True ) else : for btn in self . metaButtons : btn . setUserSelection ( btn == clickedBtn ) self . itemView . filterItemStore ( )",if activeBtns :,if activeBtns :,100,TRUE,100
1475,"def __init__ ( self , hub = None ) : # pylint: disable=unused-argument if resolver . _resolver is None : _resolver = resolver . _resolver = _DualResolver ( ) if config . resolver_nameservers : _resolver . network_resolver . nameservers [ : ] = config . resolver_nameservers <MASK> _resolver . network_resolver . lifetime = config . resolver_timeout # Different hubs in different threads could be sharing the same # resolver. assert isinstance ( resolver . _resolver , _DualResolver ) self . _resolver = resolver . _resolver",if config . resolver_timeout :,if config . resolver_timeout :,100,TRUE,100
1476,"def sub_paragraph ( self , li ) : """"""Search for checkbox in sub-paragraph."""""" found = False if len ( li ) : first = list ( li ) [ 0 ] <MASK> m = RE_CHECKBOX . match ( first . text ) if m is not None : first . text = self . markdown . htmlStash . store ( get_checkbox ( m . group ( "" state "" ) ) , safe = True ) + m . group ( "" line "" ) found = True return found","if first . tag == ""p"" and first . text is not None :",if first . text is not None :,67.79710943,FALSE,91.93
1477,"def _check_mswin_locale ( locale ) : msloc = None try : msloc = _LOCALE_NAMES [ locale [ : 5 ] ] [ : 2 ] locale = locale [ : 5 ] except KeyError : try : msloc = _LOCALE_NAMES [ locale [ : 2 ] ] [ : 2 ] locale = locale [ : 2 ] except KeyError : # US English is the outlier, all other English locales want # real English: <MASK> return ( "" en_GB "" , "" 1252 "" ) return ( None , None ) return ( locale , msloc )","if locale [ : 2 ] == ( ""en"" ) and locale [ : 5 ] != ""en_US"" :","if locale == ""en"" :",92.51068245,FALSE,83.15
1478,"def setLabel ( self , s , protect = False ) : """"""Set the label of the minibuffer."""""" c , k , w = self . c , self , self . w if w : # Support for the curses gui. <MASK> g . app . gui . set_minibuffer_label ( c , s ) w . setAllText ( s ) n = len ( s ) w . setSelectionRange ( n , n , insert = n ) if protect : k . mb_prefix = s","if hasattr ( g . app . gui , ""set_minibuffer_label"" ) :","if hasattr ( g . app . gui , ""set_minibuffer_label"" )",98.70913335,FALSE,97.85
1479,"def getProc ( su , innerTarget ) : if len ( su ) == 1 : # have a one element wedge proc = ( "" first "" , "" last "" ) else : if su . isFirst ( innerTarget ) and su . isLast ( innerTarget ) : proc = ( "" first "" , "" last "" ) # same element can be first and last <MASK> proc = ( "" first "" , ) elif su . isLast ( innerTarget ) : proc = ( "" last "" , ) else : proc = ( ) return proc",elif su . isFirst ( innerTarget ) :,elif su . isFirst ( innerTarget ) :,100,TRUE,100
1480,"def await_test_end ( self ) : iterations = 0 while True : if iterations > 100 : self . log . debug ( "" Await: iteration limit reached "" ) return status = self . master . get_status ( ) <MASK> return iterations + = 1 time . sleep ( 1.0 )","if status . get ( ""status"" ) == ""ENDED"" :","if status == ""RUNNING"" :",90.04586344,FALSE,84.24
1481,"def _handle_autocomplete_request_for_text ( text ) : if not hasattr ( text , "" autocompleter "" ) : if isinstance ( text , ( CodeViewText , ShellText ) ) and text . is_python_text ( ) : <MASK> text . autocompleter = Completer ( text ) elif isinstance ( text , ShellText ) : text . autocompleter = ShellCompleter ( text ) text . bind ( "" <1> "" , text . autocompleter . on_text_click ) else : return text . autocompleter . handle_autocomplete_request ( )","if isinstance ( text , CodeViewText ) :","if isinstance ( text , CodeViewText ) :",100,TRUE,100
1482,"def validate_party_details ( self ) : if self . party : <MASK> frappe . throw ( _ ( "" Invalid  {0} :  {1} "" ) . format ( self . party_type , self . party ) ) if self . party_account and self . party_type in ( "" Customer "" , "" Supplier "" ) : self . validate_account_type ( self . party_account , [ erpnext . get_party_account_type ( self . party_type ) ] )","if not frappe . db . exists ( self . party_type , self . party ) :",if not erpnext . validate_party_type ( self . party ) :,89.07320491,FALSE,90.94
1483,"def format ( self , formatstr ) : pieces = [ ] for i , piece in enumerate ( re_formatchars . split ( force_text ( formatstr ) ) ) : <MASK> pieces . append ( force_text ( getattr ( self , piece ) ( ) ) ) elif piece : pieces . append ( re_escaped . sub ( r "" \ 1 "" , piece ) ) return "" "" . join ( pieces )",if i % 2 :,if i == 0 :,94.26111849,FALSE,94.77
1484,"def _convert_java_pattern_to_python ( pattern ) : """"""Convert a replacement pattern from the Java-style `$5` to the Python-style `\\5`."""""" s = list ( pattern ) i = 0 while i < len ( s ) - 1 : c = s [ i ] <MASK> s [ i ] = "" \\ "" elif c == "" \\ "" and s [ i + 1 ] == "" $ "" : s [ i ] = "" "" i + = 1 i + = 1 return pattern [ : 0 ] . join ( s )","if c == ""$"" and s [ i + 1 ] in ""0123456789"" :","if c == ""\\"" :",86.57338455,FALSE,90.14
1485,"def download ( self , url , filename , * * kwargs ) : try : r = self . get ( url , timeout = 10 , stream = True , * * kwargs ) <MASK> return False with open ( filename , "" wb "" ) as f : for chunk in r . iter_content ( chunk_size = 1024 ) : if chunk : f . write ( chunk ) helpers . chmod_as_parent ( filename ) except Exception as e : sickrage . app . log . debug ( "" Failed to download file from  {}  - ERROR:  {} "" . format ( url , e ) ) if os . path . exists ( filename ) : os . remove ( filename ) return False return True",if r . status_code >= 400 :,if not r . ok :,95.90043427,FALSE,94.08
1486,"def run ( self , paths = [ ] ) : items = [ ] for item in SideBarSelection ( paths ) . getSelectedFilesWithExtension ( "" js "" ) : items . append ( ' <script type= "" text/javascript ""  src= "" ' + item . pathAbsoluteFromProjectEncoded ( ) + ' "" ></script> ' ) if len ( items ) > 0 : sublime . set_clipboard ( "" \n "" . join ( items ) ) <MASK> sublime . status_message ( "" Items copied "" ) else : sublime . status_message ( "" Item copied "" )",if len ( items ) > 1 :,if len ( items ) > 1 :,100,TRUE,100
1487,"def work ( self ) : while True : timeout = self . timeout if idle . is_set ( ) : timeout = self . idle_timeout log . debug ( "" Wait for  {} "" . format ( timeout ) ) fetch . wait ( timeout ) <MASK> log . info ( "" Stop fetch worker "" ) break self . fetch ( )",if shutting_down . is_set ( ) :,if fetch . is_set ( ) :,98.1788164,FALSE,94.18
1488,"def check_apns_certificate ( ss ) : mode = "" start "" for s in ss . split ( "" \n "" ) : if mode == "" start "" : <MASK> mode = "" key "" elif mode == "" key "" : if "" END RSA PRIVATE KEY "" in s or "" END PRIVATE KEY "" in s : mode = "" end "" break elif s . startswith ( "" Proc-Type "" ) and "" ENCRYPTED "" in s : raise ImproperlyConfigured ( "" Encrypted APNS private keys are not supported "" ) if mode != "" end "" : raise ImproperlyConfigured ( "" The APNS certificate doesn ' t contain a private key "" )","if ""BEGIN RSA PRIVATE KEY"" in s or ""BEGIN PRIVATE KEY"" in s :","if ""RSA KEY"" in s or ""RSA KEY"" in s :",93.06096792,FALSE,94.53
1489,"def compare_lists ( self , l1 , l2 , key ) : l2_lookup = { o . get ( key ) : o for o in l2 } for obj1 in l1 : obj2 = l2_lookup . get ( obj1 . get ( key ) ) for k in obj1 : <MASK> self . assertEqual ( obj1 . get ( k ) , obj2 . get ( k ) )","if k not in ""id"" and obj1 . get ( k ) :",if k in obj2 :,88.51465088,FALSE,84.55
1490,"def before_get_object ( self , view_kwargs ) : if view_kwargs . get ( "" id "" ) is not None : try : user_favourite_event = find_user_favourite_event_by_id ( event_id = view_kwargs [ "" id "" ] ) except NoResultFound : raise ObjectNotFound ( { "" source "" : "" /data/relationships/event "" } , "" Object: not found "" ) else : <MASK> view_kwargs [ "" id "" ] = user_favourite_event . id else : view_kwargs [ "" id "" ] = None",if user_favourite_event is not None :,if user_favourite_event :,91.68478795,FALSE,96.49
1491,"def close ( self ) : super ( ) . close ( ) if not sys . is_finalizing ( ) : for sig in list ( self . _signal_handlers ) : self . remove_signal_handler ( sig ) else : <MASK> warnings . warn ( f "" Closing the loop  { self !r}   "" f "" on interpreter shutdown  "" f "" stage, skipping signal handlers removal "" , ResourceWarning , source = self , ) self . _signal_handlers . clear ( )",if self . _signal_handlers :,if self . _loop . is_shutdown ( ) :,93.72267261,FALSE,92.23
1492,"def install_script ( self , script , install_options = None ) : try : fname = utils . do_script ( script , python_exe = osp . join ( self . target , "" python.exe "" ) , architecture = self . architecture , verbose = self . verbose , install_options = install_options , ) except RuntimeError : <MASK> print ( "" Failed! "" ) raise",if not self . verbose :,if self . verbose :,84.29527679,FALSE,96.99
1493,"def GetRouterForUser ( self , username ) : """"""Returns a router corresponding to a given username."""""" for index , router in enumerate ( self . routers ) : router_id = str ( index ) <MASK> logging . debug ( "" Matched router  %s  to user  %s "" , router . __class__ . __name__ , username ) return router logging . debug ( "" No router ACL rule match for user  %s . Using default  "" "" router  %s "" , username , self . default_router . __class__ . __name__ , ) return self . default_router","if self . auth_manager . CheckPermissions ( username , router_id ) :",if router . username == username :,88.44461043,FALSE,89.45
1494,"def charset ( self ) : """"""The charset from the content type."""""" header = self . environ . get ( "" CONTENT_TYPE "" ) if header : ct , options = parse_options_header ( header ) charset = options . get ( "" charset "" ) <MASK> if is_known_charset ( charset ) : return charset return self . unknown_charset ( charset ) return self . default_charset",if charset :,if charset :,100,TRUE,100
1495,def isFinished ( self ) : # returns true if episode timesteps has reached episode length and resets the task if self . count > self . epiLen : self . res ( ) return True else : if self . count == 1 : self . pertGlasPos ( 0 ) <MASK> self . env . reset ( ) self . pertGlasPos ( 1 ) self . count + = 1 return False,if self . count == self . epiLen / 2 + 1 :,elif self . count == 2 :,68.46365842,FALSE,87.21
1496,"def mtimes_of_files ( dirnames : List [ str ] , suffix : str ) - > Iterator [ float ] : for dirname in dirnames : for root , dirs , files in os . walk ( dirname ) : for sfile in files : <MASK> try : yield path . getmtime ( path . join ( root , sfile ) ) except OSError : pass",if sfile . endswith ( suffix ) :,if sfile . endswith ( suffix ) :,100,TRUE,100
1497,"def get_all_hashes ( self ) : event_hashes = [ ] sample_hashes = [ ] for a in self . event . attributes : h = None if a . type in ( "" md5 "" , "" sha1 "" , "" sha256 "" ) : h = a . value event_hashes . append ( h ) elif a . type in ( "" filename|md5 "" , "" filename|sha1 "" , "" filename|sha256 "" ) : h = a . value . split ( "" | "" ) [ 1 ] event_hashes . append ( h ) <MASK> h = a . value . split ( "" | "" ) [ 1 ] sample_hashes . append ( h ) return event_hashes , sample_hashes","elif a . type == ""malware-sample"" :","elif a . type in ( ""sample|sample"" , ""sample|sample"" )",96.06759673,FALSE,90.7
1498,"def _validate ( self , event ) : if self . type is None : return new = self . value if not isinstance ( new , self . type ) and new is not None : <MASK> self . value = event . old types = repr ( self . type ) if isinstance ( self . type , tuple ) else self . type . __name__ raise ValueError ( "" LiteralInput expected  %s  type but value  %s   "" "" is of type  %s . "" % ( types , new , type ( new ) . __name__ ) )",if event :,if event . old is not None :,96.22047808,FALSE,94.5
1499,"def update_dict ( a , b ) : for key , value in b . items ( ) : <MASK> continue if key not in a : a [ key ] = value elif isinstance ( a [ key ] , dict ) and isinstance ( value , dict ) : update_dict ( a [ key ] , value ) elif isinstance ( a [ key ] , list ) : a [ key ] . append ( value ) else : a [ key ] = [ a [ key ] , value ]",if value is None :,"if key == ""id"" :",90.35403563,FALSE,92.8
1500,"def on_pre_save ( self , view ) : extOrClause = "" | "" . join ( s . get ( "" format_on_save_extensions "" ) ) extRegex = "" \\ .( "" + extOrClause + "" )$ "" if s . get ( "" format_on_save "" ) and re . search ( extRegex , view . file_name ( ) ) : # only auto-format on save if there are no ""lint errors"" # here are some named regions from sublimelint see https://github.com/lunixbochs/sublimelint/tree/st3 lints_regions = [ "" lint-keyword-underline "" , "" lint-keyword-outline "" ] for linter in lints_regions : <MASK> return view . run_command ( "" js_format "" )",if len ( view . get_regions ( linter ) ) :,if re . search ( linter ) :,96.57724382,FALSE,93.73
1501,"def readMemory ( self , va , size ) : for mva , mmaxva , mmap , mbytes in self . _map_defs : if mva < = va < mmaxva : mva , msize , mperms , mfname = mmap <MASK> raise envi . SegmentationViolation ( va ) offset = va - mva return mbytes [ offset : offset + size ] raise envi . SegmentationViolation ( va )",if not mperms & MM_READ :,if not mperms :,94.05412915,FALSE,92.84
1502,"def assertFilepathsEqual ( self , p1 , p2 ) : if sys . platform == "" win32 "" : <MASK> p1 = [ normcase ( normpath ( x ) ) for x in p1 ] p2 = [ normcase ( normpath ( x ) ) for x in p2 ] else : assert isinstance ( p1 , ( str , unicode ) ) p1 = normcase ( normpath ( p1 ) ) p2 = normcase ( normpath ( p2 ) ) self . assertEqual ( p1 , p2 )","if isinstance ( p1 , ( list , tuple ) ) :","if isinstance ( p1 , ( list , tuple ) ) :",100,TRUE,100
1503,"def add_directory_csv_files ( dir_path , paths = None ) : if not paths : paths = [ ] for p in listdir ( dir_path ) : path = join ( dir_path , p ) if isdir ( path ) : # call recursively for each dir paths = add_directory_csv_files ( path , paths ) <MASK> # add every file to the list paths . append ( path ) return paths","elif isfile ( path ) and path . endswith ( "".csv"" ) :",elif isfile ( path ) :,95.11006326,FALSE,89.06
1504,"def _verifySubs ( self ) : for inst in self . subs : <MASK> raise BlockError ( _error . ArgType % ( self . name , ) ) if isinstance ( inst , ( _Block , _Instantiator ) ) : if not inst . modctxt : raise BlockError ( _error . InstanceError % ( self . name , inst . callername ) )","if not isinstance ( inst , ( _Block , _Instantiator , Cosimulation ) ) :","if not isinstance ( inst , ( _Arg , _Instance ) ) :",89.69500211,FALSE,91.77
1505,"def __annotations_bytes ( self ) : if self . annotations : a = [ ] for k , v in self . annotations . items ( ) : if len ( k ) != 4 : raise errors . ProtocolError ( "" annotation key must be of length 4 "" ) <MASK> k = k . encode ( "" ASCII "" ) a . append ( struct . pack ( "" !4sH "" , k , len ( v ) ) ) a . append ( v ) return b "" "" . join ( a ) return b "" ""","if sys . version_info >= ( 3 , 0 ) :","if isinstance ( k , bytes ) :",93.63820387,FALSE,89.78
1506,"def session ( self , profile : str = "" default "" , region : str = None ) - > boto3 . Session : region = self . _get_region ( region , profile ) try : session = self . _cache_lookup ( self . _session_cache , [ profile , region ] , self . _boto3 . Session , [ ] , { "" region_name "" : region , "" profile_name "" : profile } , ) except ProfileNotFound : <MASK> raise session = self . _boto3 . Session ( region_name = region ) self . _cache_set ( self . _session_cache , [ profile , region ] , session ) return session","if profile != ""default"" :",if self . _boto3 . Session is None :,94.05072875,FALSE,93.32
1507,"def spans_score ( gold_spans , system_spans ) : correct , gi , si = 0 , 0 , 0 while gi < len ( gold_spans ) and si < len ( system_spans ) : if system_spans [ si ] . start < gold_spans [ gi ] . start : si + = 1 <MASK> gi + = 1 else : correct + = gold_spans [ gi ] . end == system_spans [ si ] . end si + = 1 gi + = 1 return Score ( len ( gold_spans ) , len ( system_spans ) , correct )",elif gold_spans [ gi ] . start < system_spans [ si ] . start :,elif system_spans [ si ] . end > gold_spans [ gi ] . end,79.8145711,FALSE,93.82
1508,"def to_api ( tag , raw_value ) : try : api_tag , converter = _QL_TO_SC [ tag ] if tag else ( "" q "" , None ) except KeyError : <MASK> raise self . error ( "" Unsupported  ' %s '  tag. Try:  %s "" % ( tag , "" ,  "" . join ( SUPPORTED ) ) ) return None , None else : value = str ( converter ( raw_value ) if converter else raw_value ) return api_tag , value",if tag not in SUPPORTED :,if tag not in SUPPORTED :,100,TRUE,100
1509,"def unpack ( self , buf ) : dpkt . Packet . unpack ( self , buf ) buf = buf [ self . __hdr_len__ : ] # single-byte IE if self . type & 0x80 : self . len = 0 self . data = b "" "" # multi-byte IE else : # special PER-encoded UUIE <MASK> self . len = struct . unpack ( "" >H "" , buf [ : 2 ] ) [ 0 ] buf = buf [ 2 : ] # normal TLV-like IE else : self . len = struct . unpack ( "" B "" , buf [ : 1 ] ) [ 0 ] buf = buf [ 1 : ] self . data = buf [ : self . len ]",if self . type == USER_TO_USER :,if self . type == 0x01 :,99.08472682,FALSE,95.61
1510,"def on_bt_search_clicked ( self , widget ) : if self . current_provider is None : return query = self . en_query . get_text ( ) @self . obtain_podcasts_with def load_data ( ) : <MASK> return self . current_provider . on_search ( query ) elif self . current_provider . kind == directory . Provider . PROVIDER_URL : return self . current_provider . on_url ( query ) elif self . current_provider . kind == directory . Provider . PROVIDER_FILE : return self . current_provider . on_file ( query )",if self . current_provider . kind == directory . Provider . PROVIDER_SEARCH :,if self . current_provider . kind == directory . Provider . PROVIDER_SEARCH :,100,TRUE,100
1511,"def _text ( bitlist ) : out = "" "" for typ , text in bitlist : if not typ : out + = text <MASK> out + = "" \\ fI %s \\ fR "" % text elif typ in [ "" strong "" , "" code "" ] : out + = "" \\ fB %s \\ fR "" % text else : raise ValueError ( "" unexpected tag  %r  inside text "" % ( typ , ) ) out = out . strip ( ) out = re . sub ( re . compile ( r "" ^ \ s+ "" , re . M ) , "" "" , out ) return out","elif typ == ""em"" :","elif typ in [ ""strong"" , ""code"" ] :",79.78478138,FALSE,91.92
1512,"def process ( self , buckets ) : with self . executor_factory ( max_workers = 3 ) as w : futures = { } results = [ ] for b in buckets : futures [ w . submit ( self . process_bucket , b ) ] = b for f in as_completed ( futures ) : <MASK> b = futures [ f ] self . log . error ( "" error modifying bucket: %s \n %s "" , b [ "" Name "" ] , f . exception ( ) ) results + = filter ( None , [ f . result ( ) ] ) return results",if f . exception ( ) :,if f . exception ( ) :,100,TRUE,100
1513,"def check_settings ( self ) : if self . settings_dict [ "" TIME_ZONE "" ] is not None : <MASK> raise ImproperlyConfigured ( "" Connection  ' %s '  cannot set TIME_ZONE because USE_TZ is  "" "" False. "" % self . alias ) elif self . features . supports_timezones : raise ImproperlyConfigured ( "" Connection  ' %s '  cannot set TIME_ZONE because its engine  "" "" handles time zones conversions natively. "" % self . alias )",if not settings . USE_TZ :,"if self . settings_dict [ ""USE_TZ"" ] is not False :",61.5159929,FALSE,86.9
1514,"def process_webhook_prop ( namespace ) : if not isinstance ( namespace . webhook_properties , list ) : return result = { } for each in namespace . webhook_properties : if each : <MASK> key , value = each . split ( "" = "" , 1 ) else : key , value = each , "" "" result [ key ] = value namespace . webhook_properties = result","if ""="" in each :","if ""="" in each :",100,TRUE,100
1515,"def _expand_query_values ( original_query_list ) : query_list = [ ] for key , value in original_query_list : <MASK> query_list . append ( ( key , value ) ) else : key_fmt = key + "" [ %s ] "" value_list = _to_kv_list ( value ) query_list . extend ( ( key_fmt % k , v ) for k , v in value_list ) return query_list","if isinstance ( value , basestring ) :","if isinstance ( value , ( list , tuple ) ) :",92.68323016,FALSE,94.05
1516,"def tags ( ) : """"""Return a dictionary of all tags in the form {hash: [tag_names, ...]}."""""" tags = { } for ( n , c ) in list_refs ( ) : if n . startswith ( "" refs/tags/ "" ) : name = n [ 10 : ] <MASK> tags [ c ] = [ ] tags [ c ] . append ( name ) # more than one tag can point at 'c' return tags",if not c in tags :,if c not in tags :,94.61820521,FALSE,97.04
1517,"def test_colorspiral ( self ) : """"""Set of 625 colours, with jitter, using get_colors()."""""" boxedge = 20 boxes_per_row = 25 rows = 0 for i , c in enumerate ( get_colors ( 625 ) ) : self . c . setFillColor ( c ) x1 = boxedge * ( i % boxes_per_row ) y1 = rows * boxedge self . c . rect ( x1 , y1 , boxedge , boxedge , fill = 1 , stroke = 0 ) <MASK> rows + = 1 self . finish ( )",if not ( i + 1 ) % boxes_per_row :,if i % 2 == 0 :,91.66523715,FALSE,89.64
1518,"def oldest_pending_update_in_days ( ) : """"""Return the datestamp of the oldest pending update"""""" pendingupdatespath = os . path . join ( prefs . pref ( "" ManagedInstallDir "" ) , "" UpdateNotificationTracking.plist "" ) try : pending_updates = FoundationPlist . readPlist ( pendingupdatespath ) except FoundationPlist . NSPropertyListSerializationException : return 0 oldest_date = now = NSDate . date ( ) for category in pending_updates : for name in pending_updates [ category ] : this_date = pending_updates [ category ] [ name ] <MASK> oldest_date = this_date return now . timeIntervalSinceDate_ ( oldest_date ) / ( 24 * 60 * 60 )",if this_date < oldest_date :,if this_date < oldest_date :,100,TRUE,100
1519,"def _try_read_gpg ( path ) : path = os . path . expanduser ( path ) cmd = _gpg_cmd ( ) + [ path ] log . debug ( "" gpg cmd:  %s "" , cmd ) try : p = subprocess . Popen ( cmd , env = os . environ , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) except OSError as e : log . error ( "" cannot decode  %s  with command  ' %s '  ( %s ) "" , path , ""   "" . join ( cmd ) , e ) else : out , err = p . communicate ( ) <MASK> log . error ( err . decode ( errors = "" replace "" ) . strip ( ) ) return None return out . decode ( errors = "" replace "" )",if p . returncode != 0 :,if err . errno != errno . EINTR :,95.79167941,FALSE,94.81
1520,"def sort_nested_dictionary_lists ( d ) : for k , v in d . items ( ) : <MASK> for i in range ( 0 , len ( v ) ) : if isinstance ( v [ i ] , dict ) : v [ i ] = await sort_nested_dictionary_lists ( v [ i ] ) d [ k ] = sorted ( v ) if isinstance ( v , dict ) : d [ k ] = await sort_nested_dictionary_lists ( v ) return d","if isinstance ( v , list ) :","if isinstance ( v , list ) :",100,TRUE,100
1521,"def _the_callback ( widget , event_id ) : point = widget . GetCenter ( ) index = widget . WIDGET_INDEX if hasattr ( callback , "" __call__ "" ) : <MASK> args = [ point , index ] else : args = [ point ] if pass_widget : args . append ( widget ) try_callback ( callback , * args ) return",if num > 1 :,if index is not None :,91.65931048,FALSE,93.14
1522,"def _add_cs ( master_cs , sub_cs , prefix , delimiter = "" . "" , parent_hp = None ) : new_parameters = [ ] for hp in sub_cs . get_hyperparameters ( ) : new_parameter = copy . deepcopy ( hp ) # Allow for an empty top-level parameter <MASK> new_parameter . name = prefix elif not prefix == "" "" : new_parameter . name = "" {} {} {} "" . format ( prefix , SPLITTER , new_parameter . name ) new_parameters . append ( new_parameter ) for hp in new_parameters : _add_hp ( master_cs , hp )","if new_parameter . name == """" :",if not new_parameter . name :,97.57023201,FALSE,95.13
1523,"def tearDown ( self ) : """"""Shutdown the server."""""" try : <MASK> self . server . stop ( ) if self . sl_hdlr : self . root_logger . removeHandler ( self . sl_hdlr ) self . sl_hdlr . close ( ) finally : BaseTest . tearDown ( self )",if self . server :,if self . server :,100,TRUE,100
1524,"def app_uninstall_all ( self , excludes = [ ] , verbose = False ) : """"""Uninstall all apps"""""" our_apps = [ "" com.github.uiautomator "" , "" com.github.uiautomator.test "" ] output , _ = self . shell ( [ "" pm "" , "" list "" , "" packages "" , "" -3 "" ] ) pkgs = re . findall ( r "" package:([^ \ s]+) "" , output ) pkgs = set ( pkgs ) . difference ( our_apps + excludes ) pkgs = list ( pkgs ) for pkg_name in pkgs : <MASK> print ( "" uninstalling "" , pkg_name , ""   "" , end = "" "" , flush = True ) ok = self . app_uninstall ( pkg_name ) <MASK> print ( "" OK "" if ok else "" FAIL "" ) return pkgs",if verbose :,if verbose :,100,TRUE,100
1525,"def httpapi ( self , arg , opts ) : sc = HttpAPIStatsCollector ( ) headers = [ "" #Item "" , "" Value "" ] table = [ ] for k , v in sc . get ( ) . getStats ( ) . items ( ) : if isinstance ( v , dict ) : v = json . dumps ( v ) row = [ ] row . append ( "" # %s "" % k ) <MASK> row . append ( formatDateTime ( v ) ) else : row . append ( v ) table . append ( row ) self . protocol . sendData ( tabulate ( table , headers , tablefmt = "" plain "" , numalign = "" left "" ) . encode ( "" ascii "" ) )","if k [ - 3 : ] == ""_at"" :","if isinstance ( v , datetime . datetime ) :",92.36156497,FALSE,90.95
1526,"def Get_Gene ( self , id ) : """"""Retreive the gene name (GN)."""""" entry = self . Get ( id ) if not entry : return None GN = "" "" for line in string . split ( entry , "" \n "" ) : <MASK> GN = string . strip ( line [ 5 : ] ) if GN [ - 1 ] == "" . "" : GN = GN [ 0 : - 1 ] return GN if line [ 0 : 2 ] == "" // "" : break return GN","if line [ 0 : 5 ] == ""GN   "" :","if line [ 0 : 5 ] == ""gnn"" :",98.34093569,FALSE,97.91
1527,"def replace_dir_vars ( path , d ) : """"""Replace common directory paths with appropriate variable references (e.g. /etc becomes ${sysconfdir})"""""" dirvars = { } # Sort by length so we get the variables we're interested in first for var in sorted ( list ( d . keys ( ) ) , key = len ) : if var . endswith ( "" dir "" ) and var . lower ( ) == var : value = d . getVar ( var ) <MASK> dirvars [ value ] = var for dirpath in sorted ( list ( dirvars . keys ( ) ) , reverse = True ) : path = path . replace ( dirpath , "" $ { %s } "" % dirvars [ dirpath ] ) return path","if value . startswith ( ""/"" ) and not ""\n"" in value and value not in dirvars :",if value is not None :,68.96824617,FALSE,87.2
1528,"def _scrub_generated_timestamps ( self , target_workdir ) : """"""Remove the first line of comment from each file if it contains a timestamp."""""" for root , _ , filenames in safe_walk ( target_workdir ) : for filename in filenames : source = os . path . join ( root , filename ) with open ( source , "" r "" ) as f : lines = f . readlines ( ) if len ( lines ) < 1 : return with open ( source , "" w "" ) as f : <MASK> f . write ( lines [ 0 ] ) for line in lines [ 1 : ] : f . write ( line )",if not self . _COMMENT_WITH_TIMESTAMP_RE . match ( lines [ 0 ] ) :,"if lines [ 0 ] . startswith ( ""generated timestamps"" ) :",84.92486584,FALSE,89.09
1529,"def get_all_active_plugins ( self ) - > List [ BotPlugin ] : """"""This returns the list of plugins in the callback ordered defined from the config."""""" all_plugins = [ ] for name in self . plugins_callback_order : # None is a placeholder for any plugin not having a defined order <MASK> all_plugins + = [ plugin for name , plugin in self . plugins . items ( ) if name not in self . plugins_callback_order and plugin . is_activated ] else : plugin = self . plugins [ name ] if plugin . is_activated : all_plugins . append ( plugin ) return all_plugins",if name is None :,if name is None :,100,TRUE,100
1530,"def test_query_level ( self ) : "" Tests querying at a level other than max "" # level 2 l2 = set ( ) for p in self . tile_paths : l2 . add ( p [ 0 : 2 ] ) for path in iterate_base4 ( 2 ) : <MASK> self . assertTrue ( self . tree . query_path ( path ) ) else : self . assertFalse ( self . tree . query_path ( path ) ) # level 1: self . assertTrue ( self . tree . query_path ( ( 0 , ) ) ) self . assertTrue ( self . tree . query_path ( ( 1 , ) ) ) self . assertTrue ( self . tree . query_path ( ( 2 , ) ) ) self . assertFalse ( self . tree . query_path ( ( 3 , ) ) )",if path in l2 :,if path in l2 :,75,TRUE,100
1531,"def program_exists ( name ) : paths = ( os . getenv ( "" PATH "" ) or os . defpath ) . split ( os . pathsep ) for p in paths : fn = "" %s / %s "" % ( p , name ) <MASK> return not os . path . isdir ( fn ) and os . access ( fn , os . X_OK )",if os . path . exists ( fn ) :,if os . path . exists ( fn ) :,100,TRUE,100
1532,"def decoration_helper ( self , patched , args , keywargs ) : extra_args = [ ] with contextlib . ExitStack ( ) as exit_stack : for patching in patched . patchings : arg = exit_stack . enter_context ( patching ) <MASK> keywargs . update ( arg ) elif patching . new is DEFAULT : extra_args . append ( arg ) args + = tuple ( extra_args ) yield ( args , keywargs )",if patching . attribute_name is not None :,if patching . new is KEY :,93.83665416,FALSE,92.29
1533,"def update_neighbor ( neigh_ip_address , changes ) : rets = [ ] for k , v in changes . items ( ) : if k == neighbors . MULTI_EXIT_DISC : rets . append ( _update_med ( neigh_ip_address , v ) ) <MASK> rets . append ( update_neighbor_enabled ( neigh_ip_address , v ) ) if k == neighbors . CONNECT_MODE : rets . append ( _update_connect_mode ( neigh_ip_address , v ) ) return all ( rets )",if k == neighbors . ENABLED :,if k == neighbors . neighbor_enabled :,98.45096215,FALSE,96.28
1534,"def calcUniqueStates ( self ) : # Here we show which colors can be relied on to map to an # internal state.  The current position will be at the first # character in the buffer styled that color, so this might not # work in all cases. self . uniqueStates = { } for k in self . holdUniqueStates . keys ( ) : v = self . holdUniqueStates [ k ] <MASK> self . uniqueStates [ k ] = v . keys ( ) [ 0 ] log . debug ( "" Map style [ %s ] to state [ %s ] "" , k , v . keys ( ) [ 0 ] ) log . debug ( "" Style [ %s ] maps to states [ %s ] "" , k , "" ,  "" . join ( v . keys ( ) ) ) self . holdUniqueStates = None",if len ( v . keys ( ) ) == 1 :,"if isinstance ( v , dict ) :",71.81887305,FALSE,93.26
1535,"def init_logger ( ) : configured_loggers = [ log_config . get ( "" root "" , { } ) ] + [ logger for logger in log_config . get ( "" loggers "" , { } ) . values ( ) ] used_handlers = { handler for log in configured_loggers for handler in log . get ( "" handlers "" , [ ] ) } for handler_id , handler in list ( log_config [ "" handlers "" ] . items ( ) ) : <MASK> del log_config [ "" handlers "" ] [ handler_id ] elif "" filename "" in handler . keys ( ) : filename = handler [ "" filename "" ] logfile_path = Path ( filename ) . expanduser ( ) . resolve ( ) handler [ "" filename "" ] = str ( logfile_path ) logging . config . dictConfig ( log_config )",if handler_id not in used_handlers :,if handler_id in used_handlers :,99.06038736,FALSE,98.61
1536,"def _selected_machines ( self , virtual_machines ) : selected_machines = [ ] for machine in virtual_machines : <MASK> selected_machines . append ( machine ) if self . tags and self . _tags_match ( machine . tags , self . tags ) : selected_machines . append ( machine ) if self . locations and machine . location in self . locations : selected_machines . append ( machine ) return selected_machines",if self . _args . host and self . _args . host == machine . name :,if self . _machine_match ( machine . machine ) :,82.63961775,FALSE,85.28
1537,"def init ( self ) : r = self . get_redis ( ) if r : key = "" pocsuite_target "" info_msg = "" [PLUGIN] try fetch targets from redis... "" logger . info ( info_msg ) targets = r . get ( key ) count = 0 if targets : for target in targets : <MASK> count + = 1 info_msg = "" [PLUGIN] get  {0}  target(s) from redis "" . format ( count ) logger . info ( info_msg )",if self . add_target ( target ) :,"if target . get ( ""type"" ) == ""pocsuite_target"" :",93.26250813,FALSE,87.68
1538,"def tearDown ( self ) : suffix = str ( os . getgid ( ) ) cli = monitoring_v3 . MetricServiceClient ( ) for md in cli . list_metric_descriptors ( "" projects/ {} "" . format ( PROJECT ) ) : <MASK> try : cli . delete_metric_descriptor ( md . name ) except Exception : pass","if ""OpenCensus"" in md . name and suffix in md . name :",if md . name . endswith ( suffix ) :,86.74799629,FALSE,84.26
1539,"def InitializeColours ( self ) : """"""Initializes the 16 custom colours in :class:`CustomPanel`."""""" curr = self . _colourData . GetColour ( ) self . _colourSelection = - 1 for i in range ( 16 ) : c = self . _colourData . GetCustomColour ( i ) if c . IsOk ( ) : self . _customColours [ i ] = self . _colourData . GetCustomColour ( i ) else : self . _customColours [ i ] = wx . WHITE <MASK> self . _colourSelection = i",if c == curr :,if curr == c :,96.76145568,FALSE,96.03
1540,"def __getitem__ ( self , index ) : if self . _check ( ) : if isinstance ( index , int ) : if index < 0 or index > = len ( self . features ) : raise IndexError ( index ) if self . features [ index ] is None : feature = self . device . feature_request ( FEATURE . FEATURE_SET , 0x10 , index ) <MASK> ( feature , ) = _unpack ( "" !H "" , feature [ : 2 ] ) self . features [ index ] = FEATURE [ feature ] return self . features [ index ] elif isinstance ( index , slice ) : indices = index . indices ( len ( self . features ) ) return [ self . __getitem__ ( i ) for i in range ( * indices ) ]",if feature :,if len ( feature ) > 2 :,97.26140596,FALSE,95.55
1541,"def _get_data_from_buffer ( obj ) : try : view = memoryview ( obj ) except TypeError : # try to use legacy buffer protocol if 2.7, otherwise re-raise <MASK> view = memoryview ( buffer ( obj ) ) warnings . warn ( "" using old buffer interface to unpack  %s ;  "" "" this leads to unpacking errors if slicing is used and  "" "" will be removed in a future version "" % type ( obj ) , RuntimeWarning , stacklevel = 3 , ) else : raise if view . itemsize != 1 : raise ValueError ( "" cannot unpack from multi-byte object "" ) return view",if PY2 :,"if sys . version_info < ( 2 , 7 ) :",97.207453,FALSE,90.33
1542,"def import_modules ( modules , safe = True ) : """"""Safely import a list of *modules*"""""" all = [ ] for mname in modules : if mname . endswith ( "" .* "" ) : to_load = expand_star ( mname ) else : to_load = [ mname ] for module in to_load : try : all . append ( import_module ( module ) ) except ImportError : <MASK> raise return all",if not safe :,if safe :,96.4481006,FALSE,97.32
1543,"def pack ( types , * args ) : if len ( types ) != len ( args ) : raise Exception ( "" number of arguments does not match format string "" ) port = StringIO ( ) for ( type , value ) in zip ( types , args ) : if type == "" V "" : write_vuint ( port , value ) <MASK> write_vint ( port , value ) elif type == "" s "" : write_bvec ( port , value ) else : raise Exception ( ' unknown xpack format string item  "" ' + type + ' "" ' ) return port . getvalue ( )","elif type == ""v"" :","elif type == ""v"" :",100,TRUE,100
1544,"def create_local_app_folder ( local_app_path ) : if exists ( local_app_path ) : raise ValueError ( "" There is already a  ' %s '  folder! Aborting! "" % local_app_path ) for folder in subfolders ( local_app_path ) : <MASK> os . mkdir ( folder ) init_path = join ( folder , "" __init__.py "" ) if not exists ( init_path ) : create_file ( init_path )",if not exists ( folder ) :,if not os . path . exists ( folder ) :,94.91020256,FALSE,95.1
1545,"def _get_node_type_specific_fields ( self , node_id : str , fields_key : str ) - > Any : fields = self . config [ fields_key ] node_tags = self . provider . node_tags ( node_id ) if TAG_RAY_USER_NODE_TYPE in node_tags : node_type = node_tags [ TAG_RAY_USER_NODE_TYPE ] <MASK> raise ValueError ( f "" Unknown node type tag:  { node_type } . "" ) node_specific_config = self . available_node_types [ node_type ] if fields_key in node_specific_config : fields = node_specific_config [ fields_key ] return fields",if node_type not in self . available_node_types :,if node_type not in self . available_node_types :,100,TRUE,100
1546,"def _maybe_fix_sequence_in_union ( aliases : List [ Alias ] , typecst : cst . SubscriptElement ) - > cst . SubscriptElement : slc = typecst . slice if isinstance ( slc , cst . Index ) : val = slc . value <MASK> return cst . ensure_type ( typecst . deep_replace ( val , _get_clean_type_from_subscript ( aliases , val ) ) , cst . SubscriptElement , ) return typecst","if isinstance ( val , cst . Subscript ) :","if isinstance ( val , cst . SubscriptElement ) :",98.36508703,FALSE,97.44
1547,"def cancel_download ( self , downloads ) : # Make sure we're always dealing with a list if isinstance ( downloads , Download ) : downloads = [ downloads ] for download in downloads : <MASK> self . cancel_current_download ( ) else : self . __paused = True new_queue = queue . Queue ( ) while not self . __queue . empty ( ) : queued_download = self . __queue . get ( ) if download == queued_download : download . cancel ( ) else : new_queue . put ( queued_download ) self . __queue = new_queue self . __paused = False",if download == self . __current_download :,if self . __paused :,72.33860614,FALSE,93.54
1548,"def migrate_account_metadata ( account_id ) : from inbox . models . session import session_scope from inbox . models import Account with session_scope ( versioned = False ) as db_session : account = db_session . query ( Account ) . get ( account_id ) if account . discriminator == "" easaccount "" : create_categories_for_easfoldersyncstatuses ( account , db_session ) else : create_categories_for_folders ( account , db_session ) <MASK> set_labels_for_imapuids ( account , db_session ) db_session . commit ( )","if account . discriminator == ""gmailaccount"" :","if account . discriminator == ""imapuids"" :",98.49540604,FALSE,98.04
1549,"def __init__ ( self , fmt = None , * args ) : if not isinstance ( fmt , BaseException ) : Error . __init__ ( self , fmt , * args ) else : e = fmt cls = e . __class__ fmt = "" %s . %s :  %s "" % ( cls . __module__ , cls . __name__ , e ) tb = sys . exc_info ( ) [ 2 ] <MASK> fmt + = "" \n "" fmt + = "" "" . join ( traceback . format_tb ( tb ) ) Error . __init__ ( self , fmt )",if tb :,if fmt :,97.55560731,FALSE,98.16
1550,"def setLabel ( self , label ) : if label is None : <MASK> self . label . scene ( ) . removeItem ( self . label ) self . label = None else : if self . label is None : self . label = TextItem ( ) self . label . setParentItem ( self ) self . label . setText ( label ) self . _updateLabel ( )",if self . label is not None :,if self . label is not None :,100,TRUE,100
1551,"def serve_until_stopped ( self ) - > None : while True : rd , wr , ex = select . select ( [ self . socket . fileno ( ) ] , [ ] , [ ] , self . timeout ) <MASK> self . handle_request ( ) if self . event is not None and self . event . is_set ( ) : break",if rd :,if rd :,100,TRUE,100
1552,"def generateCompressedFile ( inputfile , outputfile , formatstring ) : try : <MASK> in_file = open ( inputfile , "" rb "" ) in_data = in_file . read ( ) out_file = open ( inputfile + "" .xz "" , "" wb "" ) out_file . write ( xz . compress ( in_data ) ) in_file . close ( ) out_file . close ( ) else : tarout = tarfile . open ( outputfile , formatstring ) tarout . add ( inputfile , arcname = os . path . basename ( inputfile ) ) tarout . close ( ) except Exception as e : print ( e ) return False return True","if formatstring == ""w:xz"" :","if formatstring == ""gzip"" :",98.91449637,FALSE,96.7
1553,"def _datastore_get_handler ( signal , sender , keys , * * kwargs ) : txn = current_transaction ( ) if txn : for key in keys : <MASK> raise PreventedReadError ( "" Attempted to read key ( %s : %s ) inside a transaction  "" "" where it was marked protected "" % ( key . kind ( ) , key . id_or_name ( ) ) ) txn . _fetched_keys . update ( set ( keys ) )",if key in txn . _protected_keys :,"if key . kind ( ) in ( ""protected"" , ""protected"" ) :",90.08135161,FALSE,86.36
1554,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) <MASK> self . set_access_token ( d . getPrefixedString ( ) ) continue if tt == 16 : self . set_expiration_time ( d . getVarInt64 ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 10 :,if tt == 10 :,100,TRUE,100
1555,"def write_vuint ( port , x ) : if x < 0 : raise Exception ( "" vuints must not be negative "" ) elif x == 0 : port . write ( "" \0 "" ) else : while x : seven_bits = x & 0x7F x >> = 7 <MASK> port . write ( chr ( 0x80 | seven_bits ) ) else : port . write ( chr ( seven_bits ) )",if x :,if x & 0x80 :,88.82459293,FALSE,95.97
1556,"def _expand_srcs ( self ) : """"""Expand src to [(src, full_path)]"""""" result = [ ] for src in self . srcs : full_path = self . _source_file_path ( src ) <MASK> # Assume generated full_path = self . _target_file_path ( src ) result . append ( ( src , full_path ) ) return result",if not os . path . exists ( full_path ) :,if not full_path :,87.64834206,FALSE,89.43
1557,"def pytest_collection_modifyitems ( items ) : for item in items : if item . nodeid . startswith ( "" tests/ops "" ) : if "" stage "" not in item . keywords : item . add_marker ( pytest . mark . stage ( "" unit "" ) ) <MASK> item . add_marker ( pytest . mark . init ( rng_seed = 123 ) )","if ""init"" not in item . keywords :","if ""init"" not in item . keywords :",100,TRUE,100
1558,"def set_shape ( self , shape ) : """"""Sets a shape."""""" if self . _shape is not None : logger . warning ( ' Modifying the shape of Placeholder  "" %s "" . ' , self . name ) if not isinstance ( shape , ( list , tuple ) ) : shape = ( shape , ) shape = tuple ( x if x != "" None "" else None for x in shape ) for x in shape : <MASK> raise ParsingError ( ' All entries in  "" shape ""  must be integers, or in special  ' "" cases None. Shape is:  {} "" . format ( shape ) ) self . _shape = shape","if not isinstance ( x , ( int , type ( None ) ) ) :","if not isinstance ( x , ( int , float ) ) :",92.61682279,FALSE,96.26
1559,"def _get_field_actual ( cant_be_number , raw_string , field_names ) : for line in raw_string . splitlines ( ) : for field_name in field_names : field_name = field_name . lower ( ) if "" : "" in line : left , right = line . split ( "" : "" , 1 ) left = left . strip ( ) . lower ( ) right = right . strip ( ) <MASK> if cant_be_number : if not right . isdigit ( ) : return right else : return right return None",if left == field_name and len ( right ) > 0 :,"if left . startswith ( ""field_"" ) :",90.62243657,FALSE,90.49
1560,"def validate_attributes ( self ) : for attribute in self . get_all_attributes ( ) : value = getattr ( self , attribute . code , None ) if value is None : <MASK> raise ValidationError ( _ ( "" %(attr)s  attribute cannot be blank "" ) % { "" attr "" : attribute . code } ) else : try : attribute . validate_value ( value ) except ValidationError as e : raise ValidationError ( _ ( "" %(attr)s  attribute  %(err)s "" ) % { "" attr "" : attribute . code , "" err "" : e } )",if attribute . required :,if not self . blank :,93.8085058,FALSE,95.73
1561,"def append ( self , s ) : buf = self . buf if buf is None : strbuf = self . strbuf <MASK> self . strbuf = strbuf + s return buf = self . _create_buffer ( ) buf . append ( s ) # use buf.__len__ rather than len(buf) FBO of not getting # OverflowError on Python 2 sz = buf . __len__ ( ) if not self . overflowed : if sz > = self . overflow : self . _set_large_buffer ( )",if len ( strbuf ) + len ( s ) < STRBUF_LIMIT :,if strbuf :,80.46900031,FALSE,87.68
1562,"def billing_invoice_show_validator ( namespace ) : from azure . cli . core . azclierror import ( RequiredArgumentMissingError , MutuallyExclusiveArgumentError , ) valid_combs = ( "" only --account-name, --name / --name / --name, --by-subscription is valid "" ) if namespace . account_name is not None : <MASK> raise MutuallyExclusiveArgumentError ( valid_combs ) if namespace . name is None : raise RequiredArgumentMissingError ( "" --name is also required "" ) <MASK> if namespace . name is None : raise RequiredArgumentMissingError ( "" --name is also required "" )",if namespace . by_subscription is not None :,if namespace . by_subscription is not None :,100,TRUE,100
1563,"def Handle ( self , args , context = None ) : for client_id in args . client_ids : cid = str ( client_id ) data_store . REL_DB . RemoveClientLabels ( cid , context . username , args . labels ) labels_to_remove = set ( args . labels ) existing_labels = data_store . REL_DB . ReadClientLabels ( cid ) for label in existing_labels : labels_to_remove . discard ( label . name ) <MASK> idx = client_index . ClientIndex ( ) idx . RemoveClientLabels ( cid , labels_to_remove )",if labels_to_remove :,if labels_to_remove :,100,TRUE,100
1564,"def delete_snapshot ( self , snapshot ) : snap_name = self . _get_snap_name ( snapshot [ "" id "" ] ) LOG . debug ( "" Deleting snapshot ( %s ) "" , snapshot [ "" id "" ] ) self . client_login ( ) try : self . client . delete_snapshot ( snap_name , self . backend_type ) except exception . DotHillRequestError as ex : # if the volume wasn't found, ignore the error <MASK> return LOG . exception ( "" Deleting snapshot  %s  failed "" , snapshot [ "" id "" ] ) raise exception . Invalid ( ex ) finally : self . client_logout ( )","if ""The volume was not found on this system."" in ex . args :",if ex . status == 404 :,94.5082776,FALSE,89.03
1565,"def jobs ( self ) : # How many jobs have we done? total_processed = 0 for jobEntity in self . jobItems . query_entities ( ) : # Process the items in the page yield AzureJob . fromEntity ( jobEntity ) total_processed + = 1 <MASK> # Produce some feedback for the user, because this can take # a long time on, for example, Azure logger . debug ( "" Processed  %d  total jobs "" % total_processed ) logger . debug ( "" Processed  %d  total jobs "" % total_processed )",if total_processed % 1000 == 0 :,if total_processed % 10000 == 0 :,73.69606532,FALSE,97.8
1566,def run ( self ) : while not self . completed : if self . block : time . sleep ( self . period ) else : self . _completed . wait ( self . period ) self . counter + = 1 try : self . callback ( self . counter ) except Exception : self . stop ( ) if self . timeout is not None : dt = time . time ( ) - self . _start_time if dt > self . timeout : self . stop ( ) <MASK> self . stop ( ),if self . counter == self . count :,elif dt < self . timeout :,91.78239114,FALSE,91.06
1567,"def get_instance ( cls , pool_size = None ) : if cls . _instance is not None : return cls . _instance # Lazy init with cls . _SINGLETON_LOCK : <MASK> cls . _instance = cls ( ARCTIC_ASYNC_NWORKERS if pool_size is None else pool_size ) return cls . _instance",if cls . _instance is None :,if cls . _instance is None :,100,TRUE,100
1568,"def set_state ( self , state ) : if self . _inhibit_play : # PLAYING, PAUSED change the state for after buffering is finished, # everything else aborts buffering <MASK> # abort self . __set_inhibit_play ( False ) self . bin . set_state ( state ) return self . _wanted_state = state else : self . bin . set_state ( state )","if state not in ( Gst . State . PLAYING , Gst . State . PAUSED ) :",if self . _wanted_state == state :,88.81255547,FALSE,82.38
1569,"def seen_add ( options ) : seen_name = options . add_value if is_imdb_url ( seen_name ) : console ( "" IMDB url detected, try to parse ID "" ) imdb_id = extract_id ( seen_name ) <MASK> seen_name = imdb_id else : console ( "" Could not parse IMDB ID "" ) db . add ( seen_name , "" cli_add "" , { "" cli_add "" : seen_name } ) console ( "" Added  %s  as seen. This will affect all tasks. "" % seen_name )",if imdb_id :,if imdb_id :,100,TRUE,100
1570,"def test_204_invalid_content_length ( self ) : # 204 status with non-zero content length is malformed with ExpectLog ( gen_log , "" .*Response with code 204 should not have body "" ) : response = self . fetch ( "" /?error=1 "" ) if not self . http1 : self . skipTest ( "" requires HTTP/1.x "" ) <MASK> self . skipTest ( "" curl client accepts invalid headers "" ) self . assertEqual ( response . code , 599 )",if self . http_client . configured_class != SimpleAsyncHTTPClient :,if response . status_code != 200 :,70.44930967,FALSE,88.94
1571,"def set_related_perm ( _mapper : Mapper , _connection : Connection , target : Slice ) - > None : src_class = target . cls_model id_ = target . datasource_id if id_ : ds = db . session . query ( src_class ) . filter_by ( id = int ( id_ ) ) . first ( ) <MASK> target . perm = ds . perm target . schema_perm = ds . schema_perm",if ds :,if ds :,100,TRUE,100
1572,"def on_modified_async ( self , view ) : if self . is_command_line ( view ) : <MASK> view . run_command ( "" text_pastry_selection_preview "" )","if view . size ( ) > 6 and view . substr ( sublime . Region ( 0 , 6 ) ) . lower ( ) == ""search"" :","if view . run_command ( ""text_pastry_selection_preview"" )",32.6201778,FALSE,56.56
1573,"def _improve_answer_span ( doc_tokens , input_start , input_end , tokenizer , orig_answer_text ) : """"""Returns tokenized answer spans that better match the annotated answer."""""" tok_answer_text = ""   "" . join ( tokenizer . tokenize ( orig_answer_text ) ) for new_start in range ( input_start , input_end + 1 ) : for new_end in range ( input_end , new_start - 1 , - 1 ) : text_span = ""   "" . join ( doc_tokens [ new_start : ( new_end + 1 ) ] ) <MASK> return new_start , new_end return input_start , input_end",if text_span == tok_answer_text :,if text_span == tok_answer_text :,100,TRUE,100
1574,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) <MASK> self . set_url ( d . getPrefixedString ( ) ) continue if tt == 18 : self . set_app_version_id ( d . getPrefixedString ( ) ) continue if tt == 26 : self . set_method ( d . getPrefixedString ( ) ) continue if tt == 34 : self . set_queue ( d . getPrefixedString ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 10 :,if tt == 10 :,100,TRUE,100
1575,"def _add_resource_group ( obj ) : if isinstance ( obj , list ) : for array_item in obj : _add_resource_group ( array_item ) elif isinstance ( obj , dict ) : try : <MASK> if obj [ "" id "" ] : obj [ "" resourceGroup "" ] = _parse_id ( obj [ "" id "" ] ) [ "" resource-group "" ] except ( KeyError , IndexError , TypeError ) : pass for item_key in obj : if item_key != "" sourceVault "" : _add_resource_group ( obj [ item_key ] )","if ""resourcegroup"" not in [ x . lower ( ) for x in obj . keys ( ) ] :","if ""resource-group"" not in obj :",73.94996249,FALSE,86.41
1576,"def build ( opt ) : dpath = os . path . join ( opt [ "" datapath "" ] , DECODE ) version = DECODE_VERSION if not build_data . built ( dpath , version_string = version ) : print ( "" [building data:  "" + dpath + "" ] "" ) <MASK> # An older version exists, so remove these outdated files. build_data . remove_dir ( dpath ) build_data . make_dir ( dpath ) # Download the data. for downloadable_file in RESOURCES : downloadable_file . download_file ( dpath ) # Mark the data as built. build_data . mark_done ( dpath , version_string = version )",if build_data . built ( dpath ) :,if build_data . built ( dpath ) :,100,TRUE,100
1577,"def toterminal ( self , tw ) : # the entries might have different styles last_style = None for i , entry in enumerate ( self . reprentries ) : <MASK> tw . line ( "" "" ) entry . toterminal ( tw ) if i < len ( self . reprentries ) - 1 : next_entry = self . reprentries [ i + 1 ] if ( entry . style == "" long "" or entry . style == "" short "" and next_entry . style == "" long "" ) : tw . sep ( self . entrysep ) if self . extraline : tw . line ( self . extraline )","if entry . style == ""long"" :","if entry . style == ""short"" :",73.95107179,FALSE,98.07
1578,"def reposition_division ( f1 ) : lines = f1 . splitlines ( ) if lines [ 2 ] == division : lines . pop ( 2 ) found = 0 for i , line in enumerate ( lines ) : <MASK> found + = 1 if found == 2 : if division in "" \n "" . join ( lines ) : break # already in the right place lines . insert ( i + 1 , "" "" ) lines . insert ( i + 2 , division ) break return "" \n "" . join ( lines )","if line . startswith ( '""""""' ) :",if line [ 0 ] == division :,92.31110528,FALSE,90.89
1579,def run_on_module ( self ) : try : self . module_base . disable ( self . opts . module_spec ) except dnf . exceptions . MarkingErrors as e : if self . base . conf . strict : <MASK> raise e if ( e . module_depsolv_errors and e . module_depsolv_errors [ 1 ] != libdnf . module . ModulePackageContainer . ModuleErrorType_ERROR_IN_DEFAULTS ) : raise e logger . error ( str ( e ) ),if e . no_match_group_specs or e . error_group_specs :,if e . module_spec [ 0 ] != libdnf . module . ModuleErrorType,81.591737,FALSE,84.99
1580,"def test_len ( self ) : eq = self . assertEqual eq ( base64mime . base64_len ( "" hello "" ) , len ( base64mime . encode ( "" hello "" , eol = "" "" ) ) ) for size in range ( 15 ) : if size == 0 : bsize = 0 elif size < = 3 : bsize = 4 elif size < = 6 : bsize = 8 <MASK> bsize = 12 elif size < = 12 : bsize = 16 else : bsize = 20 eq ( base64mime . base64_len ( "" x "" * size ) , bsize )",elif size <= 9 :,elif size <= 8 :,92.40183377,FALSE,97.84
1581,"def is_valid ( self ) : """"""Determines whether file is valid for this reader"""""" blocklist = self . open ( ) valid = True for line in blocklist : line = decode_bytes ( line ) <MASK> try : ( start , end ) = self . parse ( line ) if not re . match ( r "" ^( \ d { 1,3} \ .) {4} $ "" , start + "" . "" ) or not re . match ( r "" ^( \ d { 1,3} \ .) {4} $ "" , end + "" . "" ) : valid = False except Exception : valid = False break blocklist . close ( ) return valid",if not self . is_ignored ( line ) :,if line :,87.45416829,FALSE,92.9
1582,"def next ( self ) : while self . index < len ( self . data ) : uid = self . _read_next_word ( ) dont_care = self . _read_next_word ( ) entry = self . _read_next_string ( ) total_size = int ( 4 + 4 + len ( entry ) ) count = int ( total_size / self . SIZE ) if count == 0 : mod = self . SIZE - total_size else : mod = self . SIZE - int ( total_size - ( count * self . SIZE ) ) <MASK> remainder = self . _read_next_block ( mod ) yield ( uid , entry )",if mod > 0 :,if dont_care :,91.2515575,FALSE,96.83
1583,"def _str_param_list ( self , name ) : out = [ ] if self [ name ] : out + = self . _str_header ( name ) for param in self [ name ] : parts = [ ] if param . name : parts . append ( param . name ) if param . type : parts . append ( param . type ) out + = [ ""  :  "" . join ( parts ) ] <MASK> out + = self . _str_indent ( param . desc ) out + = [ "" "" ] return out","if param . desc and """" . join ( param . desc ) . strip ( ) :",if param . desc :,87.26899508,FALSE,87.25
1584,"def assert_backend ( self , expected_translated , language = "" cs "" ) : """"""Check that backend has correct data."""""" translation = self . get_translation ( language ) translation . commit_pending ( "" test "" , None ) store = translation . component . file_format_cls ( translation . get_filename ( ) , None ) messages = set ( ) translated = 0 for unit in store . content_units : id_hash = unit . id_hash self . assertFalse ( id_hash in messages , "" Duplicate string in in backend file! "" ) <MASK> translated + = 1 self . assertEqual ( translated , expected_translated , "" Did not found expected number of translations ( {}  !=  {} ). "" . format ( translated , expected_translated ) , )",if unit . is_translated ( ) :,if unit . is_translated :,97.532589,FALSE,97.96
1585,"def status ( self , name , error = "" No matching script logs found "" ) : with self . script_lock : <MASK> return self . script_running [ 1 : ] elif self . script_last and self . script_last [ 1 ] == name : return self . script_last [ 1 : ] else : raise ValueError ( error )",if self . script_running and self . script_running [ 1 ] == name :,if self . script_running and self . script_running [ 0 ] == name :,98.2858975,FALSE,97.15
1586,"def dict_no_value_from_proto_list ( obj_list ) : d = dict ( ) for item in obj_list : possible_dict = json . loads ( item . value_json ) <MASK> # (tss) TODO: This is protecting against legacy 'wandb_version' field. # Should investigate why the config payload even has 'wandb_version'. logger . warning ( "" key  ' {} '  has no  ' value '  attribute "" . format ( item . key ) ) continue d [ item . key ] = possible_dict [ "" value "" ] return d","if not isinstance ( possible_dict , dict ) or ""value"" not in possible_dict :","if ""value"" not in possible_dict :",83.92243737,FALSE,91.25
1587,"def visit ( self , node ) : """"""dispatcher on node's class/bases name."""""" cls = node . __class__ try : visitmethod = self . cache [ cls ] except KeyError : for subclass in cls . __mro__ : visitmethod = getattr ( self , subclass . __name__ , None ) <MASK> break else : visitmethod = self . __object self . cache [ cls ] = visitmethod visitmethod ( node )",if visitmethod is not None :,if visitmethod is None :,98.14075107,FALSE,97.43
1588,"def _get_adapter ( mcls , reversed_mro : Tuple [ type , . . . ] , collection : Dict [ Any , Dict [ type , Adapter ] ] , kwargs : Dict [ str , Any ] , ) - > Optional [ Adapter ] : registry_key = mcls . get_registry_key ( kwargs ) adapters = collection . get ( registry_key ) if adapters is None : return None result = None seen : Set [ Adapter ] = set ( ) for base in reversed_mro : for adaptee , adapter in adapters . items ( ) : found = mcls . _match_adapter ( base , adaptee , adapter ) <MASK> result = found seen . add ( found ) return result",if found and found not in seen :,if found not in seen :,95.84543213,FALSE,98.12
1589,"def test_pt_BR_rg ( self ) : for _ in range ( 100 ) : to_test = self . fake . rg ( ) <MASK> assert re . search ( r "" ^ \ d {8} X "" , to_test ) else : assert re . search ( r "" ^ \ d {9} $ "" , to_test )","if ""X"" in to_test :","if sys . version_info < ( 3 , 0 ) :",61.65173789,FALSE,85.46
1590,"def get_user_extra_data_by_client_id ( self , client_id , username ) : extra_data = { } current_client = self . clients . get ( client_id , None ) if current_client : for readable_field in current_client . get_readable_fields ( ) : attribute = list ( filter ( lambda f : f [ "" Name "" ] == readable_field , self . users . get ( username ) . attributes , ) ) <MASK> extra_data . update ( { attribute [ 0 ] [ "" Name "" ] : attribute [ 0 ] [ "" Value "" ] } ) return extra_data",if len ( attribute ) > 0 :,if len ( attribute ) == 1 :,98.32725262,FALSE,96.81
1591,"def augment ( self , resources ) : super ( ) . augment ( resources ) for r in resources : md = r . get ( "" SAMLMetadataDocument "" ) <MASK> continue root = sso_metadata ( md ) r [ "" IDPSSODescriptor "" ] = root [ "" IDPSSODescriptor "" ] return resources",if not md :,if not md :,100,TRUE,100
1592,"def __init__ ( self , mode = 0 , decode = None ) : self . regex = self . REGEX [ mode ] self . decode = decode if decode : self . header = _ ( "" ### This log has been decoded with automatic search pattern \n "" "" ### If some paths are not decoded you can manually decode them with: \n "" ) self . header + = "" ###  ' backintime --quiet  "" <MASK> self . header + = ' --profile  "" %s ""   ' % decode . config . profileName ( ) self . header + = "" --decode <path> ' \n \n "" else : self . header = "" """,if int ( decode . config . currentProfile ( ) ) > 1 :,if decode . config :,67.44319334,FALSE,91.77
1593,"def _get_dynamic_attr ( self , attname , obj , default = None ) : try : attr = getattr ( self , attname ) except AttributeError : return default if callable ( attr ) : # Check co_argcount rather than try/excepting the function and # catching the TypeError, because something inside the function # may raise the TypeError. This technique is more accurate. try : code = six . get_function_code ( attr ) except AttributeError : code = six . get_function_code ( attr . __call__ ) <MASK> # one argument is 'self' return attr ( obj ) else : return attr ( ) return attr",if code . co_argcount == 2 :,"if code == ""self"" :",97.65522724,FALSE,94.3
1594,"def grep_full_py_identifiers ( tokens ) : global pykeywords tokens = list ( tokens ) i = 0 while i < len ( tokens ) : tokentype , token = tokens [ i ] i + = 1 <MASK> continue while ( i + 1 < len ( tokens ) and tokens [ i ] == ( "" op "" , "" . "" ) and tokens [ i + 1 ] [ 0 ] == "" id "" ) : token + = "" . "" + tokens [ i + 1 ] [ 1 ] i + = 2 if token == "" "" : continue if token in pykeywords : continue if token [ 0 ] in "" .0123456789 "" : continue yield token","if tokentype != ""id"" :","if tokentype in ( ""op"" , ""ident"" ) :",97.0831637,FALSE,92.47
1595,"def _add_disk_config ( self , context , images ) : for image in images : metadata = image [ "" metadata "" ] <MASK> raw_value = metadata [ INTERNAL_DISK_CONFIG ] value = utils . bool_from_str ( raw_value ) image [ API_DISK_CONFIG ] = disk_config_to_api ( value )",if INTERNAL_DISK_CONFIG in metadata :,if INTERNAL_DISK_CONFIG in metadata :,100,TRUE,100
1596,"def test_edgeql_expr_valid_setop_07 ( self ) : expected_error_msg = "" cannot be applied to operands "" # IF ELSE with every scalar as the condition for val in get_test_values ( ) : query = f """""" SELECT 1 IF  { val }  ELSE 2; """""" <MASK> await self . assert_query_result ( query , [ 1 ] ) else : # every other combination must produce an error with self . assertRaisesRegex ( edgedb . QueryError , expected_error_msg , msg = query ) : async with self . con . transaction ( ) : await self . con . execute ( query )","if val == ""<bool>True"" :","if val == ""ELSE 2"" :",98.56809089,FALSE,96.08
1597,"def get_all_url_infos ( ) - > Dict [ str , UrlInfo ] : """"""Returns dict associating URL to UrlInfo."""""" url_infos = { } for path in _checksum_paths ( ) . values ( ) : dataset_url_infos = load_url_infos ( path ) for url , url_info in dataset_url_infos . items ( ) : <MASK> raise AssertionError ( "" URL  {}  is registered with 2+ distinct size/checksum tuples.  "" "" {}  vs  {} "" . format ( url , url_info , url_infos [ url ] ) ) url_infos . update ( dataset_url_infos ) return url_infos","if url_infos . get ( url , url_info ) != url_info :",if url_info . size != 2 :,90.96554318,FALSE,90.62
1598,"def global_fixes ( ) : """"""Yield multiple (code, function) tuples."""""" for function in list ( globals ( ) . values ( ) ) : <MASK> arguments = _get_parameters ( function ) if arguments [ : 1 ] != [ "" source "" ] : continue code = extract_code_from_function ( function ) if code : yield ( code , function )",if inspect . isfunction ( function ) :,"if not function . __module__ . startswith ( ""__"" ) :",91.66482929,FALSE,83.78
1599,"def createSocket ( self ) : skt = Port . createSocket ( self ) if self . listenMultiple : skt . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) <MASK> skt . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEPORT , 1 ) return skt","if hasattr ( socket , ""SO_REUSEPORT"" ) :",if self . listenInt :,85.43316024,FALSE,82.63
1600,"def _asStringList ( self , sep = "" "" ) : out = [ ] for item in self . _toklist : if out and sep : out . append ( sep ) <MASK> out + = item . _asStringList ( ) else : out . append ( str ( item ) ) return out","if isinstance ( item , ParseResults ) :","if isinstance ( item , BaseList ) :",97.93976264,FALSE,96.01
1601,"def parse_c_comments ( lexer , tok , ntok ) : if tok != "" / "" or ntok != "" * "" : return False quotes = lexer . quotes lexer . quotes = "" "" while True : tok = lexer . get_token ( ) ntok = lexer . get_token ( ) <MASK> lexer . quotes = quotes break else : lexer . push_token ( ntok ) return True","if tok == ""*"" and ntok == ""/"" :","if tok == ""\\"" :",88.46190881,FALSE,89.83
1602,"def doWorkForFindAll ( self , v , target , partialMatch ) : sibling = self while sibling : c1 = partialMatch and sibling . equalsTreePartial ( target ) if c1 : v . append ( sibling ) else : c2 = not partialMatch and sibling . equalsTree ( target ) <MASK> v . append ( sibling ) ### regardless of match or not, check any children for matches if sibling . getFirstChild ( ) : sibling . getFirstChild ( ) . doWorkForFindAll ( v , target , partialMatch ) sibling = sibling . getNextSibling ( )",if c2 :,if c2 :,100,TRUE,100
1603,"def __view_beside ( self , onsideof , * * kwargs ) : bounds = self . info [ "" bounds "" ] min_dist , found = - 1 , None for ui in UiObject ( self . session , Selector ( * * kwargs ) ) : dist = onsideof ( bounds , ui . info [ "" bounds "" ] ) <MASK> min_dist , found = dist , ui return found",if dist >= 0 and ( min_dist < 0 or dist < min_dist ) :,if dist < min_dist :,81.69840971,FALSE,83.63
1604,"def __eq__ ( self , other ) : if isinstance ( other , numeric_range ) : empty_self = not bool ( self ) empty_other = not bool ( other ) <MASK> return empty_self and empty_other # True if both empty else : return ( self . _start == other . _start and self . _step == other . _step and self . _get_by_index ( - 1 ) == other . _get_by_index ( - 1 ) ) else : return False",if empty_self or empty_other :,if empty_self and empty_other :,98.34060769,FALSE,97.86
1605,"def _buffered_generator ( self , size ) : buf = [ ] c_size = 0 push = buf . append while 1 : try : while c_size < size : c = next ( self . _gen ) push ( c ) if c : c_size + = 1 except StopIteration : <MASK> return yield concat ( buf ) del buf [ : ] c_size = 0",if not c_size :,if not self . _gen :,93.80757932,FALSE,93.67
1606,"def connect ( self ) : with self . _conn_lock : <MASK> raise Exception ( "" Error, database not properly initialized  "" "" before opening connection "" ) with self . exception_wrapper ( ) : self . __local . conn = self . _connect ( self . database , * * self . connect_kwargs ) self . __local . closed = False self . initialize_connection ( self . __local . conn )",if self . deferred :,if self . __local . closed :,96.22376397,FALSE,93.17
1607,"def _merge_substs ( self , subst , new_substs ) : subst = subst . copy ( ) for new_subst in new_substs : for name , var in new_subst . items ( ) : <MASK> subst [ name ] = var elif subst [ name ] is not var : subst [ name ] . PasteVariable ( var ) return subst",if name not in subst :,if name not in subst :,100,TRUE,100
1608,"def remove ( self , tag ) : """"""Removes a tag recursively from all containers."""""" new_contents = [ ] self . content_size = 0 for element in self . contents : if element . name != tag : new_contents . append ( element ) <MASK> element . remove ( tag ) self . content_size + = element . size ( ) self . contents = new_contents","if isinstance ( element , Container ) :",if element . is_tag_set ( ) :,92.6234839,FALSE,90.22
1609,"def _create_object ( self , obj_body ) : props = obj_body [ SYMBOL_PROPERTIES ] for prop_name , prop_value in props . items ( ) : if isinstance ( prop_value , dict ) and prop_value : # get the first key as the convert function func_name = list ( prop_value . keys ( ) ) [ 0 ] <MASK> func = getattr ( self , func_name ) props [ prop_name ] = func ( prop_value [ func_name ] ) if SYMBOL_TYPE in obj_body and obj_body [ SYMBOL_TYPE ] in self . fake_func_mapping : return self . fake_func_mapping [ obj_body [ SYMBOL_TYPE ] ] ( * * props ) else : return props","if func_name . startswith ( ""_"" ) :",if func_name not in props :,97.28810763,FALSE,94.97
1610,"def visit_try_stmt ( self , o : "" mypy.nodes.TryStmt "" ) - > str : a = [ o . body ] # type: List[Any] for i in range ( len ( o . vars ) ) : a . append ( o . types [ i ] ) <MASK> a . append ( o . vars [ i ] ) a . append ( o . handlers [ i ] ) if o . else_body : a . append ( ( "" Else "" , o . else_body . body ) ) if o . finally_body : a . append ( ( "" Finally "" , o . finally_body . body ) ) return self . dump ( a , o )",if o . vars [ i ] :,if o . handlers :,97.88747525,FALSE,96.29
1611,"def everythingIsUnicode ( d ) : """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k , v in d . iteritems ( ) : if isinstance ( v , dict ) and k != "" headers "" : if not everythingIsUnicode ( v ) : return False elif isinstance ( v , list ) : for i in v : if isinstance ( i , dict ) and not everythingIsUnicode ( i ) : return False <MASK> return False elif isinstance ( v , _bytes ) : return False return True","elif isinstance ( i , _bytes ) :",elif not everythingIsUnicode ( i ) :,94.82379172,FALSE,93.81
1612,"def msg_ser ( inst , sformat , lev = 0 ) : if sformat in [ "" urlencoded "" , "" json "" ] : if isinstance ( inst , Message ) : res = inst . serialize ( sformat , lev ) else : res = inst elif sformat == "" dict "" : if isinstance ( inst , Message ) : res = inst . serialize ( sformat , lev ) <MASK> res = inst elif isinstance ( inst , str ) : # Iff ID Token res = inst else : raise MessageException ( "" Wrong type:  %s "" % type ( inst ) ) else : raise PyoidcError ( "" Unknown sformat "" , inst ) return res","elif isinstance ( inst , dict ) :","elif isinstance ( inst , ( dict , list ) ) :",96.36213708,FALSE,95.41
1613,"def start_container_if_stopped ( self , container , attach_logs = False , quiet = False ) : if not container . is_running : <MASK> log . info ( "" Starting  %s "" % container . name ) if attach_logs : container . attach_log_stream ( ) return self . start_container ( container )",if not quiet :,if quiet :,88.95424352,FALSE,96.48
1614,"def layer_op ( self , input_image , mask = None ) : if not isinstance ( input_image , dict ) : self . _set_full_border ( input_image ) input_image = np . pad ( input_image , self . full_border , mode = self . mode ) return input_image , mask for name , image in input_image . items ( ) : self . _set_full_border ( image ) <MASK> tf . logging . warning ( "" could not pad, dict name  %s  not in  %s "" , name , self . image_name ) continue input_image [ name ] = np . pad ( image , self . full_border , mode = self . mode ) return input_image , mask",if name not in self . image_name :,if name not in self . image_name :,100,TRUE,100
1615,"def __Suffix_Noun_Step2b ( self , token ) : for suffix in self . __suffix_noun_step2b : <MASK> token = token [ : - 2 ] self . suffix_noun_step2b_success = True break return token",if token . endswith ( suffix ) and len ( token ) >= 5 :,if token . endswith ( suffix ) :,60.11021394,FALSE,84.5
1616,"def replace_header_items ( ps , replacments ) : match = read_while ( ps , header_item_or_end_re . match , lambda match : match is None ) while not ps . current_line . startswith ( "" */ "" ) : match = header_item_re . match ( ps . current_line ) <MASK> key = match . groupdict ( ) [ "" key "" ] if key in replacments : ps . current_line = match . expand ( "" \ g<key> \ g<space> %s \n "" % replacments [ key ] ) ps . read_line ( )",if match is not None :,if match :,93.15056999,FALSE,96.57
1617,"def __projectBookmark ( widget , location ) : script = None while widget is not None : <MASK> script = widget . scriptNode ( ) if isinstance ( script , Gaffer . ScriptNode ) : break widget = widget . parent ( ) if script is not None : p = script . context ( ) . substitute ( location ) if not os . path . exists ( p ) : try : os . makedirs ( p ) except OSError : pass return p else : return os . getcwd ( )","if hasattr ( widget , ""scriptNode"" ) :","if isinstance ( widget , Gaffer . ScriptNode ) :",90.35065565,FALSE,93.19
1618,"def events_to_str ( event_field , all_events ) : result = [ ] for ( flag , string ) in all_events : c_flag = flag if event_field & c_flag : result . append ( string ) event_field = event_field & ( ~ c_flag ) <MASK> break if event_field : result . append ( hex ( event_field ) ) return "" | "" . join ( result )",if not event_field :,if not event_field :,100,TRUE,100
1619,"def get_s3_bucket_locations ( buckets , self_log = False ) : """"""return (bucket_name, prefix) for all s3 logging targets"""""" for b in buckets : if b . get ( "" Logging "" ) : if self_log : if b [ "" Name "" ] != b [ "" Logging "" ] [ "" TargetBucket "" ] : continue yield ( b [ "" Logging "" ] [ "" TargetBucket "" ] , b [ "" Logging "" ] [ "" TargetPrefix "" ] ) <MASK> yield ( b [ "" Name "" ] , "" "" )","if not self_log and b [ ""Name"" ] . startswith ( ""cf-templates-"" ) :","elif b [ ""Name"" ] :",75.98462879,FALSE,87.87
1620,"def extract_file ( tgz , tarinfo , dst_path , buffer_size = 10 << 20 , log_function = None ) : """"""Extracts 'tarinfo' from 'tgz' and writes to 'dst_path'."""""" src = tgz . extractfile ( tarinfo ) if src is None : return dst = tf . compat . v1 . gfile . GFile ( dst_path , "" wb "" ) while 1 : buf = src . read ( buffer_size ) if not buf : break dst . write ( buf ) <MASK> log_function ( len ( buf ) ) dst . close ( ) src . close ( )",if log_function is not None :,if log_function :,95.60829537,FALSE,96.51
1621,"def make_index_fields ( rec ) : fields = { } for k , v in rec . iteritems ( ) : if k in ( "" lccn "" , "" oclc "" , "" isbn "" ) : fields [ k ] = v continue <MASK> fields [ "" title "" ] = [ read_short_title ( v ) ] return fields","if k == ""full_title"" :","if k == ""title"" :",89.70418228,FALSE,95.41
1622,"def disconnect_application ( self ) : if not self . is_app_running ( self . APP_BACKDROP ) : self . socket . send ( commands . CloseCommand ( destination_id = False ) ) start_time = time . time ( ) while not self . is_app_running ( None ) : try : self . socket . send_and_wait ( commands . StatusCommand ( ) ) except cast_socket . ConnectionTerminatedException : break current_time = time . time ( ) <MASK> raise TimeoutException ( ) time . sleep ( self . WAIT_INTERVAL ) else : logger . debug ( "" Closing not necessary. Backdrop is running ... "" )",if current_time - start_time > self . timeout :,if current_time - start_time > self . MAX_WAIT_TIME :,98.87997424,FALSE,95.58
1623,"def matches ( self , cursor_offset , line , * * kwargs ) : cs = lineparts . current_string ( cursor_offset , line ) if cs is None : return None matches = set ( ) username = cs . word . split ( os . path . sep , 1 ) [ 0 ] user_dir = os . path . expanduser ( username ) for filename in self . safe_glob ( os . path . expanduser ( cs . word ) ) : if os . path . isdir ( filename ) : filename + = os . path . sep <MASK> filename = username + filename [ len ( user_dir ) : ] matches . add ( filename ) return matches","if cs . word . startswith ( ""~"" ) :",if filename . startswith ( user_dir ) :,94.8497839,FALSE,93.75
1624,"def eventFilter ( self , obj , event ) : if event . type ( ) == QEvent . MouseButtonPress : button = event . button ( ) <MASK> self . _app . browser . back ( ) return True elif button == Qt . ForwardButton : self . _app . browser . forward ( ) return True return False",if button == Qt . BackButton :,if button == Qt . BackButton :,100,TRUE,100
1625,"def reset_parameters ( self ) : for m in self . modules ( ) : if isinstance ( m , nn . Embedding ) : continue <MASK> nn . init . constant_ ( m . weight , 0.1 ) nn . init . constant_ ( m . bias , 0 ) else : for p in m . parameters ( ) : nn . init . normal_ ( p , 0 , 0.1 )","elif isinstance ( m , nn . LayerNorm ) :","if isinstance ( m , nn . Conv2d ) :",66.2320377,FALSE,94.15
1626,"def get_scalding_core ( self ) : lib_dir = os . path . join ( self . scalding_home , "" lib "" ) for j in os . listdir ( lib_dir ) : <MASK> p = os . path . join ( lib_dir , j ) logger . debug ( "" Found scalding-core:  %s "" , p ) return p raise luigi . contrib . hadoop . HadoopJobError ( "" Could not find scalding-core. "" )","if j . startswith ( ""scalding-core-"" ) :","if j . startswith ( ""scalding-core"" ) :",98.45096215,FALSE,97.42
1627,"def save ( self ) : """"""Saves a new set of golden output frames to disk."""""" for pixels , ( relative_to_assets , filename ) in zip ( self . iter_render ( ) , self . _iter_paths ( ) ) : full_directory_path = os . path . join ( self . _ASSETS_DIR , relative_to_assets ) <MASK> os . makedirs ( full_directory_path ) path = os . path . join ( full_directory_path , filename ) _save_pixels ( pixels , path )",if not os . path . exists ( full_directory_path ) :,if not os . path . exists ( full_directory_path ) :,100,TRUE,100
1628,"def _fix_var_naming ( operators , names , mod = "" input "" ) : new_names = [ ] map = { } for op in operators : <MASK> iter = op . inputs else : iter = op . outputs for i in iter : for name in names : if i . raw_name == name and name not in map : map [ i . raw_name ] = i . full_name if len ( map ) == len ( names ) : break for name in names : new_names . append ( map [ name ] ) return new_names","if mod == ""input"" :","if mod == ""input"" :",100,TRUE,100
1629,"def Tokenize ( s ) : # type: (str) -> Iterator[Token] for item in TOKEN_RE . findall ( s ) : # The type checker can't know the true type of item! item = cast ( TupleStr4 , item ) if item [ 0 ] : typ = "" number "" val = item [ 0 ] <MASK> typ = "" name "" val = item [ 1 ] elif item [ 2 ] : typ = item [ 2 ] val = item [ 2 ] elif item [ 3 ] : typ = item [ 3 ] val = item [ 3 ] yield Token ( typ , val )",elif item [ 1 ] :,elif item [ 1 ] :,75,TRUE,100
1630,"def init_errorhandler ( ) : # http error handling for ex in default_exceptions : if ex < 500 : app . register_error_handler ( ex , error_http ) <MASK> app . register_error_handler ( ex , internal_error ) if services . ldap : # Only way of catching the LDAPException upon logging in with LDAP server down @app . errorhandler ( services . ldap . LDAPException ) def handle_exception ( e ) : log . debug ( "" LDAP server not accessible while trying to login to opds feed "" ) return error_http ( FailedDependency ( ) )",elif ex == 500 :,"elif ex . __class__ . __name__ == ""ldap.ldap.ldap.",72.16035517,FALSE,84.16
1631,"def decode ( self , ids ) : ids = pad_decr ( ids ) tokens = [ ] for int_id in ids : <MASK> tokens . append ( self . _vocab_list [ int_id ] ) else : tokens . append ( self . _oov_token ) return self . _decode_token_separator . join ( tokens )",if int_id < len ( self . _vocab_list ) :,if int_id in self . _vocab_list :,90.08014548,FALSE,91.54
1632,"def remove_contest ( contest_id ) : with SessionGen ( ) as session : contest = session . query ( Contest ) . filter ( Contest . id == contest_id ) . first ( ) if not contest : print ( "" No contest with id  %s  found. "" % contest_id ) return False contest_name = contest . name <MASK> print ( "" Not removing contest ` %s ' . "" % contest_name ) return False session . delete ( contest ) session . commit ( ) print ( "" Contest ` %s '  removed. "" % contest_name ) return True",if not ask ( contest ) :,if not session . exists ( contest_name ) :,93.45187627,FALSE,93.94
1633,def get_hi_lineno ( self ) : lineno = Node . get_hi_lineno ( self ) if self . expr1 is None : pass else : lineno = self . expr1 . get_hi_lineno ( ) if self . expr2 is None : pass else : lineno = self . expr2 . get_hi_lineno ( ) <MASK> pass else : lineno = self . expr3 . get_hi_lineno ( ) return lineno,if self . expr3 is None :,if self . expr3 is None :,100,TRUE,100
1634,"def _send_internal ( self , bytes_ ) : # buffering if self . pendings : self . pendings + = bytes_ bytes_ = self . pendings try : # reconnect if possible self . _reconnect ( ) # send message self . socket . sendall ( bytes_ ) # send finished self . pendings = None except Exception : # pylint: disable=broad-except # close socket self . _close ( ) # clear buffer if it exceeds max bufer size <MASK> # TODO: add callback handler here self . pendings = None else : self . pendings = bytes_",if self . pendings and ( len ( self . pendings ) > self . bufmax ) :,if bytes_ > self . max_bufer_size :,67.88335574,FALSE,87.36
1635,"def _unpack ( self , fmt , byt ) : d = unpack ( self . _header [ "" byteorder "" ] + fmt , byt ) [ 0 ] if fmt [ - 1 ] in self . MISSING_VALUES : nmin , nmax = self . MISSING_VALUES [ fmt [ - 1 ] ] if d < nmin or d > nmax : <MASK> return StataMissingValue ( nmax , d ) else : return None return d",if self . _missing_values :,if nmin == 0 and nmax == 0 :,91.83705018,FALSE,88.62
1636,"def tuple_iter ( self ) : for x in range ( self . center . x - self . max_radius , self . center . x + self . max_radius + 1 ) : for y in range ( self . center . y - self . max_radius , self . center . y + self . max_radius + 1 ) : <MASK> yield ( x , y )","if self . min_radius <= self . center . distance ( ( x , y ) ) <= self . max_radius :","if ( x , y ) in self . _points :",60.35585769,FALSE,76.98
1637,"def _parse_gene ( element ) : for genename_element in element : <MASK> ann_key = "" gene_ %s _ %s "" % ( genename_element . tag . replace ( NS , "" "" ) , genename_element . attrib [ "" type "" ] , ) if genename_element . attrib [ "" type "" ] == "" primary "" : self . ParsedSeqRecord . annotations [ ann_key ] = genename_element . text else : append_to_annotations ( ann_key , genename_element . text )","if ""type"" in genename_element . attrib :","if genename_element . tag . startswith ( ""annotation"" ) :",90.91977548,FALSE,91.23
1638,"def invalidateDependentSlices ( self , iFirstCurve ) : # only user defined curve can have slice dependency relationships if self . isSystemCurveIndex ( iFirstCurve ) : return nCurves = self . getNCurves ( ) for i in range ( iFirstCurve , nCurves ) : c = self . getSystemCurve ( i ) if isinstance ( c . getSymbol ( ) . getSymbolType ( ) , SymbolType . PieSliceSymbolType ) : c . invalidate ( ) <MASK> # if first curve isn't a slice, break # there are no dependent slices",elif i == iFirstCurve :,elif c . isDependent ( ) :,72.10645791,FALSE,93.17
1639,"def gen_app_versions ( self ) : for app_config in apps . get_app_configs ( ) : name = app_config . verbose_name app = app_config . module version = self . get_app_version ( app ) <MASK> yield app . __name__ , name , version",if version :,if version is not None :,93.70614929,FALSE,93.48
1640,"def verify_relative_valid_path ( root , path ) : if len ( path ) < 1 : raise PackagerError ( "" Empty chown path "" ) checkpath = root parts = path . split ( os . sep ) for part in parts : if part in ( "" . "" , "" .. "" ) : raise PackagerError ( "" . and .. is not allowed in chown path "" ) checkpath = os . path . join ( checkpath , part ) relpath = checkpath [ len ( root ) + 1 : ] <MASK> raise PackagerError ( f "" chown path  { relpath }  does not exist "" ) if os . path . islink ( checkpath ) : raise PackagerError ( f "" chown path  { relpath }  is a soft link "" )",if not os . path . exists ( checkpath ) :,if not os . path . exists ( checkpath ) :,100,TRUE,100
1641,"def create_or_update_tag_at_scope ( cmd , resource_id = None , tags = None , tag_name = None ) : rcf = _resource_client_factory ( cmd . cli_ctx ) if resource_id is not None : <MASK> raise IncorrectUsageError ( "" Tags could not be empty. "" ) Tags = cmd . get_models ( "" Tags "" ) tag_obj = Tags ( tags = tags ) return rcf . tags . create_or_update_at_scope ( scope = resource_id , properties = tag_obj ) return rcf . tags . create_or_update ( tag_name = tag_name )",if not tags :,if not tags :,100,TRUE,100
1642,"def generate_auto_complete ( self , base , iterable_var ) : sugg = [ ] for entry in iterable_var : compare_entry = entry compare_base = base if self . settings . get ( IGNORE_CASE_SETTING ) : compare_entry = compare_entry . lower ( ) compare_base = compare_base . lower ( ) <MASK> if entry not in sugg : sugg . append ( entry ) return sugg","if self . compare_entries ( compare_entry , compare_base ) :",if compare_entry == base and compare_base == base :,90.85361715,FALSE,87.59
1643,"def createFields ( self ) : yield String ( self , "" dict_start "" , 2 ) while not self . eof : addr = self . absolute_address + self . current_size <MASK> for field in parsePDFType ( self ) : yield field else : break yield String ( self , "" dict_end "" , 2 )","if self . stream . readBytes ( addr , 2 ) != "">>"" :",if self . is_pdf :,80.51830763,FALSE,80.06
1644,"def Visit_and_test ( self , node ) : # pylint: disable=invalid-name # and_test ::= not_test ('and' not_test)* for child in node . children : self . Visit ( child ) <MASK> _AppendTokenSubtype ( child , format_token . Subtype . BINARY_OPERATOR )","if isinstance ( child , pytree . Leaf ) and child . value == ""and"" :","if isinstance ( child , pytree . Leaf ) and child . value == ""and"" :",75,TRUE,100
1645,"def getfiledata ( directories ) : columns = None data = [ ] counter = 1 for directory in directories : for f in os . listdir ( directory ) : if not os . path . isfile ( os . path . join ( directory , f ) ) : continue counter + = 1 st = os . stat ( os . path . join ( directory , f ) ) <MASK> columns = [ "" rowid "" , "" name "" , "" directory "" ] + [ x for x in dir ( st ) if x . startswith ( "" st_ "" ) ] data . append ( [ counter , f , directory ] + [ getattr ( st , x ) for x in columns [ 3 : ] ] ) return columns , data",if columns is None :,if columns is None :,100,TRUE,100
1646,"def copy_attributes ( info_add , obj , name_fmt , attributes , formatter = None ) : for attr in attributes : value = getattr ( obj , attr , None ) if value is None : continue name = name_fmt % attr <MASK> value = formatter ( attr , value ) info_add ( name , value )",if formatter is not None :,if formatter is not None :,100,TRUE,100
1647,"def main ( args ) : ap = argparse . ArgumentParser ( ) ap . add_argument ( "" job_ids "" , nargs = "" + "" , type = int , help = "" ID of a running job "" ) ns = ap . parse_args ( args ) _stash = globals ( ) [ "" _stash "" ] """""":type : StaSh"""""" for job_id in ns . job_ids : <MASK> print ( "" killing job  {}  ... "" . format ( job_id ) ) worker = _stash . runtime . worker_registry . get_worker ( job_id ) worker . kill ( ) time . sleep ( 1 ) else : print ( "" error: no such job with id:  {} "" . format ( job_id ) ) break",if job_id in _stash . runtime . worker_registry :,if _stash . runtime . is_running ( job_id ) :,95.41269729,FALSE,94.69
1648,"def _check_choice ( self ) : if self . type == "" choice "" : if self . choices is None : raise OptionError ( "" must supply a list of choices for type  ' choice ' "" , self ) <MASK> raise OptionError ( "" choices must be a list of strings ( ' %s '  supplied) "" % str ( type ( self . choices ) ) . split ( "" ' "" ) [ 1 ] , self , ) elif self . choices is not None : raise OptionError ( "" must not supply choices for type  %r "" % self . type , self )","elif type ( self . choices ) not in ( types . TupleType , types . ListType ) :","if not isinstance ( self . choices , ( list , tuple ) ) :",65.44203149,FALSE,88.68
1649,"def add_file ( pipe , srcpath , tgtpath ) : with open ( srcpath , "" rb "" ) as handle : <MASK> write ( pipe , enc ( "" M 100755 inline  %s \n "" % tgtpath ) ) else : write ( pipe , enc ( "" M 100644 inline  %s \n "" % tgtpath ) ) data = handle . read ( ) write ( pipe , enc ( "" data  %d \n "" % len ( data ) ) ) write ( pipe , enc ( data ) ) write ( pipe , enc ( "" \n "" ) )","if os . access ( srcpath , os . X_OK ) :","if sys . platform == ""win32"" :",92.63439826,FALSE,89.33
1650,"def cdf ( self , x ) : if x == numpy . inf : return 1.0 else : # Inefficient sum. <MASK> raise RuntimeError ( "" Invalid value. "" ) c = 0.0 for i in xrange ( x + 1 ) : c + = self . probability ( i ) return c",if x != int ( x ) :,if x < 0 :,69.77557365,FALSE,88.28
1651,"def convert_to_strings ( self , out , seq_len ) : results = [ ] for b , batch in enumerate ( out ) : utterances = [ ] for p , utt in enumerate ( batch ) : size = seq_len [ b ] [ p ] <MASK> transcript = "" "" . join ( map ( lambda x : self . int_to_char [ x . item ( ) ] , utt [ 0 : size ] ) ) else : transcript = "" "" utterances . append ( transcript ) results . append ( utterances ) return results",if size > 0 :,if size :,95.31448117,FALSE,96.91
1652,"def get_date_range ( self ) : if not hasattr ( self , "" start "" ) or not hasattr ( self , "" end "" ) : args = ( self . today . year , self . today . month ) form = self . get_form ( ) <MASK> args = ( int ( form . cleaned_data [ "" year "" ] ) , int ( form . cleaned_data [ "" month "" ] ) ) self . start = self . get_start ( * args ) self . end = self . get_end ( * args ) return self . start , self . end",if form . is_valid ( ) :,if form . is_valid ( ) :,100,TRUE,100
1653,"def save_stats ( self ) : LOGGER . info ( "" Saving task-level statistics. "" ) has_headers = os . path . isfile ( paths . TABLE_COUNT_PATH ) with open ( paths . TABLE_COUNT_PATH , "" a "" ) as csvfile : headers = [ "" start_time "" , "" database_name "" , "" number_tables "" ] writer = csv . DictWriter ( csvfile , delimiter = "" , "" , lineterminator = "" \n "" , fieldnames = headers ) <MASK> writer . writeheader ( ) writer . writerow ( { "" start_time "" : self . start_time , "" database_name "" : self . database_name , "" number_tables "" : self . count , } )",if not has_headers :,if has_headers :,97.1400246,FALSE,98.35
1654,"def _CheckCanaryCommand ( self ) : <MASK> # fast path return with self . _lock : <MASK> return logging . info ( "" Testing OpenStack CLI command is installed and working "" ) cmd = os_utils . OpenStackCLICommand ( self , "" image "" , "" list "" ) stdout , stderr , _ = cmd . Issue ( ) if stderr : raise errors . Config . InvalidValue ( "" OpenStack CLI test command failed. Please make sure the OpenStack  "" "" CLI client is installed and properly configured "" ) OpenStackVirtualMachine . command_works = True",if OpenStackVirtualMachine . command_works :,if OpenStackVirtualMachine . command_works :,75,TRUE,100
1655,"def test_windows_hidden ( self ) : if not sys . platform == "" win32 "" : self . skipTest ( "" sys.platform is not windows "" ) return # FILE_ATTRIBUTE_HIDDEN = 2 (0x2) from GetFileAttributes documentation. hidden_mask = 2 with tempfile . NamedTemporaryFile ( ) as f : # Hide the file using success = ctypes . windll . kernel32 . SetFileAttributesW ( f . name , hidden_mask ) <MASK> self . skipTest ( "" unable to set file attributes "" ) self . assertTrue ( hidden . is_hidden ( f . name ) )",if not success :,if not success :,75,TRUE,100
1656,"def recv_some ( p , t = 0.1 , e = 1 , tr = 5 , stderr = 0 ) : if tr < 1 : tr = 1 x = time . time ( ) + t y = [ ] r = "" "" if stderr : pr = p . recv_err else : pr = p . recv while time . time ( ) < x or r : r = pr ( ) if r is None : break <MASK> y . append ( r ) else : time . sleep ( max ( ( x - time . time ( ) ) / tr , 0 ) ) return b "" "" . join ( y )",elif r :,if e :,67.88563189,FALSE,97.24
1657,"def _is_xml ( accepts ) : if accepts . startswith ( b "" application/ "" ) : has_xml = accepts . find ( b "" xml "" ) <MASK> semicolon = accepts . find ( b "" ; "" ) if semicolon < 0 or has_xml < semicolon : return True return False",if has_xml > 0 :,if has_xml > 0 :,100,TRUE,100
1658,"def times ( self , value : int ) : if value is None : self . _times = None else : try : candidate = int ( value ) except ValueError : # pylint: disable:raise-missing-from raise BarException ( f "" cannot set repeat times to:  { value !r} "" ) if candidate < 0 : raise BarException ( f "" cannot set repeat times to a value less than zero:  { value } "" ) <MASK> raise BarException ( "" cannot set repeat times on a start Repeat "" ) self . _times = candidate","if self . direction == ""start"" :",if candidate > self . start_repeat :,97.1782159,FALSE,92.41
1659,"def __call__ ( self , * args , * * kwargs ) : if not NET_INITTED : return self . raw ( * args , * * kwargs ) for stack in traceback . walk_stack ( None ) : if "" self "" in stack [ 0 ] . f_locals : layer = stack [ 0 ] . f_locals [ "" self "" ] <MASK> log . pytorch_layer_name = layer_names [ layer ] print ( layer_names [ layer ] ) break out = self . obj ( self . raw , * args , * * kwargs ) # if isinstance(out,Variable): #     out=[out] return out",if layer in layer_names :,if layer in layer_names :,100,TRUE,100
1660,"def do_begin ( self , byte ) : if byte . isspace ( ) : return if byte != "" < "" : <MASK> self . _leadingBodyData = byte return "" bodydata "" self . _parseError ( "" First char of document [ {!r} ] wasn ' t < "" . format ( byte ) ) return "" tagstart """,if self . beExtremelyLenient :,if self . _leadingBodyData is None :,93.99835059,FALSE,92.48
1661,"def pretty ( self , n , comment = True ) : if isinstance ( n , ( str , bytes , list , tuple , dict ) ) : r = repr ( n ) <MASK> # then it can be inside a comment! r = r . replace ( "" */ "" , r "" \ x2a/ "" ) return r if not isinstance ( n , six . integer_types ) : return n if isinstance ( n , constants . Constant ) : if comment : return "" %s  /*  %s  */ "" % ( n , self . pretty ( int ( n ) ) ) else : return "" %s  ( %s ) "" % ( n , self . pretty ( int ( n ) ) ) elif abs ( n ) < 10 : return str ( n ) else : return hex ( n )",if not comment :,"if r . startswith ( ""/*"" ) :",88.63889342,FALSE,93.91
1662,"def test_training_script_with_max_history_set ( tmpdir ) : train_dialogue_model ( DEFAULT_DOMAIN_PATH , DEFAULT_STORIES_FILE , tmpdir . strpath , interpreter = RegexInterpreter ( ) , policy_config = "" data/test_config/max_hist_config.yml "" , kwargs = { } , ) agent = Agent . load ( tmpdir . strpath ) for policy in agent . policy_ensemble . policies : if hasattr ( policy . featurizer , "" max_history "" ) : <MASK> assert policy . featurizer . max_history == 2 else : assert policy . featurizer . max_history == 5",if type ( policy ) == FormPolicy :,"if sys . version_info < ( 3 , 0 ) :",92.31493942,FALSE,91.56
1663,"def cli_uninstall_distro ( ) : distro_list = install_distro_list ( ) if distro_list is not None : for index , _distro_dir in enumerate ( distro_list ) : log ( str ( index ) + ""   --->>   "" + _distro_dir ) user_input = read_input_uninstall ( ) <MASK> for index , _distro_dir in enumerate ( distro_list ) : if index == user_input : config . uninstall_distro_dir_name = _distro_dir unin_distro ( ) else : log ( "" No distro installed on  "" + config . usb_disk )",if user_input is not False :,if user_input is not None :,90.27796929,FALSE,98.17
1664,"def set_random_avatar ( user ) : galleries = get_available_galleries ( include_default = True ) if not galleries : raise RuntimeError ( "" no avatar galleries are set "" ) avatars_list = [ ] for gallery in galleries : <MASK> avatars_list = gallery [ "" images "" ] break else : avatars_list + = gallery [ "" images "" ] random_avatar = random . choice ( avatars_list ) store . store_new_avatar ( user , Image . open ( random_avatar . image ) )","if gallery [ ""name"" ] == DEFAULT_GALLERY :","if not gallery [ ""image_id"" ] :",92.36725586,FALSE,91.49
1665,"def make_query ( self , key , filters ) : meta = self . get_meta ( key ) q = { meta . facet_key : self . normalize_key ( meta . path ) } if filters : if filters . get ( "" has_fulltext "" ) == "" true "" : q [ "" has_fulltext "" ] = "" true "" <MASK> q [ "" publish_year "" ] = filters [ "" publish_year "" ] return q","if filters . get ( ""publish_year"" ) :","if filters . get ( ""publish_year"" ) :",100,TRUE,100
1666,"def test_named_parameters_and_constraints ( self ) : likelihood = gpytorch . likelihoods . GaussianLikelihood ( ) model = ExactGPModel ( None , None , likelihood ) for name , _param , constraint in model . named_parameters_and_constraints ( ) : if name == "" likelihood.noise_covar.raw_noise "" : self . assertIsInstance ( constraint , gpytorch . constraints . GreaterThan ) <MASK> self . assertIsNone ( constraint ) elif name == "" covar_module.raw_outputscale "" : self . assertIsInstance ( constraint , gpytorch . constraints . Positive ) elif name == "" covar_module.base_kernel.raw_lengthscale "" : self . assertIsInstance ( constraint , gpytorch . constraints . Positive )","elif name == ""mean_module.constant"" :","elif name == ""gpytorch.gpytorch.gpytorch",94.89963439,FALSE,94.62
1667,"def _test_pooling ( input_shape , * * kwargs ) : _test_pooling_iteration ( input_shape , * * kwargs ) if is_gpu_available ( ) : <MASK> input_shape = [ input_shape [ ii ] for ii in ( 0 , 3 , 1 , 2 ) ] kwargs [ "" data_format "" ] = "" NCHW "" _test_pooling_iteration ( input_shape , * * kwargs )",if len ( input_shape ) == 4 :,"if isinstance ( input_shape , ( list , tuple ) ) :",88.56009196,FALSE,89.45
1668,"def init ( self ) : r = self . get_redis ( ) if r : key = "" pocsuite_target "" info_msg = "" [PLUGIN] try fetch targets from redis... "" logger . info ( info_msg ) targets = r . get ( key ) count = 0 <MASK> for target in targets : if self . add_target ( target ) : count + = 1 info_msg = "" [PLUGIN] get  {0}  target(s) from redis "" . format ( count ) logger . info ( info_msg )",if targets :,if targets :,100,TRUE,100
1669,"def reload_json_api_settings ( * args , * * kwargs ) : django_setting = kwargs [ "" setting "" ] setting = django_setting . replace ( JSON_API_SETTINGS_PREFIX , "" "" ) value = kwargs [ "" value "" ] if setting in DEFAULTS . keys ( ) : if value is not None : setattr ( json_api_settings , setting , value ) <MASK> delattr ( json_api_settings , setting )","elif hasattr ( json_api_settings , setting ) :",elif setting in DEVELOPMENT_SETTINGS_KEYWORDS . keys ( ) :,92.22731568,FALSE,89.69
1670,"def update_metadata ( self ) : for attrname in dir ( self ) : if attrname . startswith ( "" __ "" ) : continue attrvalue = getattr ( self , attrname , None ) if attrvalue == 0 : continue if attrname == "" salt_version "" : attrname = "" version "" if hasattr ( self . metadata , "" set_ {0} "" . format ( attrname ) ) : getattr ( self . metadata , "" set_ {0} "" . format ( attrname ) ) ( attrvalue ) <MASK> try : setattr ( self . metadata , attrname , attrvalue ) except AttributeError : pass","elif hasattr ( self . metadata , attrname ) :","elif hasattr ( self . metadata , attrname ) :",100,TRUE,100
1671,"def test_02_looking_at_listdir_path_ ( name ) : for dline in listdir . json ( ) : <MASK> assert dline [ "" type "" ] in ( "" DIRECTORY "" , "" FILE "" ) , listdir . text assert dline [ "" uid "" ] == 0 , listdir . text assert dline [ "" gid "" ] == 0 , listdir . text assert dline [ "" name "" ] == name , listdir . text break else : raise AssertionError ( f "" / { path } / { name }  not found "" )","if dline [ ""path"" ] == f""{path}/{name}"" :","if dline [ ""path"" ] == path :",95.00542466,FALSE,90.99
1672,"def DeletePlugin ( ) : oid = request . form . get ( "" oid "" , "" "" ) if oid : result = Mongo . coll [ "" Plugin "" ] . find_one_and_delete ( { "" _id "" : ObjectId ( oid ) } , remove = True ) <MASK> result [ "" filename "" ] = result [ "" filename "" ] + "" .py "" if os . path . exists ( file_path + result [ "" filename "" ] ) : os . remove ( file_path + result [ "" filename "" ] ) return "" success "" return "" fail ""","if not result [ ""filename"" ] . find ( ""."" ) > - 1 :",if result :,90.56644545,FALSE,85.95
1673,"def iterparent ( self , node ) : """"""Iterator wrapper to get allowed parent and child all at once."""""" # We do not allow the marker inside a header as that # would causes an enless loop of placing a new TOC # inside previously generated TOC. for child in node : <MASK> yield node , child yield from self . iterparent ( child )","if not self . header_rgx . match ( child . tag ) and child . tag not in [ ""pre"" , ""code"" ] :",if self . marker_has_marker ( child ) :,61.81454391,FALSE,71.29
1674,"def _get_matched_layout ( command ) : # don't use command.split_script here because a layout mismatch will likely # result in a non-splitable script as per shlex cmd = command . script . split ( ""   "" ) for source_layout in source_layouts : is_all_match = True for cmd_part in cmd : if not all ( [ ch in source_layout or ch in "" -_ "" for ch in cmd_part ] ) : is_all_match = False break <MASK> return source_layout",if is_all_match :,if is_all_match :,75,TRUE,100
1675,"def _update_tileable_and_chunk_shape ( self , tileable_graph , chunk_result , failed_ops ) : for n in tileable_graph : if n . op in failed_ops : continue tiled_n = get_tiled ( n ) if has_unknown_shape ( tiled_n ) : <MASK> # some of the chunks has been fused continue new_nsplits = self . get_tileable_nsplits ( n , chunk_result = chunk_result ) for node in ( n , tiled_n ) : node . _update_shape ( tuple ( sum ( nsplit ) for nsplit in new_nsplits ) ) tiled_n . _nsplits = new_nsplits",if any ( c . key not in chunk_result for c in tiled_n . chunks ) :,if not chunk_result :,78.46872012,FALSE,87.79
1676,"def _get_items ( self , name , target = 1 ) : all_items = self . get_items ( name ) items = [ o for o in all_items if not o . disabled ] if len ( items ) < target : if len ( all_items ) < target : raise ItemNotFoundError ( "" insufficient items with name  %r "" % name ) else : raise AttributeError ( "" insufficient non-disabled items with name  %s "" % name ) on = [ ] off = [ ] for o in items : <MASK> on . append ( o ) else : off . append ( o ) return on , off",if o . selected :,if o . disabled :,97.03602232,FALSE,98.01
1677,def parse_flow_sequence_entry_mapping_value ( self ) : if self . check_token ( ValueToken ) : token = self . get_token ( ) <MASK> self . states . append ( self . parse_flow_sequence_entry_mapping_end ) return self . parse_flow_node ( ) else : self . state = self . parse_flow_sequence_entry_mapping_end return self . process_empty_scalar ( token . end_mark ) else : self . state = self . parse_flow_sequence_entry_mapping_end token = self . peek_token ( ) return self . process_empty_scalar ( token . start_mark ),"if not self . check_token ( FlowEntryToken , FlowSequenceEndToken ) :",if token . end_mark == FlowSequenceEndToken . START_FLOW :,69.01836965,FALSE,91.88
1678,"def serialize_config ( self , session , key , tid , language ) : cache_key = gen_cache_key ( key , tid , language ) cache_obj = None if cache_key not in self . cache : <MASK> cache_obj = db_admin_serialize_node ( session , tid , language ) elif key == "" notification "" : cache_obj = db_get_notification ( session , tid , language ) self . cache [ cache_key ] = cache_obj return self . cache [ cache_key ]","if key == ""node"" :","if key == ""admin"" :",98.45291226,FALSE,97.84
1679,"def get_lldp_neighbors ( self ) : commands = [ "" show lldp neighbors "" ] output = self . device . run_commands ( commands ) [ 0 ] [ "" lldpNeighbors "" ] lldp = { } for n in output : <MASK> lldp [ n [ "" port "" ] ] = [ ] lldp [ n [ "" port "" ] ] . append ( { "" hostname "" : n [ "" neighborDevice "" ] , "" port "" : n [ "" neighborPort "" ] } ) return lldp","if n [ ""port"" ] not in lldp . keys ( ) :","if n [ ""port"" ] not in lldp :",92.7876539,FALSE,95.04
1680,"def handle ( self ) : from poetry . utils . env import EnvManager manager = EnvManager ( self . poetry ) current_env = manager . get ( ) for venv in manager . list ( ) : name = venv . path . name if self . option ( "" full-path "" ) : name = str ( venv . path ) <MASK> self . line ( "" <info> {}  (Activated)</info> "" . format ( name ) ) continue self . line ( name )",if venv == current_env :,if name in current_env . get_name ( ) :,93.77086716,FALSE,89.72
1681,"def resolve_env_secrets ( config , environ ) : """"""Create copy that recursively replaces {""$env"": ""NAME""} with values from environ"""""" if isinstance ( config , dict ) : if list ( config . keys ( ) ) == [ "" $env "" ] : return environ . get ( list ( config . values ( ) ) [ 0 ] ) <MASK> return open ( list ( config . values ( ) ) [ 0 ] ) . read ( ) else : return { key : resolve_env_secrets ( value , environ ) for key , value in config . items ( ) } elif isinstance ( config , list ) : return [ resolve_env_secrets ( value , environ ) for value in config ] else : return config","elif list ( config . keys ( ) ) == [ ""$file"" ] :","elif list ( config . keys ( ) ) == [ ""$name"" ] :",74.11378326,FALSE,98.54
1682,"def _is_valid_16bit_as_path ( cls , buf ) : two_byte_as_size = struct . calcsize ( "" !H "" ) while buf : ( type_ , num_as ) = struct . unpack_from ( cls . _SEG_HDR_PACK_STR , six . binary_type ( buf ) ) if type_ is not cls . _AS_SET and type_ is not cls . _AS_SEQUENCE : return False buf = buf [ struct . calcsize ( cls . _SEG_HDR_PACK_STR ) : ] <MASK> return False buf = buf [ num_as * two_byte_as_size : ] return True",if len ( buf ) < num_as * two_byte_as_size :,if num_as * two_byte_as_size > len ( buf ) :,92.73054716,FALSE,96.42
1683,"def reparentChildren ( self , newParent ) : if newParent . childNodes : newParent . childNodes [ - 1 ] . _element . tail + = self . _element . text else : <MASK> newParent . _element . text = "" "" if self . _element . text is not None : newParent . _element . text + = self . _element . text self . _element . text = "" "" base . Node . reparentChildren ( self , newParent )",if not newParent . _element . text :,if self . _element . text is None :,83.82753101,FALSE,93.72
1684,"def get_operation_ast ( document_ast , operation_name = None ) : operation = None for definition in document_ast . definitions : if isinstance ( definition , ast . OperationDefinition ) : if not operation_name : # If no operation name is provided, only return an Operation if it is the only one present in the # document. This means that if we've encountered a second operation as we were iterating over the # definitions in the document, there are more than one Operation defined, and we should return None. if operation : return None operation = definition <MASK> return definition return operation",elif definition . name and definition . name . value == operation_name :,"elif isinstance ( definition , ast . Name ) :",94.71352489,FALSE,88.52
1685,"def reprSmart ( vw , item ) : ptype = type ( item ) if ptype is int : <MASK> return str ( item ) elif vw . isValidPointer ( item ) : return vw . reprPointer ( item ) else : return hex ( item ) elif ptype in ( list , tuple ) : return reprComplex ( vw , item ) # recurse elif ptype is dict : return "" { %s } "" % "" , "" . join ( [ "" %s : %s "" % ( reprSmart ( vw , k ) , reprSmart ( vw , v ) ) for k , v in item . items ( ) ] ) else : return repr ( item )",if - 1024 < item < 1024 :,if vw . isValidString ( item ) :,91.02748343,FALSE,94.51
1686,"def cleanDataCmd ( cmd ) : newcmd = "" AbracadabrA ** <?php  "" if cmd [ : 6 ] != "" php:// "" : <MASK> cmds = cmd . split ( "" & "" ) for c in cmds : if len ( c ) > 0 : newcmd + = "" system( ' %s ' ); "" % c else : b64cmd = base64 . b64encode ( cmd ) newcmd + = "" system(base64_decode( ' %s ' )); "" % b64cmd else : newcmd + = cmd [ 6 : ] newcmd + = "" ?> ** "" return newcmd",if reverseConn not in cmd :,"if ""&"" in cmd :",95.91464513,FALSE,96.4
1687,"def render_tasks ( self ) - > List : results = [ ] for task in self . tasks . values ( ) : job_entry = self . jobs . get ( task . job_id ) <MASK> if not self . should_render_job ( job_entry ) : continue files = self . get_file_counts ( [ task ] ) entry = ( task . job_id , task . task_id , task . state , task . type . name , task . target , files , task . pool , task . end_time , ) results . append ( entry ) return results",if job_entry :,if job_entry :,100,TRUE,100
1688,"def __call__ ( self , environ , start_response ) : for key in "" REQUEST_URL "" , "" REQUEST_URI "" , "" UNENCODED_URL "" : if key not in environ : continue request_uri = unquote ( environ [ key ] ) script_name = unquote ( environ . get ( "" SCRIPT_NAME "" , "" "" ) ) <MASK> environ [ "" PATH_INFO "" ] = request_uri [ len ( script_name ) : ] . split ( "" ? "" , 1 ) [ 0 ] break return self . app ( environ , start_response )",if request_uri . startswith ( script_name ) :,if request_uri . startswith ( script_name ) :,100,TRUE,100
1689,"def _add_role_information ( self , function_dict , role_id ) : # Make it easier to build rules based on policies attached to execution roles function_dict [ "" role_arn "" ] = role_id role_name = role_id . split ( "" / "" ) [ - 1 ] function_dict [ "" execution_role "" ] = await self . facade . awslambda . get_role_with_managed_policies ( role_name ) if function_dict . get ( "" execution_role "" ) : statements = [ ] for policy in function_dict [ "" execution_role "" ] . get ( "" policies "" ) : <MASK> statements + = policy [ "" Document "" ] [ "" Statement "" ] function_dict [ "" execution_role "" ] [ "" policy_statements "" ] = statements","if ""Document"" in policy and ""Statement"" in policy [ ""Document"" ] :","if policy [ ""Document"" ] . get ( ""PolicyType"" ) == ""PolicyType",70.8390594,FALSE,92.83
1690,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 8 : self . set_ts ( d . getVarInt64 ( ) ) continue <MASK> raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 0 :,if tt == 0 :,100,TRUE,100
1691,"def format_counts ( results , json_output = False , human_readable = False ) : if json_output : for result in results : yield json . dumps ( result ) else : for result in results : space_consumed = result . get ( "" spaceConsumed "" ) <MASK> space_consumed = _sizeof_fmt ( int ( result . get ( "" spaceConsumed "" ) ) ) yield "" %12s   %12s   %18s   %s "" % ( result . get ( "" directoryCount "" ) , result . get ( "" fileCount "" ) , space_consumed , result . get ( "" path "" ) , )",if human_readable :,if human_readable :,100,TRUE,100
1692,"def parse_edges ( self , pcb ) : edges = [ ] drawings = list ( pcb . GetDrawings ( ) ) bbox = None for m in pcb . GetModules ( ) : for g in m . GraphicalItems ( ) : drawings . append ( g ) for d in drawings : if d . GetLayer ( ) == pcbnew . Edge_Cuts : parsed_drawing = self . parse_drawing ( d ) if parsed_drawing : edges . append ( parsed_drawing ) <MASK> bbox = d . GetBoundingBox ( ) else : bbox . Merge ( d . GetBoundingBox ( ) ) if bbox : bbox . Normalize ( ) return edges , bbox",if bbox is None :,elif d . GetLayer ( ) == pcbnew . BoundingBox_Cuts :,93.23179539,FALSE,89.52
1693,"def __getitem__ ( self , k ) - > "" SimMemView "" : if isinstance ( k , slice ) : if k . step is not None : raise ValueError ( "" Slices with strides are not supported "" ) <MASK> raise ValueError ( "" Must specify start index "" ) elif k . stop is not None : raise ValueError ( "" Slices with stop index are not supported "" ) else : addr = k . start elif self . _type is not None and self . _type . _can_refine_int : return self . _type . _refine ( self , k ) else : addr = k return self . _deeper ( addr = addr )",elif k . start is None :,elif k . start is not None :,99.18808334,FALSE,98.18
1694,"def _parse ( self , stream , context ) : obj = [ ] try : if self . subcon . conflags & self . FLAG_COPY_CONTEXT : while True : subobj = self . subcon . _parse ( stream , context . __copy__ ( ) ) obj . append ( subobj ) <MASK> break else : while True : subobj = self . subcon . _parse ( stream , context ) obj . append ( subobj ) <MASK> break except ConstructError as ex : raise ArrayError ( "" missing terminator "" , ex ) return obj","if self . predicate ( subobj , context ) :",if self . _is_terminator ( subobj ) :,88.34676874,FALSE,89.16
1695,"def before_run ( self , run_context ) : if "" featurizer "" in self . model_portion and ( self . need_to_refresh or self . refresh_base_model ) : <MASK> self . refresh_base_model = True self . init_fn ( None , run_context . session , self . model_portion , self . refresh_base_model ) self . need_to_refresh = False self . refresh_base_model = False","if self . model_portion == ""whole_featurizer"" :",if self . init_fn :,90.82090495,FALSE,89.49
1696,"def run ( self ) : while True : task = self . requestQueue . get ( ) if task is None : # The ""None"" value is used as a sentinel by # ThreadPool.cleanup().  This indicates that there # are no more tasks, so we should quit. break try : <MASK> raise SCons . Errors . BuildError ( task . targets [ 0 ] , errstr = interrupt_msg ) task . execute ( ) except : task . exception_set ( ) ok = False else : ok = True self . resultsQueue . put ( ( task , ok ) )",if self . interrupted ( ) :,if task . exception_set ( ) :,98.03405004,FALSE,94.84
1697,"def get_overdue_evergreen_documents ( * , db_session ) - > List [ Optional [ Document ] ] : """"""Returns all documents that have need had a recent evergreen notification."""""" documents = ( db_session . query ( Document ) . filter ( Document . evergreen == True ) ) . all ( ) # noqa overdue_documents = [ ] now = datetime . utcnow ( ) for d in documents : next_reminder = d . evergreen_last_reminder_at + timedelta ( days = d . evergreen_reminder_interval ) <MASK> overdue_documents . append ( d ) return overdue_documents",if now > next_reminder :,if now - next_reminder > now :,97.8894772,FALSE,95.69
1698,"def create_local_app_folder ( local_app_path ) : if exists ( local_app_path ) : raise ValueError ( "" There is already a  ' %s '  folder! Aborting! "" % local_app_path ) for folder in subfolders ( local_app_path ) : if not exists ( folder ) : os . mkdir ( folder ) init_path = join ( folder , "" __init__.py "" ) <MASK> create_file ( init_path )",if not exists ( init_path ) :,if not exists ( init_path ) :,100,TRUE,100
1699,"def generate ( ) : for leaf in u . leaves : <MASK> val = leaf . get_int_value ( ) if val in ( 0 , 1 ) : yield val else : raise _NoBoolVector elif isinstance ( leaf , Symbol ) : if leaf == SymbolTrue : yield 1 elif leaf == SymbolFalse : yield 0 else : raise _NoBoolVector else : raise _NoBoolVector","if isinstance ( leaf , Integer ) :","if isinstance ( leaf , ( Int , Vector ) ) :",94.51577706,FALSE,92.18
1700,"def replace ( self , old , new ) : v_m = self . var_map size = v_m [ self . size ] if not ( size . is_const ( ) or size . is_ident ( ) ) : size . replace ( old , new ) else : <MASK> v_m [ new . value ( ) ] = new self . size = new . value ( ) else : v_m [ old ] = new",if new . is_ident ( ) :,if new . is_ident ( ) :,100,TRUE,100
1701,"def method_for_doctype ( doctype ) : method = "" xhtml "" if doctype : if doctype . startswith ( "" html "" ) : method = "" html "" <MASK> method = "" xhtml "" elif doctype . startswith ( "" svg "" ) : method = "" xml "" else : method = "" xhtml "" return method","elif doctype . startswith ( ""xhtml"" ) :","elif doctype . startswith ( ""xhtml"" ) :",100,TRUE,100
1702,"def delete ( self , trans , * * kwd ) : idnum = kwd [ self . tagged_item_id ] item = self . _get_item_from_id ( trans , idnum , check_writable = True ) if item is not None : ex_obj = self . get_item_extended_metadata_obj ( trans , item ) <MASK> self . unset_item_extended_metadata_obj ( trans , item ) self . delete_extended_metadata ( trans , ex_obj )",if ex_obj is not None :,if ex_obj is not None :,100,TRUE,100
1703,"def check_testv ( self , testv ) : test_good = True f = open ( self . home , "" rb+ "" ) for ( offset , length , operator , specimen ) in testv : data = self . _read_share_data ( f , offset , length ) <MASK> test_good = False break f . close ( ) return test_good","if not testv_compare ( data , operator , specimen ) :","if operator not in [ ""eq"" , ""eq"" , ""eq"" ] :",69.20786824,FALSE,81.47
1704,"def get_history_user ( self , instance ) : """"""Get the modifying user from instance or middleware."""""" try : return instance . _history_user except AttributeError : request = None try : <MASK> request = self . thread . request except AttributeError : pass return self . get_user ( instance = instance , request = request )",if self . thread . request . user . is_authenticated :,if self . thread :,73.41404183,FALSE,88.02
1705,"def _check ( self , name , size = None , * extra ) : func = getattr ( imageop , name ) for height in VALUES : for width in VALUES : strlen = abs ( width * height ) <MASK> strlen * = size if strlen < MAX_LEN : data = "" A "" * strlen else : data = AAAAA <MASK> arguments = ( data , size , width , height ) + extra else : arguments = ( data , width , height ) + extra try : func ( * arguments ) except ( ValueError , imageop . error ) : pass",if size :,if size :,100,TRUE,100
1706,"def __setattr__ ( self , name , value ) : if name == "" path "" : if value and value != "" "" : if value [ 0 ] != "" / "" : raise ValueError ( ' The page path should always start with a slash ( "" / "" ). ' ) elif name == "" load_time "" : <MASK> raise ValueError ( "" Page load time must be specified in integer milliseconds. "" ) object . __setattr__ ( self , name , value )","if value and not isinstance ( value , int ) :","if not isinstance ( value , int ) :",93.83245209,FALSE,96.88
1707,"def __repr__ ( self ) : if self . _in_repr : return "" <recursion> "" try : self . _in_repr = True if self . is_computed ( ) : status = "" computed,  "" <MASK> if self . value ( ) is self : status + = "" = self "" else : status + = "" =  "" + repr ( self . value ( ) ) else : status + = "" error =  "" + repr ( self . error ( ) ) else : status = "" isn ' t computed "" return "" %s  ( %s ) "" % ( type ( self ) , status ) finally : self . _in_repr = False",if self . error ( ) is None :,if self . is_function ( ) :,97.49460302,FALSE,96.19
1708,"def _exclude_node ( self , name ) : if "" exclude_nodes "" in self . node_filters : <MASK> self . loggit . info ( ' Excluding node  "" {0} ""  due to node_filters ' . format ( name ) ) return True return False","if name in self . node_filters [ ""exclude_nodes"" ] :","if self . node_filters [ ""exclude_nodes"" ] :",67.48401836,FALSE,94.87
1709,"def enumerate_projects ( ) : """"""List projects in _DEFAULT_APP_DIR."""""" src_path = os . path . join ( _DEFAULT_APP_DIR , "" src "" ) projects = { } for project in os . listdir ( src_path ) : projects [ project ] = [ ] project_path = os . path . join ( src_path , project ) for file in os . listdir ( project_path ) : <MASK> projects [ project ] . append ( file [ : - 8 ] ) return projects","if file . endswith ( "".gwt.xml"" ) :","if file . endswith ( "".py"" ) and file . endswith ( "".py"" ) :",94.5047099,FALSE,90.59
1710,"def zip_readline_read_test ( self , f , compression ) : self . make_test_archive ( f , compression ) # Read the ZIP archive with zipfile . ZipFile ( f , "" r "" ) as zipfp , zipfp . open ( TESTFN ) as zipopen : data = b "" "" while True : read = zipopen . readline ( ) <MASK> break data + = read read = zipopen . read ( 100 ) <MASK> break data + = read self . assertEqual ( data , self . data )",if not read :,if not read :,100,TRUE,100
1711,"def f ( view , s ) : if mode == modes . NORMAL : return sublime . Region ( 0 ) elif mode == modes . VISUAL : <MASK> return sublime . Region ( s . a + 1 , 0 ) else : return sublime . Region ( s . a , 0 ) elif mode == modes . INTERNAL_NORMAL : return sublime . Region ( view . full_line ( s . b ) . b , 0 ) elif mode == modes . VISUAL_LINE : <MASK> return sublime . Region ( 0 , s . b ) else : return sublime . Region ( 0 , s . a ) return s",if s . a < s . b :,if s . a < s . b :,100,TRUE,100
1712,def response ( self ) : try : response = requests . get ( str ( self ) ) rjson = response . json ( ) <MASK> raise Exception ( response . text ) return rjson except Exception as e : raise ResponseFanartError ( str ( e ) ),"if not isinstance ( rjson , dict ) :","if rjson [ ""error"" ] :",87.50024576,FALSE,85.05
1713,"def __get_type ( self , cexpr ) : """"""Returns one of the following types: 'R' - read value, 'W' - write value, 'A' - function argument"""""" child = cexpr for p in reversed ( self . parents ) : assert p , "" Failed to get type at  "" + helper . to_hex ( self . __function_address ) if p . cexpr . op == idaapi . cot_call : return "" Arg "" if not p . is_expr ( ) : return "" R "" if p . cexpr . op == idaapi . cot_asg : <MASK> return "" W "" return "" R "" child = p . cexpr",if p . cexpr . x == child :,if not p . is_expr ( ) :,94.05547116,FALSE,93.92
1714,"def _extract_lemma ( self , parse : Parse ) - > str : special_feats = [ x for x in self . SPECIAL_FEATURES if x in parse . tag ] if len ( special_feats ) == 0 : return parse . normal_form # here we process surnames and patronyms since PyMorphy lemmatizes them incorrectly for other in parse . lexeme : tag = other . tag <MASK> continue if ( tag . case == "" nomn "" and tag . gender == parse . tag . gender and tag . number == "" sing "" ) : return other . word return parse . normal_form",if any ( x not in tag for x in special_feats ) :,if tag in special_feats :,94.77350465,FALSE,91.12
1715,"def evaluateWord ( self , argument ) : wildcard_count = argument [ 0 ] . count ( "" * "" ) if wildcard_count > 0 : if wildcard_count == 1 and argument [ 0 ] . startswith ( "" * "" ) : return self . GetWordWildcard ( argument [ 0 ] [ 1 : ] , method = "" endswith "" ) <MASK> return self . GetWordWildcard ( argument [ 0 ] [ : - 1 ] , method = "" startswith "" ) else : _regex = argument [ 0 ] . replace ( "" * "" , "" .+ "" ) matched = False for w in self . words : matched = bool ( re . search ( _regex , w ) ) if matched : break return matched return self . GetWord ( argument [ 0 ] )","if wildcard_count == 1 and argument [ 0 ] . endswith ( ""*"" ) :","elif wildcard_count == 0 and argument [ 0 ] . endswith ( ""*"" ) :",96.42210694,FALSE,97.05
1716,def getAllEntries ( self ) : entries = [ ] for bucket in self . buckets : last = None for entry in bucket . entries : if last is not None : last . size = entry . virtualOffset - last . virtualOffset last = entry entries . append ( entry ) <MASK> entries [ - 1 ] . size = bucket . endOffset - entries [ - 1 ] . virtualOffset return entries,if len ( entries ) != 0 :,if len ( entries ) > 1 :,92.09626805,FALSE,94.46
1717,def clean ( self ) : if self . _ctx : <MASK> libcrypto . EVP_CIPHER_CTX_cleanup ( self . _ctx ) else : libcrypto . EVP_CIPHER_CTX_reset ( self . _ctx ) libcrypto . EVP_CIPHER_CTX_free ( self . _ctx ),"if hasattr ( libcrypto , ""EVP_CIPHER_CTX_cleanup"" ) :",if libcrypto . EVP_CIPHER_CTX_is_free ( self,51.88526523,FALSE,84.2
1718,"def _addTab ( self , name , label , idx = None ) : label = getLanguageString ( label ) tab = Tab ( self , name , label ) tab . idx = self . _makeTab ( tab , idx ) if idx != None : # Update index list when inserting tabs at arbitrary positions newIdxList = { } for tIdx , t in list ( self . _tabs_by_idx . items ( ) ) : <MASK> t . idx + = 1 newIdxList [ t . idx ] = t self . _tabs_by_idx = newIdxList self . _tabs_by_idx [ tab . idx ] = tab self . _tabs_by_name [ tab . name ] = tab return tab",if int ( tIdx ) >= idx :,"if t . name == ""tab"" :",97.0420406,FALSE,93.8
1719,"def set ( self , _key , _new_login = True ) : with self . lock : user = self . users . get ( current_user . id , None ) <MASK> self . users [ current_user . id ] = dict ( session_count = 1 , key = _key ) else : if _new_login : user [ "" session_count "" ] + = 1 user [ "" key "" ] = _key",if user is None :,if user is None :,100,TRUE,100
1720,"def stop ( self ) : # Try to shut the connection down, but if we get any sort of # errors, go ahead and ignore them.. as we're shutting down anyway try : self . rpcserver . stop ( ) <MASK> self . backend_rpcserver . stop ( ) if self . cluster_rpcserver : self . cluster_rpcserver . stop ( ) except Exception : pass if self . coordination : try : coordination . COORDINATOR . stop ( ) except Exception : pass super ( Service , self ) . stop ( graceful = True )",if self . backend_rpcserver :,if self . backend_rpcserver :,75,TRUE,100
1721,"def __genmenuOnlyAllocated ( menu ) : for submenu in menu . Submenus : __genmenuOnlyAllocated ( submenu ) if menu . OnlyUnallocated == True : tmp [ "" cache "" ] . addMenuEntries ( menu . AppDirs ) menuentries = [ ] for rule in menu . Rules : menuentries = rule . do ( tmp [ "" cache "" ] . getMenuEntries ( menu . AppDirs ) , rule . Type , 2 ) for menuentry in menuentries : <MASK> menuentry . Parents . append ( menu ) #   menuentry.Add = False #   menuentry.Allocated = True menu . MenuEntries . append ( menuentry )",if menuentry . Add == True :,if menuentry . Add :,96.38885443,FALSE,96.19
1722,"def __init__ ( self , * * options ) : self . func_name_highlighting = get_bool_opt ( options , "" func_name_highlighting "" , True ) self . disabled_modules = get_list_opt ( options , "" disabled_modules "" , [ ] ) self . _functions = set ( ) if self . func_name_highlighting : from pygments . lexers . _lua_builtins import MODULES for mod , func in iteritems ( MODULES ) : <MASK> self . _functions . update ( func ) RegexLexer . __init__ ( self , * * options )",if mod not in self . disabled_modules :,if mod in self . disabled_modules :,98.49010411,FALSE,98.11
1723,"def recv_some ( p , t = 0.1 , e = 1 , tr = 5 , stderr = 0 ) : if tr < 1 : tr = 1 x = time . time ( ) + t y = [ ] r = "" "" if stderr : pr = p . recv_err else : pr = p . recv while time . time ( ) < x or r : r = pr ( ) <MASK> break elif r : y . append ( r ) else : time . sleep ( max ( ( x - time . time ( ) ) / tr , 0 ) ) return "" "" . join ( y )",if r is None :,if e :,75.61407465,FALSE,96.44
1724,"def get_menu_items ( node ) : aList = [ ] for child in node . children : for tag in ( "" @menu "" , "" @item "" ) : if child . h . startswith ( tag ) : name = child . h [ len ( tag ) + 1 : ] . strip ( ) <MASK> aList . append ( ( "" %s   %s "" % ( tag , name ) , get_menu_items ( child ) , None ) ) else : b = g . splitLines ( "" "" . join ( child . b ) ) aList . append ( ( tag , name , b [ 0 ] if b else "" "" ) ) break return aList","if tag == ""@menu"" :",if child . b is None :,89.62330237,FALSE,94.03
1725,"def import_suffix_generator ( a_block , datatype = False ) : if datatype is False : for name , suffix in iteritems ( a_block . component_map ( Suffix ) ) : <MASK> yield name , suffix else : for name , suffix in iteritems ( a_block . component_map ( Suffix ) ) : if ( suffix . import_enabled ( ) is True ) and ( suffix . get_datatype ( ) is datatype ) : yield name , suffix",if suffix . import_enabled ( ) is True :,if suffix . import_enabled ( ) is True :,100,TRUE,100
1726,"def verify_relative_valid_path ( root , path ) : if len ( path ) < 1 : raise PackagerError ( "" Empty chown path "" ) checkpath = root parts = path . split ( os . sep ) for part in parts : if part in ( "" . "" , "" .. "" ) : raise PackagerError ( "" . and .. is not allowed in chown path "" ) checkpath = os . path . join ( checkpath , part ) relpath = checkpath [ len ( root ) + 1 : ] if not os . path . exists ( checkpath ) : raise PackagerError ( f "" chown path  { relpath }  does not exist "" ) <MASK> raise PackagerError ( f "" chown path  { relpath }  is a soft link "" )",if os . path . islink ( checkpath ) :,if not os . path . islink ( checkpath ) :,98.31114638,FALSE,98.37
1727,"def load_syntax ( syntax ) : context = _create_scheme ( ) or { } partition_scanner = PartitionScanner ( syntax . get ( "" partitions "" , [ ] ) ) scanners = { } for part_name , part_scanner in list ( syntax . get ( "" scanner "" , { } ) . items ( ) ) : scanners [ part_name ] = Scanner ( part_scanner ) formats = [ ] for fname , fstyle in list ( syntax . get ( "" formats "" , { } ) . items ( ) ) : <MASK> if fstyle . startswith ( "" % ( "" ) and fstyle . endswith ( "" )s "" ) : key = fstyle [ 2 : - 2 ] fstyle = context [ key ] else : fstyle = fstyle % context formats . append ( ( fname , fstyle ) ) return partition_scanner , scanners , formats","if isinstance ( fstyle , basestring ) :","if isinstance ( fstyle , dict ) :",99.22076566,FALSE,98.59
1728,"def should_keep_alive ( commit_msg ) : result = False ci = get_current_ci ( ) or "" "" for line in commit_msg . splitlines ( ) : parts = line . strip ( "" #  "" ) . split ( "" : "" , 1 ) ( key , val ) = parts if len ( parts ) > 1 else ( parts [ 0 ] , "" "" ) <MASK> ci_names = val . replace ( "" , "" , ""   "" ) . lower ( ) . split ( ) if val else [ ] if len ( ci_names ) == 0 or ci . lower ( ) in ci_names : result = True return result","if key == ""CI_KEEP_ALIVE"" :","if key == ""ci"" :",98.96405918,FALSE,95.51
1729,"def get_note_title_file ( note ) : mo = note_title_re . match ( note . get ( "" content "" , "" "" ) ) if mo : fn = mo . groups ( ) [ 0 ] fn = fn . replace ( ""   "" , "" _ "" ) fn = fn . replace ( "" / "" , "" _ "" ) if not fn : return "" "" <MASK> fn = unicode ( fn , "" utf-8 "" ) else : fn = unicode ( fn ) if note_markdown ( note ) : fn + = "" .mkdn "" else : fn + = "" .txt "" return fn else : return "" ""","if isinstance ( fn , str ) :","if isinstance ( fn , bytes ) :",98.98700389,FALSE,98.16
1730,"def post ( self , orgname , teamname ) : if _syncing_setup_allowed ( orgname ) : try : team = model . team . get_organization_team ( orgname , teamname ) except model . InvalidTeamException : raise NotFound ( ) config = request . get_json ( ) # Ensure that the specified config points to a valid group. status , err = authentication . check_group_lookup_args ( config ) <MASK> raise InvalidRequest ( "" Could not sync to group:  %s "" % err ) # Set the team's syncing config. model . team . set_team_syncing ( team , authentication . federated_service , config ) return team_view ( orgname , team ) raise Unauthorized ( )",if not status :,if status != 200 :,98.21995703,FALSE,96.38
1731,"def _marshalData ( self ) : if self . _cache == None : d = self . _data s = "" "" s = time . strftime ( "" % H: % M: % S "" , ( 0 , 0 , 0 ) + d + ( 0 , 0 , - 1 ) ) f = d [ 2 ] - int ( d [ 2 ] ) <MASK> s + = ( "" %g "" % f ) [ 1 : ] s + = "" Z "" self . _cache = s return self . _cache",if f != 0 :,if f > 0 :,98.78073163,FALSE,96.88
1732,"def _get_level ( levels , level_ref ) : if level_ref in levels : return levels . index ( level_ref ) if isinstance ( level_ref , six . integer_types ) : if level_ref < 0 : level_ref + = len ( levels ) <MASK> raise PatsyError ( "" specified level  %r  is out of range "" % ( level_ref , ) ) return level_ref raise PatsyError ( "" specified level  %r  not found "" % ( level_ref , ) )",if not ( 0 <= level_ref < len ( levels ) ) :,if level_ref >= levels [ level_ref ] :,86.50390711,FALSE,88.45
1733,"def iterfieldselect ( source , field , where , complement , missing ) : it = iter ( source ) hdr = next ( it ) yield tuple ( hdr ) indices = asindices ( hdr , field ) getv = operator . itemgetter ( * indices ) for row in it : try : v = getv ( row ) except IndexError : v = missing <MASK> # XOR yield tuple ( row )",if bool ( where ( v ) ) != complement :,"if where ( v , where ) :",78.13839268,FALSE,88.34
1734,"def _test_wait_read_invalid_switch ( self , sleep ) : sock1 , sock2 = socket . socketpair ( ) try : p = gevent . spawn ( util . wrap_errors ( AssertionError , socket . wait_read ) , # pylint:disable=no-member sock1 . fileno ( ) , ) gevent . get_hub ( ) . loop . run_callback ( switch_None , p ) <MASK> gevent . sleep ( sleep ) result = p . get ( ) assert isinstance ( result , AssertionError ) , result assert "" Invalid switch "" in str ( result ) , repr ( str ( result ) ) finally : sock1 . close ( ) sock2 . close ( )",if sleep is not None :,if sleep :,97.60160549,FALSE,96.74
1735,"def train ( config , args ) : gan = setup_gan ( config , inputs , args ) test_batches = [ ] for i in range ( args . steps ) : gan . step ( ) <MASK> correct_prediction = 0 total = 0 for ( x , y ) in gan . inputs . testdata ( ) : prediction = gan . generator ( x ) correct_prediction + = ( torch . argmax ( prediction , 1 ) == torch . argmax ( y , 1 ) ) . sum ( ) total + = y . shape [ 0 ] accuracy = ( float ( correct_prediction ) / total ) * 100 print ( "" accuracy:  "" , accuracy ) return sum_metrics",if i % args . sample_every == 0 and i > 0 :,if i == args . steps - 1 :,93.21527242,FALSE,91.35
1736,"def process_response ( self , request , response , spider ) : if not response . body : return response for fmt , func in six . iteritems ( self . _formats ) : new_response = func ( response ) <MASK> logger . debug ( "" Decompressed response with format:  %(responsefmt)s "" , { "" responsefmt "" : fmt } , extra = { "" spider "" : spider } , ) return new_response return response",if new_response :,if new_response is not None :,95.80554477,FALSE,95.16
1737,"def detect_ssl_option ( self ) : for option in self . ssl_options ( ) : <MASK> for other_option in self . ssl_options ( ) : if option != other_option : if scan_argv ( self . argv , other_option ) is not None : raise ConfigurationError ( "" Cannot give both  %s  and  %s "" % ( option , other_option ) ) return option","if scan_argv ( self . argv , option ) is not None :","if scan_argv ( self . argv , option ) is not None :",75,TRUE,100
1738,"def load ( cls , storefile , template_store ) : # Did we get file or filename? if not hasattr ( storefile , "" read "" ) : storefile = open ( storefile , "" rb "" ) # Adjust store to have translations store = cls . convertfile ( storefile , template_store ) for unit in store . units : <MASK> continue # HTML does this properly on loading, others need it if cls . needs_target_sync : unit . target = unit . source unit . rich_target = unit . rich_source return store",if unit . isheader ( ) :,if unit . is_html :,72.76040255,FALSE,95.98
1739,"def _pre_get_table ( self , _ctx , table_name ) : vsctl_table = self . _get_table ( table_name ) schema_helper = self . schema_helper schema_helper . register_table ( vsctl_table . table_name ) for row_id in vsctl_table . row_ids : <MASK> schema_helper . register_table ( row_id . table ) if row_id . name_column : schema_helper . register_columns ( row_id . table , [ row_id . name_column ] ) if row_id . uuid_column : schema_helper . register_columns ( row_id . table , [ row_id . uuid_column ] ) return vsctl_table",if row_id . table :,if row_id . table :,100,TRUE,100
1740,"def __init__ ( self , pin = None , pull_up = False ) : super ( InputDevice , self ) . __init__ ( pin ) try : self . pin . function = "" input "" pull = "" up "" if pull_up else "" down "" <MASK> self . pin . pull = pull except : self . close ( ) raise self . _active_state = False if pull_up else True self . _inactive_state = True if pull_up else False",if self . pin . pull != pull :,if pin :,84.10293463,FALSE,91.49
1741,"def _increment_operations_count ( self , operation , executed ) : with self . _lock : <MASK> self . _executed_operations + = 1 self . _executed [ operation . job_type ] + = 1 else : self . _skipped [ operation . job_type ] + = 1",if executed :,if executed :,100,TRUE,100
1742,"def emit ( self , type , info = None ) : # Overload emit() to send events to the proxy object at the other end ev = super ( ) . emit ( type , info ) if self . _has_proxy is True and self . _session . status > 0 : # implicit: and self._disposed is False: <MASK> self . _session . send_command ( "" INVOKE "" , self . _id , "" _emit_at_proxy "" , [ ev ] ) elif type in self . __event_types_at_proxy : self . _session . send_command ( "" INVOKE "" , self . _id , "" _emit_at_proxy "" , [ ev ] )",if type in self . __proxy_properties__ :,if type in self . __event_types_at_proxy :,98.92577898,FALSE,95.46
1743,"def validate_pull_secret ( namespace ) : if namespace . pull_secret is None : # TODO: add aka.ms link here warning = ( "" No --pull-secret provided: cluster will not include samples or operators from  "" + "" Red Hat or from certified partners. "" ) logger . warning ( warning ) else : try : <MASK> raise Exception ( ) except : raise InvalidArgumentValueError ( "" Invalid --pull-secret. "" )","if not isinstance ( json . loads ( namespace . pull_secret ) , dict ) :","if namespace . pull_secret not in ( ""samples"" , ""operators"" ) :",92.78330273,FALSE,87.32
1744,"def pack ( types , * args ) : if len ( types ) != len ( args ) : raise Exception ( "" number of arguments does not match format string "" ) port = StringIO ( ) for ( type , value ) in zip ( types , args ) : if type == "" V "" : write_vuint ( port , value ) elif type == "" v "" : write_vint ( port , value ) <MASK> write_bvec ( port , value ) else : raise Exception ( ' unknown xpack format string item  "" ' + type + ' "" ' ) return port . getvalue ( )","elif type == ""s"" :","elif type == ""bvec"" :",98.91642093,FALSE,98.01
1745,"def data ( self ) : if self . _data is not None : return self . _data else : <MASK> with open ( self . path , "" rb "" ) as jsonfile : data = jsonfile . read ( ) . decode ( "" utf8 "" ) data = json . loads ( data ) self . _data = data return self . _data else : return dict ( )",if os . path . exists ( self . path ) :,if os . path . exists ( self . path ) :,100,TRUE,100
1746,"def interact ( self ) : self . output . write ( "" \n "" ) while True : try : request = self . getline ( "" help>  "" ) <MASK> break except ( KeyboardInterrupt , EOFError ) : break request = strip ( request ) # Make sure significant trailing quotation marks of literals don't # get deleted while cleaning input if ( len ( request ) > 2 and request [ 0 ] == request [ - 1 ] in ( "" ' "" , ' "" ' ) and request [ 0 ] not in request [ 1 : - 1 ] ) : request = request [ 1 : - 1 ] if lower ( request ) in ( "" q "" , "" quit "" ) : break self . help ( request )",if not request :,if not request :,100,TRUE,100
1747,"def api_attachment_metadata ( self ) : resp = [ ] for part in self . parts : <MASK> continue k = { "" content_type "" : part . block . content_type , "" size "" : part . block . size , "" filename "" : part . block . filename , "" id "" : part . block . public_id , } content_id = part . content_id if content_id : if content_id [ 0 ] == "" < "" and content_id [ - 1 ] == "" > "" : content_id = content_id [ 1 : - 1 ] k [ "" content_id "" ] = content_id resp . append ( k ) return resp",if not part . is_attachment :,if part . block . public_id is None :,95.19931075,FALSE,93.98
1748,"def _notin_text ( term , text , verbose = False ) : index = text . find ( term ) head = text [ : index ] tail = text [ index + len ( term ) : ] correct_text = head + tail diff = _diff_text ( correct_text , text , verbose ) newdiff = [ u ( "" %s  is contained here: "" ) % py . io . saferepr ( term , maxsize = 42 ) ] for line in diff : if line . startswith ( u ( "" Skipping "" ) ) : continue <MASK> continue if line . startswith ( u ( "" +  "" ) ) : newdiff . append ( u ( ""    "" ) + line [ 2 : ] ) else : newdiff . append ( line ) return newdiff","if line . startswith ( u ( ""- "" ) ) :","if verbose and line . startswith ( u ( ""Not in "" ) ) :",96.6029032,FALSE,95.75
1749,"def get_api ( user , url ) : global API_CACHE if API_CACHE is None or API_CACHE . get ( url ) is None : API_CACHE_LOCK . acquire ( ) try : <MASK> API_CACHE = { } if API_CACHE . get ( url ) is None : API_CACHE [ url ] = ImpalaDaemonApi ( url ) finally : API_CACHE_LOCK . release ( ) api = API_CACHE [ url ] api . set_user ( user ) return api",if API_CACHE is None :,if API_CACHE is None :,100,TRUE,100
1750,"def __str__ ( self , prefix = "" "" , printElemNumber = 0 ) : res = "" "" if self . has_index_name_ : res + = prefix + ( "" index_name:  %s \n "" % self . DebugFormatString ( self . index_name_ ) ) cnt = 0 for e in self . prefix_value_ : elm = "" "" <MASK> elm = "" ( %d ) "" % cnt res + = prefix + ( "" prefix_value %s :  %s \n "" % ( elm , self . DebugFormatString ( e ) ) ) cnt + = 1 if self . has_value_prefix_ : res + = prefix + ( "" value_prefix:  %s \n "" % self . DebugFormatBool ( self . value_prefix_ ) ) return res",if printElemNumber :,if printElemNumber :,100,TRUE,100
1751,"def add_group ( x , nl , in_group , mw ) : if len ( x ) == 0 : return x if len ( x ) > 1 and not in_group : <MASK> return [ "" [[ "" ] + x + [ "" ]] "" ] mw . warn ( "" Equation will multiplex and may produce inaccurate results (see manual) "" ) return [ "" [ "" ] + x + [ "" ] "" ]","if supports_group ( x , nl ) :",if nl :,90.64428264,FALSE,90.34
1752,"def unfulfilled_items ( self ) : unfulfilled_items = 0 for order_item in self . items . all ( ) : <MASK> aggr = order_item . deliver_item . aggregate ( delivered = Sum ( "" quantity "" ) ) unfulfilled_items + = order_item . quantity - ( aggr [ "" delivered "" ] or 0 ) return unfulfilled_items",if not order_item . canceled :,if order_item . deliver_item :,94.42473635,FALSE,92.37
1753,"def _get_pattern ( self , pattern_id ) : """"""Get pattern item by id."""""" for key in ( Tag . PATTERNS1 , Tag . PATTERNS2 , Tag . PATTERNS3 ) : <MASK> data = self . tagged_blocks . get_data ( key ) for pattern in data : if pattern . pattern_id == pattern_id : return pattern return None",if key in self . tagged_blocks :,if self . tagged_blocks . get_data ( key ) :,90.41939374,FALSE,89.02
1754,"def query_lister ( domain , query = "" "" , max_items = None , attr_names = None ) : more_results = True num_results = 0 next_token = None while more_results : rs = domain . connection . query_with_attributes ( domain , query , attr_names , next_token = next_token ) for item in rs : if max_items : <MASK> raise StopIteration yield item num_results + = 1 next_token = rs . next_token more_results = next_token != None",if num_results == max_items :,if num_results >= max_items :,98.42169683,FALSE,97.91
1755,"def find_deprecated_settings ( source ) : # pragma: no cover from celery . utils import deprecated for name , opt in flatten ( NAMESPACES ) : <MASK> deprecated . warn ( description = "" The  {0!r}  setting "" . format ( name ) , deprecation = opt . deprecate_by , removal = opt . remove_by , alternative = "" Use the  {0.alt}  instead "" . format ( opt ) , ) return source","if ( opt . deprecate_by or opt . remove_by ) and getattr ( source , name , None ) :",if opt . deprecated_by and source . startswith ( name ) :,65.58377171,FALSE,81.88
1756,"def tearDown ( self ) : """"""Shutdown the server."""""" try : <MASK> self . server . stop ( 2.0 ) if self . sl_hdlr : self . root_logger . removeHandler ( self . sl_hdlr ) self . sl_hdlr . close ( ) finally : BaseTest . tearDown ( self )",if self . server :,if self . server :,100,TRUE,100
1757,"def broadcast_events ( self , events ) : LOGGER . debug ( "" Broadcasting events:  %s "" , events ) with self . _subscribers_cv : # Copy the subscribers subscribers = { conn : sub . copy ( ) for conn , sub in self . _subscribers . items ( ) } if subscribers : for connection_id , subscriber in subscribers . items ( ) : <MASK> subscriber_events = [ event for event in events if subscriber . is_subscribed ( event ) ] event_list = EventList ( events = subscriber_events ) self . _send ( connection_id , event_list . SerializeToString ( ) )",if subscriber . is_listening ( ) :,if events :,97.04449051,FALSE,93.52
1758,"def _get_info ( self , path ) : info = OrderedDict ( ) if not self . _is_mac ( ) or self . _has_xcode_tools ( ) : stdout = None try : stdout , stderr = Popen ( [ self . _find_binary ( ) , "" info "" , os . path . realpath ( path ) ] , stdout = PIPE , stderr = PIPE , ) . communicate ( ) except OSError : pass else : if stdout : for line in stdout . splitlines ( ) : line = u ( line ) . split ( "" :  "" , 1 ) <MASK> info [ line [ 0 ] ] = line [ 1 ] return info",if len ( line ) == 2 :,if len ( line ) > 1 :,84.20595314,FALSE,96.79
1759,"def test_call_extern_c_fn ( self ) : global memcmp memcmp = cffi_support . ExternCFunction ( "" memcmp "" , ( "" int memcmp ( const uint8_t * ptr1,  "" "" const uint8_t * ptr2, size_t num ) "" ) , ) @udf ( BooleanVal ( FunctionContext , StringVal , StringVal ) ) def fn ( context , a , b ) : if a . is_null != b . is_null : return False <MASK> return True if len ( a ) != b . len : return False if a . ptr == b . ptr : return True return memcmp ( a . ptr , b . ptr , a . len ) == 0",if a is None :,if a . ptr != b . ptr :,95.12944538,FALSE,94.31
1760,"def _flatten ( * args ) : ahs = set ( ) if len ( args ) > 0 : for item in args : if type ( item ) is ActionHandle : ahs . add ( item ) elif type ( item ) in ( list , tuple , dict , set ) : for ah in item : <MASK> # pragma:nocover raise ActionManagerError ( "" Bad argument type  %s "" % str ( ah ) ) ahs . add ( ah ) else : # pragma:nocover raise ActionManagerError ( "" Bad argument type  %s "" % str ( item ) ) return ahs",if type ( ah ) is not ActionHandle :,"if not isinstance ( ah , ActionHandle ) :",93.75179675,FALSE,94.03
1761,"def startElement ( self , name , attrs , connection ) : if name == "" Parameter "" : <MASK> self [ self . _current_param . name ] = self . _current_param self . _current_param = Parameter ( self ) return self . _current_param",if self . _current_param :,if self . _current_param :,100,TRUE,100
1762,"def _find_class_in_descendants ( self , search_key ) : for cls in self . primitive_classes : cls_key = ( cls . __name__ , cls . __module__ ) self . class_cache [ cls_key ] = cls <MASK> return cls",if cls_key == search_key :,if search_key in cls . __dict__ :,86.92791851,FALSE,85.35
1763,"def doWorkForFindAll ( self , v , target , partialMatch ) : sibling = self while sibling : c1 = partialMatch and sibling . equalsTreePartial ( target ) if c1 : v . append ( sibling ) else : c2 = not partialMatch and sibling . equalsTree ( target ) if c2 : v . append ( sibling ) ### regardless of match or not, check any children for matches <MASK> sibling . getFirstChild ( ) . doWorkForFindAll ( v , target , partialMatch ) sibling = sibling . getNextSibling ( )",if sibling . getFirstChild ( ) :,if sibling . hasChildNodes ( ) :,98.71356746,FALSE,97.47
1764,"def forward ( self , inputs : paddle . Tensor ) : outputs = [ ] blocks = self . block ( inputs ) route = None for i , block in enumerate ( blocks ) : if i > 0 : block = paddle . concat ( [ route , block ] , axis = 1 ) route , tip = self . yolo_blocks [ i ] ( block ) block_out = self . block_outputs [ i ] ( tip ) outputs . append ( block_out ) <MASK> route = self . route_blocks_2 [ i ] ( route ) route = self . upsample ( route ) return outputs",if i < 2 :,if route is not None :,89.76693381,FALSE,95.6
1765,"def _filter_paths ( basename , path , is_dir , exclude ) : """""".gitignore style file filtering."""""" for item in exclude : # Items ending in '/' apply only to directories. <MASK> continue # Items starting with '/' apply to the whole path. # In any other cases just the basename is used. match = path if item . startswith ( "" / "" ) else basename if fnmatch . fnmatch ( match , item . strip ( "" / "" ) ) : return True return False","if item . endswith ( ""/"" ) and not is_dir :","if is_dir and not item . startswith ( ""/"" ) :",96.14935556,FALSE,93.17
1766,"def reposition_division ( f1 ) : lines = f1 . splitlines ( ) if lines [ 2 ] == division : lines . pop ( 2 ) found = 0 for i , line in enumerate ( lines ) : if line . startswith ( ' "" "" "" ' ) : found + = 1 if found == 2 : <MASK> break # already in the right place lines . insert ( i + 1 , "" "" ) lines . insert ( i + 2 , division ) break return "" \n "" . join ( lines )","if division in ""\n"" . join ( lines ) :",if found == 1 :,83.71282809,FALSE,89.06
1767,"def buildImage ( opt ) : dpath = os . path . join ( opt [ "" datapath "" ] , "" COCO-IMG-2015 "" ) version = "" 1 "" if not build_data . built ( dpath , version_string = version ) : print ( "" [building image data:  "" + dpath + "" ] "" ) <MASK> # An older version exists, so remove these outdated files. build_data . remove_dir ( dpath ) build_data . make_dir ( dpath ) # Download the data. for downloadable_file in RESOURCES [ : 1 ] : downloadable_file . download_file ( dpath ) # Mark the data as built. build_data . mark_done ( dpath , version_string = version )",if build_data . built ( dpath ) :,if build_data . built ( dpath ) :,100,TRUE,100
1768,"def colorformat ( text ) : if text [ 0 : 1 ] == "" # "" : col = text [ 1 : ] <MASK> return col elif len ( col ) == 3 : return col [ 0 ] * 2 + col [ 1 ] * 2 + col [ 2 ] * 2 elif text == "" "" : return "" "" assert False , "" wrong color format  %r "" % text",if len ( col ) == 6 :,if len ( col ) == 4 :,98.49386537,FALSE,97.12
1769,"def tree_print ( tree ) : for key in tree : print ( key , end = ""   "" ) # end=' ' prevents a newline character tree_element = tree [ key ] # multiple lookups is expensive, even amortized O(1)! for subElem in tree_element : print ( ""  ->  "" , subElem , end = ""   "" ) <MASK> # OP wants indenting after digits print ( "" \n   "" ) # newline and a space to match indenting print ( ) # forces a newline",if type ( subElem ) != str :,"if subElem == ""digits"" :",71.14523795,FALSE,92.6
1770,"def is_dse_cluster ( path ) : try : with open ( os . path . join ( path , "" CURRENT "" ) , "" r "" ) as f : name = f . readline ( ) . strip ( ) cluster_path = os . path . join ( path , name ) filename = os . path . join ( cluster_path , "" cluster.conf "" ) with open ( filename , "" r "" ) as f : data = yaml . load ( f ) <MASK> return True except IOError : return False","if ""dse_dir"" in data :","if ""dse"" in data :",98.74189781,FALSE,96.88
1771,"def delete_old_target_output_files ( classpath_prefix ) : """"""Delete existing output files or symlinks for target."""""" directory , basename = os . path . split ( classpath_prefix ) pattern = re . compile ( r "" ^ {basename} (([0-9]+)( \ .jar)?|classpath \ .txt)$ "" . format ( basename = re . escape ( basename ) ) ) files = [ filename for filename in os . listdir ( directory ) if pattern . match ( filename ) ] for rel_path in files : path = os . path . join ( directory , rel_path ) <MASK> safe_delete ( path )",if os . path . islink ( path ) or os . path . isfile ( path ) :,if os . path . exists ( path ) :,91.20713895,FALSE,92.63
1772,"def test_files ( self ) : # get names of files to test dist_dir = os . path . join ( os . path . dirname ( __file__ ) , os . pardir , os . pardir ) names = [ ] for d in self . test_directories : test_dir = os . path . join ( dist_dir , d ) for n in os . listdir ( test_dir ) : if n . endswith ( "" .py "" ) and not n . startswith ( "" bad "" ) : names . append ( os . path . join ( test_dir , n ) ) for filename in names : <MASK> print ( "" Testing  %s "" % filename ) source = read_pyfile ( filename ) self . check_roundtrip ( source )",if test_support . verbose :,"if filename . endswith ( "".py"" ) :",72.92129737,FALSE,93.8
1773,"def __str__ ( self ) : if self . HasError ( ) : return self . ErrorAsStr ( ) else : # Format is: {action} ""{target}"" ({filename}:{lineno}) string = self . _action if self . _target is not None : string + = '   "" {target} "" ' . format ( target = self . _target ) <MASK> path = self . _filename if self . _lineno is not None : path + = "" : {lineno} "" . format ( lineno = self . _lineno ) string + = ""  ( {path} ) "" . format ( path = path ) return string",if self . _filename is not None :,if self . _filename is not None :,75,TRUE,100
1774,"def extra_action_out ( self , input_dict , state_batches , model , action_dist ) : with self . _no_grad_context ( ) : <MASK> stats_dict = extra_action_out_fn ( self , input_dict , state_batches , model , action_dist ) else : stats_dict = parent_cls . extra_action_out ( self , input_dict , state_batches , model , action_dist ) return self . _convert_to_non_torch_type ( stats_dict )",if extra_action_out_fn :,if extra_action_out_fn :,100,TRUE,100
1775,"def _retract_bindings ( fstruct , inv_bindings , fs_class , visited ) : # Visit each node only once: if id ( fstruct ) in visited : return visited . add ( id ( fstruct ) ) if _is_mapping ( fstruct ) : items = fstruct . items ( ) elif _is_sequence ( fstruct ) : items = enumerate ( fstruct ) else : raise ValueError ( "" Expected mapping or sequence "" ) for ( fname , fval ) in items : if isinstance ( fval , fs_class ) : <MASK> fstruct [ fname ] = inv_bindings [ id ( fval ) ] _retract_bindings ( fval , inv_bindings , fs_class , visited )",if id ( fval ) in inv_bindings :,if id ( fval ) in inv_bindings :,75,TRUE,100
1776,"def warehouses ( self ) - > tuple : from . . repositories import WarehouseBaseRepo repos = dict ( ) for dep in chain ( self . dependencies , [ self ] ) : <MASK> continue if not isinstance ( dep . repo , WarehouseBaseRepo ) : continue for repo in dep . repo . repos : if repo . from_config : continue repos [ repo . name ] = repo return tuple ( repos . values ( ) )",if dep . repo is None :,if dep . name in repos :,94.86353612,FALSE,94.77
1777,"def detype ( self ) : if self . _detyped is not None : return self . _detyped ctx = { } for key , val in self . _d . items ( ) : if not isinstance ( key , str ) : key = str ( key ) detyper = self . get_detyper ( key ) <MASK> # cannot be detyped continue deval = detyper ( val ) if deval is None : # cannot be detyped continue ctx [ key ] = deval self . _detyped = ctx return ctx",if detyper is None :,if detyper is None :,100,TRUE,100
1778,"def populate_obj ( self , obj , name ) : field = getattr ( obj , name , None ) if field is not None : # If field should be deleted, clean it up <MASK> field . delete ( ) return if isinstance ( self . data , FileStorage ) and not is_empty ( self . data . stream ) : if not field . grid_id : func = field . put else : func = field . replace func ( self . data . stream , filename = self . data . filename , content_type = self . data . content_type , )",if self . _should_delete :,if field . deleted :,98.04882654,FALSE,94.01
1779,"def _load ( container ) : if isinstance ( container , str ) : # If container is a filename. <MASK> with open ( container , "" rb "" ) as f : return pickle . load ( f ) # If container is a pickle string. else : return pickle . loads ( container ) # If container is an open file elif isinstance ( container , IOBase ) : return pickle . load ( container ) # What else could it be? else : l . error ( "" Cannot unpickle container of type  %s "" , type ( container ) ) return None",if all ( c in string . printable for c in container ) and os . path . exists ( container ) :,if os . path . isfile ( container ) :,92.96503757,FALSE,86.67
1780,"def append_row ( self , row ) : self . allocate_future_payments ( row ) self . set_invoice_details ( row ) self . set_party_details ( row ) self . set_ageing ( row ) if self . filters . get ( "" group_by_party "" ) : self . update_sub_total_row ( row , row . party ) <MASK> self . append_subtotal_row ( self . previous_party ) self . previous_party = row . party self . data . append ( row )",if self . previous_party and ( self . previous_party != row . party ) :,if self . previous_party :,86.73103766,FALSE,88.4
1781,"def gg1 ( ) : while 1 : tt = 3 while tt > 0 : trace . append ( tt ) val = yield <MASK> tt = 10 # <= uncomment this line trace . append ( "" breaking early... "" ) break tt - = 1 trace . append ( "" try! "" )",if val is not None :,if val is None :,97.66085836,FALSE,96.1
1782,"def migrate_common_facts ( facts ) : """"""Migrate facts from various roles into common"""""" params = { "" node "" : ( "" portal_net "" ) , "" master "" : ( "" portal_net "" ) } if "" common "" not in facts : facts [ "" common "" ] = { } # pylint: disable=consider-iterating-dictionary for role in params . keys ( ) : if role in facts : for param in params [ role ] : <MASK> facts [ "" common "" ] [ param ] = facts [ role ] . pop ( param ) return facts",if param in facts [ role ] :,"if param in facts [ ""common"" ] :",98.82614977,FALSE,96.37
1783,"def get_measurements ( self , pipeline , object_name , category ) : if self . get_categories ( pipeline , object_name ) == [ category ] : results = [ ] <MASK> if object_name == "" Image "" : results + = [ "" Correlation "" , "" Slope "" ] else : results + = [ "" Correlation "" ] if self . do_overlap : results + = [ "" Overlap "" , "" K "" ] if self . do_manders : results + = [ "" Manders "" ] if self . do_rwc : results + = [ "" RWC "" ] if self . do_costes : results + = [ "" Costes "" ] return results return [ ]",if self . do_corr_and_slope :,if self . do_correlation :,98.99434033,FALSE,95.63
1784,"def access_modes ( self ) : """"""access_modes property"""""" if self . _access_modes is None : self . _access_modes = self . get_access_modes ( ) <MASK> self . _access_modes = list ( self . _access_modes ) return self . _access_modes","if not isinstance ( self . _access_modes , list ) :","if isinstance ( self . _access_modes , list ) :",90.81017774,FALSE,96.84
1785,"def unwrap_envelope ( self , data , many ) : if many : <MASK> if isinstance ( data , InstrumentedList ) or isinstance ( data , list ) : self . context [ "" total "" ] = len ( data ) return data else : self . context [ "" total "" ] = data [ "" total "" ] else : self . context [ "" total "" ] = 0 data = { "" items "" : [ ] } return data [ "" items "" ] return data","if data [ ""items"" ] :","if ""items"" not in data :",96.18698447,FALSE,94.15
1786,"def to_string ( self , fmt = "" {:.4f} "" ) : result_str = "" "" for key in self . measures : result = self . m_dict [ key ] [ 0 ] ( ) result_str + = ( "" , "" . join ( fmt . format ( x ) for x in result ) <MASK> else fmt . format ( result ) ) result_str + = "" , "" return result_str [ : - 1 ] # trim the last comma","if isinstance ( result , tuple )","if isinstance ( result , ( list , tuple ) )",93.1164629,FALSE,94.9
1787,"def on_torrent_created ( self , result ) : if not result : return self . dialog_widget . btn_create . setEnabled ( True ) self . dialog_widget . edit_channel_create_torrent_progress_label . setText ( "" Created torrent "" ) if "" torrent "" in result : self . create_torrent_notification . emit ( { "" msg "" : "" Torrent successfully created "" } ) <MASK> self . add_torrent_to_channel ( result [ "" torrent "" ] ) self . close_dialog ( )",if self . dialog_widget . add_to_channel_checkbox . isChecked ( ) :,"if result [ ""torrent"" ] :",87.58665178,FALSE,85.1
1788,"def save ( self ) : for var_name in self . default_config : <MASK> if var_name in self . file_config : del self . file_config [ var_name ] else : self . file_config [ var_name ] = getattr ( self , var_name ) with open ( self . config_path , "" w "" ) as f : f . write ( json . dumps ( self . file_config , indent = 2 ) )","if getattr ( self , var_name , None ) == self . default_config [ var_name ] :","if not hasattr ( self , var_name ) :",76.88600116,FALSE,83.54
1789,"def get_class_parameters ( kwarg ) : ret = { "" attrs "" : [ ] } for key in ( "" rsc "" , "" fsc "" , "" usc "" ) : <MASK> ret [ "" attrs "" ] . append ( [ "" TCA_HFSC_ %s "" % key . upper ( ) , { "" m1 "" : get_rate ( kwarg [ key ] . get ( "" m1 "" , 0 ) ) , "" d "" : get_time ( kwarg [ key ] . get ( "" d "" , 0 ) ) , "" m2 "" : get_rate ( kwarg [ key ] . get ( "" m2 "" , 0 ) ) , } , ] ) return ret",if key in kwarg :,if key in kwarg :,100,TRUE,100
1790,"def forward ( self , x ) : f_x = x if self . exp : f_x = self . exp_swish ( self . exp_bn ( self . exp ( f_x ) ) ) f_x = self . dwise_swish ( self . dwise_bn ( self . dwise ( f_x ) ) ) f_x = self . se ( f_x ) f_x = self . lin_proj_bn ( self . lin_proj ( f_x ) ) if self . has_skip : <MASK> f_x = drop_connect ( f_x , effnet_cfg . EN . DC_RATIO ) f_x = x + f_x return f_x",if self . training and effnet_cfg . EN . DC_RATIO > 0.0 :,if self . dwise :,89.15436747,FALSE,90.87
1791,"def cli_uninstall_distro ( ) : distro_list = install_distro_list ( ) if distro_list is not None : for index , _distro_dir in enumerate ( distro_list ) : log ( str ( index ) + ""   --->>   "" + _distro_dir ) user_input = read_input_uninstall ( ) if user_input is not False : for index , _distro_dir in enumerate ( distro_list ) : <MASK> config . uninstall_distro_dir_name = _distro_dir unin_distro ( ) else : log ( "" No distro installed on  "" + config . usb_disk )",if index == user_input :,"if user_input == ""unin"" :",90.45705262,FALSE,94.76
1792,"def IMPORTFROM ( self , node ) : if node . module == "" __future__ "" : <MASK> self . report ( messages . LateFutureImport , node , [ n . name for n in node . names ] ) else : self . futuresAllowed = False for alias in node . names : if alias . name == "" * "" : self . scope . importStarred = True self . report ( messages . ImportStarUsed , node , node . module ) continue name = alias . asname or alias . name importation = Importation ( name , node ) if node . module == "" __future__ "" : importation . used = ( self . scope , node ) self . addBinding ( node , importation )",if not self . futuresAllowed :,if self . futuresAllowed :,96.97251571,FALSE,98.27
1793,"def _split_and_load ( batch , ctx_list ) : """"""Split data to 1 batch each device."""""" new_batch = [ ] for _ , data in enumerate ( batch ) : <MASK> new_data = [ x . as_in_context ( ctx ) for x , ctx in zip ( data , ctx_list ) ] else : new_data = [ data . as_in_context ( ctx_list [ 0 ] ) ] new_batch . append ( new_data ) return new_batch","if isinstance ( data , ( list , tuple ) ) :","if isinstance ( data , ( list , tuple ) ) :",100,TRUE,100
1794,"def wait_success ( self , timeout = 60 * 10 ) : for i in range ( timeout / / 10 ) : time . sleep ( 10 ) status = self . query_job ( ) print ( "" job  {}  status is  {} "" . format ( self . job_id , status ) ) <MASK> return True if status and status in [ StatusSet . CANCELED , StatusSet . TIMEOUT , StatusSet . FAILED , ] : return False return False",if status and status == StatusSet . SUCCESS :,if status == StatusSet . SUCCESS :,94.88613449,FALSE,97.18
1795,"def copy_tree ( self , src_dir , dst_dir , skip_variables = False ) : for src_root , _ , files in os . walk ( src_dir ) : <MASK> rel_root = os . path . relpath ( src_root , src_dir ) else : rel_root = "" "" if skip_variables and rel_root . startswith ( "" variables "" ) : continue dst_root = os . path . join ( dst_dir , rel_root ) if not os . path . exists ( dst_root ) : os . makedirs ( dst_root ) for f in files : shutil . copy ( os . path . join ( src_root , f ) , os . path . join ( dst_root , f ) )",if src_root != src_dir :,"if src_root . startswith ( ""src"" ) :",93.22570962,FALSE,95.06
1796,"def _make_padded_shapes ( self , dataset , decoders ) : padded_shapes = dataset . output_shapes for i , hparams_i in enumerate ( self . _hparams . datasets ) : <MASK> continue if not hparams_i [ "" pad_to_max_seq_length "" ] : continue text_and_id_shapes = MonoTextData . _make_padded_text_and_id_shapes ( dataset , hparams_i , decoders [ i ] , self . text_name ( i ) , self . text_id_name ( i ) ) padded_shapes . update ( text_and_id_shapes ) return padded_shapes","if not _is_text_data ( hparams_i [ ""data_type"" ] ) :","if hparams_i [ ""pad_to_min_seq_length"" ] :",91.71187879,FALSE,89.99
1797,"def format_errors ( messages ) : errors = { } for k , v in messages . items ( ) : key = camelize ( k , uppercase_first_letter = False ) <MASK> errors [ key ] = format_errors ( v ) elif isinstance ( v , list ) : errors [ key ] = v [ 0 ] return errors","if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",100,TRUE,100
1798,"def generic_visit ( self , node , parents = None ) : parents = ( parents or [ ] ) + [ node ] for field , value in iter_fields ( node ) : <MASK> for item in value : if isinstance ( item , AST ) : self . visit ( item , parents ) elif isinstance ( value , AST ) : self . visit ( value , parents )","if isinstance ( value , list ) :","if isinstance ( value , list ) :",100,TRUE,100
1799,"def get_override_css ( self ) : """"""handls allow_css_overrides setting."""""" if self . settings . get ( "" allow_css_overrides "" ) : filename = self . view . file_name ( ) filetypes = self . settings . get ( "" markdown_filetypes "" ) if filename and filetypes : for filetype in filetypes : <MASK> css_filename = filename . rpartition ( filetype ) [ 0 ] + "" .css "" if os . path . isfile ( css_filename ) : return u "" <style> %s </style> "" % load_utf8 ( css_filename ) return "" """,if filename . endswith ( filetype ) :,"if filetype . startswith ( ""css"" ) :",94.2570583,FALSE,94.38
1800,"def clean ( self ) : super ( ) . clean ( ) # If the Cluster is assigned to a Site, all Devices must be assigned to that Site. if self . cluster . site is not None : for device in self . cleaned_data . get ( "" devices "" , [ ] ) : <MASK> raise ValidationError ( { "" devices "" : "" {}  belongs to a different site ( {} ) than the cluster ( {} ) "" . format ( device , device . site , self . cluster . site ) } )",if device . site != self . cluster . site :,if device . site != self . cluster . site :,100,TRUE,100
1801,"def _setProcessPriority ( process , nice_val , disable_gc ) : org_nice_val = Computer . _process_original_nice_value try : process . nice ( nice_val ) Computer . in_high_priority_mode = nice_val != org_nice_val <MASK> gc . disable ( ) else : gc . enable ( ) return True except psutil . AccessDenied : print2err ( "" WARNING: Could not set process  {}  priority  "" "" to  {} "" . format ( process . pid , nice_val ) ) return False",if disable_gc :,if disable_gc :,100,TRUE,100
1802,"def _setResultsName ( self , name , listAllMatches = False ) : if __diag__ . warn_multiple_tokens_in_named_alternation : <MASK> warnings . warn ( "" {} : setting results name  {!r}  on  {}  expression  "" "" may only return a single token for an And alternative,  "" "" in future will return the full list of tokens "" . format ( "" warn_multiple_tokens_in_named_alternation "" , name , type ( self ) . __name__ , ) , stacklevel = 3 , ) return super ( ) . _setResultsName ( name , listAllMatches )","if any ( isinstance ( e , And ) for e in self . exprs ) :",if self . _isAnd ( ) :,87.70534718,FALSE,89.52
1803,"def make_sources ( project : RootDependency ) - > str : content = [ ] if project . readme : content . append ( project . readme . path . name ) if project . readme . markup != "" rst "" : content . append ( project . readme . to_rst ( ) . path . name ) path = project . package . path for fname in ( "" setup.cfg "" , "" setup.py "" ) : <MASK> content . append ( fname ) for package in chain ( project . package . packages , project . package . data ) : for fpath in package : fpath = fpath . relative_to ( project . package . path ) content . append ( "" / "" . join ( fpath . parts ) ) return "" \n "" . join ( content )",if ( path / fname ) . exists ( ) :,if os . path . isfile ( fname ) :,94.86895169,FALSE,94.99
1804,"def findControlPointsInMesh ( glyph , va , subsegments ) : controlPointIndices = np . zeros ( ( len ( va ) , 1 ) ) index = 0 for i , c in enumerate ( subsegments ) : segmentCount = len ( glyph . contours [ i ] . segments ) - 1 for j , s in enumerate ( c ) : if j < segmentCount : <MASK> controlPointIndices [ index ] = 1 index + = s [ 1 ] return controlPointIndices","if glyph . contours [ i ] . segments [ j ] . type == ""line"" :",if s [ 0 ] == va [ j ] :,85.7324753,FALSE,83.19
1805,"def MergeFrom ( self , other ) : if self . message_class is not None : if other . Parse ( self . message_class ) : self . message . MergeFrom ( other . message ) elif other . message_class is not None : <MASK> self . message = other . message_class ( ) self . message_class = other . message_class self . message . MergeFrom ( other . message ) else : self . message + = other . message",if not self . Parse ( other . message_class ) :,if not self . message :,93.34121302,FALSE,91.15
1806,"def remove_old_snapshot ( install_dir ) : logging . info ( "" Removing any old files in  {} "" . format ( install_dir ) ) for file in glob . glob ( "" {} /* "" . format ( install_dir ) ) : try : <MASK> os . unlink ( file ) elif os . path . isdir ( file ) : shutil . rmtree ( file ) except Exception as error : logging . error ( "" Error:  {} "" . format ( error ) ) sys . exit ( 1 )",if os . path . isfile ( file ) :,if os . path . isfile ( file ) :,100,TRUE,100
1807,"def writexml ( self , stream , indent = "" "" , addindent = "" "" , newl = "" "" , strip = 0 , nsprefixes = { } , namespace = "" "" , ) : w = _streamWriteWrapper ( stream ) if self . raw : val = self . nodeValue if not isinstance ( val , str ) : val = str ( self . nodeValue ) else : v = self . nodeValue if not isinstance ( v , str ) : v = str ( v ) <MASK> v = ""   "" . join ( v . split ( ) ) val = escape ( v ) w ( val )",if strip :,"if isinstance ( v , list ) :",88.36840738,FALSE,93.91
1808,"def validate_attributes ( self ) : for attribute in self . get_all_attributes ( ) : value = getattr ( self , attribute . code , None ) <MASK> if attribute . required : raise ValidationError ( _ ( "" %(attr)s  attribute cannot be blank "" ) % { "" attr "" : attribute . code } ) else : try : attribute . validate_value ( value ) except ValidationError as e : raise ValidationError ( _ ( "" %(attr)s  attribute  %(err)s "" ) % { "" attr "" : attribute . code , "" err "" : e } )",if value is None :,if value is None :,100,TRUE,100
1809,"def PyJsHoisted_BinaryExpression_ ( node , parent , this , arguments , var = var ) : var = Scope ( { u "" node "" : node , u "" this "" : this , u "" arguments "" : arguments , u "" parent "" : parent } , var ) var . registers ( [ u "" node "" , u "" parent "" ] ) if PyJsStrictEq ( var . get ( u "" node "" ) . get ( u "" operator "" ) , Js ( u "" in "" ) ) : <MASK> return var . get ( u "" true "" ) if var . get ( u "" t "" ) . callprop ( u "" isFor "" , var . get ( u "" parent "" ) ) : return var . get ( u "" true "" ) return Js ( False )","if var . get ( u""t"" ) . callprop ( u""isVariableDeclarator"" , var . get ( u""parent"" ) ) :","if var . get ( u""t"" ) . callprop ( u""isFor""",93.12773679,FALSE,91.47
1810,"def distinct ( expr , * on ) : fields = frozenset ( expr . fields ) _on = [ ] append = _on . append for n in on : if isinstance ( n , Field ) : if n . _child . isidentical ( expr ) : n = n . _name else : raise ValueError ( "" {0}  is not a field of  {1} "" . format ( n , expr ) ) <MASK> raise TypeError ( "" on must be a name or field, not:  {0} "" . format ( n ) ) elif n not in fields : raise ValueError ( "" {0}  is not a field of  {1} "" . format ( n , expr ) ) append ( n ) return Distinct ( expr , tuple ( _on ) )","if not isinstance ( n , _strtypes ) :","elif not isinstance ( n , ( Name , Field ) ) :",89.33875168,FALSE,94.51
1811,"def encode ( self , msg ) : """"""Encodes the message to the stream encoding."""""" stream = self . stream rv = msg + "" \n "" if ( PY2 and is_unicode ( rv ) ) or not ( PY2 or is_unicode ( rv ) or _is_text_stream ( stream ) ) : enc = self . encoding <MASK> enc = getattr ( stream , "" encoding "" , None ) or "" utf-8 "" rv = rv . encode ( enc , "" replace "" ) return rv",if enc is None :,if not enc :,79.07213456,FALSE,96.03
1812,"def color_convert ( self , to_color_space , preserve_alpha = True ) : if to_color_space == self . color_space and preserve_alpha : return self else : pixels = pixels_as_float ( self . pixels ) converted = convert_color ( pixels , self . color_space , to_color_space , preserve_alpha ) <MASK> return None return Image ( converted , to_color_space )",if converted is None :,if converted is None :,100,TRUE,100
1813,"def seek ( self , pos ) : if self . closed : raise IOError ( "" Cannot seek on a closed file "" ) for n , idx in enumerate ( self . _indexes [ : : - 1 ] ) : if idx . offset < = pos : <MASK> self . _idxiter = iter ( self . _indexes [ - ( n + 1 ) : ] ) self . _nextidx ( ) break else : raise Exception ( "" Cannot seek to pos "" ) self . _curfile . seek ( pos - self . _curidx . offset )",if idx != self . _curidx :,if self . _curidx . offset == idx . offset :,95.02647748,FALSE,91.99
1814,"def load_from_json ( self , node_data : dict , import_version : float ) : if import_version < = 0.08 : self . image_pointer = unpack_pointer_property_name ( bpy . data . images , node_data , "" image_name "" ) <MASK> proposed_name = node_data . get ( "" image_name "" ) self . info ( f "" image data not found in current  { proposed_name } "" )",if not self . image_pointer :,if self . image_pointer is None :,78.00070028,FALSE,95.07
1815,"def __init__ ( self , execution_context , aggregate_operators ) : super ( _QueryExecutionAggregateEndpointComponent , self ) . __init__ ( execution_context ) self . _local_aggregators = [ ] self . _results = None self . _result_index = 0 for operator in aggregate_operators : if operator == "" Average "" : self . _local_aggregators . append ( _AverageAggregator ( ) ) <MASK> self . _local_aggregators . append ( _CountAggregator ( ) ) elif operator == "" Max "" : self . _local_aggregators . append ( _MaxAggregator ( ) ) elif operator == "" Min "" : self . _local_aggregators . append ( _MinAggregator ( ) ) elif operator == "" Sum "" : self . _local_aggregators . append ( _SumAggregator ( ) )","elif operator == ""Count"" :","elif operator == ""Count"" :",100,TRUE,100
1816,"def attrgetter ( item ) : items = [ None ] * len ( attribute ) for i , attribute_part in enumerate ( attribute ) : item_i = item for part in attribute_part : item_i = environment . getitem ( item_i , part ) <MASK> item_i = postprocess ( item_i ) items [ i ] = item_i return items",if postprocess is not None :,if postprocess :,90.49545537,FALSE,94.24
1817,"def work ( self ) : while True : timeout = self . timeout <MASK> timeout = self . idle_timeout log . debug ( "" Wait for  {} "" . format ( timeout ) ) fetch . wait ( timeout ) if shutting_down . is_set ( ) : log . info ( "" Stop fetch worker "" ) break self . fetch ( )",if idle . is_set ( ) :,if self . idle_timeout is not None :,93.14890088,FALSE,89.06
1818,"def testCoreInterfaceIntInputData ( ) : result_testing = False for _ in range ( 10 ) : hsyncnet_instance = hsyncnet ( [ [ 1 ] , [ 2 ] , [ 3 ] , [ 20 ] , [ 21 ] , [ 22 ] ] , 2 , initial_type . EQUIPARTITION , ccore = True ) analyser = hsyncnet_instance . process ( ) <MASK> result_testing = True break assert result_testing",if len ( analyser . allocate_clusters ( 0.1 ) ) == 2 :,"if analyser . get ( ""interface_type"" ) == ""EQUIPARTITION",82.44684195,FALSE,85.47
1819,"def _gen ( ) : buf = [ ] iterable = dataset ( ) try : while len ( buf ) < buffer_size : buf . append ( next ( iterable ) ) while 1 : i = random . randint ( 0 , buffer_size - 1 ) n = next ( iterable ) yield buf [ i ] buf [ i ] = n except StopIteration : <MASK> random . shuffle ( buf ) for i in buf : yield i",if len ( buf ) :,if shuffle :,95.81614923,FALSE,93.9
1820,"def debug_tree ( tree ) : l = [ ] for elt in tree : if isinstance ( elt , ( int , long ) ) : l . append ( _names . get ( elt , elt ) ) <MASK> l . append ( elt ) else : l . append ( debug_tree ( elt ) ) return l","elif isinstance ( elt , str ) :","elif isinstance ( elt , ( list , tuple ) ) :",92.82256235,FALSE,91.24
1821,"def reverse_code ( apps : StateApps , schema_editor : DatabaseSchemaEditor ) - > None : PreregistrationUser = apps . get_model ( "" zerver "" , "" PreregistrationUser "" ) for user in PreregistrationUser . objects . all ( ) : <MASK> # PreregistrationUser.INVITE_AS['REALM_ADMIN'] user . invited_as_admin = True else : # PreregistrationUser.INVITE_AS['MEMBER'] user . invited_as_admin = False user . save ( update_fields = [ "" invited_as_admin "" ] )",if user . invited_as == 2 :,if user . invited_as_admin :,93.54863881,FALSE,95.99
1822,"def _fastqc_data_section ( self , section_name ) : out = [ ] in_section = False data_file = os . path . join ( self . _dir , "" fastqc_data.txt "" ) if os . path . exists ( data_file ) : with open ( data_file ) as in_handle : for line in in_handle : if line . startswith ( "" >> %s "" % section_name ) : in_section = True elif in_section : <MASK> break out . append ( line . rstrip ( "" \r \n "" ) ) return out","if line . startswith ( "">>END"" ) :",if not in_section :,90.78779257,FALSE,91.39
1823,"def determine_block_hints ( self , text ) : hints = "" "" if text : if text [ 0 ] in ""   \n \x85 \u2028 \u2029 "" : hints + = str ( self . best_indent ) <MASK> hints + = "" - "" elif len ( text ) == 1 or text [ - 2 ] in "" \n \x85 \u2028 \u2029 "" : hints + = "" + "" return hints","if text [ - 1 ] not in ""\n\x85\u2028\u2029"" :","elif text [ 0 ] in "" \n\x85\u2028\u",72.42546328,FALSE,88.4
1824,"def database_app ( request ) : if request . param == "" postgres_app "" : if not which ( "" initdb "" ) : pytest . skip ( "" initdb must be on PATH for postgresql fixture "" ) if not psycopg2 : pytest . skip ( "" psycopg2 must be installed for postgresql fixture "" ) if request . param == "" sqlite_rabbitmq_app "" : <MASK> pytest . skip ( "" rabbitmq tests will be skipped if GALAXY_TEST_AMQP_INTERNAL_CONNECTION env var is unset "" ) return request . getfixturevalue ( request . param )","if not os . environ . get ( ""GALAXY_TEST_AMQP_INTERNAL_CONNECTION"" ) :",if not GALAXY_TEST_AMQP_INTERNAL_CONNECTION :,91.82831304,FALSE,90.62
1825,"def do_rollout ( agent , env , num_steps , render = False ) : total_rew = 0 ob = env . reset ( ) for t in range ( num_steps ) : a = agent . act ( ob ) ( ob , reward , done , _info ) = env . step ( a ) total_rew + = reward <MASK> env . render ( ) if done : break return total_rew , t + 1",if render and t % 3 == 0 :,if render :,91.86751201,FALSE,91.02
1826,"def _handle_subrepos ( self , ctx , dirty_trees ) : substate = util . parse_hgsubstate ( ctx [ "" .hgsubstate "" ] . data ( ) . splitlines ( ) ) sub = util . OrderedDict ( ) if "" .hgsub "" in ctx : sub = util . parse_hgsub ( ctx [ "" .hgsub "" ] . data ( ) . splitlines ( ) ) for path , sha in substate . iteritems ( ) : # Ignore non-Git repositories keeping state in .hgsubstate. <MASK> continue d = os . path . dirname ( path ) dirty_trees . add ( d ) tree = self . _dirs . setdefault ( d , dulobjs . Tree ( ) ) tree . add ( os . path . basename ( path ) , dulobjs . S_IFGITLINK , sha )","if path in sub and not sub [ path ] . startswith ( ""[git]"" ) :",if path in sub :,95.66899029,FALSE,90.24
1827,"def get_property_file_image_choices ( self , pipeline ) : columns = pipeline . get_measurement_columns ( ) image_names = [ ] for column in columns : object_name , feature , coltype = column [ : 3 ] choice = feature [ ( len ( C_FILE_NAME ) + 1 ) : ] <MASK> image_names . append ( choice ) return image_names","if object_name == ""Image"" and ( feature . startswith ( C_FILE_NAME ) ) :","if self . is_image_available ( pipeline , object_name , feature , coltype",71.97708759,FALSE,79.56
1828,"def check_all_decorator_order ( ) : """"""Check that in all test files, the slow decorator is always last."""""" errors = [ ] for fname in os . listdir ( PATH_TO_TESTS ) : <MASK> filename = os . path . join ( PATH_TO_TESTS , fname ) new_errors = check_decorator_order ( filename ) errors + = [ f "" -  { filename } , line  { i } "" for i in new_errors ] if len ( errors ) > 0 : msg = "" \n "" . join ( errors ) raise ValueError ( f "" The parameterized decorator (and its variants) should always be first, but this is not the case in the following files: \n { msg } "" )","if fname . endswith ( "".py"" ) :","if fname . endswith ( "".py"" ) :",100,TRUE,100
1829,"def on_edit_button_clicked ( self , event = None , a = None , col = None ) : tree , tree_id = self . treeView . get_selection ( ) . get_selected ( ) watchdir_id = str ( self . store . get_value ( tree_id , 0 ) ) if watchdir_id : if col and col . get_title ( ) == _ ( "" Active "" ) : <MASK> client . autoadd . disable_watchdir ( watchdir_id ) else : client . autoadd . enable_watchdir ( watchdir_id ) else : self . opts_dialog . show ( self . watchdirs [ watchdir_id ] , watchdir_id )","if self . watchdirs [ watchdir_id ] [ ""enabled"" ] :",if client . autoadd . get_active ( ) :,89.9325277,FALSE,90.79
1830,"def get_conv_output_size ( input_size , kernel_size , stride , padding , dilation ) : ndim = len ( input_size ) output_size = [ ] for i in range ( ndim ) : size = ( input_size [ i ] + 2 * padding [ i ] - dilation [ i ] * ( kernel_size [ i ] - 1 ) - 1 ) / / stride [ i ] + 1 <MASK> output_size . append ( 1 ) else : output_size . append ( size ) return output_size",if kernel_size [ i ] == - 1 :,if size == 0 :,85.06174469,FALSE,91.26
1831,"def from_location ( cls , location , basename , metadata = None , * * kw ) : project_name , version , py_version , platform = [ None ] * 4 basename , ext = os . path . splitext ( basename ) if ext . lower ( ) in ( "" .egg "" , "" .egg-info "" ) : match = EGG_NAME ( basename ) <MASK> project_name , version , py_version , platform = match . group ( "" name "" , "" ver "" , "" pyver "" , "" plat "" ) return cls ( location , metadata , project_name = project_name , version = version , py_version = py_version , platform = platform , * * kw )",if match :,if match :,100,TRUE,100
1832,"def __new__ ( metacls , typename , bases , namespace ) : annotations = namespace . get ( "" __annotations__ "" , { } ) for t in annotations . values ( ) : <MASK> for ut in t . __args__ : _assert_tensorizer_type ( ut ) else : _assert_tensorizer_type ( t ) return super ( ) . __new__ ( metacls , typename , bases , namespace )","if getattr ( t , ""__origin__"" , """" ) is Union :","if hasattr ( t , ""__args__"" ) :",89.08229243,FALSE,88.89
1833,"def decode_content ( self ) : """"""Return the best possible representation of the response body."""""" ct = self . headers . get ( "" content-type "" ) if ct : ct , options = parse_options_header ( ct ) charset = options . get ( "" charset "" ) <MASK> return self . json ( charset ) elif ct . startswith ( "" text/ "" ) : return self . text ( charset ) elif ct == FORM_URL_ENCODED : return parse_qsl ( self . content . decode ( charset ) , keep_blank_values = True ) return self . content",if ct in JSON_CONTENT_TYPES :,"if ct . startswith ( ""application/json"" ) :",95.82049023,FALSE,92.06
1834,"def get_full_path ( path ) : if "" :// "" not in path : path = os . path . join ( self . AUTO_COLL_TEMPL , path , "" "" ) <MASK> path = os . path . join ( abs_path , path ) return path",if abs_path :,if os . path . isabs ( path ) :,73.23044561,FALSE,85.8
1835,"def __getitem__ ( self , name_or_path ) : if isinstance ( name_or_path , integer_types ) : return list . __getitem__ ( self , name_or_path ) elif isinstance ( name_or_path , tuple ) : try : val = self for fid in name_or_path : <MASK> raise KeyError # path contains base value val = val [ fid ] return val except ( KeyError , IndexError ) : raise KeyError ( name_or_path ) else : raise TypeError ( self . _INDEX_ERROR % name_or_path )","if not isinstance ( val , FeatStruct ) :",if fid not in val :,91.67992826,FALSE,93.8
1836,"def scan ( scope ) : for s in scope . children : if s . start_pos < = position < = s . end_pos : if isinstance ( s , ( tree . Scope , tree . Flow ) ) : return scan ( s ) or s <MASK> return scan ( s ) return None","elif s . type in ( ""suite"" , ""decorated"" ) :","elif isinstance ( s , tree . Scope ) :",87.9397814,FALSE,82.77
1837,"def _get_key ( self ) : if not self . key : self . _channel . send ( u "" pake "" , self . msg1 ) pake_msg = self . _channel . get ( u "" pake "" ) self . key = self . sp . finish ( pake_msg ) self . verifier = self . derive_key ( u "" wormhole:verifier "" ) <MASK> return confkey = self . derive_key ( u "" wormhole:confirmation "" ) nonce = os . urandom ( CONFMSG_NONCE_LENGTH ) confmsg = make_confmsg ( confkey , nonce ) self . _channel . send ( u "" _confirm "" , confmsg )",if not self . _send_confirm :,if self . verifier is None :,95.63932865,FALSE,94.39
1838,"def executeScript ( self , script ) : if len ( script ) > 0 : commands = [ ] for l in script : extracted = self . extract_command ( l ) <MASK> commands . append ( extracted ) for command in commands : cmd , argv = command self . dispatch_command ( cmd , argv )",if extracted :,if extracted is not None :,94.91516869,FALSE,93.08
1839,"def create_path ( n , fullname , meta ) : if meta : meta . create_path ( fullname ) else : # These fallbacks are important -- meta could be null if, for # example, save created a ""fake"" item, i.e. a new strip/graft # path element, etc.  You can find cases like that by # searching for ""Metadata()"". unlink ( fullname ) if stat . S_ISDIR ( n . mode ) : mkdirp ( fullname ) <MASK> os . symlink ( n . readlink ( ) , fullname )",elif stat . S_ISLNK ( n . mode ) :,elif stat . S_ISLINK ( n . mode ) :,73.72133754,FALSE,97.95
1840,def get_cycle ( self ) : if self . has_cycle ( ) : cross_node = self . path [ - 1 ] <MASK> return self . path [ self . path . index ( cross_node ) : ] else : return self . path return [ ],if self . path . count ( cross_node ) > 1 :,if cross_node in self . get_nodes ( ) :,59.39882941,FALSE,83.55
1841,"def _select_block ( str_in , start_tag , end_tag ) : """"""Select first block delimited by start_tag and end_tag"""""" start_pos = str_in . find ( start_tag ) if start_pos < 0 : raise ValueError ( "" start_tag not found "" ) depth = 0 for pos in range ( start_pos , len ( str_in ) ) : <MASK> depth + = 1 elif str_in [ pos ] == end_tag : depth - = 1 if depth == 0 : break sel = str_in [ start_pos + 1 : pos ] return sel",if str_in [ pos ] == start_tag :,if str_in [ pos ] == start_tag :,100,TRUE,100
1842,"def device ( self ) : """"""Device on which the data array of this variable reside."""""" # lazy initialization for performance if self . _device is None : <MASK> self . _device = backend . CpuDevice ( ) else : self . _device = backend . get_device_from_array ( self . _data [ 0 ] ) return self . _device",if self . _data [ 0 ] is None :,if self . _data [ 0 ] is None :,75,TRUE,100
1843,"def function_out ( * args , * * kwargs ) : try : return function_in ( * args , * * kwargs ) except dbus . exceptions . DBusException as e : if e . get_dbus_name ( ) == DBUS_UNKNOWN_METHOD : raise ItemNotFoundException ( "" Item does not exist! "" ) <MASK> raise ItemNotFoundException ( e . get_dbus_message ( ) ) if e . get_dbus_name ( ) in ( DBUS_NO_REPLY , DBUS_NOT_SUPPORTED ) : raise SecretServiceNotAvailableException ( e . get_dbus_message ( ) ) raise",if e . get_dbus_name ( ) == DBUS_NO_SUCH_OBJECT :,if e . get_dbus_name ( ) == DBUS_NOT_FOUND :,98.63201675,FALSE,95.38
1844,"def run ( self ) : """"""Continual loop evaluating when_statements"""""" while len ( self . library ) > 0 : for name , expression in self . library . items ( ) : <MASK> del self . library [ name ] else : expression . evaluate ( ) sleep ( 0.01 ) return",if expression . remove_me == True :,"if isinstance ( expression , _Case ) :",91.06182335,FALSE,86.79
1845,"def tamper ( payload , * * kwargs ) : junk_chars = "" !#$ % &()*~+-_.,:;?@[/| \ ]^` "" retval = "" "" for i , char in enumerate ( payload , start = 1 ) : amount = random . randint ( 10 , 15 ) <MASK> retval + = "" > "" for _ in range ( amount ) : retval + = random . choice ( junk_chars ) elif char == "" < "" : retval + = "" < "" for _ in range ( amount ) : retval + = random . choice ( junk_chars ) elif char == ""   "" : for _ in range ( amount ) : retval + = random . choice ( junk_chars ) else : retval + = char return retval","if char == "">"" :","if char == "">"" :",100,TRUE,100
1846,"def _source_target_path ( source , source_path , source_location ) : target_path_attr = source . target_path or source . resdef . target_path if source . preserve_path : <MASK> log . warning ( "" target-path  ' %s '  specified with preserve-path - ignoring "" , target_path_attr , ) return os . path . relpath ( os . path . dirname ( source_path ) , source_location ) else : return target_path_attr or source . resdef . target_path or "" """,if target_path_attr :,if target_path_attr :,100,TRUE,100
1847,"def _load_user_from_header ( self , header ) : if self . _header_callback : user = self . _header_callback ( header ) <MASK> app = current_app . _get_current_object ( ) user_loaded_from_header . send ( app , user = user ) return user return None",if user is not None :,if user :,72.92699301,FALSE,93.76
1848,"def setup ( cls ) : "" Check dependencies and warn about firewalling "" pathCheck ( "" brctl "" , moduleName = "" bridge-utils "" ) # Disable Linux bridge firewalling so that traffic can flow! for table in "" arp "" , "" ip "" , "" ip6 "" : cmd = "" sysctl net.bridge.bridge-nf-call- %s tables "" % table out = quietRun ( cmd ) . strip ( ) <MASK> warn ( "" Warning: Linux bridge may not work with "" , out , "" \n "" )","if out . endswith ( ""1"" ) :",if out :,71.64278354,FALSE,91.8
1849,"def _browse_your_music ( web_client , variant ) : if not web_client . logged_in : return [ ] if variant in ( "" tracks "" , "" albums "" ) : items = flatten ( [ page . get ( "" items "" , [ ] ) for page in web_client . get_all ( f "" me/ { variant } "" , params = { "" market "" : "" from_token "" , "" limit "" : 50 } , ) if page ] ) <MASK> return list ( translator . web_to_track_refs ( items ) ) else : return list ( translator . web_to_album_refs ( items ) ) else : return [ ]","if variant == ""tracks"" :","if variant == ""tracks"" :",75,TRUE,100
1850,"def reset_styling ( self ) : for edge in self . fsm_graph . edges_iter ( ) : style_attr = self . fsm_graph . style_attributes . get ( "" edge "" , { } ) . get ( "" default "" ) edge . attr . update ( style_attr ) for node in self . fsm_graph . nodes_iter ( ) : <MASK> style_attr = self . fsm_graph . style_attributes . get ( "" node "" , { } ) . get ( "" inactive "" ) node . attr . update ( style_attr ) for sub_graph in self . fsm_graph . subgraphs_iter ( ) : style_attr = self . fsm_graph . style_attributes . get ( "" graph "" , { } ) . get ( "" default "" ) sub_graph . graph_attr . update ( style_attr )","if ""point"" not in node . attr [ ""shape"" ] :",if node . inactive :,91.27512109,FALSE,92.75
1851,"def set_message_type_visibility ( self , message_type : MessageType ) : try : rows = { i for i , msg in enumerate ( self . proto_analyzer . messages ) <MASK> } if message_type . show : self . ui . tblViewProtocol . show_rows ( rows ) else : self . ui . tblViewProtocol . hide_rows ( rows ) except Exception as e : logger . exception ( e )",if msg . message_type == message_type,if msg . visibility_type == message_type . visibility_type,64.88901986,FALSE,93.05
1852,"def POP ( cpu , * regs ) : for reg in regs : val = cpu . stack_pop ( cpu . address_bit_size / / 8 ) <MASK> cpu . _set_mode_by_val ( val ) val = val & ~ 0x1 reg . write ( val )","if reg . reg in ( ""PC"" , ""R15"" ) :",if cpu . _set_mode_by_val ( val ) :,82.15409396,FALSE,81.72
1853,"def processMovie ( self , atom ) : for field in atom : if "" track "" in field : self . processTrack ( field [ "" track "" ] ) <MASK> self . processMovieHeader ( field [ "" movie_hdr "" ] )","if ""movie_hdr"" in field :","if ""movie_hdr"" in field :",100,TRUE,100
1854,"def check_update_function ( url , folder , update_setter , version_setter , auto ) : remote_version = urllib . urlopen ( url ) . read ( ) if remote_version . isdigit ( ) : local_version = get_local_timestamp ( folder ) <MASK> if auto : update_setter . set_value ( True ) version_setter . set_value ( remote_version ) return True else : return False else : return False",if remote_version > local_version :,if local_version . isdigit ( ) :,63.98337594,FALSE,92.91
1855,"def init ( self , view , items = None ) : selections = [ ] if view . sel ( ) : for region in view . sel ( ) : selections . append ( view . substr ( region ) ) values = [ ] for idx , index in enumerate ( map ( int , items ) ) : if idx > = len ( selections ) : break i = index - 1 <MASK> values . append ( selections [ i ] ) else : values . append ( None ) # fill up for idx , value in enumerate ( selections ) : if len ( values ) + 1 < idx : values . append ( value ) self . stack = values",if i >= 0 and i < len ( selections ) :,if i >= 0 :,89.85256045,FALSE,93.83
1856,"def find_int_identifiers ( directory ) : results = find_rules ( directory , has_int_identifier ) print ( "" Number of rules with integer identifiers:  %d "" % len ( results ) ) for result in results : rule_path = result [ 0 ] product_yaml_path = result [ 1 ] product_yaml = None <MASK> product_yaml = yaml . open_raw ( product_yaml_path ) fix_file ( rule_path , product_yaml , fix_int_identifier )",if product_yaml_path is not None :,if product_yaml_path is not None :,100,TRUE,100
1857,"def condition ( self ) : if self . __condition is None : if len ( self . flat_conditions ) == 1 : # Avoid an extra indirection in the common case of only one condition. self . __condition = self . flat_conditions [ 0 ] <MASK> # Possible, if unlikely, due to filter predicate rewriting self . __condition = lambda _ : True else : self . __condition = lambda x : all ( cond ( x ) for cond in self . flat_conditions ) return self . __condition",elif len ( self . flat_conditions ) == 0 :,elif len ( self . flat_conditions ) == 2 :,98.73106218,FALSE,97.91
1858,"def get_scene_exceptions_by_season ( self , season = - 1 ) : scene_exceptions = [ ] for scene_exception in self . scene_exceptions : <MASK> continue scene_name , scene_season = scene_exception . split ( "" | "" ) if season == scene_season : scene_exceptions . append ( scene_name ) return scene_exceptions",if not len ( scene_exception ) == 2 :,"if scene_exception . startswith ( ""Scene "" ) :",87.09474383,FALSE,89.03
1859,"def init ( self , view , items = None ) : selections = [ ] if view . sel ( ) : for region in view . sel ( ) : selections . append ( view . substr ( region ) ) values = [ ] for idx , index in enumerate ( map ( int , items ) ) : <MASK> break i = index - 1 if i > = 0 and i < len ( selections ) : values . append ( selections [ i ] ) else : values . append ( None ) # fill up for idx , value in enumerate ( selections ) : if len ( values ) + 1 < idx : values . append ( value ) self . stack = values",if idx >= len ( selections ) :,if index == 0 :,90.33847282,FALSE,94.07
1860,"def to_tool_path ( self , path_or_uri_like , * * kwds ) : if "" :// "" not in path_or_uri_like : path = path_or_uri_like else : uri_like = path_or_uri_like <MASK> raise Exception ( "" Invalid URI passed to get_tool_source "" ) scheme , rest = uri_like . split ( "" : "" , 2 ) if scheme not in self . resolver_classes : raise Exception ( "" Unknown tool scheme [ {} ] for URI [ {} ] "" . format ( scheme , uri_like ) ) path = self . resolver_classes [ scheme ] ( ) . get_tool_source_path ( uri_like ) return path","if "":"" not in path_or_uri_like :","if not uri_like . startswith ( ""tool://"" ) :",94.38325748,FALSE,92.33
1861,def mainWindow ( ) : global MW if not MW : for i in qApp . topLevelWidgets ( ) : <MASK> MW = i return MW return None else : return MW,"if i . objectName ( ) == ""MainWindow"" :",if i . isWindow ( ) :,84.87792286,FALSE,78.37
1862,"def async_get_service ( hass , config , discovery_info = None ) : # pylint: disable=unused-argument """"""Get the demo notification service."""""" for account , account_dict in hass . data [ DATA_ALEXAMEDIA ] [ "" accounts "" ] . items ( ) : for key , _ in account_dict [ "" devices "" ] [ "" media_player "" ] . items ( ) : <MASK> _LOGGER . debug ( "" %s : Media player  %s  not loaded yet; delaying load "" , hide_email ( account ) , hide_serial ( key ) , ) return False return AlexaNotificationService ( hass )","if key not in account_dict [ ""entities"" ] [ ""media_player"" ] :","if not config [ ""load_media_player"" ] [ key ] :",95.70082644,FALSE,91.27
1863,"def _migrate_bool ( self , name : str , true_value : str , false_value : str ) - > None : if name not in self . _settings : return values = self . _settings [ name ] if not isinstance ( values , dict ) : return for scope , val in values . items ( ) : <MASK> new_value = true_value if val else false_value self . _settings [ name ] [ scope ] = new_value self . changed . emit ( )","if isinstance ( val , bool ) :","if isinstance ( val , bool ) :",100,TRUE,100
1864,"def send ( self , data , flags = 0 ) : self . _checkClosed ( ) if self . _sslobj : <MASK> raise ValueError ( "" non-zero flags not allowed in calls to send() on  %s "" % self . __class__ ) return self . _sslobj . write ( data ) else : return socket . send ( self , data , flags )",if flags != 0 :,if flags != 0 :,100,TRUE,100
1865,"def rec_deps ( services , container_by_name , cnt , init_service ) : deps = cnt [ "" _deps "" ] for dep in deps . copy ( ) : dep_cnts = services . get ( dep ) if not dep_cnts : continue dep_cnt = container_by_name . get ( dep_cnts [ 0 ] ) <MASK> # TODO: avoid creating loops, A->B->A if init_service and init_service in dep_cnt [ "" _deps "" ] : continue new_deps = rec_deps ( services , container_by_name , dep_cnt , init_service ) deps . update ( new_deps ) return deps",if dep_cnt :,if dep_cnt :,100,TRUE,100
1866,"def as_dict ( path = "" "" , version = "" latest "" , section = "" meta-data "" ) : result = { } dirs = dir ( path , version , section ) if not dirs : return None for item in dirs : if item . endswith ( "" / "" ) : records = as_dict ( path + item , version , section ) if records : result [ item [ : - 1 ] ] = records <MASK> idx , name = is_dict . match ( item ) . groups ( ) records = as_dict ( path + idx + "" / "" , version , section ) if records : result [ name ] = records else : result [ item ] = valueconv ( get ( path + item , version , section ) ) return result",elif is_dict . match ( item ) :,"elif item . startswith ( ""/"" ) :",94.78592633,FALSE,95.12
1867,"def PrintColGroup ( col_names , schema ) : """"""Print HTML colgroup element, used for JavaScript sorting."""""" print ( ""   <colgroup> "" ) for i , col in enumerate ( col_names ) : if col . endswith ( "" _HREF "" ) : continue # CSS class is used for sorting <MASK> css_class = "" number "" else : css_class = "" case-insensitive "" # NOTE: id is a comment only; not used print ( '     <col id= "" {} ""  type= "" {} ""  /> ' . format ( col , css_class ) ) print ( ""   </colgroup> "" )",if schema . IsNumeric ( col ) :,if i == 0 :,96.548095,FALSE,94.54
1868,"def check_region ( self , region ) : for other in self . regions : <MASK> continue if ( other . start < region . start < other . end ) or ( other . start < region . end < other . end ) : raise Exception ( "" %r  overlaps with  %r "" % ( region , other ) )",if other is region :,if region . start == other . end :,89.12288319,FALSE,87.67
1869,"def _write_value ( self , rng , value , scalar ) : if rng . api and value : # it is assumed by this stage that value is a list of lists <MASK> value = value [ 0 ] [ 0 ] else : rng = rng . resize ( len ( value ) , len ( value [ 0 ] ) ) rng . raw_value = value",if scalar :,if scalar :,100,TRUE,100
1870,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 10 : length = d . getVarInt32 ( ) tmp = ProtocolBuffer . Decoder ( d . buffer ( ) , d . pos ( ) , d . pos ( ) + length ) d . skip ( length ) self . mutable_cost ( ) . TryMerge ( tmp ) continue if tt == 24 : self . add_version ( d . getVarInt64 ( ) ) continue <MASK> raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 0 :,if tt == 0 :,100,TRUE,100
1871,"def generate_sv_faces ( dcel_mesh , point_index , only_select = False , del_flag = None ) : # This part of function creates faces in SV format # It ignores  boundless super face sv_faces = [ ] for i , face in enumerate ( dcel_mesh . faces ) : <MASK> "" Face ( {} ) has inner components! Sverchok cant show polygons with holes. "" . format ( i ) if not face . outer or del_flag in face . flags : continue if only_select and not face . select : continue sv_faces . append ( [ point_index [ hedge . origin ] for hedge in face . outer . loop_hedges ] ) return sv_faces",if face . inners and face . outer :,if not face . outer :,97.6735784,FALSE,96.36
1872,"def _get_x_for_y ( self , xValue , x , y ) : # print(""searching ""+x+"" with the value ""+str(xValue)+"" and want to give back ""+y) x_value = str ( xValue ) for anime in self . xmlMap . findall ( "" anime "" ) : try : <MASK> return int ( anime . get ( y , 0 ) ) except ValueError as e : continue return 0","if anime . get ( x , False ) == x_value :","if anime . get ( x , 0 ) == x_value :",73.21286018,FALSE,97.64
1873,"def dir_copy ( src_dir , dest_dir , merge_if_exists = True ) : try : if not os . path . exists ( dest_dir ) : shutil . copytree ( src_dir , dest_dir ) <MASK> merge_dir ( src_dir , dest_dir ) except OSError as e : # If source is not a directory, copy with shutil.copy if e . errno == errno . ENOTDIR : shutil . copy ( src_dir , dest_dir ) else : logging . error ( "" Could not copy  %s  to  %s "" , src_dir , dest_dir )",elif merge_if_exists :,if merge_if_exists :,77.23082075,FALSE,98.09
1874,"def mapping ( self ) : m = { } if getGdriveCredentialsFile ( ) is not None : m [ "" gdrive "" ] = "" "" unknown = 0 for f in self . scan : bits = f . split ( "" # "" , 2 ) if len ( bits ) == 1 : label = os . path . basename ( f ) else : label = bits [ 1 ] <MASK> label = "" L "" + str ( unknown ) unknown + = 1 m [ label ] = bits [ 0 ] return m","if not label or len ( label ) == 0 or label == """" :",if unknown :,86.37129932,FALSE,84.44
1875,"def get_tag_values ( self , event ) : http = event . interfaces . get ( "" sentry.interfaces.Http "" ) if not http : return [ ] if not http . headers : return [ ] headers = http . headers # XXX: transitional support for workers if isinstance ( headers , dict ) : headers = headers . items ( ) output = [ ] for key , value in headers : if key != "" User-Agent "" : continue ua = Parse ( value ) <MASK> continue result = self . get_tag_from_ua ( ua ) if result : output . append ( result ) return output",if not ua :,if not ua :,100,TRUE,100
1876,"def __iter__ ( self ) : it = DiskHashMerger . __iter__ ( self ) direct_upstreams = self . direct_upstreams for k , groups in it : t = list ( [ [ ] for _ in range ( self . size ) ] ) for i , g in enumerate ( groups ) : if g : <MASK> t [ i ] = g else : g . sort ( key = itemgetter ( 0 ) ) g1 = [ ] for _ , vs in g : g1 . extend ( vs ) t [ i ] = g1 yield k , tuple ( t )",if i in direct_upstreams :,if i in direct_upstreams :,100,TRUE,100
1877,"def process_question ( qtxt ) : question = "" "" skip = False for letter in qtxt : <MASK> skip = True if letter == "" > "" : skip = False if skip : continue if letter . isalnum ( ) or letter == ""   "" : if letter == ""   "" : letter = "" _ "" question + = letter . lower ( ) return question","if letter == ""<"" :","if letter == ""<"" :",100,TRUE,100
1878,"def _module_repr_from_spec ( spec ) : """"""Return the repr to use for the module."""""" # We mostly replicate _module_repr() using the spec attributes. name = "" ? "" if spec . name is None else spec . name if spec . origin is None : if spec . loader is None : return "" <module  {!r} > "" . format ( name ) else : return "" <module  {!r}  ( {!r} )> "" . format ( name , spec . loader ) else : <MASK> return "" <module  {!r}  from  {!r} > "" . format ( name , spec . origin ) else : return "" <module  {!r}  ( {} )> "" . format ( spec . name , spec . origin )",if spec . has_location :,if spec . name is None :,73.63283132,FALSE,97.38
1879,"def test_row ( self , row ) : for idx , test in self . patterns . items ( ) : try : value = row [ idx ] except IndexError : value = "" "" result = test ( value ) if self . any_match : if result : return not self . inverse # True else : <MASK> return self . inverse # False if self . any_match : return self . inverse # False else : return not self . inverse # True",if not result :,if self . any_match and not result :,98.05620115,FALSE,92.44
1880,"def frequent_thread_switches ( ) : """"""Make concurrency bugs more likely to manifest."""""" interval = None if not sys . platform . startswith ( "" java "" ) : <MASK> interval = sys . getswitchinterval ( ) sys . setswitchinterval ( 1e-6 ) else : interval = sys . getcheckinterval ( ) sys . setcheckinterval ( 1 ) try : yield finally : if not sys . platform . startswith ( "" java "" ) : if hasattr ( sys , "" setswitchinterval "" ) : sys . setswitchinterval ( interval ) else : sys . setcheckinterval ( interval )","if hasattr ( sys , ""getswitchinterval"" ) :","if hasattr ( sys , ""setswitchinterval"" ) :",98.81398574,FALSE,97.86
1881,"def record_expected_exportable_production ( self , ticks ) : """"""Record the amount of production that should be transferred to other islands."""""" for ( quota_holder , resource_id ) , amount in self . _low_priority_requests . items ( ) : <MASK> self . _settlement_manager_id [ quota_holder ] = WorldObject . get_object_by_id ( int ( quota_holder [ 1 : ] . split ( "" , "" ) [ 0 ] ) ) . settlement_manager . worldid self . trade_storage [ self . _settlement_manager_id [ quota_holder ] ] [ resource_id ] + = ( ticks * amount )",if quota_holder not in self . _settlement_manager_id :,if quota_holder not in self . trade_storage :,97.64582112,FALSE,95.33
1882,"def _method_events_callback ( self , values ) : try : previous_echoed = ( values [ "" child_result_list "" ] [ - 1 ] . decode ( ) . split ( "" \n "" ) [ - 2 ] . strip ( ) ) <MASK> return "" echo foo2 \n "" elif previous_echoed . endswith ( "" foo2 "" ) : return "" echo foo3 \n "" elif previous_echoed . endswith ( "" foo3 "" ) : return "" exit \n "" else : raise Exception ( "" Unexpected output  {0!r} "" . format ( previous_echoed ) ) except IndexError : return "" echo foo1 \n ""","if previous_echoed . endswith ( ""foo1"" ) :","if previous_echoed . endswith ( ""foo1"" ) :",100,TRUE,100
1883,"def describe_cluster_snapshots ( self , cluster_identifier = None , snapshot_identifier = None ) : if cluster_identifier : cluster_snapshots = [ ] for snapshot in self . snapshots . values ( ) : <MASK> cluster_snapshots . append ( snapshot ) if cluster_snapshots : return cluster_snapshots if snapshot_identifier : if snapshot_identifier in self . snapshots : return [ self . snapshots [ snapshot_identifier ] ] raise ClusterSnapshotNotFoundError ( snapshot_identifier ) return self . snapshots . values ( )",if snapshot . cluster . cluster_identifier == cluster_identifier :,if snapshot . name == cluster_identifier :,94.69237062,FALSE,94.24
1884,def get_snippet_edit_handler ( model ) : if model not in SNIPPET_EDIT_HANDLERS : <MASK> # use the edit handler specified on the page class edit_handler = model . edit_handler else : panels = extract_panel_definitions_from_model_class ( model ) edit_handler = ObjectList ( panels ) SNIPPET_EDIT_HANDLERS [ model ] = edit_handler . bind_to ( model = model ) return SNIPPET_EDIT_HANDLERS [ model ],"if hasattr ( model , ""edit_handler"" ) :","if hasattr ( model , ""edit_handler"" ) :",100,TRUE,100
1885,"def start ( ) : if os . environ . get ( "" RUN_MAIN "" ) != "" true "" : try : exit_code = restart_with_reloader ( ) <MASK> os . kill ( os . getpid ( ) , - exit_code ) else : sys . exit ( exit_code ) except KeyboardInterrupt : pass",if exit_code < 0 :,if exit_code < 0 :,100,TRUE,100
1886,"def discover ( self , * objlist ) : ret = [ ] for l in self . splitlines ( ) : if len ( l ) < 5 : continue <MASK> continue try : int ( l [ 2 ] ) int ( l [ 3 ] ) except : continue #           ret.append(improve(l[0])) ret . append ( l [ 0 ] ) ret . sort ( ) for item in objlist : ret . append ( item ) return ret","if l [ 0 ] == ""Filename"" :","if l [ 0 ] == ""ID"" :",98.56762157,FALSE,97.49
1887,"def ipfs_publish ( self , lib ) : with tempfile . NamedTemporaryFile ( ) as tmp : self . ipfs_added_albums ( lib , tmp . name ) try : <MASK> cmd = "" ipfs add --nocopy -q  "" . split ( ) else : cmd = "" ipfs add -q  "" . split ( ) cmd . append ( tmp . name ) output = util . command_output ( cmd ) except ( OSError , subprocess . CalledProcessError ) as err : msg = "" Failed to publish library. Error:  {0} "" . format ( err ) self . _log . error ( msg ) return False self . _log . info ( "" hash of library:  {0} "" , output )","if self . config [ ""nocopy"" ] :","if lib . name == ""noopy"" :",94.03502766,FALSE,94
1888,"def spends ( self ) : # Return spends indexed by hashX spends = defaultdict ( list ) utxos = self . mempool_utxos ( ) for tx_hash , tx in self . txs . items ( ) : for n , input in enumerate ( tx . inputs ) : if input . is_generation ( ) : continue prevout = ( input . prev_hash , input . prev_idx ) <MASK> hashX , value = utxos . pop ( prevout ) else : hashX , value = self . db_utxos [ prevout ] spends [ hashX ] . append ( prevout ) return spends",if prevout in utxos :,if n == tx_hash :,72.74468772,FALSE,93.76
1889,"def terminate ( self ) : if self . returncode is None : try : os . kill ( self . pid , TERM_SIGNAL ) except OSError as exc : if getattr ( exc , "" errno "" , None ) != errno . ESRCH : <MASK> raise",if self . wait ( timeout = 0.1 ) is None :,if exc . errno != errno . EINTR :,79.49367873,FALSE,82.15
1890,"def _getVolumeScalar ( self ) : if self . _volumeScalar is not None : return self . _volumeScalar # use default elif self . _value in dynamicStrToScalar : return dynamicStrToScalar [ self . _value ] else : thisDynamic = self . _value # ignore leading s like in sf <MASK> thisDynamic = thisDynamic [ 1 : ] # ignore closing z like in fz if thisDynamic [ - 1 ] == "" z "" : thisDynamic = thisDynamic [ : - 1 ] if thisDynamic in dynamicStrToScalar : return dynamicStrToScalar [ thisDynamic ] else : return dynamicStrToScalar [ None ]","if ""s"" in thisDynamic :","if thisDynamic [ 0 ] == ""s"" :",72.24357174,FALSE,92.37
1891,"def init_values ( self ) : config = self . _raw_config for valname , value in self . overrides . iteritems ( ) : if "" . "" in valname : realvalname , key = valname . split ( "" . "" , 1 ) config . setdefault ( realvalname , { } ) [ key ] = value else : config [ valname ] = value for name in config : <MASK> self . __dict__ [ name ] = config [ name ] del self . _raw_config",if name in self . values :,if name not in self . __dict__ :,97.57171814,FALSE,91.73
1892,"def modified ( self ) : paths = set ( ) dictionary_list = [ ] for op_list in self . _operations : if not isinstance ( op_list , list ) : op_list = ( op_list , ) for item in chain ( * op_list ) : if item is None : continue dictionary = item . dictionary <MASK> continue paths . add ( dictionary . path ) dictionary_list . append ( dictionary ) return dictionary_list",if dictionary . path in paths :,if dictionary . path in paths :,100,TRUE,100
1893,"def __getitem__ ( self , key , _get_mode = False ) : if not _get_mode : <MASK> return self . _list [ key ] elif isinstance ( key , slice ) : return self . __class__ ( self . _list [ key ] ) ikey = key . lower ( ) for k , v in self . _list : if k . lower ( ) == ikey : return v # micro optimization: if we are in get mode we will catch that # exception one stack level down so we can raise a standard # key error instead of our special one. if _get_mode : raise KeyError ( ) raise BadRequestKeyError ( key )","if isinstance ( key , ( int , long ) ) :","if isinstance ( key , str ) :",93.71467299,FALSE,95.61
1894,"def _get_items ( self , name , target = 1 ) : all_items = self . get_items ( name ) items = [ o for o in all_items if not o . disabled ] if len ( items ) < target : <MASK> raise ItemNotFoundError ( "" insufficient items with name  %r "" % name ) else : raise AttributeError ( "" insufficient non-disabled items with name  %s "" % name ) on = [ ] off = [ ] for o in items : if o . selected : on . append ( o ) else : off . append ( o ) return on , off",if len ( all_items ) < target :,if len ( items ) == target :,96.90611116,FALSE,95.43
1895,"def get_genome_dir ( gid , galaxy_dir , data ) : """"""Return standard location of genome directories."""""" if galaxy_dir : refs = genome . get_refs ( gid , None , galaxy_dir , data ) seq_file = tz . get_in ( [ "" fasta "" , "" base "" ] , refs ) <MASK> return os . path . dirname ( os . path . dirname ( seq_file ) ) else : gdirs = glob . glob ( os . path . join ( _get_data_dir ( ) , "" genomes "" , "" * "" , gid ) ) if len ( gdirs ) == 1 and os . path . exists ( gdirs [ 0 ] ) : return gdirs [ 0 ]",if seq_file and os . path . exists ( seq_file ) :,if seq_file :,88.45755673,FALSE,92.33
1896,"def _PrintFuncs ( self , names ) : # type: (List[str]) -> int status = 0 for name in names : <MASK> print ( name ) # TODO: Could print LST for -f, or render LST.  Bash does this.  'trap' # could use that too. else : status = 1 return status",if name in self . funcs :,"if name . startswith ( ""-f"" ) :",69.77432125,FALSE,88.56
1897,"def package_files ( self ) : seen_package_directories = ( ) directories = self . distribution . package_dir or { } empty_directory_exists = "" "" in directories packages = self . distribution . packages or [ ] for package in packages : if package in directories : package_directory = directories [ package ] elif empty_directory_exists : package_directory = os . path . join ( directories [ "" "" ] , package ) else : package_directory = package <MASK> seen_package_directories + = ( package_directory + "" . "" , ) yield package_directory",if not package_directory . startswith ( seen_package_directories ) :,if package_directory not in seen_package_directories :,93.09858601,FALSE,92.95
1898,"def apply_conf_file ( fn , conf_filename ) : for env in LSF_CONF_ENV : conf_file = get_conf_file ( conf_filename , env ) <MASK> with open ( conf_file ) as conf_handle : value = fn ( conf_handle ) if value : return value return None",if conf_file :,if os . path . exists ( conf_file ) :,88.69732452,FALSE,86.83
1899,"def on_text ( self , text ) : if text != self . chosen_text : self . fail_test ( ' Expected  "" {} "" , received  "" {} "" ' . format ( self . chosen_text , text ) ) else : self . checks_passed + = 1 <MASK> self . pass_test ( ) else : self . _select_next_text ( )",if self . checks_passed >= self . number_of_checks :,if self . checks_passed >= self . max_checks :,73.1460973,FALSE,95.23
1900,"def test_field_attr_existence ( self ) : for name , item in ast . __dict__ . items ( ) : if self . _is_ast_node ( name , item ) : <MASK> # Index(value) just returns value now. # The argument is required. continue x = item ( ) if isinstance ( x , ast . AST ) : self . assertEqual ( type ( x . _fields ) , tuple )","if name == ""Index"" :","if isinstance ( item , int ) :",66.01960979,FALSE,92.21
1901,"def apply ( self , response ) : updated_headers = self . update_headers ( response ) if updated_headers : response . headers . update ( updated_headers ) warning_header_value = self . warning ( response ) <MASK> response . headers . update ( { "" Warning "" : warning_header_value } ) return response",if warning_header_value is not None :,if warning_header_value :,90.95522502,FALSE,94
1902,"def validate ( self ) : self . assertEqual ( len ( self . inputs ) , len ( self . outputs ) ) for batch_in , batch_out in zip ( self . inputs , self . outputs ) : self . assertEqual ( len ( batch_in ) , len ( batch_out ) ) if self . use_parallel_executor and not self . use_double_buffer : self . validate_unordered_batch ( batch_in , batch_out ) else : for in_data , out_data in zip ( batch_in , batch_out ) : self . assertEqual ( in_data . shape , out_data . shape ) <MASK> self . assertTrue ( ( in_data == out_data ) . all ( ) )",if not self . use_parallel_executor :,if self . use_parallel_executor :,97.59195721,FALSE,98.45
1903,def finalize ( self ) : if self . _started : <MASK> self . _queue . put ( None ) self . _queue . join ( ) self . _consumer . join ( ) self . _started = False self . _finalized = True,if not self . _finalized :,if self . _queue :,87.18991809,FALSE,90.67
1904,"def _get_ilo_version ( self ) : try : self . _get_ilo2 ( ' <?xml version= "" 1.0 "" ?><RIBCL VERSION= "" 2.0 "" ></RIBCL> ' ) except ResponseError as e : <MASK> if e . code == 405 : return 3 if e . code == 501 : return 1 raise return 2","if hasattr ( e , ""code"" ) :","if e . code in ( 3 , 5 ) :",88.43303981,FALSE,89.59
1905,"def _check_data ( self , source , expected_bytes , expected_duration ) : received_bytes = 0 received_seconds = 0.0 bytes_to_read = 1024 while True : data = source . get_audio_data ( bytes_to_read ) <MASK> break received_bytes + = data . length received_seconds + = data . duration self . assertEqual ( data . length , len ( data . data ) ) self . assertAlmostEqual ( expected_duration , received_seconds , places = 1 ) self . assertAlmostEqual ( expected_bytes , received_bytes , delta = 5 )",if data is None :,if not data :,94.78196781,FALSE,96.52
1906,"def __randomize_interval_task ( self ) : for job in self . aps_scheduler . get_jobs ( ) : <MASK> self . aps_scheduler . modify_job ( job . id , next_run_time = datetime . now ( ) + timedelta ( seconds = randrange ( job . trigger . interval . total_seconds ( ) * 0.75 , job . trigger . interval . total_seconds ( ) , ) ) , )","if isinstance ( job . trigger , IntervalTrigger ) :",if job . trigger . interval :,66.52684646,FALSE,91.74
1907,"def find_approximant ( x ) : c = 1e-4 it = sympy . ntheory . continued_fraction_convergents ( sympy . ntheory . continued_fraction_iterator ( x ) ) for i in it : p , q = i . as_numer_denom ( ) tol = c / q * * 2 <MASK> return i if tol < machine_epsilon : break return x",if abs ( i - x ) <= tol :,if p == sympy . ntheory . number_of_convergents (,86.4327234,FALSE,83.75
1908,"def fix_newlines ( lines ) : """"""Convert newlines to unix."""""" for i , line in enumerate ( lines ) : if line . endswith ( "" \r \n "" ) : lines [ i ] = line [ : - 2 ] + "" \n "" <MASK> lines [ i ] = line [ : - 1 ] + "" \n ""","elif line . endswith ( ""\r"" ) :","elif line . endswith ( ""\r\n"" ) :",87.78195165,FALSE,95.88
1909,"def payment_control_render ( self , request : HttpRequest , payment : OrderPayment ) : template = get_template ( "" pretixplugins/paypal/control.html "" ) sale_id = None for trans in payment . info_data . get ( "" transactions "" , [ ] ) : for res in trans . get ( "" related_resources "" , [ ] ) : <MASK> sale_id = res [ "" sale "" ] [ "" id "" ] ctx = { "" request "" : request , "" event "" : self . event , "" settings "" : self . settings , "" payment_info "" : payment . info_data , "" order "" : payment . order , "" sale_id "" : sale_id , } return template . render ( ctx )","if ""sale"" in res and ""id"" in res [ ""sale"" ] :","if ""sale"" in res :",92.81486994,FALSE,92.47
1910,"def for_name ( self , name ) : try : name_resources = self . _resources [ name ] except KeyError : raise LookupError ( name ) else : for res in name_resources : try : inst = res . inst ( ) except Exception as e : <MASK> log . exception ( "" error initializing  %s "" , res ) else : log . error ( "" error initializing  %s :  %s "" , res , e ) else : yield inst",if log . getEffectiveLevel ( ) <= logging . DEBUG :,if log . getEffectiveLevel ( ) <= logging . DEBUG :,100,TRUE,100
1911,"def describe ( self , done = False ) : description = ShellCommand . describe ( self , done ) if done : <MASK> description = [ "" compile "" ] description . append ( "" %d  projects "" % self . getStatistic ( "" projects "" , 0 ) ) description . append ( "" %d  files "" % self . getStatistic ( "" files "" , 0 ) ) warnings = self . getStatistic ( "" warnings "" , 0 ) if warnings > 0 : description . append ( "" %d  warnings "" % warnings ) errors = self . getStatistic ( "" errors "" , 0 ) if errors > 0 : description . append ( "" %d  errors "" % errors ) return description",if not description :,if not description :,100,TRUE,100
1912,"def parse_list ( tl ) : ls = [ ] nm = [ ] while True : term , nmt , tl = parse_term ( tl ) ls . append ( term ) <MASK> nm . append ( nmt ) if tl [ 0 ] != "" , "" : break tl = tl [ 1 : ] return ls , nm , tl",if nmt is not None :,"if tl [ 0 ] == ""N"" :",90.23967187,FALSE,86.41
1913,"def infer_dataset_impl ( path ) : if IndexedRawTextDataset . exists ( path ) : return "" raw "" elif IndexedDataset . exists ( path ) : with open ( index_file_path ( path ) , "" rb "" ) as f : magic = f . read ( 8 ) <MASK> return "" cached "" elif magic == MMapIndexedDataset . Index . _HDR_MAGIC [ : 8 ] : return "" mmap "" else : return None elif FastaDataset . exists ( path ) : return "" fasta "" else : return None",if magic == IndexedDataset . _HDR_MAGIC :,if magic == IndexedDataset . _HDR_MAGIC [ : 8 ] :,68.31298739,FALSE,95.22
1914,"def _get ( self ) : fut = item = None with self . _mutex : # Critical section never blocks. <MASK> fut = Future ( ) fut . add_done_callback ( lambda f : self . _get_complete ( ) if not f . cancelled ( ) else None ) self . _getters . append ( fut ) else : item = self . _get_item ( ) self . _get_complete ( ) return item , fut",if not self . _queue or self . _getters :,if self . _getters :,95.80347455,FALSE,92.51
1915,"def validate ( self ) : dates = [ ] for d in self . get ( "" leave_block_list_dates "" ) : # date is not repeated <MASK> frappe . msgprint ( _ ( "" Date is repeated "" ) + "" : "" + d . block_date , raise_exception = 1 ) dates . append ( d . block_date )",if d . block_date in dates :,if d . block_date not in dates :,98.46497307,FALSE,96.86
1916,"def on_choose_watch_dir_clicked ( self ) : if self . window ( ) . watchfolder_enabled_checkbox . isChecked ( ) : previous_watch_dir = self . window ( ) . watchfolder_location_input . text ( ) or "" "" watch_dir = QFileDialog . getExistingDirectory ( self . window ( ) , "" Please select the watch folder "" , previous_watch_dir , QFileDialog . ShowDirsOnly , ) <MASK> return self . window ( ) . watchfolder_location_input . setText ( watch_dir )",if not watch_dir :,if not watch_dir :,100,TRUE,100
1917,"def log_generator ( self , limit = 6000 , * * kwargs ) : # Generator for show_log_panel skip = 0 while True : logs = self . log ( limit = limit , skip = skip , * * kwargs ) if not logs : break for entry in logs : yield entry <MASK> break skip = skip + limit",if len ( logs ) < limit :,if not self . show_log_panel :,94.55932848,FALSE,87.21
1918,"def _setUpClass ( cls ) : global solver import pyomo . environ from pyomo . solvers . tests . io . writer_test_cases import testCases for test_case in testCases : <MASK> solver [ ( test_case . name , test_case . io ) ] = True","if ( ( test_case . name , test_case . io ) in solver ) and ( test_case . available ) :","if ( test_case . name , test_case . io ) not in ( ""py",80.35894087,FALSE,81.04
1919,"def _get_file_data ( self , normpath , normrev ) : data = self . client . cat ( normpath , normrev ) if has_expanded_svn_keywords ( data ) : # Find out if this file has any keyword expansion set. # If it does, collapse these keywords. This is because SVN # will return the file expanded to us, which would break patching. keywords = self . client . propget ( "" svn:keywords "" , normpath , normrev , recurse = True ) <MASK> data = collapse_svn_keywords ( data , force_bytes ( keywords [ normpath ] ) ) return data",if normpath in keywords :,if keywords :,98.12877483,FALSE,97.21
1920,"def add_controller_list ( path ) : if not os . path . exists ( os . path . join ( path , "" __init__.py "" ) ) : bb . fatal ( "" Controllers directory  %s  exists but is missing __init__.py "" % path ) files = sorted ( [ f for f in os . listdir ( path ) if f . endswith ( "" .py "" ) and not f . startswith ( "" _ "" ) ] ) for f in files : module = "" oeqa.controllers. "" + f [ : - 3 ] <MASK> controllerslist . append ( module ) else : bb . warn ( "" Duplicate controller module found for  %s , only one added. Layers should create unique controller module names "" % module )",if module not in controllerslist :,if os . path . exists ( module ) :,92.53164939,FALSE,94.33
1921,"def on_session2 ( event ) : new_xmpp . get_roster ( ) new_xmpp . send_presence ( ) logging . info ( roster [ 0 ] ) data = roster [ 0 ] [ "" roster "" ] [ "" items "" ] logging . info ( data ) for jid , item in data . items ( ) : <MASK> new_xmpp . send_presence ( ptype = "" subscribe "" , pto = jid ) new_xmpp . update_roster ( jid , name = item [ "" name "" ] , groups = item [ "" groups "" ] ) new_xmpp . disconnect ( )","if item [ ""subscription"" ] != ""none"" :","if ""name"" in item and ""groups"" in item :",93.784395,FALSE,91.46
1922,"def _parse_class_simplified ( symbol ) : results = { } name = symbol . name + "" ( "" name + = "" ,  "" . join ( [ analyzer . expand_attribute ( base ) for base in symbol . bases ] ) name + = "" ) "" for sym in symbol . body : <MASK> result = _parse_function_simplified ( sym , symbol . name ) results . update ( result ) elif isinstance ( sym , ast . ClassDef ) : result = _parse_class_simplified ( sym ) results . update ( result ) lineno = symbol . lineno for decorator in symbol . decorator_list : lineno + = 1 results [ lineno ] = ( name , "" c "" ) return results","if isinstance ( sym , ast . FunctionDef ) :","if isinstance ( sym , ast . FunctionDef ) :",100,TRUE,100
1923,"def check_args ( args ) : """"""Checks that the args are coherent."""""" check_args_has_attributes ( args ) if args . v : non_version_attrs = [ v for k , v in args . __dict__ . items ( ) if k != "" v "" ] print ( "" non_version_attrs "" , non_version_attrs ) <MASK> fail ( "" Cannot show the version number with another command. "" ) return if args . i is None : fail ( "" Cannot draw ER diagram of no database. "" ) if args . o is None : fail ( "" Cannot draw ER diagram with no output file. "" )",if len ( [ v for v in non_version_attrs if v is not None ] ) != 0 :,if non_version_attrs :,83.10951411,FALSE,87.22
1924,"def handle ( self , * args , * * options ) : if not settings . ST_BASE_DIR . endswith ( "" spirit "" ) : raise CommandError ( "" settings.ST_BASE_DIR is not the spirit root folder, are you overriding it? "" ) for root , dirs , files in os . walk ( settings . ST_BASE_DIR ) : <MASK> continue with utils . pushd ( root ) : call_command ( "" makemessages "" , stdout = self . stdout , stderr = self . stderr , * * options ) self . stdout . write ( "" ok "" )","if ""locale"" not in dirs :","if ""spirit"" in dirs :",98.20003485,FALSE,96.57
1925,"def scan ( scope ) : for s in scope . children : if s . start_pos < = position < = s . end_pos : <MASK> return scan ( s ) or s elif s . type in ( "" suite "" , "" decorated "" ) : return scan ( s ) return None","if isinstance ( s , ( tree . Scope , tree . Flow ) ) :","if s . type == ""module"" :",85.07306448,FALSE,79.36
1926,def run_sync ( self ) : count = 0 while count < self . args . num_messages : batch = self . receiver . fetch_next ( max_batch_size = self . args . num_messages - count ) <MASK> for msg in batch : msg . complete ( ) count + = len ( batch ),if self . args . peeklock :,if len ( batch ) :,89.69942164,FALSE,90.75
1927,"def __getitem__ ( self , item ) : if self . _datas is not None : ret = [ ] for data in self . _datas : <MASK> ret . append ( data [ self . _offset ] ) else : ret . append ( data . iloc [ self . _offset ] ) self . _offset + = 1 return ret else : return self . _get_data ( item )","if isinstance ( data , np . ndarray ) :",if self . _offset < len ( data . iloc ) :,91.08939723,FALSE,88.7
1928,"def removedir ( self , path ) : # type: (Text) -> None _path = self . validatepath ( path ) if _path == "" / "" : raise errors . RemoveRootError ( ) with ftp_errors ( self , path ) : try : self . ftp . rmd ( _encode ( _path , self . ftp . encoding ) ) except error_perm as error : code , _ = _parse_ftp_error ( error ) if code == "" 550 "" : <MASK> raise errors . DirectoryExpected ( path ) if not self . isempty ( path ) : raise errors . DirectoryNotEmpty ( path ) raise # pragma: no cover",if self . isfile ( path ) :,if not self . isempty ( path ) :,73.1712148,FALSE,96.5
1929,"def replaces_in_file ( file , replacement_list ) : rs = [ ( re . compile ( regexp ) , repl ) for ( regexp , repl ) in replacement_list ] file_tmp = file + "" . "" + str ( os . getpid ( ) ) + "" .tmp "" with open ( file , "" r "" ) as f : with open ( file_tmp , "" w "" ) as f_tmp : for line in f : for r , replace in rs : match = r . search ( line ) <MASK> line = replace + "" \n "" f_tmp . write ( line ) shutil . move ( file_tmp , file )",if match :,if match :,100,TRUE,100
1930,"def _get_path_check_mem ( self , i , size ) : if size > 0 : <MASK> p = self . _get_path ( i , - 1 ) else : p = self . _get_path ( i , size ) if p . startswith ( "" /dev/shm "" ) : env . meminfo . add ( size ) else : p = self . _get_path ( i , size ) return p",if env . meminfo . rss + size > env . meminfo . mem_limit_soft :,if i == 0 :,82.09700764,FALSE,82.06
1931,"def find_widget_by_id ( self , id , parent = None ) : """"""Recursively searches for widget with specified ID"""""" if parent == None : if id in self : return self [ id ] # Do things fast if possible parent = self [ "" editor "" ] for c in parent . get_children ( ) : <MASK> if c . get_id ( ) == id : return c if isinstance ( c , Gtk . Container ) : r = self . find_widget_by_id ( id , c ) if not r is None : return r return None","if hasattr ( c , ""get_id"" ) :","if isinstance ( c , Gtk . Widget ) :",97.18944145,FALSE,93.18
1932,"def _deserialize ( cls , io ) : flags = VideoFlags ( ) flags . byte = U8 . read ( io ) if flags . bit . type == VIDEO_FRAME_TYPE_COMMAND_FRAME : data = VideoCommandFrame . deserialize ( io ) else : <MASK> data = AVCVideoData . deserialize ( io ) else : data = io . read ( ) return cls ( flags . bit . type , flags . bit . codec , data )",if flags . bit . codec == VIDEO_CODEC_ID_AVC :,if flags . bit . type == AVC_FRAME_TYPE_AVC_DATA :,95.29167798,FALSE,88.67
1933,"def asciiLogData ( data , maxlen = 64 , replace = False ) : ellipses = ""  ... "" try : <MASK> dd = data [ : maxlen ] + ellipses else : dd = data return dd . decode ( "" utf8 "" , errors = "" replace "" if replace else "" strict "" ) except : return "" 0x "" + binLogData ( data , maxlen )",if len ( data ) > maxlen - len ( ellipses ) :,if len ( data ) > maxlen :,89.98299475,FALSE,92.02
1934,"def _check_units ( self , new_unit_system ) : # If no unit system has been specified for me yet, adopt the incoming # system if self . unit_system is None : self . unit_system = new_unit_system else : # Otherwise, make sure they match <MASK> raise ValueError ( "" Unit system mismatch  %d  v.  %d "" % ( self . unit_system , new_unit_system ) )",if self . unit_system != new_unit_system :,if self . unit_system != new_unit_system :,75,TRUE,100
1935,"def command ( filenames , dirnames , fix ) : for filename in gather_files ( dirnames , filenames ) : visitor = process_file ( filename ) <MASK> print ( "" %s :  %s "" % ( filename , visitor . get_stats ( ) ) ) if fix : print ( "" Fixing:  %s "" % filename ) fix_file ( filename )",if visitor . needs_fix ( ) :,if visitor :,93.3489145,FALSE,90.39
1936,"def assign_attributes_to_variants ( variant_attributes ) : for value in variant_attributes : pk = value [ "" pk "" ] defaults = value [ "" fields "" ] defaults [ "" variant_id "" ] = defaults . pop ( "" variant "" ) defaults [ "" assignment_id "" ] = defaults . pop ( "" assignment "" ) assigned_values = defaults . pop ( "" values "" ) assoc , created = AssignedVariantAttribute . objects . update_or_create ( pk = pk , defaults = defaults ) <MASK> assoc . values . set ( AttributeValue . objects . filter ( pk__in = assigned_values ) )",if created :,if created :,100,TRUE,100
1937,"def _info ( self , userlist ) : for strng in userlist : group_matched = False for env in self . base . comps . environments_by_pattern ( strng ) : self . output . display_groups_in_environment ( env ) group_matched = True for group in self . base . comps . groups_by_pattern ( strng ) : self . output . display_pkgs_in_groups ( group ) group_matched = True <MASK> logger . error ( _ ( "" Warning: Group  %s  does not exist. "" ) , strng ) return 0 , [ ]",if not group_matched :,if not group_matched :,100,TRUE,100
1938,"def parse_implements_interfaces ( parser ) : types = [ ] if parser . token . value == "" implements "" : advance ( parser ) while True : types . append ( parse_named_type ( parser ) ) <MASK> break return types","if not peek ( parser , TokenKind . NAME ) :",if not parser . token . value :,85.11838938,FALSE,84.48
1939,"def generate ( ) : for leaf in u . leaves : if isinstance ( leaf , Integer ) : val = leaf . get_int_value ( ) if val in ( 0 , 1 ) : yield val else : raise _NoBoolVector elif isinstance ( leaf , Symbol ) : if leaf == SymbolTrue : yield 1 <MASK> yield 0 else : raise _NoBoolVector else : raise _NoBoolVector",elif leaf == SymbolFalse :,elif leaf == SymbolFalse :,100,TRUE,100
1940,"def update_gstin ( context ) : dirty = False for key , value in iteritems ( frappe . form_dict ) : if key != "" party "" : address_name = frappe . get_value ( "" Address "" , key ) <MASK> address = frappe . get_doc ( "" Address "" , address_name ) address . gstin = value . upper ( ) address . save ( ignore_permissions = True ) dirty = True if dirty : frappe . db . commit ( ) context . updated = True",if address_name :,if address_name :,100,TRUE,100
1941,"def everythingIsUnicode ( d ) : """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k , v in d . iteritems ( ) : <MASK> if not everythingIsUnicode ( v ) : return False elif isinstance ( v , list ) : for i in v : if isinstance ( i , dict ) and not everythingIsUnicode ( i ) : return False elif isinstance ( i , _bytes ) : return False elif isinstance ( v , _bytes ) : return False return True","if isinstance ( v , dict ) and k != ""headers"" :","if isinstance ( v , dict ) :",93.39625736,FALSE,92.11
1942,"def check_graph ( graph ) : # pragma: no cover for c in graph : <MASK> raise RuntimeError ( "" cannot have fuse "" ) for inp in c . inputs : if isinstance ( inp . op , Fuse ) : raise RuntimeError ( "" cannot have fuse "" )","if isinstance ( c . op , Fuse ) :","if isinstance ( c . op , Gaffer . Conv2d ) :",72.08079175,FALSE,92.86
1943,"def __getattr__ ( self , key ) : try : value = self . __parent . contents [ key ] except KeyError : pass else : <MASK> if isinstance ( value , _ModuleMarker ) : return value . mod_ns else : assert isinstance ( value , _MultipleClassMarker ) return value . attempt_get ( self . __parent . path , key ) raise AttributeError ( "" Module  %r  has no mapped classes  "" "" registered under the name  %r "" % ( self . __parent . name , key ) )",if value is not None :,"if isinstance ( value , _MappingClassMarker ) :",94.08124748,FALSE,92.77
1944,"def filter_ports ( self , dpid , in_port , nw_id , allow_nw_id_external = None ) : assert nw_id != self . nw_id_unknown ret = [ ] for port in self . get_ports ( dpid ) : nw_id_ = port . network_id if port . port_no == in_port : continue <MASK> ret . append ( port . port_no ) elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external : ret . append ( port . port_no ) return ret",if nw_id_ == nw_id :,if port . port_no == nw_id :,95.04159049,FALSE,95.47
1945,"def _parse ( self , contents ) : entries = [ ] for line in contents . splitlines ( ) : if not len ( line . strip ( ) ) : entries . append ( ( "" blank "" , [ line ] ) ) continue ( head , tail ) = chop_comment ( line . strip ( ) , "" # "" ) <MASK> entries . append ( ( "" all_comment "" , [ line ] ) ) continue entries . append ( ( "" option "" , [ head . split ( None ) , tail ] ) ) return entries",if not len ( head ) :,if head is None :,91.25699941,FALSE,94.48
1946,"def _get_documented_completions ( self , table , startswith = None ) : names = [ ] for key , command in table . items ( ) : if getattr ( command , "" _UNDOCUMENTED "" , False ) : # Don't tab complete undocumented commands/params continue <MASK> continue if getattr ( command , "" positional_arg "" , False ) : continue names . append ( key ) return names",if startswith is not None and not key . startswith ( startswith ) :,if startswith and key . startswith ( startswith ) :,95.57987396,FALSE,93.03
1947,"def _convert_example ( example , use_bfloat16 ) : """"""Cast int64 into int32 and float32 to bfloat16 if use_bfloat16."""""" for key in list ( example . keys ( ) ) : val = example [ key ] <MASK> val = tf . sparse . to_dense ( val ) if val . dtype == tf . int64 : val = tf . cast ( val , tf . int32 ) if use_bfloat16 and val . dtype == tf . float32 : val = tf . cast ( val , tf . bfloat16 ) example [ key ] = val",if tf . keras . backend . is_sparse ( val ) :,"if isinstance ( val , tf . sparse . SparseTensor ) :",79.15742227,FALSE,91.77
1948,"def _get_lang_zone ( self , lang ) : if lang not in self . _lang_zone_from_lang : <MASK> self . _lang_zone_from_lang [ lang ] = MultiLangZone ( self . mgr , lang ) else : self . _lang_zone_from_lang [ lang ] = LangZone ( self . mgr , lang ) return self . _lang_zone_from_lang [ lang ]",if self . mgr . is_multilang ( lang ) :,if self . _multi_lang :,86.47416108,FALSE,90.8
1949,"def dispatch ( self , request , * args , * * kwargs ) : try : return super ( Handler , self ) . dispatch ( request , * args , * * kwargs ) except Http404 as e : <MASK> try : request . original_path_info = request . path_info request . path_info = settings . FEINCMS_CMS_404_PAGE response = super ( Handler , self ) . dispatch ( request , * args , * * kwargs ) response . status_code = 404 return response except Http404 : raise e else : raise",if settings . FEINCMS_CMS_404_PAGE :,if settings . FEINCMS_404_PAGE :,98.70131274,FALSE,97.66
1950,"def _maybe_update_dropout ( self , step ) : for i in range ( len ( self . dropout_steps ) ) : <MASK> self . model . update_dropout ( self . dropout [ i ] ) logger . info ( "" Updated dropout to  %f  from step  %d "" % ( self . dropout [ i ] , step ) )",if step > 1 and step == self . dropout_steps [ i ] + 1 :,if self . dropout [ i ] > 0.0 :,60.92798871,FALSE,82.07
1951,"def bulk_move ( * args , * * kwargs ) : for arg in args : <MASK> raise PopupException ( _ ( "" Source path and destination path cannot be same "" ) ) request . fs . rename ( urllib . unquote ( arg [ "" src_path "" ] ) , urllib . unquote ( arg [ "" dest_path "" ] ) )","if arg [ ""src_path"" ] == arg [ ""dest_path"" ] :","if arg [ ""src_path"" ] != arg [ ""dest_path"" ] :",73.27716695,FALSE,97.12
1952,"def asisWrite ( self , root ) : at , c = self , self . c try : c . endEditing ( ) c . init_error_dialogs ( ) fileName = at . initWriteIvars ( root , root . atAsisFileNodeName ( ) ) <MASK> at . addToOrphanList ( root ) return at . openOutputStream ( ) for p in root . self_and_subtree ( copy = False ) : at . writeAsisNode ( p ) contents = at . closeOutputStream ( ) at . replaceFile ( contents , at . encoding , fileName , root ) except Exception : at . writeException ( fileName , root )","if not at . precheck ( fileName , root ) :","if fileName == ""automatic"" :",91.79715224,FALSE,91.61
1953,"def next_event ( it ) : """"""read an event from an eventstream"""""" while True : try : line = next ( it ) except StopIteration : return <MASK> return json . loads ( line . split ( "" : "" , 1 ) [ 1 ] )","if line . startswith ( ""data:"" ) :","if line . startswith ( ""event:"" ) :",97.62692892,FALSE,96.01
1954,"def process_formdata ( self , valuelist ) : if valuelist : if valuelist [ 0 ] == "" __None "" : self . data = None else : <MASK> self . data = None return try : obj = self . queryset . get ( pk = valuelist [ 0 ] ) self . data = obj except DoesNotExist : self . data = None",if self . queryset is None :,"if valuelist [ 0 ] == ""__None"" :",74.25516462,FALSE,84.73
1955,"def _setResultsName ( self , name , listAllMatches = False ) : if __diag__ . warn_multiple_tokens_in_named_alternation : <MASK> warnings . warn ( "" {} : setting results name  {!r}  on  {}  expression  "" "" will return a list of all parsed tokens in an And alternative,  "" "" in prior versions only the first token was returned "" . format ( "" warn_multiple_tokens_in_named_alternation "" , name , type ( self ) . __name__ , ) , stacklevel = 3 , ) return super ( ) . _setResultsName ( name , listAllMatches )","if any ( isinstance ( e , And ) for e in self . exprs ) :",if self . _isAnd ( ) :,87.81712305,FALSE,89.67
1956,"def add ( request ) : form_type = "" servers "" if request . method == "" POST "" : form = BookMarkForm ( request . POST ) if form . is_valid ( ) : form_type = form . save ( ) messages . add_message ( request , messages . INFO , "" Bookmark created "" ) else : messages . add_message ( request , messages . INFO , form . errors ) <MASK> url = reverse ( "" servers "" ) else : url = reverse ( "" metrics "" ) return redirect ( url ) else : return redirect ( reverse ( "" servers "" ) )","if form_type == ""server"" :","if form_type == ""servers"" :",98.88426089,FALSE,98.03
1957,"def __init__ ( self , post_id , artist , page , tzInfo = None , dateFormat = None ) : self . imageUrls = list ( ) self . imageResizedUrls = list ( ) self . imageId = int ( post_id ) self . _tzInfo = tzInfo self . dateFormat = dateFormat if page is not None : post_json = demjson . decode ( page ) <MASK> artist_id = post_json [ "" data "" ] [ "" item "" ] [ "" user "" ] [ "" id "" ] self . artist = SketchArtist ( artist_id , page , tzInfo , dateFormat ) else : self . artist = artist self . parse_post ( post_json [ "" data "" ] [ "" item "" ] )",if artist is None :,"if post_json [ ""data"" ] [ ""item"" ] [ ""user"" ]",70.29790826,FALSE,88.14
1958,"def _create_batch_iterator ( self , mark_as_delete : Callable [ [ Any ] , None ] , to_key : Callable [ [ Any ] , Any ] , to_value : Callable [ [ Any ] , Any ] , batch : Iterable [ EventT ] , ) - > Iterable [ Tuple [ Any , Any ] ] : for event in batch : key = to_key ( event . key ) # to delete keys in the table we set the raw value to None <MASK> mark_as_delete ( key ) continue yield key , to_value ( event . value )",if event . message . value is None :,if event . value is None :,98.73865686,FALSE,97.84
1959,"def test_lc_numeric_nl_langinfo ( self ) : # Test nl_langinfo against known values tested = False for loc in candidate_locales : try : setlocale ( LC_NUMERIC , loc ) setlocale ( LC_CTYPE , loc ) except Error : continue for li , lc in ( ( RADIXCHAR , "" decimal_point "" ) , ( THOUSEP , "" thousands_sep "" ) ) : <MASK> tested = True if not tested : self . skipTest ( "" no suitable locales "" )","if self . numeric_tester ( ""nl_langinfo"" , nl_langinfo ( li ) , lc , loc ) :","if setlocale ( LC_NUMERIC , li , lc ) :",67.61686933,FALSE,81.61
1960,"def _level_up_logging ( self ) : for handler in self . log . handlers : <MASK> if handler . level != logging . DEBUG : handler . setLevel ( logging . DEBUG ) self . log . debug ( "" Leveled up log file verbosity "" )","if issubclass ( handler . __class__ , logging . FileHandler ) :","if hasattr ( handler , ""level"" ) :",62.57921679,FALSE,79.38
1961,def _show_axes_changed ( self ) : marker = self . marker if ( self . _vtk_control is not None ) and ( marker is not None ) : <MASK> marker . interactor = None marker . enabled = False else : marker . interactor = self . interactor marker . enabled = True self . render ( ),if not self . show_axes :,if self . _vtk_control . axes_changed :,88.45145694,FALSE,86.29
1962,"def handle_keypress ( self , rawKey , modifiers , key , * args ) : if self . recordKeyboard and self . __delayPassed ( ) : <MASK> self . insideKeys = True self . targetParent . start_key_sequence ( ) modifierCount = len ( modifiers ) if ( modifierCount > 1 or ( modifierCount == 1 and Key . SHIFT not in modifiers ) or ( Key . SHIFT in modifiers and len ( rawKey ) > 1 ) ) : self . targetParent . append_hotkey ( rawKey , modifiers ) elif key not in MODIFIERS : self . targetParent . append_key ( key )",if not self . insideKeys :,if self . insideKeys :,93.61051423,FALSE,97.92
1963,"def transform ( self , data ) : with timer ( "" transform  %s "" % self . name , logging . DEBUG ) : if self . operator in { "" lat "" , "" latitude "" } : return self . series ( data ) . apply ( GeoIP . get_latitude ) <MASK> return self . series ( data ) . apply ( GeoIP . get_longitude ) elif self . operator in { "" acc "" , "" accuracy "" } : return self . series ( data ) . apply ( GeoIP . get_accuracy ) raise NameError ( "" Unknown GeoIP operator [lat, lon, acc]:  %s "" % self . operator )","elif self . operator in { ""lon"" , ""longitude"" } :","elif self . operator in { ""lon"" , ""longitude"" } :",100,TRUE,100
1964,"def _get_sidebar_selected ( self ) : sidebar_selected = None if self . businessline_id : sidebar_selected = "" bl_ %s "" % self . businessline_id <MASK> sidebar_selected + = "" _s_ %s "" % self . service_id if self . environment_id : sidebar_selected + = "" _env_ %s "" % self . environment_id return sidebar_selected",if self . service_id :,if self . service_id :,100,TRUE,100
1965,"def _run_response_middleware ( self , request , response , request_name = None ) : named_middleware = self . named_response_middleware . get ( request_name , deque ( ) ) applicable_middleware = self . response_middleware + named_middleware if applicable_middleware : for middleware in applicable_middleware : _response = middleware ( request , response ) <MASK> _response = await _response if _response : response = _response break return response",if isawaitable ( _response ) :,if asyncio . iscoroutine ( _response ) :,93.79867753,FALSE,95.55
1966,"def populate_obj ( self , obj , name ) : field = getattr ( obj , name , None ) if field is not None : # If field should be deleted, clean it up if self . _should_delete : field . delete ( ) return <MASK> if not field . grid_id : func = field . put else : func = field . replace func ( self . data . stream , filename = self . data . filename , content_type = self . data . content_type , )","if isinstance ( self . data , FileStorage ) and not is_empty ( self . data . stream ) :",if self . _should_put :,91.07496271,FALSE,82.59
1967,"def _import_hash ( self , operator ) : # Import required modules into local namespace so that pipelines # may be evaluated directly for key in sorted ( operator . import_hash . keys ( ) ) : module_list = "" ,  "" . join ( sorted ( operator . import_hash [ key ] ) ) <MASK> exec ( "" from  {}  import  {} "" . format ( key [ 4 : ] , module_list ) ) else : exec ( "" from  {}  import  {} "" . format ( key , module_list ) ) for var in operator . import_hash [ key ] : self . operators_context [ var ] = eval ( var )","if key . startswith ( ""tpot."" ) :","if key . startswith ( ""module"" ) :",73.96936433,FALSE,97.57
1968,"def remove_files ( folder , file_extensions ) : for f in os . listdir ( folder ) : f_path = os . path . join ( folder , f ) <MASK> extension = os . path . splitext ( f_path ) [ 1 ] if extension in file_extensions : os . remove ( f_path )",if os . path . isfile ( f_path ) :,if os . path . isfile ( f_path ) :,100,TRUE,100
1969,"def clearBuffer ( self ) : if self . shouldLose == - 1 : return if self . producer : self . producer . resumeProducing ( ) if self . buffer : <MASK> self . logFile . write ( "" loopback receiving  %s \n "" % repr ( self . buffer ) ) buffer = self . buffer self . buffer = b "" "" self . target . dataReceived ( buffer ) if self . shouldLose == 1 : self . shouldLose = - 1 self . target . connectionLost ( failure . Failure ( main . CONNECTION_DONE ) )",if self . logFile :,if self . logFile :,100,TRUE,100
1970,"def write ( self , data ) : if mock_target . _mirror_on_stderr : if self . _write_line : sys . stderr . write ( fn + "" :  "" ) if bytes : sys . stderr . write ( data . decode ( "" utf8 "" ) ) else : sys . stderr . write ( data ) <MASK> self . _write_line = True else : self . _write_line = False super ( Buffer , self ) . write ( data )","if ( data [ - 1 ] ) == ""\n"" :",if not self . _write_line :,88.67727973,FALSE,86.79
1971,def stop ( self ) : self . queue_com . state_lock . acquire ( ) try : <MASK> self . queue_com . state = STOPPED self . remove ( ) return True return False finally : self . queue_com . state_lock . release ( ),if self . queue_com . state == RUNNING and self . stop_task ( ) :,if self . queue_com . state == STOPPED :,81.69334189,FALSE,85.11
1972,"def _handle_special_args ( self , pyobjects ) : if len ( pyobjects ) == len ( self . arguments . args ) : if self . arguments . vararg : pyobjects . append ( rope . base . builtins . get_list ( ) ) <MASK> pyobjects . append ( rope . base . builtins . get_dict ( ) )",if self . arguments . kwarg :,if self . arguments . vararg :,98.0251428,FALSE,96.56
1973,"def go_to_last_edit_location ( self ) : if self . last_edit_cursor_pos is not None : filename , position = self . last_edit_cursor_pos if not osp . isfile ( filename ) : self . last_edit_cursor_pos = None return else : self . load ( filename ) editor = self . get_current_editor ( ) <MASK> editor . set_cursor_position ( position )",if position < editor . document ( ) . characterCount ( ) :,if editor is not None :,78.94236353,FALSE,87.89
1974,"def _create_sentence_objects ( self ) : """"""Returns a list of Sentence objects from the raw text."""""" sentence_objects = [ ] sent_tokenizer = SentenceTokenizer ( locale = self . language . code ) seq = Sequence ( self . raw ) seq = sent_tokenizer . transform ( seq ) for start_index , end_index in zip ( seq . idx [ : - 1 ] , seq . idx [ 1 : ] ) : # Sentences share the same models as their parent blob sent = seq . text [ start_index : end_index ] . strip ( ) <MASK> continue s = Sentence ( sent , start_index = start_index , end_index = end_index ) s . detected_languages = self . detected_languages sentence_objects . append ( s ) return sentence_objects",if not sent :,"if sent == """" :",98.41194226,FALSE,96.41
1975,"def to_json_schema ( self , parent = None ) : schema = { } if not parent : schema [ "" title "" ] = self . title <MASK> schema [ "" description "" ] = self . description if self . has_default : schema [ "" default "" ] = self . default schema [ "" _required_ "" ] = self . required if self . null : schema [ "" type "" ] = [ "" string "" , "" null "" ] else : schema [ "" type "" ] = "" string "" if self . enum is not None : schema [ "" enum "" ] = self . enum return schema",if self . description :,if self . description is not None :,94.97491861,FALSE,96.54
1976,def rmdir ( dirname ) : if dirname [ - 1 ] == os . sep : dirname = dirname [ : - 1 ] if os . path . islink ( dirname ) : return # do not clear link - we can get out of dir for f in os . listdir ( dirname ) : <MASK> continue path = dirname + os . sep + f if os . path . isdir ( path ) : rmdir ( path ) else : os . unlink ( path ) os . rmdir ( dirname ),"if f in ( ""."" , "".."" ) :","if f . startswith ( ""__"" ) :",96.88198769,FALSE,91.28
1977,"def convert_whole_dir ( path = Path ( "" marian_ckpt/ "" ) ) : for subdir in tqdm ( list ( path . ls ( ) ) ) : dest_dir = f "" marian_converted/ { subdir . name } "" <MASK> continue convert ( source_dir , dest_dir )","if ( dest_dir / ""pytorch_model.bin"" ) . exists ( ) :",if not os . path . exists ( dest_dir ) :,87.18587729,FALSE,82.71
1978,"def colorformat ( text ) : if text [ 0 : 1 ] == "" # "" : col = text [ 1 : ] if len ( col ) == 6 : return col <MASK> return col [ 0 ] * 2 + col [ 1 ] * 2 + col [ 2 ] * 2 elif text == "" "" : return "" "" assert False , "" wrong color format  %r "" % text",elif len ( col ) == 3 :,elif len ( col ) == 3 :,100,TRUE,100
1979,"def _init_rel_seek ( self ) : "" Sets the file object ' s position to the relative location set above. "" rs , fo = self . _rel_seek , self . _file_obj if rs == 0.0 : fo . seek ( 0 , os . SEEK_SET ) else : fo . seek ( 0 , os . SEEK_END ) size = fo . tell ( ) <MASK> self . _cur_pos = size else : target = int ( size * rs ) fo . seek ( target , os . SEEK_SET ) self . _align_to_newline ( ) self . _cur_pos = fo . tell ( )",if rs == 1.0 :,if size == 0 :,93.10659825,FALSE,96.57
1980,"def parse_command_line ( self , argv = None ) : """"""Parse the command line"""""" if self . config : parser = argparse . ArgumentParser ( add_help = False ) self . settings [ "" config "" ] . add_argument ( parser ) opts , _ = parser . parse_known_args ( argv ) if opts . config is not None : self . set ( "" config "" , opts . config ) self . params . update ( self . import_from_module ( ) ) parser = self . parser ( ) opts = parser . parse_args ( argv ) for k , v in opts . __dict__ . items ( ) : <MASK> continue self . set ( k . lower ( ) , v )",if v is None :,"if k . startswith ( ""_"" ) :",95.04266446,FALSE,94.11
1981,"def process ( self , resources , event = None ) : client = local_session ( self . manager . session_factory ) . client ( "" shield "" , region_name = "" us-east-1 "" ) protections = get_type_protections ( client , self . manager . get_model ( ) ) protected_resources = { p [ "" ResourceArn "" ] for p in protections } state = self . data . get ( "" state "" , False ) results = [ ] for arn , r in zip ( self . manager . get_arns ( resources ) , resources ) : r [ "" c7n:ShieldProtected "" ] = shielded = arn in protected_resources <MASK> results . append ( r ) elif not shielded and not state : results . append ( r ) return results",if shielded and state :,if arn in self . data :,95.1263132,FALSE,95.89
1982,"def removeTrailingWs ( self , aList ) : i = 0 while i < len ( aList ) : <MASK> j = i i = self . skip_ws ( aList , i ) assert j < i if i > = len ( aList ) or aList [ i ] == "" \n "" : # print ""removing trailing ws:"", `i-j` del aList [ j : i ] i = j else : i + = 1",if self . is_ws ( aList [ i ] ) :,"if aList [ i ] == ""\n"" :",89.28853875,FALSE,89.69
1983,"def predict ( request : Request ) : form = await request . form ( ) files , entry = convert_input ( form ) try : <MASK> return JSONResponse ( ALL_FEATURES_PRESENT_ERROR , status_code = 400 ) try : resp = model . predict ( data_dict = [ entry ] ) . to_dict ( "" records "" ) [ 0 ] return JSONResponse ( resp ) except Exception as e : logger . error ( "" Error:  {} "" . format ( str ( e ) ) ) return JSONResponse ( COULD_NOT_RUN_INFERENCE_ERROR , status_code = 500 ) finally : for f in files : os . remove ( f . name )",if ( entry . keys ( ) & input_features ) != input_features :,"if entry in [ ""*"" , ""*"" ] :",73.1096465,FALSE,88.66
1984,"def reset ( self ) : logger . debug ( "" Arctic.reset() "" ) with self . _lock : <MASK> self . __conn . close ( ) self . __conn = None for _ , l in self . _library_cache . items ( ) : if hasattr ( l , "" _reset "" ) and callable ( l . _reset ) : logger . debug ( "" Library reset()  %s "" % l ) l . _reset ( ) # the existence of _reset() is not guaranteed/enforced, it also triggers re-auth",if self . __conn is not None :,if self . __conn :,91.82158991,FALSE,96.31
1985,"def read ( self ) : if op . isfile ( self . fileName ) : with textfile_open ( self . fileName , "" rt "" ) as fid : items = json . load ( fid ) # TODO: catch JSON exception... <MASK> items = dict ( ) else : items = dict ( ) self . _items . clear ( ) self . _items . update ( items ) self . _haveReadData = True",if items is None :,if items is None :,100,TRUE,100
1986,"def get_django_comment ( text : str , i : int ) - > str : end = i + 4 unclosed_end = 0 while end < = len ( text ) : if text [ end - 2 : end ] == "" #} "" : return text [ i : end ] <MASK> unclosed_end = end end + = 1 raise TokenizationException ( "" Unclosed comment "" , text [ i : unclosed_end ] )","if not unclosed_end and text [ end ] == ""<"" :","if text [ end - 2 : end ] == ""#}="" :",89.86186963,FALSE,89.66
1987,"def _wrap_forwarded ( self , key , value ) : if isinstance ( value , SourceCode ) and value . late_binding : # get cached return value if present value_ = self . _late_binding_returnvalues . get ( key , KeyError ) <MASK> # evaluate the late-bound function value_ = self . _eval_late_binding ( value ) schema = self . late_bind_schemas . get ( key ) if schema is not None : value_ = schema . validate ( value_ ) # cache result of late bound func self . _late_binding_returnvalues [ key ] = value_ return value_ else : return value",if value_ is KeyError :,if value_ is None :,98.7918803,FALSE,98.12
1988,"def connect ( * args , * * ckwargs ) : if "" give_content_type "" in kwargs : <MASK> kwargs [ "" give_content_type "" ] ( args [ 6 ] [ "" content-type "" ] ) else : kwargs [ "" give_content_type "" ] ( "" "" ) if "" give_connect "" in kwargs : kwargs [ "" give_connect "" ] ( * args , * * ckwargs ) status = code_iter . next ( ) etag = etag_iter . next ( ) timestamp = timestamps_iter . next ( ) if status == - 1 : raise HTTPException ( ) return FakeConn ( status , etag , body = kwargs . get ( "" body "" , "" "" ) , timestamp = timestamp )","if len ( args ) >= 7 and ""content_type"" in args [ 6 ] :",if args [ 6 ] :,92.85268833,FALSE,90.34
1989,"def _reset ( self ) : self . _handle_connect ( ) if self . rewarder_session : <MASK> env_id = random . choice ( self . _sample_env_ids ) logger . info ( "" Randomly sampled env_id= {} "" . format ( env_id ) ) else : env_id = None self . rewarder_session . reset ( env_id = env_id ) else : logger . info ( "" No rewarder session exists, so cannot send a reset via the rewarder channel "" ) self . _reset_mask ( ) return [ None ] * self . n",if self . _sample_env_ids :,if self . _sample_env_ids :,100,TRUE,100
1990,"def _create_architecture_list ( architectures , current_arch ) : if not architectures : return [ _Architecture ( build_on = [ current_arch ] ) ] build_architectures : List [ str ] = [ ] architecture_list : List [ _Architecture ] = [ ] for item in architectures : if isinstance ( item , str ) : build_architectures . append ( item ) <MASK> architecture_list . append ( _Architecture ( build_on = item . get ( "" build-on "" ) , run_on = item . get ( "" run-on "" ) ) ) if build_architectures : architecture_list . append ( _Architecture ( build_on = build_architectures ) ) return architecture_list","if isinstance ( item , dict ) :","elif isinstance ( item , dict ) :",74.69617809,FALSE,98.31
1991,"def inspect ( self , pokemon ) : # Make sure it was not caught! for caught_pokemon in self . cache : same_latitude = "" {0:.4f} "" . format ( pokemon [ "" latitude "" ] ) == "" {0:.4f} "" . format ( caught_pokemon [ "" latitude "" ] ) same_longitude = "" {0:.4f} "" . format ( pokemon [ "" longitude "" ] ) == "" {0:.4f} "" . format ( caught_pokemon [ "" longitude "" ] ) <MASK> return if len ( self . cache ) > = 200 : self . cache . pop ( 0 ) self . cache . append ( pokemon )",if same_latitude and same_longitude :,if same_latitude and same_longitude :,75,TRUE,100
1992,"def parley ( self ) : for x in [ 0 , 1 ] : a = self . agents [ x ] . act ( ) <MASK> if "" [DONE] "" in a [ "" text "" ] : self . agents [ x - 1 ] . observe ( { "" id "" : "" World "" , "" text "" : "" The other agent has ended the chat. "" } ) self . episodeDone = True else : self . agents [ x - 1 ] . observe ( a )",if a is not None :,"if a [ ""id"" ] == ""Chat"" :",94.3799781,FALSE,89.38
1993,"def _prepare_subset ( full_data : torch . Tensor , full_targets : torch . Tensor , num_samples : int , digits : Sequence , ) : classes = { d : 0 for d in digits } indexes = [ ] for idx , target in enumerate ( full_targets ) : label = target . item ( ) if classes . get ( label , float ( "" inf "" ) ) > = num_samples : continue indexes . append ( idx ) classes [ label ] + = 1 <MASK> break data = full_data [ indexes ] targets = full_targets [ indexes ] return data , targets",if all ( classes [ k ] >= num_samples for k in classes ) :,if classes [ label ] >= num_samples :,89.82505211,FALSE,91.1
1994,"def get_work_root ( self , flags ) : _flags = flags . copy ( ) _flags [ "" is_toplevel "" ] = True target = self . _get_target ( _flags ) if target : _flags [ "" target "" ] = target . name tool = self . get_tool ( _flags ) <MASK> return target . name + "" - "" + tool else : raise SyntaxError ( "" Failed to determine work root. Could not resolve tool for target  "" + target . name ) else : raise SyntaxError ( "" Failed to determine work root. Could not resolve target "" )",if tool :,if tool :,100,TRUE,100
1995,"def run_command ( self , data ) : """"""Run editor commands."""""" parts = data . split ( ""   "" ) cmd = parts [ 0 ] . lower ( ) if cmd in self . operations . keys ( ) : return self . run_operation ( cmd ) args = ""   "" . join ( parts [ 1 : ] ) self . logger . debug ( "" Looking for command  ' {0} ' "" . format ( cmd ) ) if cmd in self . modules . modules . keys ( ) : self . logger . debug ( "" Trying to run command  ' {0} ' "" . format ( cmd ) ) self . get_editor ( ) . store_action_state ( cmd ) <MASK> return False else : self . set_status ( "" Command  ' {0} '  not found. "" . format ( cmd ) ) return False return True","if not self . run_module ( cmd , args ) :","if self . run_command ( cmd , args ) :",86.66037676,FALSE,97.34
1996,"def get_main_chain_layers ( self ) : """"""Return a list of layer IDs in the main chain."""""" main_chain = self . get_main_chain ( ) ret = [ ] for u in main_chain : for v , layer_id in self . adj_list [ u ] : <MASK> ret . append ( layer_id ) return ret",if v in main_chain and u in main_chain :,if v == layer_id :,90.1474017,FALSE,87.47
1997,"def hash ( self , context ) : with context : <MASK> return IECore . MurmurHash ( ) h = GafferDispatch . TaskNode . hash ( self , context ) h . append ( self [ "" fileName "" ] . hash ( ) ) h . append ( self [ "" in "" ] . hash ( ) ) h . append ( self . __parameterHandler . hash ( ) ) return h","if not self [ ""fileName"" ] . getValue ( ) or self [ ""in"" ] . source ( ) == self [ ""in"" ] :",if self . __parameterHandler is None :,81.13922323,FALSE,68.63
1998,"def consume_buf ( ) : ty = state [ "" ty "" ] - 1 for i in xrange ( state [ "" buf "" ] . shape [ 1 ] / / N ) : tx = x / / N + i src = state [ "" buf "" ] [ : , i * N : ( i + 1 ) * N , : ] <MASK> with self . tile_request ( tx , ty , readonly = False ) as dst : mypaintlib . tile_convert_rgba8_to_rgba16 ( src , dst , self . EOTF ) if state [ "" progress "" ] : try : state [ "" progress "" ] . completed ( ty - ty0 ) except Exception : logger . exception ( "" Progress.completed() failed "" ) state [ "" progress "" ] = None","if src [ : , : , 3 ] . any ( ) :",if self . EOTF :,92.25145678,FALSE,92.03
1999,"def check_permissions ( self , obj ) : request = self . context . get ( "" request "" ) for Perm in permissions : perm = Perm ( ) if not perm . has_permission ( request , self ) : return False <MASK> return False return True","if not perm . has_object_permission ( request , self , obj ) :","if not perm . has_permission ( obj , self ) :",87.89978455,FALSE,87.35
2000,"def _post_order ( op ) : if isinstance ( op , tvm . tir . Allocate ) : lift_stmt [ - 1 ] . append ( op ) return op . body if isinstance ( op , tvm . tir . AttrStmt ) : if op . attr_key == "" storage_scope "" : lift_stmt [ - 1 ] . append ( op ) return op . body <MASK> return _merge_block ( lift_stmt . pop ( ) + [ op ] , op . body ) return op if isinstance ( op , tvm . tir . For ) : return _merge_block ( lift_stmt . pop ( ) + [ op ] , op . body ) raise RuntimeError ( "" not reached "" )","if op . attr_key == ""virtual_thread"" :","if isinstance ( op , tvm . tir . For ) :",94.82929466,FALSE,91.73
2001,"def task_done ( self ) : with self . _cond : if not self . _unfinished_tasks . acquire ( False ) : raise ValueError ( "" task_done() called too many times "" ) <MASK> self . _cond . notify_all ( )",if self . _unfinished_tasks . _semlock . _is_zero ( ) :,if self . _unfinished_tasks . acquire ( False ) :,90.13490351,FALSE,86.53
2002,"def get_json ( self ) : if not hasattr ( self , "" _json "" ) : self . _json = None <MASK> self . _json = json . loads ( self . request . body ) return self . _json","if self . request . headers . get ( ""Content-Type"" , """" ) . startswith ( ""application/json"" ) :","if self . request . method == ""GET"" :",63.01629193,FALSE,68.71
2003,"def userfullname ( ) : """"""Get the user's full name."""""" global _userfullname <MASK> uid = os . getuid ( ) entry = pwd_from_uid ( uid ) if entry : _userfullname = entry [ 4 ] . split ( "" , "" ) [ 0 ] or entry [ 0 ] <MASK> _userfullname = "" user %d "" % uid return _userfullname",if not _userfullname :,if not _userfullname :,100,TRUE,100
2004,"def test_scatter ( self ) : for rank in range ( self . world_size ) : tensor = [ ] <MASK> tensor = [ torch . tensor ( i ) for i in range ( self . world_size ) ] result = comm . get ( ) . scatter ( tensor , rank , size = ( ) ) self . assertTrue ( torch . is_tensor ( result ) ) self . assertEqual ( result . item ( ) , self . rank )",if self . rank == rank :,if rank == 0 :,84.92722052,FALSE,93.8
2005,"def decompile ( decompiler ) : for pos , next_pos , opname , arg in decompiler . instructions : if pos in decompiler . targets : decompiler . process_target ( pos ) method = getattr ( decompiler , opname , None ) if method is None : throw ( DecompileError ( "" Unsupported operation:  %s "" % opname ) ) decompiler . pos = pos decompiler . next_pos = next_pos x = method ( * arg ) <MASK> decompiler . stack . append ( x )",if x is not None :,if x is not None :,100,TRUE,100
2006,"def print_scenario_ran ( self , scenario ) : if scenario . passed : self . wrt ( "" OK "" ) elif scenario . failed : reason = self . scenarios_and_its_fails [ scenario ] <MASK> self . wrt ( "" FAILED "" ) else : self . wrt ( "" ERROR "" ) self . wrt ( "" \n "" )","if isinstance ( reason . exception , AssertionError ) :",if reason . failed :,79.57261174,FALSE,88.85
2007,"def detect_ssl_option ( self ) : for option in self . ssl_options ( ) : if scan_argv ( self . argv , option ) is not None : for other_option in self . ssl_options ( ) : if option != other_option : <MASK> raise ConfigurationError ( "" Cannot give both  %s  and  %s "" % ( option , other_option ) ) return option","if scan_argv ( self . argv , other_option ) is not None :",if other_option in self . options :,65.38353224,FALSE,84.93
2008,"def print_po_snippet ( en_loc_old_lists , context ) : for m , localized , old in zip ( * en_loc_old_lists ) : if m == "" "" : continue <MASK> localized = old print ( "" #:  {file} : {line} \n "" ' msgid  "" {context} {en_month} "" \n ' ' msgstr  "" {localized_month} "" \n ' . format ( context = context , file = filename , line = print_po_snippet . line , en_month = m , localized_month = localized , ) ) print_po_snippet . line + = 1",if m == localized :,"if localized == """" :",93.43463936,FALSE,96.06
2009,"def set_status ( self , dict_new ) : for i , value in dict_new . items ( ) : self . dict_bili [ i ] = value <MASK> self . dict_bili [ "" pcheaders "" ] [ "" cookie "" ] = value self . dict_bili [ "" appheaders "" ] [ "" cookie "" ] = value","if i == ""cookie"" :","if ""cookie"" in self . dict_bili [ ""pcheaders"" ]",59.91826363,FALSE,82.94
2010,"def makeSomeFiles ( pathobj , dirdict ) : pathdict = { } for ( key , value ) in dirdict . items ( ) : child = pathobj . child ( key ) <MASK> pathdict [ key ] = child child . setContent ( value ) elif isinstance ( value , dict ) : child . createDirectory ( ) pathdict [ key ] = makeSomeFiles ( child , value ) else : raise ValueError ( "" only strings and dicts allowed as values "" ) return pathdict","if isinstance ( value , bytes ) :","if isinstance ( child , str ) :",86.36074295,FALSE,95.37
2011,"def _truncate_to_length ( generator , len_map = None ) : for example in generator : example = list ( example ) if len_map is not None : for key , max_len in len_map . items ( ) : example_len = example [ key ] . shape <MASK> example [ key ] = np . resize ( example [ key ] , max_len ) yield tuple ( example )",if example_len > max_len :,if example_len > max_len :,100,TRUE,100
2012,"def check ( self , * * kw ) : if not kw : return exists ( self . strpath ) if len ( kw ) == 1 : if "" dir "" in kw : return not kw [ "" dir "" ] ^ isdir ( self . strpath ) <MASK> return not kw [ "" file "" ] ^ isfile ( self . strpath ) return super ( LocalPath , self ) . check ( * * kw )","if ""file"" in kw :","if ""file"" in kw :",100,TRUE,100
2013,"def next_instruction_is_function_or_class ( lines ) : """"""Is the first non-empty, non-commented line of the cell either a function or a class?"""""" parser = StringParser ( "" python "" ) for i , line in enumerate ( lines ) : if parser . is_quoted ( ) : parser . read_line ( line ) continue parser . read_line ( line ) if not line . strip ( ) : # empty line if i > 0 and not lines [ i - 1 ] . strip ( ) : return False continue if line . startswith ( "" def  "" ) or line . startswith ( "" class  "" ) : return True <MASK> continue return False return False","if line . startswith ( ( ""#"" , ""@"" , "" "" , "")"" ) ) :","if line . startswith ( ""function "" ) :",95.86532379,FALSE,89.52
2014,"def askCheckReadFile ( self , localFile , remoteFile ) : if not kb . bruteMode : message = "" do you want confirmation that the remote file  ' %s '   "" % remoteFile message + = "" has been successfully downloaded from the back-end  "" message + = "" DBMS file system? [Y/n]  "" <MASK> return self . _checkFileLength ( localFile , remoteFile , True ) return None","if readInput ( message , default = ""Y"" , boolean = True ) :","if not self . _promptFile ( localFile , message ) :",85.26943974,FALSE,84.37
2015,"def process_tag ( hive_name , company , company_key , tag , default_arch ) : with winreg . OpenKeyEx ( company_key , tag ) as tag_key : version = load_version_data ( hive_name , company , tag , tag_key ) if version is not None : # if failed to get version bail major , minor , _ = version arch = load_arch_data ( hive_name , company , tag , tag_key , default_arch ) <MASK> exe_data = load_exe ( hive_name , company , company_key , tag ) if exe_data is not None : exe , args = exe_data return company , major , minor , arch , exe , args",if arch is not None :,if arch is not None :,100,TRUE,100
2016,"def _get_matching_bracket ( self , s , pos ) : if s [ pos ] != "" { "" : return None end = len ( s ) depth = 1 pos + = 1 while pos != end : c = s [ pos ] if c == "" { "" : depth + = 1 <MASK> depth - = 1 if depth == 0 : break pos + = 1 if pos < end and s [ pos ] == "" } "" : return pos return None","elif c == ""}"" :","elif c == ""}"" :",100,TRUE,100
2017,"def pred ( field , value , item ) : for suffix , p in _BUILTIN_PREDS . iteritems ( ) : if field . endswith ( suffix ) : f = field [ : field . index ( suffix ) ] <MASK> return False return p ( getattr ( item , f ) , value ) if not hasattr ( item , field ) or getattr ( item , field ) is None : return False if isinstance ( value , type ( lambda x : x ) ) : return value ( getattr ( item , field ) ) return getattr ( item , field ) == value","if not hasattr ( item , f ) or getattr ( item , f ) is None :","if not hasattr ( item , f ) :",92.20169581,FALSE,91.65
2018,"def init_weights ( self ) : """"""Initialize model weights."""""" for _ , m in self . multi_deconv_layers . named_modules ( ) : if isinstance ( m , nn . ConvTranspose2d ) : normal_init ( m , std = 0.001 ) elif isinstance ( m , nn . BatchNorm2d ) : constant_init ( m , 1 ) for m in self . multi_final_layers . modules ( ) : <MASK> normal_init ( m , std = 0.001 , bias = 0 )","if isinstance ( m , nn . Conv2d ) :","if isinstance ( m , nn . ConvTranspose3d ) :",98.57680042,FALSE,97.76
2019,"def test_byteswap ( self ) : if self . typecode == "" u "" : example = "" \U00100100 "" else : example = self . example a = array . array ( self . typecode , example ) self . assertRaises ( TypeError , a . byteswap , 42 ) if a . itemsize in ( 1 , 2 , 4 , 8 ) : b = array . array ( self . typecode , example ) b . byteswap ( ) <MASK> self . assertEqual ( a , b ) else : self . assertNotEqual ( a , b ) b . byteswap ( ) self . assertEqual ( a , b )",if a . itemsize == 1 :,if a . itemsize == 1 :,100,TRUE,100
2020,"def _remove_blocks_from_variables ( variables ) : new_variables = [ ] for name , variable in variables : <MASK> new_variables . extend ( variable . locals ) new_variables . append ( ( name , variable . result ) ) else : new_variables . append ( ( name , variable ) ) return new_variables",if variable . is_block ( ) :,"if isinstance ( variable , Block ) :",90.29533843,FALSE,90.67
2021,def scope ( self ) : <MASK> self . lazy_init_lock_ . acquire ( ) try : <MASK> self . scope_ = Scope ( ) finally : self . lazy_init_lock_ . release ( ) return self . scope_,if self . scope_ is None :,if self . scope_ is None :,100,TRUE,100
2022,"def translate ( ) : assert Lex . next ( ) is AttributeList reader . read ( ) # Discard attribute list from reader. attrs = { } d = AttributeList . match . groupdict ( ) for k , v in d . items ( ) : if v is not None : if k == "" attrlist "" : v = subs_attrs ( v ) <MASK> parse_attributes ( v , attrs ) else : AttributeList . attrs [ k ] = v AttributeList . subs ( attrs ) AttributeList . attrs . update ( attrs )",if v :,"elif k == ""attr"" :",72.61264271,FALSE,92.22
2023,"def parse ( self , response ) : try : content = response . content . decode ( "" utf-8 "" , "" ignore "" ) content = json . loads ( content , strict = False ) except : self . logger . error ( "" Fail to parse the response in json format "" ) return for item in content [ "" data "" ] : <MASK> img_url = self . _decode_url ( item [ "" objURL "" ] ) elif "" hoverURL "" in item : img_url = item [ "" hoverURL "" ] else : continue yield dict ( file_url = img_url )","if ""objURL"" in item :","if ""objURL"" in item :",100,TRUE,100
2024,"def canonicalize_instruction_name ( instr ) : name = instr . insn_name ( ) . upper ( ) # XXX bypass a capstone bug that incorrectly labels some insns as mov if name == "" MOV "" : if instr . mnemonic . startswith ( "" lsr "" ) : return "" LSR "" elif instr . mnemonic . startswith ( "" lsl "" ) : return "" LSL "" <MASK> return "" ASR "" return OP_NAME_MAP . get ( name , name )","elif instr . mnemonic . startswith ( ""asr"" ) :","elif instr . mnemonic . startswith ( ""asr"" ) :",100,TRUE,100
2025,"def _clean_regions ( items , region ) : """"""Intersect region with target file if it exists"""""" variant_regions = bedutils . population_variant_regions ( items , merged = True ) with utils . tmpfile ( ) as tx_out_file : target = subset_variant_regions ( variant_regions , region , tx_out_file , items ) <MASK> if isinstance ( target , six . string_types ) and os . path . isfile ( target ) : target = _load_regions ( target ) else : target = [ target ] return target",if target :,if target :,100,TRUE,100
2026,def reader_leaves ( self ) : self . mutex . acquire ( ) try : self . active_readers - = 1 <MASK> self . active_writers + = 1 self . waiting_writers - = 1 self . can_write . release ( ) finally : self . mutex . release ( ),if self . active_readers == 0 and self . waiting_writers != 0 :,if self . active_readers == 0 and self . waiting_writers == 0 :,97.93032097,FALSE,96.7
2027,"def _bpe_to_words ( sentence , delimiter = "" @@ "" ) : """"""Convert a sequence of bpe words into sentence."""""" words = [ ] word = "" "" delimiter_len = len ( delimiter ) for subwords in sentence : <MASK> word + = subwords [ : - delimiter_len ] else : word + = subwords words . append ( word ) word = "" "" return words",if len ( subwords ) >= delimiter_len and subwords [ - delimiter_len : ] == delimiter :,if subwords [ - delimiter_len : ] == delimiter :,88.1683259,FALSE,88.66
2028,"def _make_var_names ( exog ) : if hasattr ( exog , "" name "" ) : var_names = exog . name elif hasattr ( exog , "" columns "" ) : var_names = exog . columns else : raise ValueError ( "" exog is not a Series or DataFrame or is unnamed. "" ) try : var_names = ""   "" . join ( var_names ) except TypeError : # cannot have names that are numbers, pandas default from statsmodels . base . data import _make_exog_names <MASK> var_names = "" x1 "" else : var_names = ""   "" . join ( _make_exog_names ( exog ) ) return var_names",if exog . ndim == 1 :,if exog is None :,97.60891897,FALSE,95.46
2029,"def __start_element_handler ( self , name , attrs ) : if name == "" mime-type "" : <MASK> for extension in self . extensions : self [ extension ] = self . type self . type = attrs [ "" type "" ] . lower ( ) self . extensions = [ ] elif name == "" glob "" : pattern = attrs [ "" pattern "" ] if pattern . startswith ( "" *. "" ) : self . extensions . append ( pattern [ 1 : ] . lower ( ) )",if self . type :,if self . extensions :,98.70807951,FALSE,97.61
2030,"def nodes ( self , id = None , name = None ) : for node_dict in self . node_ls ( id = id , name = name ) : node_id = node_dict [ "" ID "" ] node = DockerNode ( self , node_id , inspect = node_dict ) <MASK> continue yield node",if self . _node_prefix and not node . name . startswith ( self . _node_prefix ) :,if not node . is_valid ( ) :,78.5932636,FALSE,76.52
2031,"def fix_repeating_arguments ( self ) : """"""Fix elements that should accumulate/increment values."""""" either = [ list ( child . children ) for child in transform ( self ) . children ] for case in either : for e in [ child for child in case if case . count ( child ) > 1 ] : if type ( e ) is Argument or type ( e ) is Option and e . argcount : if e . value is None : e . value = [ ] elif type ( e . value ) is not list : e . value = e . value . split ( ) <MASK> e . value = 0 return self",if type ( e ) is Command or type ( e ) is Option and e . argcount == 0 :,elif type ( e . value ) is Option and e . argcount :,79.77417599,FALSE,89.57
2032,"def vi_search ( self , rng ) : for i in rng : line_history = self . _history . history [ i ] pos = line_history . get_line_text ( ) . find ( self . _vi_search_text ) <MASK> self . _history . history_cursor = i self . l_buffer . line_buffer = list ( line_history . line_buffer ) self . l_buffer . point = pos self . vi_undo_restart ( ) return True self . _bell ( ) return False",if pos >= 0 :,if pos != - 1 :,93.14721555,FALSE,95.47
2033,"def visitIf ( self , node , scope ) : for test , body in node . tests : <MASK> if type ( test . value ) in self . _const_types : if not test . value : continue self . visit ( test , scope ) self . visit ( body , scope ) if node . else_ : self . visit ( node . else_ , scope )","if isinstance ( test , ast . Const ) :","if isinstance ( test , ast . If ) :",98.39662859,FALSE,96.94
2034,"def collect ( self ) : for nickname in self . squid_hosts . keys ( ) : squid_host = self . squid_hosts [ nickname ] fulldata = self . _getData ( squid_host [ "" host "" ] , squid_host [ "" port "" ] ) <MASK> fulldata = fulldata . splitlines ( ) for data in fulldata : matches = self . stat_pattern . match ( data ) if matches : self . publish_counter ( "" %s . %s "" % ( nickname , matches . group ( 1 ) ) , float ( matches . group ( 2 ) ) )",if fulldata is not None :,if self . strip_newlines :,82.12614013,FALSE,94.72
2035,"def convert ( x , base , exponents ) : out = [ ] for e in exponents : d = int ( x / ( base * * e ) ) x - = d * ( base * * e ) out . append ( digits [ d ] ) <MASK> break return out",if x == 0 and e < 0 :,if d == 0 :,89.31985192,FALSE,88.42
2036,"def print_doc ( manager , options ) : plugin_name = options . doc plugin = plugins . get ( plugin_name , None ) if plugin : <MASK> console ( "" Plugin  %s  does not have documentation "" % plugin_name ) else : console ( "" "" ) console ( trim ( plugin . instance . __doc__ ) ) console ( "" "" ) else : console ( "" Could not find plugin  %s "" % plugin_name )",if not plugin . instance . __doc__ :,if not plugin . instance . __doc__ :,100,TRUE,100
2037,"def _set_attrs ( self , attrs ) : for attr in self . ATTRS : if attr in attrs : setattr ( self , attr , attrs [ attr ] ) del attrs [ attr ] else : <MASK> setattr ( self , attr , NO_DEFAULT ) else : setattr ( self , attr , None ) if attrs : attrs = sorted ( attrs . keys ( ) ) raise OptionError ( "" invalid keyword arguments:  %s "" % "" ,  "" . join ( attrs ) , self )","if attr == ""default"" :","if attr == ""default"" :",100,TRUE,100
2038,"def _get_set_scope ( ir_set : irast . Set , scope_tree : irast . ScopeTreeNode ) - > irast . ScopeTreeNode : if ir_set . path_scope_id : new_scope = scope_tree . root . find_by_unique_id ( ir_set . path_scope_id ) <MASK> raise errors . InternalServerError ( f "" dangling scope pointer to node with uid "" f "" : { ir_set . path_scope_id }  in  { ir_set !r} "" ) else : new_scope = scope_tree return new_scope",if new_scope is None :,if new_scope is None :,100,TRUE,100
2039,"def test_leave_one_out ( self ) : correct = 0 k = 3 model = kNN . train ( xs , ys , k ) predictions = [ 1 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 1 ] for i in range ( len ( predictions ) ) : model = kNN . train ( xs [ : i ] + xs [ i + 1 : ] , ys [ : i ] + ys [ i + 1 : ] , k ) prediction = kNN . classify ( model , xs [ i ] ) self . assertEqual ( prediction , predictions [ i ] ) <MASK> correct + = 1 self . assertEqual ( correct , 13 )",if prediction == ys [ i ] :,if i == 0 :,95.29563315,FALSE,95.22
2040,"def import_files ( self , files ) : """"""Import a list of MORE (.csv) files."""""" c = self . c if files : changed = False self . tab_width = c . getTabWidth ( c . p ) for fileName in files : g . setGlobalOpenDir ( fileName ) p = self . import_file ( fileName ) <MASK> p . contract ( ) p . setDirty ( ) c . setChanged ( True ) changed = True if changed : c . redraw ( p )",if p :,if p :,100,TRUE,100
2041,"def getPageTemplate ( payload , place ) : retVal = ( kb . originalPage , kb . errorIsNone ) if payload and place : <MASK> page , _ , _ = Request . queryPage ( payload , place , content = True , raise404 = False ) kb . pageTemplates [ ( payload , place ) ] = ( page , kb . lastParserStatus is None ) retVal = kb . pageTemplates [ ( payload , place ) ] return retVal","if ( payload , place ) not in kb . pageTemplates :","if ( payload , place ) not in kb . pageTemplates :",100,TRUE,100
2042,"def _skip_trivial ( constraint_data ) : if skip_trivial_constraints : if isinstance ( constraint_data , LinearCanonicalRepn ) : if constraint_data . variables is None : return True else : <MASK> return True return False",if constraint_data . body . polynomial_degree ( ) == 0 :,"if constraint_data . variables . get ( ""trivial"" ) :",85.78355018,FALSE,81.78
2043,"def get_unique_attribute ( self , name : str ) : feat = None for f in self . features : if self . _return_feature ( f ) and hasattr ( f , name ) : <MASK> raise RuntimeError ( "" The attribute was not unique. "" ) feat = f if feat is None : raise RuntimeError ( "" The attribute did not exist "" ) return getattr ( feat , name )",if feat is not None :,if not f . unique :,88.11777262,FALSE,93.75
2044,"def hideEvent ( self , event ) : """"""Reimplement Qt method"""""" if not self . light : for plugin in self . widgetlist : <MASK> plugin . visibility_changed ( True ) QMainWindow . hideEvent ( self , event )",if plugin . isAncestorOf ( self . last_focused_widget ) :,"if isinstance ( plugin , QWidget ) :",83.48234141,FALSE,79.12
2045,"def move_stdout_to_stderr ( self ) : to_remove = [ ] to_add = [ ] for consumer_level , consumer in self . consumers : <MASK> to_remove . append ( ( consumer_level , consumer ) ) to_add . append ( ( consumer_level , sys . stderr ) ) for item in to_remove : self . consumers . remove ( item ) self . consumers . extend ( to_add )",if consumer == sys . stdout :,if consumer . is_stdout :,92.79376083,FALSE,94.54
2046,"def create ( exported_python_target ) : if exported_python_target not in created : self . context . log . info ( "" Creating setup.py project for  {} "" . format ( exported_python_target ) ) subject = self . derived_by_original . get ( exported_python_target , exported_python_target ) setup_dir , dependencies = self . create_setup_py ( subject , dist_dir ) created [ exported_python_target ] = setup_dir <MASK> for dep in dependencies : if is_exported_python_target ( dep ) : create ( dep )",if self . _recursive :,if dependencies :,94.31803735,FALSE,95.74
2047,"def __add__ ( self , other ) : other = ArithmeticExpression . try_unpack_const ( other ) if not self . symbolic and type ( other ) is int : return SpOffset ( self . _bits , self . _to_signed ( self . offset + other ) ) else : <MASK> return SpOffset ( self . _bits , self . offset + other ) else : return SpOffset ( self . _bits , ArithmeticExpression ( ArithmeticExpression . Add , ( self . offset , other , ) , ) , )",if self . symbolic :,if self . symbolic :,100,TRUE,100
2048,"def check_connection ( conn ) : tables = [ r [ 0 ] for r in conn . execute ( "" select name from sqlite_master where type= ' table ' "" ) . fetchall ( ) ] for table in tables : try : conn . execute ( f "" PRAGMA table_info( { escape_sqlite ( table ) } ); "" , ) except sqlite3 . OperationalError as e : <MASK> raise SpatialiteConnectionProblem ( e ) else : raise ConnectionProblem ( e )","if e . args [ 0 ] == ""no such module: VirtualSpatialIndex"" :","if ""Spatialite connection is not available"" in str ( e ) :",88.15315556,FALSE,85.3
2049,"def _get_github_client ( self ) - > "" Github "" : from github import Github if self . access_token_secret is not None : # If access token secret specified, load it access_token = Secret ( self . access_token_secret ) . get ( ) else : # Otherwise, fallback to loading from local secret or environment variable access_token = prefect . context . get ( "" secrets "" , { } ) . get ( "" GITHUB_ACCESS_TOKEN "" ) <MASK> access_token = os . getenv ( "" GITHUB_ACCESS_TOKEN "" ) return Github ( access_token )",if access_token is None :,if access_token is None :,100,TRUE,100
2050,"def make_tab ( lists ) : if hasattr ( lists , "" tolist "" ) : lists = lists . tolist ( ) ut = [ ] for rad in lists : <MASK> ut . append ( "" \t "" . join ( [ "" %s "" % x for x in rad ] ) ) else : ut . append ( "" %s "" % rad ) return "" \n "" . join ( ut )","if type ( rad ) in [ list , tuple ] :","if isinstance ( rad , ( list , tuple ) ) :",90.87601916,FALSE,90.15
2051,"def _ensure_ffi_initialized ( cls ) : with cls . _init_lock : <MASK> cls . lib = build_conditional_library ( lib , CONDITIONAL_NAMES ) cls . _lib_loaded = True # initialize the SSL library cls . lib . SSL_library_init ( ) # adds all ciphers/digests for EVP cls . lib . OpenSSL_add_all_algorithms ( ) # loads error strings for libcrypto and libssl functions cls . lib . SSL_load_error_strings ( ) cls . _register_osrandom_engine ( )",if not cls . _lib_loaded :,if not cls . _lib_loaded :,100,TRUE,100
2052,def writer_leaves ( self ) : self . mutex . acquire ( ) try : self . active_writers - = 1 if self . waiting_writers != 0 : self . active_writers + = 1 self . waiting_writers - = 1 self . can_write . release ( ) <MASK> t = self . waiting_readers self . waiting_readers = 0 self . active_readers + = t while t > 0 : self . can_read . release ( ) t - = 1 finally : self . mutex . release ( ),elif self . waiting_readers != 0 :,if self . waiting_readers == 0 :,93.62781198,FALSE,95.71
2053,"def _spans ( self , operands ) : spans = { } k = 0 j = 0 for mode in ( self . FLOAT , self . MPMATH ) : for i , operand in enumerate ( operands [ k : ] ) : if operand [ 0 ] > mode : break j = i + k + 1 <MASK> # only init state? then ignore. j = 0 spans [ mode ] = slice ( k , j ) k = j spans [ self . SYMBOLIC ] = slice ( k , len ( operands ) ) return spans",if k == 0 and j == 1 :,if j == len ( operands ) - 1 :,81.24302122,FALSE,92.97
2054,"def _report_error ( self , completion_routine , response = None , message = None ) : if response : # Only include the text in case of error. <MASK> status = location . Status ( response . status_code , response . text ) else : status = location . Status ( response . status_code ) else : status = location . Status ( 500 , message ) if response is None or not response . ok : if completion_routine : return completion_routine ( status ) raise IOError ( response . text ) else : if completion_routine : completion_routine ( status ) return location . Status ( 200 , response . content )",if not response . ok :,if response . ok :,98.80784289,FALSE,98.12
2055,"def readinto ( self , buf ) : if self . current_frame : n = self . current_frame . readinto ( buf ) if n == 0 and len ( buf ) != 0 : self . current_frame = None n = len ( buf ) buf [ : ] = self . file_read ( n ) return n <MASK> raise UnpicklingError ( "" pickle exhausted before end of frame "" ) return n else : n = len ( buf ) buf [ : ] = self . file_read ( n ) return n",if n < len ( buf ) :,if n == 0 :,93.40708486,FALSE,94.21
2056,"def __getitem__ ( self , name , set = set , getattr = getattr , id = id ) : visited = set ( ) mydict = self . basedict while 1 : value = mydict [ name ] <MASK> return value myid = id ( mydict ) assert myid not in visited visited . add ( myid ) mydict = mydict . Parent if mydict is None : return",if value is not None :,"if getattr ( value , ""parent"" , None ) :",87.5960587,FALSE,86.76
2057,"def _handle_Mul ( self , expr ) : arg0 , arg1 = expr . args expr_0 = self . _expr ( arg0 ) if expr_0 is None : return None expr_1 = self . _expr ( arg1 ) if expr_1 is None : return None try : <MASK> # self.tyenv is not used mask = ( 1 << expr . result_size ( self . tyenv ) ) - 1 return ( expr_0 * expr_1 ) & mask else : return expr_0 * expr_1 except TypeError as e : self . l . warning ( e ) return None","if isinstance ( expr_0 , int ) and isinstance ( expr_1 , int ) :",if expr . result_size ( ) == 1 :,79.34357867,FALSE,87.61
2058,"def end_request ( self , request_id ) : """"""Removes the information associated with given request_id."""""" with self . _lock : del self . _request_wsgi_environ [ request_id ] del self . _request_id_to_server_configuration [ request_id ] <MASK> del self . _request_id_to_instance [ request_id ]",if request_id in self . _request_id_to_instance :,if request_id in self . _request_id_to_instance :,100,TRUE,100
2059,def generate ( ) : <MASK> decoder = zlib . decompressobj ( 16 + zlib . MAX_WBITS ) while True : chunk = self . raw . read ( chunk_size ) if not chunk : break <MASK> chunk = decoder . decompress ( chunk ) yield chunk,if self . _gzipped :,if chunk_size :,74.81528453,FALSE,82.11
2060,"def handle ( self ) : from poetry . utils . env import EnvManager manager = EnvManager ( self . poetry ) current_env = manager . get ( ) for venv in manager . list ( ) : name = venv . path . name <MASK> name = str ( venv . path ) if venv == current_env : self . line ( "" <info> {}  (Activated)</info> "" . format ( name ) ) continue self . line ( name )","if self . option ( ""full-path"" ) :","if name == ""default"" :",92.46430924,FALSE,91.19
2061,"def addAggregators ( sheet , cols , aggrnames ) : "" Add each aggregator in list of *aggrnames* to each of *cols*. "" for aggrname in aggrnames : aggrs = vd . aggregators . get ( aggrname ) aggrs = aggrs if isinstance ( aggrs , list ) else [ aggrs ] for aggr in aggrs : for c in cols : <MASK> c . aggregators = [ ] if aggr and aggr not in c . aggregators : c . aggregators + = [ aggr ]","if not hasattr ( c , ""aggregators"" ) :","if not hasattr ( c , ""aggregators"" ) :",100,TRUE,100
2062,"def on_pre_output_coercion ( directive_args : Dict [ str , Any ] , next_directive : Callable , value : Any , ctx : Optional [ Any ] , info : "" ResolveInfo "" , ) : value = await next_directive ( value , ctx , info ) if value is None : return value try : py_enum = _ENUM_MAP [ directive_args [ "" name "" ] ] <MASK> return [ None if item is None else py_enum ( item ) . name for item in value ] return py_enum ( value ) . name except Exception : pass return value","if isinstance ( value , list ) :","if isinstance ( value , ( list , tuple ) ) :",80.79199828,FALSE,95.23
2063,def cut ( sentence ) : sentence = strdecode ( sentence ) blocks = re_han . split ( sentence ) for blk in blocks : <MASK> for word in __cut ( blk ) : if word not in Force_Split_Words : yield word else : for c in word : yield c else : tmp = re_skip . split ( blk ) for x in tmp : if x : yield x,if re_han . match ( blk ) :,if blk in SENTINEL_BLOCK_BLOCKS :,93.55827702,FALSE,89.79
2064,"def refresh_archive_action ( self ) : archive_name = self . selected_archive_name ( ) if archive_name is not None : params = BorgInfoArchiveThread . prepare ( self . profile ( ) , archive_name ) <MASK> thread = BorgInfoArchiveThread ( params [ "" cmd "" ] , params , parent = self . app ) thread . updated . connect ( self . _set_status ) thread . result . connect ( self . refresh_archive_result ) self . _toggle_all_buttons ( False ) thread . start ( )","if params [ ""ok"" ] :",if params :,81.26106873,FALSE,94.32
2065,"def get_resource_public_actions ( resource_class ) : resource_class_members = inspect . getmembers ( resource_class ) resource_methods = { } for name , member in resource_class_members : if not name . startswith ( "" _ "" ) : if not name [ 0 ] . isupper ( ) : if not name . startswith ( "" wait_until "" ) : <MASK> resource_methods [ name ] = member return resource_methods",if is_resource_action ( member ) :,if inspect . isclass ( member ) and inspect . isfunction ( member ) :,91.34311636,FALSE,89.08
2066,"def _get_compressor ( compress_type , compresslevel = None ) : if compress_type == ZIP_DEFLATED : <MASK> return zlib . compressobj ( compresslevel , zlib . DEFLATED , - 15 ) return zlib . compressobj ( zlib . Z_DEFAULT_COMPRESSION , zlib . DEFLATED , - 15 ) elif compress_type == ZIP_BZIP2 : <MASK> return bz2 . BZ2Compressor ( compresslevel ) return bz2 . BZ2Compressor ( ) # compresslevel is ignored for ZIP_LZMA elif compress_type == ZIP_LZMA : return LZMACompressor ( ) else : return None",if compresslevel is not None :,if compresslevel is not None :,100,TRUE,100
2067,"def parse_header ( plyfile , ext ) : # Variables line = [ ] properties = [ ] num_points = None while b "" end_header "" not in line and line != b "" "" : line = plyfile . readline ( ) if b "" element "" in line : line = line . split ( ) num_points = int ( line [ 2 ] ) <MASK> line = line . split ( ) properties . append ( ( line [ 2 ] . decode ( ) , ext + ply_dtypes [ line [ 1 ] ] ) ) return num_points , properties","elif b""property"" in line :","if b""end_header"" in line and num_points == 0 :",71.75861323,FALSE,88.35
2068,"def download_release_artifacts ( self , version ) : try : os . mkdir ( self . artifacts_dir ) except FileExistsError : pass for job_name in self . build_ids : build_number = self . build_ids . get ( job_name ) build_status = self . _get_build_status ( job_name , build_number ) <MASK> self . _download_job_artifact ( job_name , build_number , version ) else : print ( "" Build for  {}  is not fininished "" . format ( job_name ) ) print ( "" \t Run  ' build '  action to check status of  {} "" . format ( job_name ) )","if build_status == ""built"" :","if build_status == "" fininished"" :",98.53897649,FALSE,98.32
2069,"def update_metadata ( self ) : for attrname in dir ( self ) : <MASK> continue attrvalue = getattr ( self , attrname , None ) if attrvalue == 0 : continue if attrname == "" salt_version "" : attrname = "" version "" if hasattr ( self . metadata , "" set_ {0} "" . format ( attrname ) ) : getattr ( self . metadata , "" set_ {0} "" . format ( attrname ) ) ( attrvalue ) elif hasattr ( self . metadata , attrname ) : try : setattr ( self . metadata , attrname , attrvalue ) except AttributeError : pass","if attrname . startswith ( ""__"" ) :","if not attrname . startswith ( ""_"" ) :",69.43530291,FALSE,97.43
2070,"def check_heuristic_in_sql ( ) : heurs = set ( ) excluded = [ "" Equal assembly or pseudo-code "" , "" All or most attributes "" ] for heur in HEURISTICS : name = heur [ "" name "" ] if name in excluded : continue sql = heur [ "" sql "" ] <MASK> print ( ( "" SQL command not correctly associated to  %s "" % repr ( name ) ) ) print ( sql ) assert sql . find ( name ) != - 1 heurs . add ( name ) print ( "" Heuristics: "" ) import pprint pprint . pprint ( heurs )",if sql . lower ( ) . find ( name . lower ( ) ) == - 1 :,if not sql :,82.86171363,FALSE,85.1
2071,def gettext ( rv ) : for child in rv . childNodes : <MASK> yield child . nodeValue if child . nodeType == child . ELEMENT_NODE : for item in gettext ( child ) : yield item,if child . nodeType == child . TEXT_NODE :,if child . nodeType == child . TEXT_NODE :,100,TRUE,100
2072,"def update ( self ) : """"""Update properties over dbus."""""" self . _check_dbus ( ) _LOGGER . info ( "" Updating service information "" ) self . _services . clear ( ) try : systemd_units = await self . sys_dbus . systemd . list_units ( ) for service_data in systemd_units [ 0 ] : <MASK> continue self . _services . add ( ServiceInfo . read_from ( service_data ) ) except ( HassioError , IndexError ) : _LOGGER . warning ( "" Can ' t update host service information! "" )","if not service_data [ 0 ] . endswith ( "".service"" ) or service_data [ 2 ] != ""loaded"" :","if service_data [ 0 ] == ""host"" :",88.92893291,FALSE,84.19
2073,"def filtercomments ( source ) : """"""NOT USED: strips trailing comments and put them at the top."""""" trailing_comments = [ ] comment = True while comment : <MASK> comment = source [ 0 , source . index ( "" */ "" ) + 2 ] elif re . search ( r "" ^ \ s* \ / \ / "" , source ) : comment = re . search ( r "" ^ \ s* \ / \ / "" , source ) . group ( 0 ) else : comment = None if comment : source = re . sub ( r "" ^ \ s+ "" , "" "" , source [ len ( comment ) : ] ) trailing_comments . append ( comment ) return "" \n "" . join ( trailing_comments ) + source","if re . search ( r""^\s*\/\*"" , source ) :","if re . search ( r""^\s*/"" , source ) :",99.1453293,FALSE,96.98
2074,"def _getSourceStamp_sync ( self , ssid ) : if ssid in self . sourcestamps : ssdict = self . sourcestamps [ ssid ] . copy ( ) ssdict [ "" ssid "" ] = ssid patchid = ssdict [ "" patchid "" ] <MASK> ssdict . update ( self . patches [ patchid ] ) ssdict [ "" patchid "" ] = patchid else : ssdict [ "" patch_body "" ] = None ssdict [ "" patch_level "" ] = None ssdict [ "" patch_subdir "" ] = None ssdict [ "" patch_author "" ] = None ssdict [ "" patch_comment "" ] = None return ssdict else : return None",if patchid :,if patchid in self . patches :,89.6420092,FALSE,95.71
2075,"def parseImpl ( self , instring , loc , doActions = True ) : try : loc , tokens = self . expr . _parse ( instring , loc , doActions , callPreParse = False ) except ( ParseException , IndexError ) : if self . defaultValue is not self . __optionalNotMatched : <MASK> tokens = ParseResults ( [ self . defaultValue ] ) tokens [ self . expr . resultsName ] = self . defaultValue else : tokens = [ self . defaultValue ] else : tokens = [ ] return loc , tokens",if self . expr . resultsName :,if self . expr . resultsName :,100,TRUE,100
2076,"def _find_exceptions ( ) : for _name , obj in iteritems ( globals ( ) ) : try : is_http_exception = issubclass ( obj , HTTPException ) except TypeError : is_http_exception = False if not is_http_exception or obj . code is None : continue __all__ . append ( obj . __name__ ) old_obj = default_exceptions . get ( obj . code , None ) <MASK> continue default_exceptions [ obj . code ] = obj","if old_obj is not None and issubclass ( obj , old_obj ) :",if old_obj is not None and old_obj . code != obj . code :,90.03610797,FALSE,91.86
2077,"def generator ( self , data ) : for ( proc_as , key_buf_ptr ) in data : key_buf = proc_as . read ( key_buf_ptr , 24 ) <MASK> continue key = "" "" . join ( "" %02X "" % ord ( k ) for k in key_buf ) yield ( 0 , [ str ( key ) , ] , )",if not key_buf :,if not key_buf :,100,TRUE,100
2078,"def calculateEnableMargins ( self ) : self . cnc . resetEnableMargins ( ) for block in self . blocks : <MASK> CNC . vars [ "" xmin "" ] = min ( CNC . vars [ "" xmin "" ] , block . xmin ) CNC . vars [ "" ymin "" ] = min ( CNC . vars [ "" ymin "" ] , block . ymin ) CNC . vars [ "" zmin "" ] = min ( CNC . vars [ "" zmin "" ] , block . zmin ) CNC . vars [ "" xmax "" ] = max ( CNC . vars [ "" xmax "" ] , block . xmax ) CNC . vars [ "" ymax "" ] = max ( CNC . vars [ "" ymax "" ] , block . ymax ) CNC . vars [ "" zmax "" ] = max ( CNC . vars [ "" zmax "" ] , block . zmax )",if block . enable :,if block . is_horizontal :,99.25347384,FALSE,97.34
2079,"def __init__ ( self , client , job_id , callback = None ) : self . client = client self . job_id = job_id # If a job event has been received already then we must set an Event # to wait for this job to finish. # Otherwise we create a new stub for the job with the Event for when # the job event arrives to use existing event. with client . _jobs_lock : job = client . _jobs . get ( job_id ) self . event = None <MASK> self . event = job . get ( "" __ready "" ) if self . event is None : self . event = job [ "" __ready "" ] = Event ( ) job [ "" __callback "" ] = callback",if job :,if job :,100,TRUE,100
2080,"def asset ( * paths ) : for path in paths : fspath = www_root + "" /assets/ "" + path etag = "" "" try : if env . cache_static : etag = asset_etag ( fspath ) else : os . stat ( fspath ) except FileNotFoundError as e : if path == paths [ - 1 ] : <MASK> tell_sentry ( e , { } ) else : continue except Exception as e : tell_sentry ( e , { } ) return asset_url + path + ( etag and "" ?etag= "" + etag )","if not os . path . exists ( fspath + "".spt"" ) :",if env . cache_static :,85.91477942,FALSE,87.86
2081,"def set_conf ( ) : """"""Collapse all object_trail config into cherrypy.request.config."""""" base = cherrypy . config . copy ( ) # Note that we merge the config from each node # even if that node was None. for name , obj , conf , segleft in object_trail : base . update ( conf ) <MASK> base [ "" tools.staticdir.section "" ] = "" / "" + "" / "" . join ( fullpath [ 0 : fullpath_len - segleft ] ) return base","if ""tools.staticdir.dir"" in conf :",if segleft :,97.20717613,FALSE,90.75
2082,"def __init__ ( self ) : self . setLayers ( None , None ) self . interface = None self . event_callbacks = { } self . __stack = None self . lock = threading . Lock ( ) members = inspect . getmembers ( self , predicate = inspect . ismethod ) for m in members : <MASK> fname = m [ 0 ] fn = m [ 1 ] self . event_callbacks [ fn . event_callback ] = getattr ( self , fname )","if hasattr ( m [ 1 ] , ""event_callback"" ) :","if isinstance ( m , tuple ) :",91.77080957,FALSE,88.53
2083,def multi_dev_generator ( self ) : for data in self . _data_loader ( ) : <MASK> self . _tail_data + = data if len ( self . _tail_data ) == self . _base_number : yield self . _tail_data self . _tail_data = [ ],if len ( self . _tail_data ) < self . _base_number :,if data [ 0 ] == self . _tail_data [ 0 ] :,83.63506453,FALSE,84.16
2084,"def replace_field_to_value ( layout , cb ) : for i , lo in enumerate ( layout . fields ) : if isinstance ( lo , Field ) or issubclass ( lo . __class__ , Field ) : layout . fields [ i ] = ShowField ( cb , * lo . fields , attrs = lo . attrs , wrapper_class = lo . wrapper_class ) elif isinstance ( lo , basestring ) : layout . fields [ i ] = ShowField ( cb , lo ) <MASK> replace_field_to_value ( lo , cb )","elif hasattr ( lo , ""get_field_names"" ) :","elif isinstance ( lo , ( Field , FieldWrapper ) ) :",67.20826478,FALSE,91.13
2085,"def function_out ( * args , * * kwargs ) : try : return function_in ( * args , * * kwargs ) except dbus . exceptions . DBusException as e : if e . get_dbus_name ( ) == DBUS_UNKNOWN_METHOD : raise ItemNotFoundException ( "" Item does not exist! "" ) if e . get_dbus_name ( ) == DBUS_NO_SUCH_OBJECT : raise ItemNotFoundException ( e . get_dbus_message ( ) ) <MASK> raise SecretServiceNotAvailableException ( e . get_dbus_message ( ) ) raise","if e . get_dbus_name ( ) in ( DBUS_NO_REPLY , DBUS_NOT_SUPPORTED ) :",if e . get_dbus_name ( ) == DBUS_NO_SUCH_,92.16203512,FALSE,89.74
2086,"def results_iter ( self ) : if self . connection . ops . oracle : from django . db . models . fields import DateTimeField fields = [ DateTimeField ( ) ] else : needs_string_cast = self . connection . features . needs_datetime_string_cast offset = len ( self . query . extra_select ) for rows in self . execute_sql ( MULTI ) : for row in rows : date = row [ offset ] if self . connection . ops . oracle : date = self . resolve_columns ( row , fields ) [ offset ] <MASK> date = typecast_timestamp ( str ( date ) ) yield date",elif needs_string_cast :,if needs_string_cast :,98.69948743,FALSE,98.1
2087,"def handle_label ( self , path , * * options ) : verbosity = int ( options . get ( "" verbosity "" , 1 ) ) result = finders . find ( path , all = options [ "" all "" ] ) path = smart_unicode ( path ) if result : if not isinstance ( result , ( list , tuple ) ) : result = [ result ] output = u "" \n    "" . join ( ( smart_unicode ( os . path . realpath ( path ) ) for path in result ) ) self . stdout . write ( smart_str ( u "" Found  ' %s '  here: \n    %s \n "" % ( path , output ) ) ) else : <MASK> self . stderr . write ( smart_str ( "" No matching file found for  ' %s ' . \n "" % path ) )",if verbosity >= 1 :,if verbosity < 1 :,99.19433716,FALSE,97.98
2088,"def name ( self ) : """"""Get the enumeration name of this storage class."""""" if self . _name_map is None : self . _name_map = { } for key , value in list ( StorageClass . __dict__ . items ( ) ) : <MASK> self . _name_map [ value ] = key return self . _name_map [ self ]","if isinstance ( value , StorageClass ) :",if key not in self . _name_map :,73.82390505,FALSE,88.37
2089,"def index ( self , value ) : if self . _growing : if self . _start < = value < self . _stop : q , r = divmod ( value - self . _start , self . _step ) if r == self . _zero : return int ( q ) else : <MASK> q , r = divmod ( self . _start - value , - self . _step ) if r == self . _zero : return int ( q ) raise ValueError ( "" {}  is not in numeric range "" . format ( value ) )",if self . _start >= value > self . _stop :,if self . _start > value < self . _stop :,98.19124607,FALSE,96.6
2090,"def extract_cookie ( cookie_header , cookie_name ) : inx = cookie_header . find ( cookie_name ) if inx > = 0 : end_inx = cookie_header . find ( "" ; "" , inx ) <MASK> value = cookie_header [ inx : end_inx ] else : value = cookie_header [ inx : ] return value return "" """,if end_inx > 0 :,if end_inx >= 0 :,97.89883298,FALSE,96.94
2091,"def get_size ( self , shape_info ) : # The size is the data, that have constant size. state = np . random . RandomState ( ) . get_state ( ) size = 0 for elem in state : if isinstance ( elem , str ) : size + = len ( elem ) <MASK> size + = elem . size * elem . itemsize elif isinstance ( elem , int ) : size + = np . dtype ( "" int "" ) . itemsize elif isinstance ( elem , float ) : size + = np . dtype ( "" float "" ) . itemsize else : raise NotImplementedError ( ) return size","elif isinstance ( elem , np . ndarray ) :","elif isinstance ( elem , ( Base , Base ) ) :",72.80878351,FALSE,95.08
2092,"def createFields ( self ) : size = self . size / 8 if size > 2 : <MASK> yield UInt8 ( self , "" cs "" , "" 10ms units, values from 0 to 199 "" ) yield Bits ( self , "" 2sec "" , 5 , "" seconds/2 "" ) yield Bits ( self , "" min "" , 6 , "" minutes "" ) yield Bits ( self , "" hour "" , 5 , "" hours "" ) yield Bits ( self , "" day "" , 5 , "" (1-31) "" ) yield Bits ( self , "" month "" , 4 , "" (1-12) "" ) yield Bits ( self , "" year "" , 7 , "" (0 = 1980, 127 = 2107) "" )",if size > 4 :,if size % 2 == 0 :,96.39713797,FALSE,95.73
2093,"def detect ( get_page ) : retval = False for vector in WAF_ATTACK_VECTORS : page , headers , code = get_page ( get = vector ) retval = ( re . search ( r "" incap_ses|visid_incap "" , headers . get ( HTTP_HEADER . SET_COOKIE , "" "" ) , re . I ) is not None ) retval | = re . search ( r "" Incapsula "" , headers . get ( "" X-CDN "" , "" "" ) , re . I ) is not None <MASK> break return retval",if retval :,if retval :,100,TRUE,100
2094,"def _get_order_information ( self , node_id , timeout = 1200 , check_interval = 5 ) : mask = { "" billingItem "" : "" "" , "" powerState "" : "" "" , "" operatingSystem "" : { "" passwords "" : "" "" } , "" provisionDate "" : "" "" , } for i in range ( 0 , timeout , check_interval ) : res = self . connection . request ( "" SoftLayer_Virtual_Guest "" , "" getObject "" , id = node_id , object_mask = mask ) . object <MASK> return res time . sleep ( check_interval ) raise SoftLayerException ( "" Timeout on getting node details "" )","if res . get ( ""provisionDate"" , None ) :","if res . get ( ""status"" ) == ""available"" :",95.25229817,FALSE,93.8
2095,"def _process_param_change ( self , msg ) : msg = super ( Select , self ) . _process_param_change ( msg ) labels , values = self . labels , self . values if "" value "" in msg : msg [ "" value "" ] = [ labels [ indexOf ( v , values ) ] for v in msg [ "" value "" ] if isIn ( v , values ) ] if "" options "" in msg : msg [ "" options "" ] = labels <MASK> self . value = [ v for v in self . value if isIn ( v , values ) ] return msg","if any ( not isIn ( v , values ) for v in self . value ) :",if self . value :,66.45704082,FALSE,87.9
2096,"def get_object_from_name ( self , name , check_symlinks = True ) : if not name : return None name = name . rstrip ( "" \\ "" ) for a , o in self . objects . items ( ) : if not o . name : continue if o . name . lower ( ) == name . lower ( ) : return o if check_symlinks : m = [ sl [ 1 ] for sl in self . symlinks if name . lower ( ) == sl [ 0 ] . lower ( ) ] <MASK> name = m [ 0 ] return self . get_object_from_name ( name , False )",if m :,if len ( m ) == 1 :,96.60054959,FALSE,94.03
2097,"def run ( self ) : for k , v in iteritems ( self . objs ) : <MASK> continue if v [ "" _class "" ] == "" User "" : if v [ "" email "" ] == "" "" : v [ "" email "" ] = None if v [ "" ip "" ] == "" 0.0.0.0 "" : v [ "" ip "" ] = None return self . objs","if k . startswith ( ""_"" ) :","if k . startswith ( ""_"" ) :",100,TRUE,100
2098,"def _providers ( self , descriptor ) : res = [ ] for _md in self . metadata . values ( ) : for ent_id , ent_desc in _md . items ( ) : if descriptor in ent_desc : <MASK> # print(""duplicated entity_id: %s"" % res) pass else : res . append ( ent_id ) return res",if ent_id in res :,if ent_id in res :,100,TRUE,100
2099,"def test_add_participant ( self ) : async with self . chat_client : await self . _create_thread ( ) async with self . chat_thread_client : share_history_time = datetime . utcnow ( ) share_history_time = share_history_time . replace ( tzinfo = TZ_UTC ) new_participant = ChatThreadParticipant ( user = self . new_user , display_name = "" name "" , share_history_time = share_history_time , ) await self . chat_thread_client . add_participant ( new_participant ) <MASK> await self . chat_client . delete_chat_thread ( self . thread_id )",if not self . is_playback ( ) :,if self . is_deleted :,94.10166796,FALSE,95.23
2100,"def url ( regex , view , kwargs = None , name = None , prefix = "" "" ) : if isinstance ( view , ( list , tuple ) ) : # For include(...) processing. urlconf_module , app_name , namespace = view return RegexURLResolver ( regex , urlconf_module , kwargs , app_name = app_name , namespace = namespace ) else : if isinstance ( view , basestring ) : if not view : raise ImproperlyConfigured ( "" Empty URL pattern view name not permitted (for pattern  %r ) "" % regex ) <MASK> view = prefix + "" . "" + view return RegexURLPattern ( regex , view , kwargs , name )",if prefix :,if prefix :,100,TRUE,100
2101,"def tx ( ) : # Sync receiver ready to avoid loss of first packets while not sub_ready . ready ( ) : pub . send ( b "" test BEGIN "" ) eventlet . sleep ( 0.005 ) for i in range ( 1 , 101 ) : msg = "" test  {0} "" . format ( i ) . encode ( ) <MASK> pub . send ( msg ) else : pub . send ( b "" test LAST "" ) sub_last . wait ( ) # XXX: putting a real delay of 1ms here fixes sporadic failures on Travis # just yield eventlet.sleep(0) doesn't cut it eventlet . sleep ( 0.001 ) pub . send ( b "" done DONE "" )",if i != 50 :,if sub_ready . ready ( ) :,72.88520128,FALSE,94.07
2102,"def remove_tmp_snapshot_file ( self , files ) : for filepath in files : path = Path ( filepath ) if path . is_dir ( ) and path . exists ( ) : shutil . rmtree ( path ) <MASK> path . unlink ( )",elif path . is_file ( ) and path . exists ( ) :,elif path . is_file ( ) and path . is_file ( ) :,97.70833537,FALSE,93.19
2103,"def f ( view , s ) : if mode == modes . INTERNAL_NORMAL : if count == 1 : <MASK> eol = view . line ( s . b ) . b return R ( s . b , eol ) return s return s",if view . line ( s . b ) . size ( ) > 0 :,if view . line ( s . b ) . b != s . b :,87.64970255,FALSE,88.32
2104,"def get_ids ( self , * * kwargs ) : id = [ ] if "" id "" in kwargs : id = kwargs [ "" id "" ] # Coerce ids to list <MASK> id = id . split ( "" , "" ) # Ensure ids are integers try : id = list ( map ( int , id ) ) except Exception : decorators . error ( "" Invalid id "" ) return id","if not isinstance ( id , list ) :","if "","" in id :",94.9403587,FALSE,90.6
2105,"def param_value ( self ) : # This is part of the ""handle quoted extended parameters"" hack. for token in self : <MASK> return token . stripped_value if token . token_type == "" quoted-string "" : for token in token : if token . token_type == "" bare-quoted-string "" : for token in token : <MASK> return token . stripped_value return "" ""","if token . token_type == ""value"" :","if token . type == ""extended"" :",69.2742355,FALSE,87.62
2106,"def get_all_start_methods ( self ) : if sys . platform == "" win32 "" : return [ "" spawn "" ] else : methods = [ "" spawn "" , "" fork "" ] if sys . platform == "" darwin "" else [ "" fork "" , "" spawn "" ] <MASK> methods . append ( "" forkserver "" ) return methods",if reduction . HAVE_SEND_HANDLE :,if self . _is_server ( ) :,85.40952773,FALSE,88.76
2107,"def _process_watch ( self , watched_event ) : logger . debug ( "" process_watch:  %r "" , watched_event ) with handle_exception ( self . _tree . _error_listeners ) : <MASK> assert self . _parent is None , "" unexpected CREATED on non-root "" self . on_created ( ) elif watched_event . type == EventType . DELETED : self . on_deleted ( ) elif watched_event . type == EventType . CHANGED : self . _refresh_data ( ) elif watched_event . type == EventType . CHILD : self . _refresh_children ( )",if watched_event . type == EventType . CREATED :,if watched_event . type == EventType . CREATED :,100,TRUE,100
2108,"def assert_open ( self , sock , * rest ) : if isinstance ( sock , fd_types ) : self . __assert_fd_open ( sock ) else : fileno = sock . fileno ( ) assert isinstance ( fileno , fd_types ) , fileno sockname = sock . getsockname ( ) assert isinstance ( sockname , tuple ) , sockname <MASK> self . __assert_fd_open ( fileno ) else : self . _assert_sock_open ( sock ) if rest : self . assert_open ( rest [ 0 ] , * rest [ 1 : ] )",if not WIN :,if fileno :,96.10016931,FALSE,97.06
2109,"def detype ( self ) : """"""De-types the instance, allowing it to be exported to the environment."""""" style = self . style if self . _detyped is None : self . _detyped = "" : "" . join ( [ key + "" = "" + "" ; "" . join ( [ LsColors . target_value <MASK> else ansi_color_name_to_escape_code ( v , cmap = style ) for v in val ] ) for key , val in sorted ( self . _d . items ( ) ) ] ) return self . _detyped",if key in self . _targets,"if isinstance ( val , ( list , tuple ) )",76.84947974,FALSE,90.82
2110,"def gather_metrics ( dry_run = False ) : today = datetime . date . today ( ) first = today . replace ( day = 1 ) last_month = first - datetime . timedelta ( days = 1 ) filename = "" form_types_ {} .csv "" . format ( last_month . strftime ( "" % Y- % m "" ) ) with connection . cursor ( ) as cursor : cursor . execute ( REGISTRATION_METRICS_SQL ) <MASK> for row in cursor . fetchall ( ) : logger . info ( encode_row ( row ) ) else : write_raw_data ( cursor = cursor , filename = filename )",if dry_run :,if dry_run :,100,TRUE,100
2111,"def cat ( tensors , dim = 0 ) : assert isinstance ( tensors , list ) , "" input to cat must be a list "" if len ( tensors ) == 1 : return tensors [ 0 ] from . autograd_cryptensor import AutogradCrypTensor if any ( isinstance ( t , AutogradCrypTensor ) for t in tensors ) : <MASK> tensors [ 0 ] = AutogradCrypTensor ( tensors [ 0 ] , requires_grad = False ) return tensors [ 0 ] . cat ( * tensors [ 1 : ] , dim = dim ) else : return get_default_backend ( ) . cat ( tensors , dim = dim )","if not isinstance ( tensors [ 0 ] , AutogradCrypTensor ) :",if len ( tensors ) == 1 :,88.43021763,FALSE,91.86
2112,"def is_installed ( self , dlc_title = "" "" ) - > bool : installed = False if dlc_title : dlc_version = self . get_dlc_info ( "" version "" , dlc_title ) installed = True if dlc_version else False # Start: Code for compatibility with minigalaxy 1.0 if not installed : status = self . legacy_get_dlc_status ( dlc_title ) installed = True if status in [ "" installed "" , "" updatable "" ] else False # End: Code for compatibility with minigalaxy 1.0 else : <MASK> installed = True return installed",if self . install_dir and os . path . exists ( self . install_dir ) :,"if self . legacy_get_dlc_status ( dlc_title ) == """,93.47969862,FALSE,87.05
2113,"def on_copy ( self ) : source_objects = self . __getSelection ( ) for source in source_objects : <MASK> new_obj = model . Phrase ( "" "" , "" "" ) else : new_obj = model . Script ( "" "" , "" "" ) new_obj . copy ( source ) self . cutCopiedItems . append ( new_obj )","if isinstance ( source , model . Phrase ) :",if self . is_phrases ( source ) :,88.28858115,FALSE,90.83
2114,"def FetchFn ( type_name ) : """"""Fetches all hunt results of a given type."""""" offset = 0 while True : results = data_store . REL_DB . ReadHuntResults ( hunt_id , offset = offset , count = self . _RESULTS_PAGE_SIZE , with_type = type_name ) <MASK> break for r in results : msg = r . AsLegacyGrrMessage ( ) msg . source_urn = source_urn yield msg offset + = self . _RESULTS_PAGE_SIZE",if not results :,if not results :,100,TRUE,100
2115,"def get_blob_type_declaration_sql ( self , column ) : length = column . get ( "" length "" ) if length : <MASK> return "" TINYBLOB "" if length < = self . LENGTH_LIMIT_BLOB : return "" BLOB "" if length < = self . LENGTH_LIMIT_MEDIUMBLOB : return "" MEDIUMBLOB "" return "" LONGBLOB """,if length <= self . LENGTH_LIMIT_TINYBLOB :,if length <= self . LENGTH_LIMIT_TINYBLOB :,100,TRUE,100
2116,"def decode ( cls , data ) : while data : ( length , atype , ) = unpack ( cls . Header . PACK , data [ : cls . Header . LEN ] ) <MASK> raise AttributesError ( "" Buffer underrun  %d  <  %d "" % ( len ( data ) , length ) ) payload = data [ cls . Header . LEN : length ] yield atype , payload data = data [ int ( ( length + 3 ) / 4 ) * 4 : ]",if len ( data ) < length :,if length < 0 :,91.08275181,FALSE,92.85
2117,"def test_join_diffs ( db , series_of_diffs , expected ) : diffs = [ ] for changes in series_of_diffs : tracker = DBDiffTracker ( ) for key , val in changes . items ( ) : <MASK> del tracker [ key ] else : tracker [ key ] = val diffs . append ( tracker . diff ( ) ) DBDiff . join ( diffs ) . apply_to ( db ) assert db == expected",if val is None :,if key in tracker :,93.5365618,FALSE,95.06
2118,"def ant_map ( m ) : tmp = "" rows  %s \n cols  %s \n "" % ( len ( m ) , len ( m [ 0 ] ) ) players = { } for row in m : tmp + = "" m  "" for col in row : if col == LAND : tmp + = "" . "" elif col == BARRIER : tmp + = "" % "" elif col == FOOD : tmp + = "" * "" <MASK> tmp + = "" ? "" else : players [ col ] = True tmp + = chr ( col + 97 ) tmp + = "" \n "" tmp = ( "" players  %s \n "" % len ( players ) ) + tmp return tmp",elif col == UNSEEN :,elif col == FOOD :,86.91496615,FALSE,98.3
2119,"def _report_error ( self , completion_routine , response = None , message = None ) : if response : # Only include the text in case of error. if not response . ok : status = location . Status ( response . status_code , response . text ) else : status = location . Status ( response . status_code ) else : status = location . Status ( 500 , message ) if response is None or not response . ok : <MASK> return completion_routine ( status ) raise IOError ( response . text ) else : <MASK> completion_routine ( status ) return location . Status ( 200 , response . content )",if completion_routine :,if completion_routine :,100,TRUE,100
2120,"def _generate_examples ( self , src_path = None , tgt_path = None , replace_unk = None ) : """"""Yields examples."""""" with tf . io . gfile . GFile ( src_path ) as f_d , tf . io . gfile . GFile ( tgt_path ) as f_s : for i , ( doc_text , sum_text ) in enumerate ( zip ( f_d , f_s ) ) : <MASK> yield i , { _DOCUMENT : doc_text . strip ( ) . replace ( "" <unk> "" , "" UNK "" ) , _SUMMARY : sum_text . strip ( ) . replace ( "" <unk> "" , "" UNK "" ) , } else : yield i , { _DOCUMENT : doc_text . strip ( ) , _SUMMARY : sum_text . strip ( ) }",if replace_unk :,if replace_unk :,100,TRUE,100
2121,"def escape ( text , newline = False ) : """"""Escape special html characters."""""" if isinstance ( text , str ) : if "" & "" in text : text = text . replace ( "" & "" , "" &amp; "" ) if "" > "" in text : text = text . replace ( "" > "" , "" &gt; "" ) if "" < "" in text : text = text . replace ( "" < "" , "" &lt; "" ) if ' "" ' in text : text = text . replace ( ' "" ' , "" &quot; "" ) if "" ' "" in text : text = text . replace ( "" ' "" , "" &quot; "" ) <MASK> if "" \n "" in text : text = text . replace ( "" \n "" , "" <br> "" ) return text",if newline :,if newline :,100,TRUE,100
2122,"def _handle_url_click ( self , event ) : url = _extract_click_text ( self . info_text , event , "" url "" ) if url is not None : <MASK> import webbrowser webbrowser . open ( url ) elif os . path . sep in url : os . makedirs ( url , exist_ok = True ) open_path_in_system_file_manager ( url ) else : self . _start_show_package_info ( url )","if url . startswith ( ""http:"" ) or url . startswith ( ""https:"" ) :","if os . name == ""nt"" :",80.10935451,FALSE,82.85
2123,"def SConsignFile ( self , name = "" .sconsign "" , dbm_module = None ) : if name is not None : name = self . subst ( name ) <MASK> name = os . path . join ( str ( self . fs . SConstruct_dir ) , name ) if name : name = os . path . normpath ( name ) sconsign_dir = os . path . dirname ( name ) if sconsign_dir and not os . path . exists ( sconsign_dir ) : self . Execute ( SCons . Defaults . Mkdir ( sconsign_dir ) ) SCons . SConsign . File ( name , dbm_module )",if not os . path . isabs ( name ) :,"if not name . startswith ( ""."" ) :",94.50632706,FALSE,94.3
2124,"def on_train_start ( self , trainer : Trainer , pl_module : LightningModule ) - > None : super ( ) . on_train_start ( trainer , pl_module ) submodule_dict = dict ( pl_module . named_modules ( ) ) self . _hook_handles = [ ] for name in self . _get_submodule_names ( pl_module ) : <MASK> rank_zero_warn ( f "" { name }  is not a valid identifier for a submodule in  { pl_module . __class__ . __name__ } , "" ""  skipping this key. "" ) continue handle = self . _register_hook ( name , submodule_dict [ name ] ) self . _hook_handles . append ( handle )",if name not in submodule_dict :,if name not in submodule_dict :,100,TRUE,100
2125,"def validate_configuration ( self , configuration : Optional [ ExpectationConfiguration ] ) : super ( ) . validate_configuration ( configuration ) if configuration is None : configuration = self . configuration try : assert "" value_set "" in configuration . kwargs , "" value_set is required "" assert isinstance ( configuration . kwargs [ "" value_set "" ] , ( list , set , dict ) ) , "" value_set must be a list or a set "" <MASK> assert ( "" $PARAMETER "" in configuration . kwargs [ "" value_set "" ] ) , ' Evaluation Parameter dict for value_set kwarg must have  "" $PARAMETER ""  key. ' except AssertionError as e : raise InvalidExpectationConfigurationError ( str ( e ) ) return True","if isinstance ( configuration . kwargs [ ""value_set"" ] , dict ) :","if isinstance ( configuration . kwargs [ ""value_set"" ] , dict ) :",100,TRUE,100
2126,"def check_refcounts ( expected , timeout = 10 ) : start = time . time ( ) while True : try : _check_refcounts ( expected ) break except AssertionError as e : <MASK> raise e else : time . sleep ( 0.1 )",if time . time ( ) - start > timeout :,if time . time ( ) - start >= timeout :,97.5834214,FALSE,95.59
2127,"def pickline ( file , key , casefold = 1 ) : try : f = open ( file , "" r "" ) except IOError : return None pat = re . escape ( key ) + "" : "" prog = re . compile ( pat , casefold and re . IGNORECASE ) while 1 : line = f . readline ( ) <MASK> break if prog . match ( line ) : text = line [ len ( key ) + 1 : ] while 1 : line = f . readline ( ) if not line or not line [ 0 ] . isspace ( ) : break text = text + line return text . strip ( ) return None",if not line :,if not line or not line [ 0 ] . isspace ( ) :,95.15175292,FALSE,91.56
2128,def _is_perf_file ( file_path ) : f = get_file ( file_path ) for line in f : <MASK> continue r = event_regexp . search ( line ) if r : f . close ( ) return True f . close ( ) return False,"if line [ 0 ] == ""#"" :","if line . startswith ( ""#"" ) :",89.65500421,FALSE,87.67
2129,"def link_pantsrefs ( soups , precomputed ) : """"""Transorm soups: <a pantsref=""foo""> becomes <a href=""../foo_page.html#foo"">"""""" for ( page , soup ) in soups . items ( ) : for a in soup . find_all ( "" a "" ) : <MASK> continue pantsref = a [ "" pantsref "" ] if pantsref not in precomputed . pantsref : raise TaskError ( f ' Page  { page }  has pantsref  "" { pantsref } ""  and I cannot find pantsmark for it ' ) a [ "" href "" ] = rel_href ( page , precomputed . pantsref [ pantsref ] )","if not a . has_attr ( ""pantsref"" ) :","if ""href"" not in a :",91.49407434,FALSE,92.04
2130,"def __init__ ( self , querylist = None ) : self . query_id = - 1 if querylist is None : self . querylist = [ ] else : self . querylist = querylist for query in self . querylist : <MASK> self . query_id = query . query_id else : if self . query_id != query . query_id : raise ValueError ( "" query in list must be same query_id "" )",if self . query_id == - 1 :,if self . query_id is None :,91.82555737,FALSE,94.41
2131,"def _draw_number ( screen , x_offset , y_offset , number , token = Token . Clock , transparent = False ) : "" Write number at position. "" fg = Char ( ""   "" , token ) bg = Char ( ""   "" , Token ) for y , row in enumerate ( _numbers [ number ] ) : screen_row = screen . data_buffer [ y + y_offset ] for x , n in enumerate ( row ) : <MASK> screen_row [ x + x_offset ] = fg elif not transparent : screen_row [ x + x_offset ] = bg","if n == ""#"" :",if n == 0 :,95.76915497,FALSE,96.49
2132,"def init ( self ) : self . sock . setblocking ( True ) if self . parser is None : # wrap the socket if needed <MASK> self . sock = ssl . wrap_socket ( self . sock , server_side = True , * * self . cfg . ssl_options ) # initialize the parser self . parser = http . RequestParser ( self . cfg , self . sock )",if self . cfg . is_ssl :,if ssl . is_available ( self . sock ) :,95.52714472,FALSE,89.58
2133,"def intersect_face ( pt ) : # todo: rewrite! inefficient! nonlocal vis_faces2D for f , vs in vis_faces2D : v0 = vs [ 0 ] for v1 , v2 in iter_pairs ( vs [ 1 : ] , False ) : <MASK> return f return None","if intersect_point_tri_2d ( pt , v0 , v1 , v2 ) :",if v0 == v1 and v2 == pt :,65.66811074,FALSE,76.86
2134,"def IMPORTFROM ( self , node ) : if node . module == "" __future__ "" : if not self . futuresAllowed : self . report ( messages . LateFutureImport , node , [ n . name for n in node . names ] ) else : self . futuresAllowed = False for alias in node . names : <MASK> self . scope . importStarred = True self . report ( messages . ImportStarUsed , node , node . module ) continue name = alias . asname or alias . name importation = Importation ( name , node ) if node . module == "" __future__ "" : importation . used = ( self . scope , node ) self . addBinding ( node , importation )","if alias . name == ""*"" :",if alias . asname and alias . name in self . scope . importStarred :,79.21112082,FALSE,92.13
2135,"def PyObject_Bytes ( obj ) : if type ( obj ) == bytes : return obj if hasattr ( obj , "" __bytes__ "" ) : res = obj . __bytes__ ( ) <MASK> raise TypeError ( "" __bytes__ returned non-bytes (type  %s ) "" % type ( res ) . __name__ ) return PyBytes_FromObject ( obj )","if not isinstance ( res , bytes ) :","if not isinstance ( res , bytes ) :",100,TRUE,100
2136,"def on_bt_search_clicked ( self , widget ) : if self . current_provider is None : return query = self . en_query . get_text ( ) @self . obtain_podcasts_with def load_data ( ) : if self . current_provider . kind == directory . Provider . PROVIDER_SEARCH : return self . current_provider . on_search ( query ) elif self . current_provider . kind == directory . Provider . PROVIDER_URL : return self . current_provider . on_url ( query ) <MASK> return self . current_provider . on_file ( query )",elif self . current_provider . kind == directory . Provider . PROVIDER_FILE :,elif self . current_provider . kind == directory . Provider . PROVIDER_FILE :,100,TRUE,100
2137,"def remove ( self , name ) : for s in [ self . __storage ( self . __category ) , self . __storage ( None ) ] : for i , b in enumerate ( s ) : <MASK> del s [ i ] if b . persistent : self . __save ( ) return raise KeyError ( name )",if b . name == name :,if b . name == name :,100,TRUE,100
2138,"def _wrapper ( data , axis = None , keepdims = False ) : if not keepdims : return func ( data , axis = axis ) else : <MASK> axis = axis if isinstance ( axis , int ) else axis [ 0 ] out_shape = list ( data . shape ) out_shape [ axis ] = 1 else : out_shape = [ 1 for _ in range ( len ( data . shape ) ) ] return func ( data , axis = axis ) . reshape ( out_shape )",if axis is not None :,if axis is not None :,100,TRUE,100
2139,"def authn_info ( self ) : res = [ ] for astat in self . assertion . authn_statement : context = astat . authn_context try : authn_instant = astat . authn_instant except AttributeError : authn_instant = "" "" <MASK> try : aclass = context . authn_context_class_ref . text except AttributeError : aclass = "" "" try : authn_auth = [ a . text for a in context . authenticating_authority ] except AttributeError : authn_auth = [ ] res . append ( ( aclass , authn_auth , authn_instant ) ) return res",if context :,if context :,100,TRUE,100
2140,"def _persist_metadata ( self , dirname , filename ) : metadata_path = "" {0} / {1} .json "" . format ( dirname , filename ) if self . media_metadata or self . comments or self . include_location : if self . posts : <MASK> self . merge_json ( { "" GraphImages "" : self . posts } , metadata_path ) else : self . save_json ( { "" GraphImages "" : self . posts } , metadata_path ) if self . stories : <MASK> self . merge_json ( { "" GraphStories "" : self . stories } , metadata_path ) else : self . save_json ( { "" GraphStories "" : self . stories } , metadata_path )",if self . latest :,if self . include_location :,97.99826288,FALSE,94.3
2141,"def update_record_image_detail ( input_image_record , updated_image_detail , session = None ) : if not session : session = db . Session image_record = { } image_record . update ( input_image_record ) image_record . pop ( "" created_at "" , None ) image_record . pop ( "" last_updated "" , None ) if image_record [ "" image_type "" ] == "" docker "" : for tag_record in updated_image_detail : <MASK> image_record [ "" image_detail "" ] . append ( tag_record ) return update_record ( image_record , session = session ) return image_record","if tag_record not in image_record [ ""image_detail"" ] :","if tag_record not in image_record [ ""image_detail"" ] :",100,TRUE,100
2142,"def backup ( self ) : for ds in [ ( "" activedirectory "" , "" AD "" ) , ( "" ldap "" , "" LDAP "" ) , ( "" nis "" , "" NIS "" ) ] : <MASK> try : ds_cache = self . middleware . call_sync ( "" cache.get "" , f "" { ds [ 1 ] } _cache "" ) with open ( f "" /var/db/system/. { ds [ 1 ] } _cache_backup "" , "" wb "" ) as f : pickle . dump ( ds_cache , f ) except KeyError : self . logger . debug ( "" No cache exists for directory service [ %s ]. "" , ds [ 0 ] )","if ( self . middleware . call_sync ( f""{ds[0]}.config"" ) ) [ ""enable"" ] :",if ds [ 0 ] :,81.10287953,FALSE,84.37
2143,"def parse_setup_cfg ( self ) : # type: () -> Dict[STRING_TYPE, Any] if self . setup_cfg is not None and self . setup_cfg . exists ( ) : contents = self . setup_cfg . read_text ( ) base_dir = self . setup_cfg . absolute ( ) . parent . as_posix ( ) try : parsed = setuptools_parse_setup_cfg ( self . setup_cfg . as_posix ( ) ) except Exception : if six . PY2 : contents = self . setup_cfg . read_bytes ( ) parsed = parse_setup_cfg ( contents , base_dir ) <MASK> return { } return parsed return { }",if not parsed :,if parsed is None :,73.0153264,FALSE,97.14
2144,"def parts ( ) : for l in lists . leaves : head_name = l . get_head_name ( ) <MASK> yield l . leaves elif head_name != "" System`Missing "" : raise MessageException ( "" Catenate "" , "" invrp "" , l )","if head_name == ""System`List"" :","if head_name == ""System`List"" :",100,TRUE,100
2145,"def _get_callback_and_order ( self , hook ) : if callable ( hook ) : return hook , None elif isinstance ( hook , tuple ) and len ( hook ) == 2 : callback , order = hook # test that callback is a callable <MASK> raise ValueError ( "" Hook callback is not a callable "" ) # test that number is an int try : int ( order ) except ValueError : raise ValueError ( "" Hook order is not a number "" ) return callback , order else : raise ValueError ( "" Invalid hook definition, neither a callable nor a 2-tuple (callback, order):  {!r} "" . format ( hook ) )",if not callable ( callback ) :,if not callable ( callback ) :,100,TRUE,100
2146,"def _resize_masks ( self , results ) : """"""Resize masks with ``results['scale']``"""""" for key in results . get ( "" mask_fields "" , [ ] ) : if results [ key ] is None : continue <MASK> results [ key ] = results [ key ] . rescale ( results [ "" scale "" ] ) else : results [ key ] = results [ key ] . resize ( results [ "" img_shape "" ] [ : 2 ] )",if self . keep_ratio :,"if isinstance ( results [ key ] , MaskMask ) :",91.12299357,FALSE,90.39
2147,"def getDataMax ( self ) : result = - Double . MAX_VALUE nCurves = self . chart . getNCurves ( ) for i in range ( nCurves ) : c = self . getSystemCurve ( i ) if not c . isVisible ( ) : continue <MASK> nPoints = c . getNPoints ( ) for j in range ( nPoints ) : result = self . maxIgnoreNaNAndMaxValue ( result , c . getPoint ( j ) . getY ( ) ) if result == - Double . MAX_VALUE : return Double . NaN return result",if c . getYAxis ( ) == Y_AXIS :,if c . isVisible ( ) :,95.45080568,FALSE,91.93
2148,"def _check_token ( self ) : if settings . app . sso_client_cache and self . server_auth_token : doc = self . sso_client_cache_collection . find_one ( { "" user_id "" : self . user . id , "" server_id "" : self . server . id , "" device_id "" : self . device_id , "" device_name "" : self . device_name , "" auth_token "" : self . server_auth_token , } ) <MASK> self . has_token = True",if doc :,"if doc and doc [ ""auth_token"" ] :",68.90213092,FALSE,91.69
2149,"def parse_header ( plyfile , ext ) : # Variables line = [ ] properties = [ ] num_points = None while b "" end_header "" not in line and line != b "" "" : line = plyfile . readline ( ) <MASK> line = line . split ( ) num_points = int ( line [ 2 ] ) elif b "" property "" in line : line = line . split ( ) properties . append ( ( line [ 2 ] . decode ( ) , ext + ply_dtypes [ line [ 1 ] ] ) ) return num_points , properties","if b""element"" in line :","if b""num_points"" in line :",73.83418461,FALSE,96.37
2150,"def __codeanalysis_settings_changed ( self , current_finfo ) : if self . data : run_pyflakes , run_pep8 = self . pyflakes_enabled , self . pep8_enabled for finfo in self . data : self . __update_editor_margins ( finfo . editor ) finfo . cleanup_analysis_results ( ) <MASK> if current_finfo is not finfo : finfo . run_code_analysis ( run_pyflakes , run_pep8 )",if ( run_pyflakes or run_pep8 ) and current_finfo is not None :,if run_pyflakes :,83.3508486,FALSE,84.95
2151,"def __modules ( self ) : raw_output = self . __module_avail_output ( ) . decode ( "" utf-8 "" ) for line in StringIO ( raw_output ) : line = line and line . strip ( ) <MASK> continue line_modules = line . split ( ) for module in line_modules : if module . endswith ( self . default_indicator ) : module = module [ 0 : - len ( self . default_indicator ) ] . strip ( ) module_parts = module . split ( "" / "" ) module_version = None if len ( module_parts ) == 2 : module_version = module_parts [ 1 ] module_name = module_parts [ 0 ] yield module_name , module_version","if not line or line . startswith ( ""-"" ) :",if not line :,73.26638607,FALSE,93.55
2152,"def _set_trailing_size ( self , size ) : if self . is_free ( ) : next_chunk = self . next_chunk ( ) <MASK> self . state . memory . store ( next_chunk . base , size , self . state . arch . bytes )",if next_chunk is not None :,if next_chunk :,62.50526567,FALSE,92.86
2153,"def _execute_for_all_tables ( self , app , bind , operation , skip_tables = False ) : app = self . get_app ( app ) if bind == "" __all__ "" : binds = [ None ] + list ( app . config . get ( "" SQLALCHEMY_BINDS "" ) or ( ) ) elif isinstance ( bind , string_types ) or bind is None : binds = [ bind ] else : binds = bind for bind in binds : extra = { } <MASK> tables = self . get_tables_for_bind ( bind ) extra [ "" tables "" ] = tables op = getattr ( self . Model . metadata , operation ) op ( bind = self . get_engine ( app , bind ) , * * extra )",if not skip_tables :,if skip_tables :,96.48842668,FALSE,98.45
2154,"def getFileName ( ) : extension = "" .json "" file = "" %s -stats "" % self . clusterName counter = 0 while True : suffix = str ( counter ) . zfill ( 3 ) + extension fullName = os . path . join ( self . statsPath , file + suffix ) <MASK> return fullName counter + = 1",if not os . path . exists ( fullName ) :,if not os . path . exists ( fullName ) :,100,TRUE,100
2155,def logic ( ) : # direction if goRight == ACTIVE : dir . next = DirType . RIGHT run . next = True elif goLeft == ACTIVE : dir . next = DirType . LEFT run . next = True # stop if stop == ACTIVE : run . next = False # counter action if run : <MASK> q . next [ 4 : 1 ] = q [ 3 : ] q . next [ 0 ] = not q [ 3 ] else : q . next [ 3 : ] = q [ 4 : 1 ] q . next [ 3 ] = not q [ 0 ],if dir == DirType . LEFT :,if q [ 0 ] :,72.32479867,FALSE,93.87
2156,"def test_broadcast ( self ) : """"""Test example broadcast functionality."""""" self . create_lang_connection ( "" 1000000000 "" , "" en "" ) self . create_lang_connection ( "" 1000000001 "" , "" en "" ) self . create_lang_connection ( "" 1000000002 "" , "" en "" ) self . create_lang_connection ( "" 1000000003 "" , "" es "" ) self . create_lang_connection ( "" 1000000004 "" , "" es "" ) app . lang_broadcast ( ) self . assertEqual ( 2 , len ( self . outbound ) ) for message in self . outbound : if message . text == "" hello "" : self . assertEqual ( 3 , len ( message . connections ) ) <MASK> self . assertEqual ( 2 , len ( message . connections ) )","elif message . text == ""hola"" :","elif message . text == ""hello"" :",99.07796763,FALSE,98.53
2157,"def get_ovf_env ( dirname ) : env_names = ( "" ovf-env.xml "" , "" ovf_env.xml "" , "" OVF_ENV.XML "" , "" OVF-ENV.XML "" ) for fname in env_names : full_fn = os . path . join ( dirname , fname ) <MASK> try : contents = util . load_file ( full_fn ) return ( fname , contents ) except Exception : util . logexc ( LOG , "" Failed loading ovf file  %s "" , full_fn ) return ( None , False )",if os . path . isfile ( full_fn ) :,if os . path . exists ( full_fn ) :,89.02661417,FALSE,97.95
2158,"def _calc_offsets_children ( self , offset , is_last ) : if self . elems : elem_last = self . elems [ - 1 ] for elem in self . elems : offset = elem . _calc_offsets ( offset , ( elem is elem_last ) ) offset + = _BLOCK_SENTINEL_LENGTH elif not self . props or self . id in _ELEMS_ID_ALWAYS_BLOCK_SENTINEL : <MASK> offset + = _BLOCK_SENTINEL_LENGTH return offset",if not is_last :,if is_last :,79.23182953,FALSE,97.6
2159,"def publish_state ( cls , payload , state ) : try : <MASK> if state == action_constants . LIVEACTION_STATUS_REQUESTED : cls . process ( payload ) else : worker . get_worker ( ) . process ( payload ) except Exception : traceback . print_exc ( ) print ( payload )","if isinstance ( payload , LiveActionDB ) :",if state != action_constants . LIVEACTION_STATUS_NONE :,64.64806519,FALSE,81.83
2160,"def log_predictive_density ( self , x_test , y_test , Y_metadata = None ) : if isinstance ( x_test , list ) : x_test , y_test , ind = util . multioutput . build_XY ( x_test , y_test ) <MASK> Y_metadata = { "" output_index "" : ind , "" trials "" : np . ones ( ind . shape ) } return super ( MultioutputGP , self ) . log_predictive_density ( x_test , y_test , Y_metadata )",if Y_metadata is None :,if Y_metadata is None :,100,TRUE,100
2161,"def minimalBases ( classes ) : """"""Reduce a list of base classes to its ordered minimum equivalent"""""" if not __python3 : # pragma: no cover classes = [ c for c in classes if c is not ClassType ] candidates = [ ] for m in classes : for n in classes : <MASK> break else : # m has no subclasses in 'classes' if m in candidates : candidates . remove ( m ) # ensure that we're later in the list candidates . append ( m ) return candidates","if issubclass ( n , m ) and m is not n :",if n is not ClassType :,69.37061089,FALSE,89.44
2162,"def apply ( self , operations , rotations = None , * * kwargs ) : rotations = rotations or [ ] # apply the circuit operations for i , operation in enumerate ( operations ) : <MASK> raise DeviceError ( "" Operation  {}  cannot be used after other Operations have already been applied  "" "" on a  {}  device. "" . format ( operation . name , self . short_name ) ) for operation in operations : self . _apply_operation ( operation ) # store the pre-rotated state self . _pre_rotated_state = self . _state # apply the circuit rotations for operation in rotations : self . _apply_operation ( operation )","if i > 0 and isinstance ( operation , ( QubitStateVector , BasisState ) ) :",if i > 0 :,94.94902425,FALSE,91
2163,"def __str__ ( self ) : txt = str ( self . _called ) if self . call_gas or self . call_value : gas = f "" gas:  { self . call_gas } "" if self . call_gas else "" "" value = f "" value:  { self . call_value } "" if self . call_value else "" "" salt = f "" salt:  { self . call_salt } "" if self . call_salt else "" "" <MASK> options = [ gas , value , salt ] txt + = "" { "" + "" , "" . join ( [ o for o in options if o != "" "" ] ) + "" } "" return txt + "" ( "" + "" , "" . join ( [ str ( a ) for a in self . _arguments ] ) + "" ) """,if gas or value or salt :,if self . _arguments :,80.63224885,FALSE,96.33
2164,"def pop ( self ) : """"""Pop a nonterminal.  (Internal)"""""" popdfa , popstate , popnode = self . stack . pop ( ) newnode = self . convert ( self . grammar , popnode ) if newnode is not None : <MASK> dfa , state , node = self . stack [ - 1 ] node . children . append ( newnode ) else : self . rootnode = newnode",if self . stack :,if popdfa is not None :,90.57578432,FALSE,93.3
2165,"def pollpacket ( self , wait ) : self . _stage0 ( ) if len ( self . buffer ) < self . bufneed : r , w , x = select . select ( [ self . sock . fileno ( ) ] , [ ] , [ ] , wait ) <MASK> return None try : s = self . sock . recv ( BUFSIZE ) except socket . error : raise EOFError if len ( s ) == 0 : raise EOFError self . buffer + = s self . _stage0 ( ) return self . _stage1 ( )",if len ( r ) == 0 :,if len ( r ) == 0 :,100,TRUE,100
2166,"def increaseToolReach ( self ) : if self . draggingFace is not None : d = ( 1 , - 1 ) [ self . draggingFace & 1 ] <MASK> # xxxxx y d = - d self . draggingY + = d x , y , z = self . editor . mainViewport . cameraPosition pos = [ x , y , z ] pos [ self . draggingFace >> 1 ] + = d self . editor . mainViewport . cameraPosition = tuple ( pos ) else : self . cloneCameraDistance = self . editor . _incrementReach ( self . cloneCameraDistance ) return True",if self . draggingFace >> 1 != 1 :,if self . draggingFace >> 1 == 0 :,98.46254699,FALSE,96.33
2167,"def selectionToChunks ( self , remove = False , add = False ) : box = self . selectionBox ( ) if box : if box == self . level . bounds : self . selectedChunks = set ( self . level . allChunks ) return selectedChunks = self . selectedChunks boxedChunks = set ( box . chunkPositions ) if boxedChunks . issubset ( selectedChunks ) : remove = True <MASK> selectedChunks . difference_update ( boxedChunks ) else : selectedChunks . update ( boxedChunks ) self . selectionTool . selectNone ( )",if remove and not add :,if remove :,92.9993321,FALSE,95.41
2168,"def __init__ ( self , * args , * * kwargs ) : super ( ProjectForm , self ) . __init__ ( * args , * * kwargs ) if self . instance . id : <MASK> self . fields [ "" localfiletype "" ] . widget . attrs [ "" disabled "" ] = True self . fields [ "" localfiletype "" ] . required = False if ( self . instance . treestyle != "" auto "" and self . instance . translationproject_set . count ( ) and self . instance . treestyle == self . instance . _detect_treestyle ( ) ) : self . fields [ "" treestyle "" ] . widget . attrs [ "" disabled "" ] = True self . fields [ "" treestyle "" ] . required = False",if Store . objects . filter ( translation_project__project = self . instance ) . count ( ) :,"if self . instance . localfiletype != ""auto"" :",89.32653815,FALSE,87.88
2169,"def _infer_return_type ( * args ) : """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args : if arg is None : continue if isinstance ( arg , bytes ) : <MASK> raise TypeError ( "" Can ' t mix bytes and non-bytes in  "" "" path components. "" ) return_type = bytes else : if return_type is bytes : raise TypeError ( "" Can ' t mix bytes and non-bytes in  "" "" path components. "" ) return_type = str if return_type is None : return str # tempfile APIs return a str by default. return return_type",if return_type is str :,if return_type is None :,95.9220082,FALSE,98.21
2170,"def deleteDuplicates ( gadgets , callback = None ) : toReturn = [ ] inst = set ( ) count = 0 added = False len_gadgets = len ( gadgets ) for i , gadget in enumerate ( gadgets ) : inst . add ( gadget . _gadget ) if len ( inst ) > count : count = len ( inst ) toReturn . append ( gadget ) added = True <MASK> callback ( gadget , added , float ( i + 1 ) / ( len_gadgets ) ) added = False return toReturn",if callback :,if callback :,100,TRUE,100
2171,"def send_all ( self , data : bytes ) : with self . _conflict_detector : <MASK> raise _core . ClosedResourceError ( "" this pipe is already closed "" ) if not data : await _core . checkpoint ( ) return try : written = await _core . write_overlapped ( self . _handle_holder . handle , data ) except BrokenPipeError as ex : raise _core . BrokenResourceError from ex # By my reading of MSDN, this assert is guaranteed to pass so long # as the pipe isn't in nonblocking mode, but... let's just # double-check. assert written == len ( data )",if self . _handle_holder . closed :,if self . _closed :,88.78523542,FALSE,95.76
2172,"def setup_parameter_node ( self , param_node ) : if param_node . bl_idname == "" SvNumberNode "" : <MASK> value = self . sv_get ( ) [ 0 ] [ 0 ] print ( "" V "" , value ) if isinstance ( value , int ) : param_node . selected_mode = "" int "" param_node . int_ = value elif isinstance ( value , float ) : param_node . selected_mode = "" float "" param_node . float_ = value",if self . use_prop or self . get_prop_name ( ) :,if self . sv_get ( ) :,88.07873822,FALSE,89.88
2173,"def collect_active_inst_idx_list ( inst_beams , word_prob , inst_idx_to_position_map ) : active_inst_idx_list = [ ] for inst_idx , inst_position in inst_idx_to_position_map . items ( ) : is_inst_complete = inst_beams [ inst_idx ] . advance ( word_prob [ inst_position ] ) <MASK> active_inst_idx_list + = [ inst_idx ] return active_inst_idx_list",if not is_inst_complete :,if is_inst_complete :,93.30697984,FALSE,97.87
2174,"def compare_member_req_resp_without_key ( self , request , response ) : for user_response in resp_json ( response ) [ "" data "" ] : for user_request in request : <MASK> assert user_request [ "" role "" ] == user_response [ "" role "" ]","if user_request [ ""user_id"" ] == user_response [ ""user_id"" ] :","if ""role"" in user_request and ""role"" in user_response :",62.5265622,FALSE,76.74
2175,"def __init__ ( self , dir ) : self . module_names = set ( ) for name in os . listdir ( dir ) : if name . endswith ( "" .py "" ) : self . module_names . add ( name [ : - 3 ] ) <MASK> self . module_names . add ( name )","elif ""."" not in name :","elif name . startswith ( ""py"" ) :",90.16487117,FALSE,88.62
2176,"def _read_filter ( self , data ) : if data : if self . expected_inner_sha256 : self . inner_sha . update ( data ) <MASK> self . inner_md5 . update ( data ) return data",if self . expected_inner_md5sum :,if self . expected_inner_md5 :,96.64370469,FALSE,95.16
2177,"def _p_basicstr_content ( s , content = _basicstr_re ) : res = [ ] while True : res . append ( s . expect_re ( content ) . group ( 0 ) ) <MASK> break if s . consume_re ( _newline_esc_re ) : pass elif s . consume_re ( _short_uni_re ) or s . consume_re ( _long_uni_re ) : res . append ( _chr ( int ( s . last ( ) . group ( 1 ) , 16 ) ) ) else : s . expect_re ( _escapes_re ) res . append ( _escapes [ s . last ( ) . group ( 0 ) ] ) return "" "" . join ( res )","if not s . consume ( ""\\"" ) :",if s . consume_re ( _eol_esc_re ) :,94.69346484,FALSE,92.92
2178,"def process_response ( self , request , response ) : if ( response . status_code == 404 and request . path_info . endswith ( "" / "" ) and not is_valid_path ( request . path_info ) and is_valid_path ( request . path_info [ : - 1 ] ) ) : # Use request.path because we munged app/locale in path_info. newurl = request . path [ : - 1 ] <MASK> with safe_query_string ( request ) : newurl + = "" ? "" + request . META . get ( "" QUERY_STRING "" , "" "" ) return HttpResponsePermanentRedirect ( newurl ) else : return response",if request . GET :,"if request . META . get ( ""QUERY_STRING"" ) :",97.76083979,FALSE,92.25
2179,"def convertDict ( obj ) : obj = dict ( obj ) for k , v in obj . items ( ) : del obj [ k ] <MASK> k = dumps ( k ) # Keep track of which keys need to be decoded when loading. if Types . KEYS not in obj : obj [ Types . KEYS ] = [ ] obj [ Types . KEYS ] . append ( k ) obj [ k ] = convertObjects ( v ) return obj","if not ( isinstance ( k , str ) or isinstance ( k , unicode ) ) :","if isinstance ( k , dict ) :",81.9434725,FALSE,85.96
2180,"def __repr__ ( self ) : if self . _in_repr : return "" <recursion> "" try : self . _in_repr = True <MASK> status = "" computed,  "" if self . error ( ) is None : if self . value ( ) is self : status + = "" = self "" else : status + = "" =  "" + repr ( self . value ( ) ) else : status + = "" error =  "" + repr ( self . error ( ) ) else : status = "" isn ' t computed "" return "" %s  ( %s ) "" % ( type ( self ) , status ) finally : self . _in_repr = False",if self . is_computed ( ) :,if self . _in_repr :,98.25311132,FALSE,95.7
2181,"def allocate_network ( ipv = "" ipv4 "" ) : global dtcd_uuid global network_pool global allocations network = None try : cx = httplib . HTTPConnection ( "" localhost:7623 "" ) cx . request ( "" POST "" , "" /v1/network/ %s / "" % ipv , body = dtcd_uuid ) resp = cx . getresponse ( ) <MASK> network = netaddr . IPNetwork ( resp . read ( ) . decode ( "" utf-8 "" ) ) cx . close ( ) except Exception : pass if network is None : network = network_pool [ ipv ] . pop ( ) allocations [ network ] = True return network",if resp . status == 200 :,if resp . status_code == 200 :,98.91816607,FALSE,97.39
2182,"def change_args_to_dict ( string ) : if string is None : return None ans = [ ] strings = string . split ( "" \n "" ) ind = 1 start = 0 while ind < = len ( strings ) : if ind < len ( strings ) and strings [ ind ] . startswith ( ""   "" ) : ind + = 1 else : if start < ind : ans . append ( "" \n "" . join ( strings [ start : ind ] ) ) start = ind ind + = 1 d = { } for line in ans : <MASK> lines = line . split ( "" : "" ) d [ lines [ 0 ] ] = lines [ 1 ] . strip ( ) return d","if "":"" in line and len ( line ) > 0 :","if line . startswith ( ""args:"" ) :",92.26255913,FALSE,92.35
2183,"def kill_members ( members , sig , hosts = nodes ) : for member in sorted ( members ) : try : <MASK> print ( "" killing  %s "" % member ) proc = hosts [ member ] [ "" proc "" ] # Not sure if cygwin makes sense here... if sys . platform in ( "" win32 "" , "" cygwin "" ) : os . kill ( proc . pid , signal . CTRL_C_EVENT ) else : os . kill ( proc . pid , sig ) except OSError : <MASK> print ( "" %s  already dead? "" % member )",if ha_tools_debug :,"if ""proc"" in hosts :",89.19077147,FALSE,89.69
2184,"def check ( self ) : for path in self . paths : response = self . http_request ( method = "" GET "" , path = path , ) <MASK> continue if any ( map ( lambda x : x in response . text , [ "" report.db.server.name "" , "" report.db.server.sa.pass "" , "" report.db.server.user.pass "" , ] , ) ) : self . valid = path return True # target is vulnerable return False # target not vulnerable",if response is None :,if response is None :,100,TRUE,100
2185,"def get_to_download_runs_ids ( session , headers ) : last_date = 0 result = [ ] while 1 : r = session . get ( RUN_DATA_API . format ( last_date = last_date ) , headers = headers ) if r . ok : run_logs = r . json ( ) [ "" data "" ] [ "" records "" ] result . extend ( [ i [ "" logs "" ] [ 0 ] [ "" stats "" ] [ "" id "" ] for i in run_logs ] ) last_date = r . json ( ) [ "" data "" ] [ "" lastTimestamp "" ] since_time = datetime . utcfromtimestamp ( last_date / 1000 ) print ( f "" pares keep ids data since  { since_time } "" ) time . sleep ( 1 ) # spider rule <MASK> break return result",if not last_date :,if since_time > time . time ( ) :,97.84840095,FALSE,94.34
2186,"def button_press_cb ( self , tdw , event ) : self . _update_zone_and_cursors ( tdw , event . x , event . y ) if self . _zone in ( _EditZone . CREATE_FRAME , _EditZone . REMOVE_FRAME ) : button = event . button <MASK> self . _click_info = ( button , self . _zone ) return False return super ( FrameEditMode , self ) . button_press_cb ( tdw , event )",if button == 1 and event . type == Gdk . EventType . BUTTON_PRESS :,"if button in ( _EditZone . DELETE_FRAME , _EditZone . EDIT_FRAME",65.93934679,FALSE,84.46
2187,"def first_timestep ( ) : assignment = self . has_previous . assign ( value = tf_util . constant ( value = True , dtype = "" bool "" ) , read_value = False ) with tf . control_dependencies ( control_inputs = ( assignment , ) ) : <MASK> current = x else : current = tf . expand_dims ( input = x , axis = ( self . axis + 1 ) ) multiples = tuple ( self . length if dims == self . axis + 1 else 1 for dims in range ( self . output_spec ( ) . rank + 1 ) ) return tf . tile ( input = current , multiples = multiples )",if self . concatenate :,if self . axis == 0 :,96.55429235,FALSE,96.1
2188,"def main ( ) - > None : onefuzz = Onefuzz ( ) jobs = onefuzz . jobs . list ( ) for job in jobs : print ( "" job: "" , str ( job . job_id ) [ : 8 ] , "" : "" . join ( [ job . config . project , job . config . name , job . config . build ] ) , ) for task in onefuzz . tasks . list ( job_id = job . job_id ) : <MASK> continue print ( ""      "" , str ( task . task_id ) [ : 8 ] , task . config . task . type , task . config . task . target_exe , )","if task . state in [ ""stopped"" , ""stopping"" ] :","if task . config . task . type != ""test"" :",94.08737619,FALSE,91.79
2189,"def update_stack ( self , full_name , template_url , parameters , tags ) : """"""Updates an existing stack in CloudFormation."""""" try : logger . info ( "" Attempting to update stack  %s . "" , full_name ) self . conn . cloudformation . update_stack ( full_name , template_url = template_url , parameters = parameters , tags = tags , capabilities = [ "" CAPABILITY_IAM "" ] , ) return SUBMITTED except BotoServerError as e : <MASK> logger . info ( "" Stack  %s  did not change, not updating. "" , full_name ) return SKIPPED raise","if ""No updates are to be performed."" in e . message :",if e . status == 404 :,93.06608053,FALSE,89.98
2190,"def header_tag_files ( env , files , legal_header , script_files = False ) : """"""Apply the legal_header to the list of files"""""" try : import apply_legal_header except : xbc . cdie ( "" XED ERROR: mfile.py could not find scripts directory "" ) for g in files : print ( "" G:  "" , g ) for f in mbuild . glob ( g ) : print ( "" F:  "" , f ) <MASK> apply_legal_header . apply_header_to_data_file ( legal_header , f ) else : apply_legal_header . apply_header_to_source_file ( legal_header , f )",if script_files :,if script_files :,100,TRUE,100
2191,"def cleanDataCmd ( cmd ) : newcmd = "" AbracadabrA ** <?php  "" if cmd [ : 6 ] != "" php:// "" : if reverseConn not in cmd : cmds = cmd . split ( "" & "" ) for c in cmds : <MASK> newcmd + = "" system( ' %s ' ); "" % c else : b64cmd = base64 . b64encode ( cmd ) newcmd + = "" system(base64_decode( ' %s ' )); "" % b64cmd else : newcmd + = cmd [ 6 : ] newcmd + = "" ?> ** "" return newcmd",if len ( c ) > 0 :,"if c . startswith ( ""base64:"" ) :",92.25027843,FALSE,92.32
2192,"def test_form ( self ) : n_qubits = 6 random_operator = get_fermion_operator ( random_interaction_operator ( n_qubits ) ) chemist_operator = chemist_ordered ( random_operator ) for term , _ in chemist_operator . terms . items ( ) : <MASK> pass else : self . assertTrue ( term [ 0 ] [ 1 ] ) self . assertTrue ( term [ 2 ] [ 1 ] ) self . assertFalse ( term [ 1 ] [ 1 ] ) self . assertFalse ( term [ 3 ] [ 1 ] ) self . assertTrue ( term [ 0 ] [ 0 ] > term [ 2 ] [ 0 ] ) self . assertTrue ( term [ 1 ] [ 0 ] > term [ 3 ] [ 0 ] )",if len ( term ) == 2 or not len ( term ) :,if len ( term ) == 3 :,94.40183712,FALSE,94.82
2193,"def do ( server , handler , config , modargs ) : data = [ ] clients = server . get_clients ( handler . default_filter ) if not clients : return for client in clients : tags = config . tags ( client . node ( ) ) <MASK> tags . remove ( * modargs . remove ) if modargs . add : tags . add ( * modargs . add ) data . append ( { "" ID "" : client . node ( ) , "" TAGS "" : tags } ) config . save ( project = modargs . write_project , user = modargs . write_user ) handler . display ( Table ( data ) )",if modargs . remove :,if modargs . remove :,100,TRUE,100
2194,"def validate ( self ) : if self . data . get ( "" state "" ) == "" enabled "" : <MASK> raise PolicyValidationError ( ( "" redshift logging enablement requires `bucket`  "" "" and `prefix` specification on  %s "" % ( self . manager . data , ) ) ) return self","if ""bucket"" not in self . data :","if self . data . get ( ""bucket"" ) and self . data . get ( """,79.79648036,FALSE,78.22
2195,"def renumber ( self , x1 , y1 , x2 , y2 , dx , dy ) : out = [ ] for part in re . split ( "" ( \ w+) "" , self . formula ) : m = re . match ( "" ^([A-Z]+)([1-9][0-9]*)$ "" , part ) <MASK> sx , sy = m . groups ( ) x = colname2num ( sx ) y = int ( sy ) if x1 < = x < = x2 and y1 < = y < = y2 : part = cellname ( x + dx , y + dy ) out . append ( part ) return FormulaCell ( "" "" . join ( out ) , self . fmt , self . alignment )",if m is not None :,if m :,73.12507644,FALSE,97.06
2196,"def update_sysconfig_file ( fn , adjustments , allow_empty = False ) : if not adjustments : return ( exists , contents ) = read_sysconfig_file ( fn ) updated_am = 0 for ( k , v ) in adjustments . items ( ) : if v is None : continue v = str ( v ) if len ( v ) == 0 and not allow_empty : continue contents [ k ] = v updated_am + = 1 if updated_am : lines = [ str ( contents ) , ] <MASK> lines . insert ( 0 , util . make_header ( ) ) util . write_file ( fn , "" \n "" . join ( lines ) + "" \n "" , 0o644 )",if not exists :,if exists :,97.729886,FALSE,98.32
2197,"def getElement ( self , aboutUri , namespace , name ) : for desc in self . rdfRoot . getElementsByTagNameNS ( RDF_NAMESPACE , "" Description "" ) : if desc . getAttributeNS ( RDF_NAMESPACE , "" about "" ) == aboutUri : attr = desc . getAttributeNodeNS ( namespace , name ) <MASK> yield attr for element in desc . getElementsByTagNameNS ( namespace , name ) : yield element",if attr != None :,if attr :,94.25391721,FALSE,94.16
2198,"def get_store_name_from_connection_string ( connection_string ) : if is_valid_connection_string ( connection_string ) : segments = dict ( seg . split ( "" = "" , 1 ) for seg in connection_string . split ( "" ; "" ) ) endpoint = segments . get ( "" Endpoint "" ) <MASK> return endpoint . split ( "" // "" ) [ 1 ] . split ( "" . "" ) [ 0 ] return None",if endpoint :,if endpoint :,100,TRUE,100
2199,"def insertLoopTemplate ( self , layout ) : col = layout . column ( align = True ) for socket in self . activeNode . outputs : <MASK> props = col . operator ( "" an.insert_loop_for_iterator "" , text = "" Loop through  {} "" . format ( repr ( socket . getDisplayedName ( ) ) ) , icon = "" MOD_ARRAY "" , ) props . nodeIdentifier = self . activeNode . identifier props . socketIndex = socket . getIndex ( )",if not socket . hide and isList ( socket . bl_idname ) :,"if socket . getDisplayedName ( ) != ""loop"" :",87.62247035,FALSE,87.49
2200,"def do_task ( self , task ) : self . running_task + = 1 result = yield gen . Task ( self . fetcher . fetch , task ) type , task , response = result . args self . processor . on_task ( task , response ) # do with message while not self . processor . inqueue . empty ( ) : _task , _response = self . processor . inqueue . get ( ) self . processor . on_task ( _task , _response ) # do with results while not self . processor . result_queue . empty ( ) : _task , _result = self . processor . result_queue . get ( ) <MASK> self . result_worker . on_result ( _task , _result ) self . running_task - = 1",if self . result_worker :,if _task is not None and _result is not None :,97.33736748,FALSE,92.91
2201,"def _parse_config_result ( data ) : command_list = ""  ;  "" . join ( [ x . strip ( ) for x in data [ 0 ] ] ) config_result = data [ 1 ] if isinstance ( config_result , list ) : result = "" "" <MASK> for key in config_result [ 0 ] : result + = config_result [ 0 ] [ key ] config_result = result else : config_result = config_result [ 0 ] return [ command_list , config_result ]","if isinstance ( config_result [ 0 ] , dict ) :",if config_result [ 0 ] :,85.62375867,FALSE,93.18
2202,"def load_api_handler ( self , mod_name ) : for name , hdl in API_HANDLERS : name = name . lower ( ) <MASK> handler = self . mods . get ( name ) if not handler : handler = hdl ( self . emu ) self . mods . update ( { name : handler } ) return handler return None",if mod_name and name == mod_name . lower ( ) :,if name . lower ( ) == mod_name :,90.54186094,FALSE,89.52
2203,def heal ( self ) : if not self . doctors : return proc_ids = self . _get_process_ids ( ) for proc_id in proc_ids : # get proc every time for latest state proc = PipelineProcess . objects . get ( id = proc_id ) if not proc . is_alive or proc . is_frozen : continue for dr in self . doctors : <MASK> dr . cure ( proc ) break,if dr . confirm ( proc ) :,if dr . is_alive :,96.52257426,FALSE,94.1
2204,"def __new__ ( cls , * args , * * kwargs ) : if len ( args ) == 1 : if len ( kwargs ) : raise ValueError ( "" You can either use  {}  with one positional argument or with keyword arguments, not both. "" . format ( cls . __name__ ) ) <MASK> return super ( ) . __new__ ( cls ) if isinstance ( args [ 0 ] , cls ) : return cls return super ( ) . __new__ ( cls , * args , * * kwargs )",if not args [ 0 ] :,if args [ 0 ] is None :,95.43917475,FALSE,95.78
2205,"def __lt__ ( self , other ) : # 0: clock 1: timestamp 3: process id try : A , B = self [ 0 ] , other [ 0 ] # uses logical clock value first <MASK> # use logical clock if available if A == B : # equal clocks use lower process id return self [ 2 ] < other [ 2 ] return A < B return self [ 1 ] < other [ 1 ] # ... or use timestamp except IndexError : return NotImplemented",if A and B :,if A != B :,73.48475285,FALSE,96.6
2206,"def _get_client ( rp_mapping , resource_provider ) : for key , value in rp_mapping . items ( ) : if str . lower ( key ) == str . lower ( resource_provider ) : <MASK> return GeneralPrivateEndpointClient ( key , value [ "" api_version "" ] , value [ "" support_list_or_not "" ] , value [ "" resource_get_api_version "" ] , ) return value ( ) raise CLIError ( "" Resource type must be one of  {} "" . format ( "" ,  "" . join ( rp_mapping . keys ( ) ) ) )","if isinstance ( value , dict ) :","if ""api_get_api_version"" in value :",93.8631086,FALSE,90.88
2207,"def test_progressbar_format_pos ( runner , pos , length ) : with _create_progress ( length , length_known = length != 0 , pos = pos ) as progress : result = progress . format_pos ( ) <MASK> assert result == f "" { pos } / { length } "" else : assert result == str ( pos )",if progress . length_known :,if runner . is_test :,92.93525924,FALSE,92.29
2208,"def optimize ( self , graph : Graph ) : MAX_TEXTURE_SIZE = config . WEBGL_MAX_TEXTURE_SIZE flag_changed = False for v in traverse . listup_variables ( graph ) : if not Placeholder . check_resolved ( v . size ) : continue height , width = TextureShape . get ( v ) if height < = MAX_TEXTURE_SIZE and width < = MAX_TEXTURE_SIZE : continue <MASK> flag_changed = True v . attributes . add ( SplitTarget ( ) ) return graph , flag_changed",if not v . has_attribute ( SplitTarget ) :,if v . attributes . get ( SplitTarget ( ) ) :,93.7037284,FALSE,92.29
2209,"def ant_map ( m ) : tmp = "" rows  %s \n cols  %s \n "" % ( len ( m ) , len ( m [ 0 ] ) ) players = { } for row in m : tmp + = "" m  "" for col in row : <MASK> tmp + = "" . "" elif col == BARRIER : tmp + = "" % "" elif col == FOOD : tmp + = "" * "" elif col == UNSEEN : tmp + = "" ? "" else : players [ col ] = True tmp + = chr ( col + 97 ) tmp + = "" \n "" tmp = ( "" players  %s \n "" % len ( players ) ) + tmp return tmp",if col == LAND :,if col == ANONYMOUS :,99.07712831,FALSE,98.3
2210,"def reset ( self ) : logger . debug ( "" Arctic.reset() "" ) with self . _lock : if self . __conn is not None : self . __conn . close ( ) self . __conn = None for _ , l in self . _library_cache . items ( ) : <MASK> logger . debug ( "" Library reset()  %s "" % l ) l . _reset ( ) # the existence of _reset() is not guaranteed/enforced, it also triggers re-auth","if hasattr ( l , ""_reset"" ) and callable ( l . _reset ) :",if l . _reset is not None :,88.19862063,FALSE,86.68
2211,"def add_cand_to_check ( cands ) : for cand in cands : x = cand . creator <MASK> continue if x not in fan_out : # `len(fan_out)` is in order to avoid comparing `x` heapq . heappush ( cand_funcs , ( - x . rank , len ( fan_out ) , x ) ) fan_out [ x ] + = 1",if x is None :,if x . rank < 0 :,89.83054157,FALSE,93.69
2212,"def on_task_modify ( self , task , config ) : for entry in task . entries : <MASK> size = entry [ "" torrent "" ] . size / 1024 / 1024 log . debug ( "" %s  size:  %s  MB "" % ( entry [ "" title "" ] , size ) ) entry [ "" content_size "" ] = size","if ""torrent"" in entry :","if ""torrent"" in entry and entry [ ""torrent_type"" ] == """,58.17686336,FALSE,84.36
2213,"def get_measurements ( self , pipeline , object_name , category ) : if self . get_categories ( pipeline , object_name ) == [ category ] : results = [ ] if self . do_corr_and_slope : if object_name == "" Image "" : results + = [ "" Correlation "" , "" Slope "" ] else : results + = [ "" Correlation "" ] if self . do_overlap : results + = [ "" Overlap "" , "" K "" ] if self . do_manders : results + = [ "" Manders "" ] if self . do_rwc : results + = [ "" RWC "" ] <MASK> results + = [ "" Costes "" ] return results return [ ]",if self . do_costes :,if self . do_costes :,100,TRUE,100
2214,"def create_root ( cls , site = None , title = "" Root "" , request = None , * * kwargs ) : if not site : site = Site . objects . get_current ( ) root_nodes = cls . objects . root_nodes ( ) . filter ( site = site ) if not root_nodes : article = Article ( ) revision = ArticleRevision ( title = title , * * kwargs ) <MASK> revision . set_from_request ( request ) article . add_revision ( revision , save = True ) article . save ( ) root = cls . objects . create ( site = site , article = article ) article . add_object_relation ( root ) else : root = root_nodes [ 0 ] return root",if request :,if request :,100,TRUE,100
2215,"def get ( self , key ) : filename = self . _get_filename ( key ) try : with open ( filename , "" rb "" ) as f : pickle_time = pickle . load ( f ) <MASK> return pickle . load ( f ) else : os . remove ( filename ) return None except ( IOError , OSError , pickle . PickleError ) : return None",if pickle_time == 0 or pickle_time >= time ( ) :,if pickle_time . st_age == time . time ( ) :,81.46085267,FALSE,89.75
2216,"def build_message ( self , options , target ) : message = multipart . MIMEMultipart ( ) for name , value in list ( options . items ( ) ) : <MASK> self . add_body ( message , value ) elif name == "" EMAIL_ATTACHMENT "" : self . add_attachment ( message , value ) else : # From, To, Subject, etc. self . set_option ( message , name , value , target ) return message","if name == ""EMAIL_BODY"" :","if name == ""CONTENT_TYPE"" :",98.41141637,FALSE,95.66
2217,"def updateVar ( name , data , mode = None ) : if mode : <MASK> core . config . globalVariables [ name ] . append ( data ) elif mode == "" add "" : core . config . globalVariables [ name ] . add ( data ) else : core . config . globalVariables [ name ] = data","if mode == ""append"" :","if mode == ""add"" :",98.03188766,FALSE,96.25
2218,"def insert_errors ( el , errors , form_id = None , form_index = None , error_class = "" error "" , error_creator = default_error_creator , ) : el = _find_form ( el , form_id = form_id , form_index = form_index ) for name , error in errors . items ( ) : <MASK> continue for error_el , message in _find_elements_for_name ( el , name , error ) : assert isinstance ( message , ( basestring , type ( None ) , ElementBase ) ) , ( "" Bad message:  %r "" % message ) _insert_error ( error_el , message , error_class , error_creator )",if error is None :,"if name . startswith ( ""_"" ) :",93.3203767,FALSE,93.92
2219,"def read ( self , item , recursive = False , sort = False ) : item = _normalize_path ( item ) if item in self . _store : <MASK> del self . _store [ item ] raise KeyError ( item ) return PathResult ( item , value = self . _store [ item ] ) else : return self . _read_dir ( item , recursive = recursive , sort = sort )",if item in self . _expire_time and self . _expire_time [ item ] < datetime . now ( ) :,if not recursive and not sort :,79.14343768,FALSE,74.73
2220,"def _stash_splitter ( states ) : keep , split = [ ] , [ ] if state_func is not None : for s in states : ns = state_func ( s ) if isinstance ( ns , SimState ) : split . append ( ns ) <MASK> split . extend ( ns ) else : split . append ( s ) if stash_func is not None : split = stash_func ( states ) if to_stash is not stash : keep = states return keep , split","elif isinstance ( ns , ( list , tuple , set ) ) :","elif isinstance ( ns , SimListState ) :",91.75526726,FALSE,92.22
2221,"def run ( self ) : while self . runflag : <MASK> with self . lock : tasks = list ( self . queue ) self . queue . clear ( ) while len ( tasks ) > 0 : pathname , remotepath = tasks . pop ( 0 ) self . bcloud_app . upload_page . add_bg_task ( pathname , remotepath ) self . last = time ( ) else : sleep ( 1 )",if time ( ) - self . last > 5 and self . qsize ( ) > 0 :,if self . last - self . last > self . timeout :,88.77456739,FALSE,85.5
2222,"def _append_patch ( self , patch_dir , patch_files ) : for patch in patch_files : <MASK> tmp = patch patch = { } for key in tmp . keys ( ) : patch [ os . path . join ( patch_dir , key ) ] = tmp [ key ] self . patches . append ( patch ) else : self . patches . append ( os . path . join ( patch_dir , patch ) )",if type ( patch ) is dict :,"if isinstance ( patch , dict ) :",93.31541073,FALSE,93.38
2223,"def __remote_port ( self ) : port = 22 if self . git_has_remote : m = re . match ( r "" ^(.*?)?@([^/:]*):?([0-9]+)? "" , self . git_remote . url ) if m : <MASK> port = m . group ( 3 ) return int ( port )",if m . group ( 3 ) :,if m . group ( 3 ) :,100,TRUE,100
2224,"def _create_or_get_helper ( self , infer_mode : Optional [ bool ] = None , * * kwargs ) - > Helper : # Prefer creating a new helper when at least one kwarg is specified. prefer_new = len ( kwargs ) > 0 kwargs . update ( infer_mode = infer_mode ) is_training = not infer_mode if infer_mode is not None else self . training helper = self . _train_helper if is_training else self . _infer_helper if prefer_new or helper is None : helper = self . create_helper ( * * kwargs ) <MASK> self . _train_helper = helper elif not is_training and self . _infer_helper is None : self . _infer_helper = helper return helper",if is_training and self . _train_helper is None :,if is_training and self . _train_helper is None :,75,TRUE,100
2225,"def flushChangeClassifications ( self , schedulerid , less_than = None ) : if less_than is not None : classifications = self . classifications . setdefault ( schedulerid , { } ) for changeid in list ( classifications ) : <MASK> del classifications [ changeid ] else : self . classifications [ schedulerid ] = { } return defer . succeed ( None )",if changeid < less_than :,if less_than . get ( changeid ) < less_than :,92.56065505,FALSE,88.16
2226,"def pid_from_name ( name ) : processes = [ ] for pid in os . listdir ( "" /proc "" ) : try : pid = int ( pid ) pname , cmdline = SunProcess . _name_args ( pid ) if name in pname : return pid <MASK> return pid except : pass raise ProcessException ( "" No process with such name:  %s "" % name )","if name in cmdline . split ( "" "" , 1 ) [ 0 ] :",if cmdline :,84.90778275,FALSE,82.32
2227,"def spew ( ) : seenUID = False start ( ) for part in query : <MASK> seenUID = True if part . type == "" body "" : yield self . spew_body ( part , id , msg , write , flush ) else : f = getattr ( self , "" spew_ "" + part . type ) yield f ( id , msg , write , flush ) if part is not query [ - 1 ] : space ( ) if uid and not seenUID : space ( ) yield self . spew_uid ( id , msg , write , flush ) finish ( ) flush ( )","if part . type == ""uid"" :","if part . name == ""uid"" :",97.06396271,FALSE,98
2228,"def rx ( ) : while True : rx_i = rep . recv ( ) <MASK> rep . send ( b "" done "" ) break rep . send ( b "" i "" )","if rx_i == b""1000"" :","if rx_i == b""0"" :",96.64370469,FALSE,94.41
2229,"def test_search_incorrect_base_exception_1 ( self ) : self . connection_1c . bind ( ) try : result = self . connection_1c . search ( "" o=nonexistant "" , "" (cn=*) "" , search_scope = SUBTREE , attributes = [ "" cn "" , "" sn "" ] ) <MASK> _ , result = self . connection_1c . get_response ( result ) self . fail ( "" exception not raised "" ) except LDAPNoSuchObjectResult : pass",if not self . connection_1c . strategy . sync :,if not self . connection_1c . bind ( ) :,66.59760322,FALSE,95.87
2230,"def value_from_datadict ( self , data , files , prefix ) : count = int ( data [ "" %s -count "" % prefix ] ) values_with_indexes = [ ] for i in range ( 0 , count ) : <MASK> continue values_with_indexes . append ( ( int ( data [ "" %s - %d -order "" % ( prefix , i ) ] ) , self . child_block . value_from_datadict ( data , files , "" %s - %d -value "" % ( prefix , i ) ) , ) ) values_with_indexes . sort ( ) return [ v for ( i , v ) in values_with_indexes ]","if data [ ""%s-%d-deleted"" % ( prefix , i ) ] :",if i == count - 1 :,92.87074014,FALSE,89.36
2231,"def _ensure_header_written ( self , datasize ) : if not self . _headerwritten : if not self . _nchannels : raise Error ( "" # channels not specified "" ) if not self . _sampwidth : raise Error ( "" sample width not specified "" ) <MASK> raise Error ( "" sampling rate not specified "" ) self . _write_header ( datasize )",if not self . _framerate :,if not self . _samprate :,98.13966192,FALSE,96.78
2232,def wait_til_ready ( cls ) : while True : now = time . time ( ) next_iteration = now / / 1.0 + 1 <MASK> break else : await cls . _clock . run_til ( next_iteration ) await asyncio . sleep ( 1.0 ),if cls . connector . ready :,if cls . _clock . is_ready ( ) :,87.83810896,FALSE,85.94
2233,"def lookup_actions ( self , resp ) : actions = { } for action , conditions in self . actions . items ( ) : for condition , opts in conditions : for key , val in condition : <MASK> if resp . match ( key [ : - 1 ] , val ) : break else : if not resp . match ( key , val ) : break else : actions [ action ] = opts return actions","if key [ - 1 ] == ""!"" :","if key . endswith ( ""/"" ) :",93.24095264,FALSE,89.03
2234,"def close ( self , wait = True , abort = False ) : """"""Close the socket connection."""""" if not self . closed and not self . closing : self . closing = True self . server . _trigger_event ( "" disconnect "" , self . sid , run_async = False ) if not abort : self . send ( packet . Packet ( packet . CLOSE ) ) self . closed = True self . queue . put ( None ) <MASK> self . queue . join ( )",if wait :,if wait :,100,TRUE,100
2235,"def model_parse ( self ) : for name , submodel in self . model . named_modules ( ) : for op_type in SUPPORTED_OP_TYPE : <MASK> self . target_layer [ name ] = submodel self . already_pruned [ name ] = 0","if isinstance ( submodel , op_type ) :",if name in self . target_layer and op_type in self . target_layer [,79.35098265,FALSE,74.69
2236,"def pack_identifier ( self ) : """"""Return a combined identifier for the whole pack if this has more than one episode."""""" # Currently only supports ep mode if self . id_type == "" ep "" : <MASK> return "" S %02d E %02d -E %02d "" % ( self . season , self . episode , self . episode + self . episodes - 1 , ) else : return self . identifier else : return self . identifier",if self . episodes > 1 :,if self . episode < self . episodes :,72.22100717,FALSE,94.25
2237,"def on_data ( res ) : if terminate . is_set ( ) : return if args . strings and not args . no_content : if type ( res ) == tuple : f , v = res if type ( f ) == unicode : f = f . encode ( "" utf-8 "" ) <MASK> v = v . encode ( "" utf-8 "" ) self . success ( "" {} :  {} "" . format ( f , v ) ) elif not args . content_only : self . success ( res ) else : self . success ( res )",if type ( v ) == unicode :,if type ( v ) == unicode :,100,TRUE,100
2238,"def _enable_contours_changed ( self , value ) : """"""Turns on and off the contours."""""" if self . module_manager is None : return if value : self . actor . inputs = [ self . contour ] <MASK> self . actor . mapper . scalar_mode = "" use_cell_data "" else : self . actor . inputs = [ self . grid_plane ] self . actor . mapper . scalar_mode = "" default "" self . render ( )",if self . contour . filled_contours :,if self . grid_plane is None :,93.38421294,FALSE,94.16
2239,"def _apply_abs_paths ( data , script_dir ) : for flag_data in data . values ( ) : <MASK> continue default = flag_data . get ( "" default "" ) if ( not default or not isinstance ( default , six . string_types ) or os . path . sep not in default ) : continue abs_path = os . path . join ( script_dir , default ) if os . path . exists ( abs_path ) : flag_data [ "" default "" ] = abs_path","if not isinstance ( flag_data , dict ) :","if not flag_data . get ( ""enabled"" ) :",92.79575303,FALSE,92.57
2240,"def button_release ( self , mapper ) : self . pressed = False if self . waiting_task and self . active is None and not self . action : # In HoldModifier, button released before timeout mapper . cancel_task ( self . waiting_task ) self . waiting_task = None <MASK> self . normalaction . button_press ( mapper ) mapper . schedule ( 0.02 , self . normalaction . button_release ) elif self . active : # Released held button self . active . button_release ( mapper ) self . active = None",if self . normalaction :,if self . normalaction :,100,TRUE,100
2241,"def goToPrevMarkedHeadline ( self , event = None ) : """"""Select the next marked node."""""" c = self p = c . p if not p : return p . moveToThreadBack ( ) wrapped = False while 1 : if p and p . isMarked ( ) : break <MASK> p . moveToThreadBack ( ) elif wrapped : break else : wrapped = True p = c . rootPosition ( ) if not p : g . blue ( "" done "" ) c . treeSelectHelper ( p ) # Sets focus.",elif p :,elif p :,100,TRUE,100
2242,"def status ( self , name , error = "" No matching script logs found "" ) : with self . script_lock : if self . script_running and self . script_running [ 1 ] == name : return self . script_running [ 1 : ] <MASK> return self . script_last [ 1 : ] else : raise ValueError ( error )",elif self . script_last and self . script_last [ 1 ] == name :,elif self . script_last and self . script_last [ 1 ] == name :,100,TRUE,100
2243,"def _stderr_supports_color ( ) : try : if hasattr ( sys . stderr , "" isatty "" ) and sys . stderr . isatty ( ) : <MASK> curses . setupterm ( ) if curses . tigetnum ( "" colors "" ) > 0 : return True elif colorama : if sys . stderr is getattr ( colorama . initialise , "" wrapped_stderr "" , object ( ) ) : return True except Exception : # Very broad exception handling because it's always better to # fall back to non-colored logs than to break at startup. pass return False",if curses :,if curses . getupterm ( ) is not None :,96.39871835,FALSE,92.56
2244,"def main ( ) : configFilename = "" twitterbot.ini "" if sys . argv [ 1 : ] : configFilename = sys . argv [ 1 ] try : <MASK> raise Exception ( ) load_config ( configFilename ) except Exception as e : print ( "" Error while loading ini file  %s "" % ( configFilename ) , file = sys . stderr ) print ( e , file = sys . stderr ) print ( __doc__ , file = sys . stderr ) sys . exit ( 1 ) bot = TwitterBot ( configFilename ) return bot . run ( )",if not os . path . exists ( configFilename ) :,if not os . path . exists ( configFilename ) :,100,TRUE,100
2245,def safe_to_kill ( request ) : if os . path . exists ( DRAIN_FILE ) : with open ( DRAIN_FILE ) as f : dt = datetime . datetime . fromtimestamp ( float ( f . read ( ) ) ) delta = datetime . datetime . now ( ) - dt <MASK> return Response ( status_int = 200 ) else : return Response ( status_int = 400 ) else : return Response ( status_int = 400 ),if delta . seconds > 2 :,if delta < dt :,66.74838269,FALSE,94.29
2246,"def get_class_name ( item ) : class_name , module_name = None , None for parent in reversed ( item . listchain ( ) ) : <MASK> class_name = parent . name elif isinstance ( parent , pytest . Module ) : module_name = parent . module . __name__ break # heuristic: # - better to group gpu and task tests, since tests from those modules #   are likely to share caching more # - split up the rest by class name because slow tests tend to be in #   the same module if class_name and "" .tasks. "" not in module_name : return "" {} . {} "" . format ( module_name , class_name ) else : return module_name","if isinstance ( parent , pytest . Class ) :","if isinstance ( parent , pytest . Class ) :",100,TRUE,100
2247,"def getAllFitsLite ( ) : fits = eos . db . getFitListLite ( ) shipMap = { f . shipID : None for f in fits } for shipID in shipMap : ship = eos . db . getItem ( shipID ) <MASK> shipMap [ shipID ] = ( ship . name , ship . getShortName ( ) ) fitsToPurge = set ( ) for fit in fits : try : fit . shipName , fit . shipNameShort = shipMap [ fit . shipID ] except ( KeyError , TypeError ) : fitsToPurge . add ( fit ) for fit in fitsToPurge : fits . remove ( fit ) return fits",if ship is not None :,if not ship . isShown ( ) :,89.25455275,FALSE,93.96
2248,"def _process ( self , event_data ) : self . machine . callbacks ( self . machine . prepare_event , event_data ) _LOGGER . debug ( "" %s Executed machine preparation callbacks before conditions. "" , self . machine . name ) try : for trans in self . transitions [ event_data . state . name ] : event_data . transition = trans <MASK> event_data . result = True break except Exception as err : event_data . error = err raise finally : self . machine . callbacks ( self . machine . finalize_event , event_data ) _LOGGER . debug ( "" %s Executed machine finalize callbacks "" , self . machine . name ) return event_data . result",if trans . execute ( event_data ) :,if trans . is_valid ( ) :,87.28673127,FALSE,96.05
2249,"def fetch_comments ( self , force = False , limit = None ) : comments = [ ] if ( force is True ) or ( self . badges [ "" comments "" ] > 0 ) : query_params = { "" filter "" : "" commentCard,copyCommentCard "" } <MASK> query_params [ "" limit "" ] = limit comments = self . client . fetch_json ( "" /cards/ "" + self . id + "" /actions "" , query_params = query_params ) return sorted ( comments , key = lambda comment : comment [ "" date "" ] ) return comments",if limit is not None :,if limit is not None :,100,TRUE,100
2250,"def get_changed ( self ) : if self . _is_expression ( ) : result = self . _get_node_text ( self . ast ) if result == self . source : return None return result else : collector = codeanalyze . ChangeCollector ( self . source ) last_end = - 1 for match in self . matches : start , end = match . get_region ( ) if start < last_end : <MASK> continue last_end = end replacement = self . _get_matched_text ( match ) collector . add_change ( start , end , replacement ) return collector . get_changed ( )",if not self . _is_expression ( ) :,if end == last_end :,93.3992441,FALSE,92.47
2251,"def _replace_home ( x ) : if xp . ON_WINDOWS : home = ( builtins . __xonsh__ . env [ "" HOMEDRIVE "" ] + builtins . __xonsh__ . env [ "" HOMEPATH "" ] [ 0 ] ) <MASK> x = x . replace ( home , "" ~ "" , 1 ) if builtins . __xonsh__ . env . get ( "" FORCE_POSIX_PATHS "" ) : x = x . replace ( os . sep , os . altsep ) return x else : home = builtins . __xonsh__ . env [ "" HOME "" ] <MASK> x = x . replace ( home , "" ~ "" , 1 ) return x",if x . startswith ( home ) :,if home :,68.27650862,FALSE,90.31
2252,"def project_review ( plans ) : for plan in plans : print ( "" Inspecting  {}  plan "" . format ( plan ) ) branches = get_branches_from_plan ( plan ) for branch in branches : build_results = get_results_from_branch ( branch ) for build in build_results : build_key = build . get ( "" buildResultKey "" ) or None print ( "" Inspecting build -  {} "" . format ( build_key ) ) <MASK> for status in STATUS_CLEANED_RESULTS : remove_build_result ( build_key = build_key , status = status )",if build_key :,if build_key :,100,TRUE,100
2253,"def _check_for_batch_clashes ( xs ) : """"""Check that batch names do not overlap with sample names."""""" names = set ( [ x [ "" description "" ] for x in xs ] ) dups = set ( [ ] ) for x in xs : batches = tz . get_in ( ( "" metadata "" , "" batch "" ) , x ) if batches : <MASK> batches = [ batches ] for batch in batches : if batch in names : dups . add ( batch ) if len ( dups ) > 0 : raise ValueError ( "" Batch names must be unique from sample descriptions. \n "" "" Clashing batch names:  %s "" % sorted ( list ( dups ) ) )","if not isinstance ( batches , ( list , tuple ) ) :","if not isinstance ( batches , list ) :",95.24249474,FALSE,95.96
2254,"def _check_signal ( self ) : """"""Checks if a signal was received and issues a message."""""" proc_signal = getattr ( self . proc , "" signal "" , None ) if proc_signal is None : return sig , core = proc_signal sig_str = SIGNAL_MESSAGES . get ( sig ) if sig_str : if core : sig_str + = ""  (core dumped) "" print ( sig_str , file = sys . stderr ) <MASK> self . errors + = sig_str + "" \n """,if self . errors is not None :,if self . errors :,93.42006802,FALSE,96.22
2255,"def loadLabelFile ( self , labelpath ) : labeldict = { } if not os . path . exists ( labelpath ) : f = open ( labelpath , "" w "" , encoding = "" utf-8 "" ) else : with open ( labelpath , "" r "" , encoding = "" utf-8 "" ) as f : data = f . readlines ( ) for each in data : file , label = each . split ( "" \t "" ) <MASK> label = label . replace ( "" false "" , "" False "" ) label = label . replace ( "" true "" , "" True "" ) labeldict [ file ] = eval ( label ) else : labeldict [ file ] = [ ] return labeldict",if label :,"if ""false"" in label :",85.13495293,FALSE,95.99
2256,"def exists_col_to_many ( self , select_columns : List [ str ] ) - > bool : for column in select_columns : <MASK> root_relation = get_column_root_relation ( column ) if self . is_relation_many_to_many ( root_relation ) or self . is_relation_one_to_many ( root_relation ) : return True return False",if is_column_dotted ( column ) :,"if column . name == ""relation"" :",91.77457464,FALSE,89.96
2257,"def check_sequence_matches ( seq , template ) : i = 0 for pattern in template : <MASK> pattern = { pattern } got = set ( seq [ i : i + len ( pattern ) ] ) assert got == pattern i + = len ( got )","if not isinstance ( pattern , set ) :","if not isinstance ( pattern , dict ) :",96.55591194,FALSE,95.81
2258,"def load_modules ( to_load , load , attr , modules_dict , excluded_aliases , loading_message = None ) : if loading_message : print ( loading_message ) for name in to_load : module = load ( name ) if module is None or not hasattr ( module , attr ) : continue cls = getattr ( module , attr ) if hasattr ( cls , "" initialize "" ) and not cls . initialize ( ) : continue <MASK> for alias in module . aliases ( ) : if alias not in excluded_aliases : modules_dict [ alias ] = module else : modules_dict [ name ] = module if loading_message : print ( )","if hasattr ( module , ""aliases"" ) :","if hasattr ( module , ""aliases"" ) :",100,TRUE,100
2259,"def result ( ) : # ""global"" does not work here... R , V = rays , virtual_rays if V is not None : <MASK> V = normalize_rays ( V , lattice ) if check : R = PointCollection ( V , lattice ) V = PointCollection ( V , lattice ) d = lattice . dimension ( ) if len ( V ) != d - R . dim ( ) or ( R + V ) . dim ( ) != d : raise ValueError ( "" virtual rays must be linearly  "" "" independent and with other rays span the ambient space. "" ) return RationalPolyhedralFan ( cones , R , lattice , is_complete , V )",if normalize :,if normalize :,75,TRUE,100
2260,"def communicate ( self , _input = None , _timeout = None ) - > Tuple [ bytes , bytes ] : if parse_args ( ) . print_commands : <MASK> print_stderr ( color_line ( "" =>  "" , 14 ) + ""   "" . join ( str ( arg ) for arg in self . args ) ) stdout , stderr = super ( ) . communicate ( _input , _timeout ) self . stdout_text = stdout . decode ( "" utf-8 "" ) if stdout else None self . stderr_text = stderr . decode ( "" utf-8 "" ) if stderr else None return stdout , stderr",if self . args != get_sudo_refresh_command ( ) :,if self . args :,95.84417861,FALSE,90.74
2261,"def convert ( data ) : result = [ ] for d in data : # noinspection PyCompatibility <MASK> result . append ( ( d [ 0 ] , None , d [ 1 ] ) ) elif isinstance ( d , basestring ) : result . append ( d ) return result","if isinstance ( d , tuple ) and len ( d ) == 2 :","if isinstance ( d , tuple ) and len ( d ) == 2 :",100,TRUE,100
2262,"def validate ( self , value ) : try : value = [ datetime . datetime . strptime ( range , "" % Y- % m- %d   % H: % M: % S "" ) for range in value . split ( ""  to  "" ) ] <MASK> return True else : return False except ValueError : return False",if ( len ( value ) == 2 ) and ( value [ 0 ] <= value [ 1 ] ) :,if len ( value ) == 1 :,53.57196737,FALSE,76.18
2263,"def rmdir ( dirname ) : if dirname [ - 1 ] == os . sep : dirname = dirname [ : - 1 ] if os . path . islink ( dirname ) : return # do not clear link - we can get out of dir for f in os . listdir ( dirname ) : if f in ( "" . "" , "" .. "" ) : continue path = dirname + os . sep + f <MASK> rmdir ( path ) else : os . unlink ( path ) os . rmdir ( dirname )",if os . path . isdir ( path ) :,if os . path . isdir ( path ) :,100,TRUE,100
2264,"def onCompletion ( self , text ) : res = [ ] for l in text . split ( "" \n "" ) : if not l : continue l = l . split ( "" : "" ) <MASK> continue res . append ( [ l [ 0 ] . strip ( ) , l [ 1 ] . strip ( ) ] ) self . panel . setSlides ( res )",if len ( l ) != 2 :,if len ( l ) != 2 :,100,TRUE,100
2265,"def pytest_collection_modifyitems ( items ) : for item in items : <MASK> if "" stage "" not in item . keywords : item . add_marker ( pytest . mark . stage ( "" unit "" ) ) if "" init "" not in item . keywords : item . add_marker ( pytest . mark . init ( rng_seed = 123 ) )","if item . nodeid . startswith ( ""tests/infer"" ) :","if isinstance ( item , pytest . mark . Mark ) :",91.72317046,FALSE,86.13
2266,"def build_message ( self , options , target ) : message = multipart . MIMEMultipart ( ) for name , value in list ( options . items ( ) ) : if name == "" EMAIL_BODY "" : self . add_body ( message , value ) <MASK> self . add_attachment ( message , value ) else : # From, To, Subject, etc. self . set_option ( message , name , value , target ) return message","elif name == ""EMAIL_ATTACHMENT"" :","elif name == ""EMAIL_CONTENT"" :",98.41141637,FALSE,97.44
2267,def extend_with_zeroes ( b ) : try : for x in b : x = to_constant ( x ) <MASK> yield ( x ) else : yield ( 0 ) for _ in range ( 32 ) : yield ( 0 ) except Exception as e : return,"if isinstance ( x , int ) :",if x != 0 :,62.51373349,FALSE,87.75
2268,"def _start_cluster ( * , cleanup_atexit = True ) : global _default_cluster if _default_cluster is None : cluster_addr = os . environ . get ( "" EDGEDB_TEST_CLUSTER_ADDR "" ) <MASK> conn_spec = json . loads ( cluster_addr ) _default_cluster = edgedb_cluster . RunningCluster ( * * conn_spec ) else : data_dir = os . environ . get ( "" EDGEDB_TEST_DATA_DIR "" ) _default_cluster = _init_cluster ( data_dir = data_dir , cleanup_atexit = cleanup_atexit ) return _default_cluster",if cluster_addr :,if cluster_addr :,100,TRUE,100
2269,"def preprocess_raw_enwik9 ( input_filename , output_filename ) : with open ( input_filename , "" r "" ) as f1 : with open ( output_filename , "" w "" ) as f2 : while True : line = f1 . readline ( ) <MASK> break line = list ( enwik9_norm_transform ( [ line ] ) ) [ 0 ] if line != ""   "" and line != "" "" : if line [ 0 ] == ""   "" : line = line [ 1 : ] f2 . writelines ( line + "" \n "" )",if not line :,if not line :,100,TRUE,100
2270,"def is_entirely_italic ( line ) : style = subs . styles . get ( line . style , SSAStyle . DEFAULT_STYLE ) for fragment , sty in parse_tags ( line . text , style , subs . styles ) : fragment = fragment . replace ( r "" \ h "" , ""   "" ) fragment = fragment . replace ( r "" \ n "" , "" \n "" ) fragment = fragment . replace ( r "" \ N "" , "" \n "" ) <MASK> return False return True",if not sty . italic and fragment and not fragment . isspace ( ) :,"if fragment . startswith ( ""italic"" ) :",88.64399492,FALSE,89.21
2271,def __get_all_nodes ( self ) : nodes = [ ] next_level = [ self . __tree . get_root ( ) ] while len ( next_level ) != 0 : cur_level = next_level nodes + = next_level next_level = [ ] for cur_node in cur_level : children = cur_node . get_children ( ) <MASK> next_level + = children return nodes,if children is not None :,if children :,93.49694288,FALSE,95.27
2272,"def _openvpn_stdout ( self ) : while True : line = self . process . stdout . readline ( ) if not line : <MASK> return time . sleep ( 0.05 ) continue yield try : self . server . output . push_output ( line ) except : logger . exception ( "" Failed to push vpn output "" , "" server "" , server_id = self . server . id , ) yield",if self . process . poll ( ) is not None or self . is_interrupted ( ) :,if self . server . output . is_available ( ) :,61.06327582,FALSE,85.83
2273,"def payment_received_handler ( event ) : if isinstance ( event . message . action , types . MessageActionPaymentSentMe ) : payment : types . MessageActionPaymentSentMe = event . message . action # do something after payment was received if payment . payload . decode ( "" UTF-8 "" ) == "" product A "" : await bot . send_message ( event . message . from_id , "" Thank you for buying product A! "" ) <MASK> await bot . send_message ( event . message . from_id , "" Thank you for buying product B! "" ) raise events . StopPropagation","elif payment . payload . decode ( ""UTF-8"" ) == ""product B"" :","elif payment . payload . decode ( ""UTF-8"" ) == ""product B""",98.87620111,FALSE,98.08
2274,"def spaces_after ( token , prev , next , min = - 1 , max = - 1 , min_desc = None , max_desc = None ) : if next is not None and token . end_mark . line == next . start_mark . line : spaces = next . start_mark . pointer - token . end_mark . pointer <MASK> return LintProblem ( token . start_mark . line + 1 , next . start_mark . column , max_desc ) elif min != - 1 and spaces < min : return LintProblem ( token . start_mark . line + 1 , next . start_mark . column + 1 , min_desc )",if max != - 1 and spaces > max :,if max != - 1 and spaces > max :,100,TRUE,100
2275,"def seek_to_block ( self , pos ) : baseofs = 0 ofs = 0 for b in self . blocks : <MASK> self . current_block = b break baseofs + = b . compressed_size ofs + = b . uncompressed_size else : self . current_block = None self . current_stream = BytesIO ( b "" "" ) return self . current_block_start = ofs self . stream . seek ( self . basepos + baseofs ) buf = BytesIO ( self . stream . read ( self . current_block . compressed_size ) ) self . current_stream = self . current_block . decompress ( buf )",if ofs + b . uncompressed_size > pos :,if b . offset == pos :,80.52477976,FALSE,93.66
2276,"def rewrite_hunks ( hunks ) : # type: (List[Hunk]) -> Iterator[Hunk] # Assumes `hunks` are sorted, and from the same file deltas = ( hunk . b_length - hunk . a_length for hunk in hunks ) offsets = accumulate ( deltas , initial = 0 ) for hunk , offset in zip ( hunks , offsets ) : new_b = hunk . a_start + offset if hunk_of_additions_only ( hunk ) : new_b + = 1 <MASK> new_b - = 1 yield hunk . _replace ( b_start = new_b )",elif hunk_of_removals_only ( hunk ) :,elif hunk_of_removals_only ( hunk ) :,75,TRUE,100
2277,"def do_query ( data , q ) : ret = [ ] if not q : return ret qkey = q [ 0 ] for key , value in iterate ( data ) : <MASK> if key == qkey : ret . append ( value ) elif is_iterable ( value ) : ret . extend ( do_query ( value , q ) ) else : if not is_iterable ( value ) : continue if key == qkey : ret . extend ( do_query ( value , q [ 1 : ] ) ) else : ret . extend ( do_query ( value , q ) ) return ret",if len ( q ) == 1 :,"if isinstance ( value , ( list , tuple ) ) :",93.71261758,FALSE,91.61
2278,"def get_url ( token , base_url ) : """"""Parse an <url> token."""""" if token . type == "" url "" : return _get_url_tuple ( token . value , base_url ) elif token . type == "" function "" : if token . name == "" attr "" : return check_attr_function ( token , "" url "" ) <MASK> # Ignore url modifiers # See https://drafts.csswg.org/css-values-3/#urls return _get_url_tuple ( token . arguments [ 0 ] . value , base_url )","elif token . name == ""url"" and len ( token . arguments ) in ( 1 , 2 ) :","elif token . name == ""url"" :",84.92521467,FALSE,89.79
2279,"def read ( self , count ) : if self . closed : return self . upstream . read ( count ) try : while len ( self . upstream ) < count : <MASK> with self . buf_in : self . transport . downstream_recv ( self . buf_in ) else : break return self . upstream . read ( count ) except : logger . debug ( traceback . format_exc ( ) )",if self . buf_in or self . _poll_read ( 10 ) :,if self . buf_in :,65.0561666,FALSE,87.69
2280,"def get_timestamp_for_block ( self , block_hash : HexBytes , max_tries : Optional [ int ] = 10 ) - > int : counter = 0 block : AttributeDict = None if block_hash in self . _block_cache . keys ( ) : block = self . _block_cache . get ( block_hash ) else : while block is None : <MASK> raise ValueError ( f "" Block hash  { block_hash . hex ( ) }  does not exist. "" ) counter + = 1 block = self . _block_cache . get ( block_hash ) await asyncio . sleep ( 0.5 ) return block . get ( "" timestamp "" )",if counter == max_tries :,if counter >= max_tries :,98.8782304,FALSE,98.22
2281,"def reader ( ) : batch_out = [ ] for video_name in self . video_list : video_idx = self . video_list . index ( video_name ) video_feat = self . load_file ( video_name ) batch_out . append ( ( video_feat , video_idx ) ) <MASK> yield batch_out batch_out = [ ]",if len ( batch_out ) == self . batch_size :,if len ( batch_out ) == self . batch_size :,100,TRUE,100
2282,"def cleanup ( ) : gscript . message ( _ ( "" Erasing temporary files... "" ) ) for temp_map , maptype in temp_maps : <MASK> gscript . run_command ( "" g.remove "" , flags = "" f "" , type = maptype , name = temp_map , quiet = True )","if gscript . find_file ( temp_map , element = maptype ) [ ""name"" ] :","if not gscript . run_command ( ""g.add"" , name = temp_",65.38237113,FALSE,77.77
2283,"def run ( self ) : while True : try : with DelayedKeyboardInterrupt ( ) : raw_inputs = self . _parent_task_queue . get ( ) <MASK> self . _rq . put ( raw_inputs , block = True ) break if self . _flow_type == BATCH : self . _rq . put ( raw_inputs , block = True ) elif self . _flow_type == REALTIME : try : self . _rq . put ( raw_inputs , block = False ) except : pass except KeyboardInterrupt : continue",if self . _has_stop_signal ( raw_inputs ) :,if self . _flow_type == SKIP :,67.9290277,FALSE,90.78
2284,"def handle_sent ( self , elt ) : sent = [ ] for child in elt : if child . tag in ( "" mw "" , "" hi "" , "" corr "" , "" trunc "" ) : sent + = [ self . handle_word ( w ) for w in child ] <MASK> sent . append ( self . handle_word ( child ) ) elif child . tag not in self . tags_to_ignore : raise ValueError ( "" Unexpected element  %s "" % child . tag ) return BNCSentence ( elt . attrib [ "" n "" ] , sent )","elif child . tag in ( ""w"" , ""c"" ) :","elif child . tag == ""w"" :",93.24508974,FALSE,92.09
2285,"def bind_subscribers_to_graphql_type ( self , graphql_type ) : for field , subscriber in self . _subscribers . items ( ) : <MASK> raise ValueError ( "" Field  %s  is not defined on type  %s "" % ( field , self . name ) ) graphql_type . fields [ field ] . subscribe = subscriber",if field not in graphql_type . fields :,if field not in graphql_type . fields :,100,TRUE,100
2286,"def _get_from_json ( self , * , name , version ) : url = urljoin ( self . url , posixpath . join ( name , str ( version ) , "" json "" ) ) async with aiohttp_session ( auth = self . auth ) as session : async with session . get ( url ) as response : <MASK> raise PackageNotFoundError ( package = name , url = url ) response . raise_for_status ( ) response = await response . json ( ) dist = response [ "" info "" ] [ "" requires_dist "" ] or [ ] if dist : return dist # If no requires_dist then package metadata can be broken. # Let's check distribution files. return await self . _get_from_files ( response [ "" urls "" ] )",if response . status == 404 :,if response . status_code != 404 :,98.75780486,FALSE,97.24
2287,"def is_active ( self ) : if not self . pk : log_level = get_setting ( "" LOG_MISSING_SWITCHES "" ) if log_level : logger . log ( log_level , "" Switch  %s  not found "" , self . name ) <MASK> switch , _created = Switch . objects . get_or_create ( name = self . name , defaults = { "" active "" : get_setting ( "" SWITCH_DEFAULT "" ) } ) cache = get_cache ( ) cache . set ( self . _cache_key ( self . name ) , switch ) return get_setting ( "" SWITCH_DEFAULT "" ) return self . active","if get_setting ( ""CREATE_MISSING_SWITCHES"" ) :",if self . _cache_key ( self . name ) not in Switch . objects . all,90.7821356,FALSE,87.84
2288,"def add_requirements ( self , requirements ) : if self . _legacy : self . _legacy . add_requirements ( requirements ) else : run_requires = self . _data . setdefault ( "" run_requires "" , [ ] ) always = None for entry in run_requires : <MASK> always = entry break if always is None : always = { "" requires "" : requirements } run_requires . insert ( 0 , always ) else : rset = set ( always [ "" requires "" ] ) | set ( requirements ) always [ "" requires "" ] = sorted ( rset )","if ""environment"" not in entry and ""extra"" not in entry :","if entry [ ""requires"" ] == requirements :",76.63458364,FALSE,89.34
2289,"def display_failures_for_single_test ( result : TestResult ) - > None : """"""Display a failure for a single method / endpoint."""""" display_subsection ( result ) checks = _get_unique_failures ( result . checks ) for idx , check in enumerate ( checks , 1 ) : message : Optional [ str ] if check . message : message = f "" { idx } .  { check . message } "" else : message = None example = cast ( Case , check . example ) # filtered in `_get_unique_failures` display_example ( example , check . name , message , result . seed ) # Display every time except the last check <MASK> click . echo ( "" \n "" )",if idx != len ( checks ) :,if idx == 0 :,97.44612866,FALSE,95.39
2290,"def __call__ ( self , frame : FrameType , event : str , arg : Any ) - > "" CallTracer "" : code = frame . f_code if ( event not in SUPPORTED_EVENTS or code . co_name == "" trace_types "" or self . should_trace and not self . should_trace ( code ) ) : return self try : if event == EVENT_CALL : self . handle_call ( frame ) <MASK> self . handle_return ( frame , arg ) else : logger . error ( "" Cannot handle event  %s "" , event ) except Exception : logger . exception ( "" Failed collecting trace "" ) return self",elif event == EVENT_RETURN :,elif event == EVENT_RETURN :,100,TRUE,100
2291,"def get_maps ( test ) : pages = set ( ) for addr in test [ "" pre "" ] [ "" memory "" ] . keys ( ) : pages . add ( addr >> 12 ) for addr in test [ "" pos "" ] [ "" memory "" ] . keys ( ) : pages . add ( addr >> 12 ) maps = [ ] for p in sorted ( pages ) : <MASK> maps [ - 1 ] = ( maps [ - 1 ] [ 0 ] , maps [ - 1 ] [ 1 ] + 0x1000 ) else : maps . append ( ( p << 12 , 0x1000 ) ) return maps",if len ( maps ) > 0 and maps [ - 1 ] [ 0 ] + maps [ - 1 ] [ 1 ] == p << 12 :,if p == 0 :,80.34707849,FALSE,79.27
2292,"def process_rotate_aes_key ( self ) : if hasattr ( self . options , "" rotate_aes_key "" ) and isinstance ( self . options . rotate_aes_key , six . string_types ) : <MASK> self . options . rotate_aes_key = True elif self . options . rotate_aes_key . lower ( ) == "" false "" : self . options . rotate_aes_key = False","if self . options . rotate_aes_key . lower ( ) == ""true"" :","if self . options . rotate_aes_key . lower ( ) == ""true"" :",100,TRUE,100
2293,"def apply_figure ( self , figure ) : super ( legend_text_legend , self ) . apply_figure ( figure ) properties = self . properties . copy ( ) with suppress ( KeyError ) : del properties [ "" margin "" ] with suppress ( KeyError ) : texts = figure . _themeable [ "" legend_text_legend "" ] for text in texts : <MASK> # textarea text = text . _text text . set ( * * properties )","if not hasattr ( text , ""_x"" ) :","if isinstance ( text , Text ) :",93.60673371,FALSE,91.06
2294,"def tearDown ( self ) : for i in range ( len ( self . tree ) - 1 , - 1 , - 1 ) : s = os . path . join ( self . root , self . tree [ i ] ) <MASK> os . rmdir ( s ) else : os . remove ( s ) os . rmdir ( self . root )","if not ""."" in s :",if os . path . isdir ( s ) :,80.67347953,FALSE,88.16
2295,"def _get_id ( self , type , id ) : fields = id . split ( "" : "" ) if len ( fields ) > = 3 : if type != fields [ - 2 ] : logger . warning ( "" Expected id of type  %s  but found type  %s   %s "" , type , fields [ - 2 ] , id ) return fields [ - 1 ] fields = id . split ( "" / "" ) if len ( fields ) > = 3 : itype = fields [ - 2 ] <MASK> logger . warning ( "" Expected id of type  %s  but found type  %s   %s "" , type , itype , id ) return fields [ - 1 ] . split ( "" ? "" ) [ 0 ] return id",if type != itype :,if type != itype :,100,TRUE,100
2296,"def candidates ( ) - > Generator [ "" Symbol "" , None , None ] : s = self if Symbol . debug_lookup : Symbol . debug_print ( "" searching in self: "" ) print ( s . to_string ( Symbol . debug_indent + 1 ) , end = "" "" ) while True : <MASK> yield s if recurseInAnon : yield from s . children_recurse_anon else : yield from s . _children if s . siblingAbove is None : break s = s . siblingAbove if Symbol . debug_lookup : Symbol . debug_print ( "" searching in sibling: "" ) print ( s . to_string ( Symbol . debug_indent + 1 ) , end = "" "" )",if matchSelf :,if s . is_anon :,98.04497758,FALSE,95.58
2297,"def records ( account_id ) : """"""Fetch locks data"""""" s = boto3 . Session ( ) table = s . resource ( "" dynamodb "" ) . Table ( "" Sphere11.Dev.ResourceLocks "" ) results = table . scan ( ) for r in results [ "" Items "" ] : if "" LockDate "" in r : r [ "" LockDate "" ] = datetime . fromtimestamp ( r [ "" LockDate "" ] ) <MASK> r [ "" RevisionDate "" ] = datetime . fromtimestamp ( r [ "" RevisionDate "" ] ) print ( tabulate . tabulate ( results [ "" Items "" ] , headers = "" keys "" , tablefmt = "" fancy_grid "" ) )","if ""RevisionDate"" in r :","if ""RevisionDate"" in r :",100,TRUE,100
2298,"def _handle_errors ( errors ) : """"""Log out and possibly reraise errors during import."""""" if not errors : return log_all = True # pylint: disable=unused-variable err_msg = "" T2T: skipped importing  {num_missing}  data_generators modules. "" print ( err_msg . format ( num_missing = len ( errors ) ) ) for module , err in errors : err_str = str ( err ) if log_all : print ( "" Did not import module:  %s ; Cause:  %s "" % ( module , err_str ) ) <MASK> print ( "" From module  %s "" % module ) raise err","if not _is_import_err_msg ( err_str , module ) :",elif module not in data_generators :,95.60266796,FALSE,88.19
2299,"def find_needle ( self , tree , focused = None ) : if isinstance ( tree , list ) : for el in tree : res = self . find_needle ( el , focused ) <MASK> return res elif isinstance ( tree , dict ) : nodes = tree . get ( "" nodes "" , [ ] ) + tree . get ( "" floating_nodes "" , [ ] ) if focused : for node in nodes : if node [ "" id "" ] == focused [ "" id "" ] : return tree elif tree [ "" focused "" ] : return tree return self . find_needle ( nodes , focused ) return { }",if res :,if res :,100,TRUE,100
2300,"def available_datasets ( self ) : """"""Automatically determine datasets provided by this file"""""" res = self . resolution coordinates = [ "" pixel_longitude "" , "" pixel_latitude "" ] for var_name , val in self . file_content . items ( ) : if isinstance ( val , netCDF4 . Variable ) : ds_info = { "" file_type "" : self . filetype_info [ "" file_type "" ] , "" resolution "" : res , } <MASK> ds_info [ "" coordinates "" ] = coordinates yield DatasetID ( name = var_name , resolution = res ) , ds_info",if not self . is_geo :,if coordinates :,85.97462425,FALSE,94.3
2301,"def get_subkeys ( self , key ) : # TODO: once we revamp the registry emulation, # make this better parent_path = key . get_path ( ) subkeys = [ ] for k in self . keys : test_path = k . get_path ( ) <MASK> sub = test_path [ len ( parent_path ) : ] if sub . startswith ( "" \\ "" ) : sub = sub [ 1 : ] end_slash = sub . find ( "" \\ "" ) if end_slash > = 0 : sub = sub [ : end_slash ] if not sub : continue subkeys . append ( sub ) return subkeys",if test_path . lower ( ) . startswith ( parent_path . lower ( ) ) :,if test_path . startswith ( parent_path ) :,70.86459789,FALSE,93.08
2302,"def default ( self , o ) : try : if type ( o ) == datetime . datetime : return str ( o ) else : # remove unwanted attributes from the provider object during conversion to json if hasattr ( o , "" profile "" ) : del o . profile <MASK> del o . credentials if hasattr ( o , "" metadata_path "" ) : del o . metadata_path if hasattr ( o , "" services_config "" ) : del o . services_config return vars ( o ) except Exception as e : return str ( o )","if hasattr ( o , ""credentials"" ) :","if hasattr ( o , ""credentials"" ) :",75,TRUE,100
2303,"def submit ( self , fn , * args , * * kwargs ) : with self . _shutdown_lock : <MASK> raise RuntimeError ( "" cannot schedule new futures after shutdown "" ) f = _base . Future ( ) w = _WorkItem ( f , fn , args , kwargs ) self . _work_queue . put ( w ) self . _adjust_thread_count ( ) return f",if self . _shutdown :,if self . _shutdown :,100,TRUE,100
2304,"def __viewerKeyPress ( viewer , event ) : view = viewer . view ( ) if not isinstance ( view , GafferSceneUI . SceneView ) : return False if event == __editSourceKeyPress : selectedPath = __sceneViewSelectedPath ( view ) <MASK> __editSourceNode ( view . getContext ( ) , view [ "" in "" ] , selectedPath ) return True elif event == __editTweaksKeyPress : selectedPath = __sceneViewSelectedPath ( view ) <MASK> __editTweaksNode ( view . getContext ( ) , view [ "" in "" ] , selectedPath ) return True",if selectedPath is not None :,if selectedPath :,90.67685539,FALSE,92.03
2305,"def _split_to_option_groups_and_paths ( self , args ) : opt_groups = [ ] current = [ ] for arg in args : <MASK> opts = self . _arg_parser . parse_args ( current ) [ 0 ] opt_groups . append ( opts ) current = [ ] else : current . append ( arg ) if opt_groups : return opt_groups , current raise ValueError ( "" Nothing to split "" )","if arg . replace ( ""-"" , """" ) == """" and len ( arg ) >= 3 :","if arg == ""-"" :",75.47933701,FALSE,81.08
2306,"def _on_change ( self ) : changed = False self . save ( ) for key , value in self . data . items ( ) : if isinstance ( value , bool ) : <MASK> changed = True break if isinstance ( value , int ) : if value != 1 : changed = True break elif value is None : continue elif len ( value ) != 0 : changed = True break self . _reset_button . disabled = not changed",if value :,if value :,100,TRUE,100
2307,"def wait_for_child ( pid , timeout = 1.0 ) : deadline = mitogen . core . now ( ) + timeout while timeout < mitogen . core . now ( ) : try : target_pid , status = os . waitpid ( pid , os . WNOHANG ) if target_pid == pid : return except OSError : e = sys . exc_info ( ) [ 1 ] <MASK> return time . sleep ( 0.05 ) assert False , "" wait_for_child() timed out """,if e . args [ 0 ] == errno . ECHILD :,if e . errno == errno . EINTR :,92.89735132,FALSE,92.72
2308,"def _get_os_version_lsb_release ( ) : try : output = subprocess . check_output ( "" lsb_release -sri "" , shell = True ) lines = output . strip ( ) . split ( ) name , version = lines <MASK> version = "" "" return name , version except : return _get_os_version_uname ( )","if version . lower ( ) == ""rolling"" :","if version == ""0"" :",76.56329971,FALSE,90.19
2309,"def _check_snapshot_status_healthy ( self , snapshot_uuid ) : status = "" "" try : while True : status , locked = self . _get_snapshot_status ( snapshot_uuid ) <MASK> break eventlet . sleep ( 2 ) except Exception : with excutils . save_and_reraise_exception ( ) : LOG . exception ( "" Failed to get snapshot status. [ %s ] "" , snapshot_uuid ) LOG . debug ( "" Lun [ %(snapshot)s ], status [ %(status)s ]. "" , { "" snapshot "" : snapshot_uuid , "" status "" : status } , ) return status == "" Healthy """,if not locked :,"if status == ""Alive"" :",91.89762349,FALSE,94.73
2310,"def CountButtons ( self ) : """"""Returns the number of visible buttons in the docked pane."""""" n = 0 if self . HasCaption ( ) or self . HasCaptionLeft ( ) : if isinstance ( wx . GetTopLevelParent ( self . window ) , AuiFloatingFrame ) : return 1 <MASK> n + = 1 if self . HasMaximizeButton ( ) : n + = 1 if self . HasMinimizeButton ( ) : n + = 1 if self . HasPinButton ( ) : n + = 1 return n",if self . HasCloseButton ( ) :,if self . HasMinimizeButton ( ) :,98.67233756,FALSE,97.49
2311,"def _url_encode_impl ( obj , charset , encode_keys , sort , key ) : from . datastructures import iter_multi_items iterable = iter_multi_items ( obj ) if sort : iterable = sorted ( iterable , key = key ) for key , value in iterable : <MASK> continue if not isinstance ( key , bytes ) : key = text_type ( key ) . encode ( charset ) if not isinstance ( value , bytes ) : value = text_type ( value ) . encode ( charset ) yield _fast_url_quote_plus ( key ) + "" = "" + _fast_url_quote_plus ( value )",if value is None :,if not encode_keys :,83.57300728,FALSE,95.99
2312,"def get_response ( self , exc_fmt = None ) : self . callback = None if __debug__ : self . parent . _log ( 3 , "" %s : %s .ready.wait "" % ( self . name , self . tag ) ) self . ready . wait ( ) if self . aborted is not None : typ , val = self . aborted <MASK> exc_fmt = "" %s  -  %% s "" % typ raise typ ( exc_fmt % str ( val ) ) return self . response",if exc_fmt is None :,if exc_fmt is None :,100,TRUE,100
2313,"def extract_items ( self ) : responses = self . fetch ( ) items = [ ] for response in responses : page_key = response . meta . get ( "" page_key "" ) or response . url item = { "" key "" : page_key , "" items "" : None , "" templates "" : None } extracted_items = [ dict ( i ) for i in self . spider . parse ( response ) if not isinstance ( i , Request ) ] <MASK> item [ "" items "" ] = extracted_items item [ "" templates "" ] = [ i [ "" _template "" ] for i in extracted_items if i . get ( "" _template "" ) ] items . append ( item ) return items",if extracted_items :,if extracted_items :,100,TRUE,100
2314,"def fit_one ( self , x ) : for i , xi in x . items ( ) : if self . with_centering : self . median [ i ] . update ( xi ) <MASK> self . iqr [ i ] . update ( xi ) return self",if self . with_scaling :,elif self . with_iqr :,71.55197617,FALSE,91.13
2315,"def find_word_bounds ( self , text , index , allowed_chars ) : right = left = index done = False while not done : if left == 0 : done = True <MASK> left - = 1 else : done = True done = False while not done : if right == len ( text ) : done = True elif not self . word_boundary_char ( text [ right ] ) : right + = 1 else : done = True return left , right",elif not self . word_boundary_char ( text [ left - 1 ] ) :,elif allowed_chars and not self . word_boundary_char ( text [ left ],77.06564423,FALSE,93.06
2316,"def _validate_duplicate_detection_history_time_window ( namespace ) : if namespace . duplicate_detection_history_time_window : if iso8601pattern . match ( namespace . duplicate_detection_history_time_window ) : pass <MASK> pass else : raise CLIError ( "" --duplicate-detection-history-time-window Value Error :  {0}  value is not in ISO 8601 timespan / duration format. e.g. PT10M for duration of 10 min or 00:10:00 for duration of 10 min "" . format ( namespace . duplicate_detection_history_time_window ) )",elif timedeltapattern . match ( namespace . duplicate_detection_history_time_window ) :,elif durationpattern . match ( namespace . duplicate_detection_history_time_window ),71.71030128,FALSE,96.08
2317,"def get_subkeys ( self , key ) : # TODO: once we revamp the registry emulation, # make this better parent_path = key . get_path ( ) subkeys = [ ] for k in self . keys : test_path = k . get_path ( ) if test_path . lower ( ) . startswith ( parent_path . lower ( ) ) : sub = test_path [ len ( parent_path ) : ] if sub . startswith ( "" \\ "" ) : sub = sub [ 1 : ] end_slash = sub . find ( "" \\ "" ) if end_slash > = 0 : sub = sub [ : end_slash ] <MASK> continue subkeys . append ( sub ) return subkeys",if not sub :,"if sub . startswith ( ""\\"" ) :",72.75594336,FALSE,93.48
2318,"def generator ( self , data ) : <MASK> silent_vars = self . _get_silent_vars ( ) for task in data : for var , val in task . environment_variables ( ) : <MASK> if var in silent_vars : continue yield ( 0 , [ int ( task . UniqueProcessId ) , str ( task . ImageFileName ) , Address ( task . Peb . ProcessParameters . Environment ) , str ( var ) , str ( val ) , ] , )",if self . _config . SILENT :,if self . _is_silent_process ( task ) :,88.08913049,FALSE,83.5
2319,"def start_requests ( self ) : if self . fail_before_yield : 1 / 0 for s in range ( 100 ) : qargs = { "" total "" : 10 , "" seed "" : s } url = self . mockserver . url ( "" /follow? %s "" ) % urlencode ( qargs , doseq = 1 ) yield Request ( url , meta = { "" seed "" : s } ) <MASK> 2 / 0 assert self . seedsseen , "" All start requests consumed before any download happened """,if self . fail_yielding :,if self . fail_before_yield :,98.69454294,FALSE,95.84
2320,"def populateGridlines ( self ) : cTicks = self . getSystemCurve ( self . ticksId ) cGridlines = self . getSystemCurve ( self . gridlinesId ) cGridlines . clearPoints ( ) nTicks = cTicks . getNPoints ( ) for iTick in range ( nTicks ) : <MASK> p = cTicks . getPoint ( iTick ) cGridlines . addPoint ( p . getX ( ) , p . getY ( ) )",if self . hasGridlines and ( iTick % self . ticksPerGridline ) == 0 :,if cGridlines . getPoint ( iTick ) . isPoint ( ) :,82.99729465,FALSE,83.27
2321,"def handle_before_events ( request , event_list ) : if not event_list : return "" "" if not hasattr ( event_list , "" __iter__ "" ) : project = event_list . project event_list = [ event_list ] else : projects = set ( e . project for e in event_list ) <MASK> project = projects . pop ( ) else : project = None for plugin in plugins . for_project ( project ) : safe_execute ( plugin . before_events , request , event_list ) return "" """,if len ( projects ) == 1 :,if projects :,77.04216329,FALSE,93.19
2322,"def handle_parse_result ( self , ctx , opts , args ) : if self . name in opts : <MASK> self . _raise_exclusive_error ( ) if self . multiple and len ( set ( opts [ self . name ] ) ) > 1 : self . _raise_exclusive_error ( ) return super ( MutuallyExclusiveOption , self ) . handle_parse_result ( ctx , opts , args )",if self . mutually_exclusive . intersection ( opts ) :,if self . exclusive and len ( opts [ self . name ] ) > 1 :,89.79139623,FALSE,86.71
2323,"def current_word ( cursor_offset , line ) : """"""the object.attribute.attribute just before or under the cursor"""""" pos = cursor_offset start = pos end = pos word = None for m in current_word_re . finditer ( line ) : <MASK> start = m . start ( 1 ) end = m . end ( 1 ) word = m . group ( 1 ) if word is None : return None return LinePart ( start , end , word )",if m . start ( 1 ) < pos and m . end ( 1 ) >= pos :,if start is None :,77.33027833,FALSE,82.8
2324,"def query_to_script_path ( path , query ) : if path != "" * "" : script = os . path . join ( path , query . split ( ""   "" ) [ 0 ] ) <MASK> raise IOError ( "" Script  ' {} '  not found in script directory "" . format ( query ) ) return os . path . join ( path , query ) . split ( ""   "" ) return query",if not os . path . exists ( script ) :,if not os . path . exists ( script ) :,100,TRUE,100
2325,"def expand ( self , pbegin ) : # TODO(b/151921205): we have to do an identity map for unmodified # PCollections below because otherwise we get an error from beam. identity_map = "" Identity "" >> beam . Map ( lambda x : x ) if self . _dataset_key . is_flattened_dataset_key ( ) : <MASK> return self . _flat_pcollection | identity_map else : return list ( self . _pcollection_dict . values ( ) ) | "" FlattenAnalysisInputs "" >> beam . Flatten ( pipeline = pbegin . pipeline ) else : return self . _pcollection_dict [ self . _dataset_key ] | identity_map",if self . _flat_pcollection :,if self . _pcollection_dict . get ( self . _dataset_key ) is,71.83346298,FALSE,90.37
2326,"def processCoords ( coords ) : newcoords = deque ( ) for ( x , y , z ) in coords : for _dir , offsets in faceDirections : if _dir == FaceYIncreasing : continue dx , dy , dz = offsets p = ( x + dx , y + dy , z + dz ) if p not in box : continue nx , ny , nz = p <MASK> level . setBlockAt ( nx , ny , nz , waterID ) newcoords . append ( p ) return newcoords","if level . blockAt ( nx , ny , nz ) == 0 :",if nx != nz :,88.09767765,FALSE,86.94
2327,"def delete_byfilter ( userId , remove = True , session = None , * * dbfilter ) : if not session : session = db . Session ret = False results = session . query ( ObjectStorageMetadata ) . filter_by ( * * dbfilter ) if results : for result in results : <MASK> session . delete ( result ) else : result . update ( { "" record_state_key "" : "" to_delete "" , "" record_state_val "" : str ( time . time ( ) ) , } ) ret = True return ret",if remove :,if remove :,100,TRUE,100
2328,"def fields ( self , fields ) : fields_xml = "" "" for field in fields : field_dict = DEFAULT_FIELD . copy ( ) field_dict . update ( field ) <MASK> field_dict [ "" required "" ] = "" true "" fields_xml + = FIELD_XML_TEMPLATE % field_dict + "" \n "" self . xml = force_unicode ( force_unicode ( self . xml ) . replace ( u "" <!-- REPLACE FIELDS --> "" , force_unicode ( fields_xml ) ) )","if self . unique_key_field == field [ ""name"" ] :","if field . get ( ""required"" ) :",92.73866331,FALSE,87.42
2329,"def get_all_users ( self , access_token , timeout = None ) : if timeout is None : timeout = DEFAULT_TIMEOUT headers = self . retrieve_header ( access_token ) try : response = await self . standard_request ( "" get "" , "" /walkoff/api/users "" , timeout = DEFAULT_TIMEOUT , headers = headers ) <MASK> resp = await response . json ( ) return resp , "" Success "" else : return "" Invalid Credentials "" except asyncio . CancelledError : return False , "" TimedOut """,if response . status == 200 :,if response . status == 200 :,100,TRUE,100
2330,"def set_val ( ) : idx = 0 for idx in range ( 0 , len ( model ) ) : row = model [ idx ] if value and row [ 0 ] == value : break <MASK> idx = - 1 os_widget . set_active ( idx ) if idx == - 1 : os_widget . set_active ( 0 ) if idx > = 0 : return row [ 1 ] if self . show_all_os : return None",if idx == len ( os_widget . get_model ( ) ) - 1 :,if idx == 0 :,84.89538389,FALSE,85.56
2331,"def translate_module_name ( module : str , relative : int ) - > Tuple [ str , int ] : for pkg in VENDOR_PACKAGES : for alt in "" six.moves "" , "" six "" : substr = "" {} . {} "" . format ( pkg , alt ) if module . endswith ( "" . "" + substr ) or ( module == substr and relative ) : return alt , 0 <MASK> return alt + "" . "" + module . partition ( "" . "" + substr + "" . "" ) [ 2 ] , 0 return module , relative","if ""."" + substr + ""."" in module :","elif module . startswith ( alt + ""."" ) and ( module == substr and relative ) :",92.00215815,FALSE,86.37
2332,"def escape ( m ) : all , tail = m . group ( 0 , 1 ) assert all . startswith ( "" \\ "" ) esc = simple_escapes . get ( tail ) if esc is not None : return esc if tail . startswith ( "" x "" ) : hexes = tail [ 1 : ] <MASK> raise ValueError ( "" invalid hex string escape ( ' \\ %s ' ) "" % tail ) try : i = int ( hexes , 16 ) except ValueError : raise ValueError ( "" invalid hex string escape ( ' \\ %s ' ) "" % tail ) else : try : i = int ( tail , 8 ) except ValueError : raise ValueError ( "" invalid octal string escape ( ' \\ %s ' ) "" % tail ) return chr ( i )",if len ( hexes ) < 2 :,if hexes == 0 :,94.91723766,FALSE,95.53
2333,"def __get_k8s_container_name ( self , job_wrapper ) : # These must follow a specific regex for Kubernetes. raw_id = job_wrapper . job_destination . id if isinstance ( raw_id , str ) : cleaned_id = re . sub ( "" [^-a-z0-9] "" , "" - "" , raw_id ) <MASK> cleaned_id = "" x %s x "" % cleaned_id return cleaned_id return "" job-container ""","if cleaned_id . startswith ( ""-"" ) or cleaned_id . endswith ( ""-"" ) :","if cleaned_id . startswith ( ""x"" ) :",68.64900434,FALSE,88.38
2334,"def _power_exact ( y , xc , yc , xe ) : yc , ye = y . int , y . exp while yc % 10 == 0 : yc / / = 10 ye + = 1 if xc == 1 : xe * = yc while xe % 10 == 0 : xe / / = 10 ye + = 1 if ye < 0 : return None exponent = xe * 10 * * ye <MASK> xc = exponent else : xc = 0 return 5",if y and xe :,if abs ( exponent ) < 1e-5 :,90.6737581,FALSE,92.59
2335,"def lpush ( key , * vals , * * kwargs ) : ttl = kwargs . get ( "" ttl "" ) cap = kwargs . get ( "" cap "" ) if not ttl and not cap : _client . lpush ( key , * vals ) else : pipe = _client . pipeline ( ) pipe . lpush ( key , * vals ) <MASK> pipe . ltrim ( key , 0 , cap ) if ttl : pipe . expire ( key , ttl ) pipe . execute ( )",if cap :,if cap :,100,TRUE,100
2336,"def render_headers ( self ) - > bytes : if not hasattr ( self , "" _headers "" ) : parts = [ b "" Content-Disposition: form-data;  "" , format_form_param ( "" name "" , self . name ) , ] if self . filename : filename = format_form_param ( "" filename "" , self . filename ) parts . extend ( [ b "" ;  "" , filename ] ) <MASK> content_type = self . content_type . encode ( ) parts . extend ( [ b "" \r \n Content-Type:  "" , content_type ] ) parts . append ( b "" \r \n \r \n "" ) self . _headers = b "" "" . join ( parts ) return self . _headers",if self . content_type is not None :,if self . content_type :,97.83251209,FALSE,97.17
2337,"def validate_custom_field_data ( field_type : int , field_data : ProfileFieldData ) - > None : try : <MASK> # Choice type field must have at least have one choice if len ( field_data ) < 1 : raise JsonableError ( _ ( "" Field must have at least one choice. "" ) ) validate_choice_field_data ( field_data ) elif field_type == CustomProfileField . EXTERNAL_ACCOUNT : validate_external_account_field_data ( field_data ) except ValidationError as error : raise JsonableError ( error . message )",if field_type == CustomProfileField . CHOICE :,if field_type == CustomProfileField . Choice :,98.52952258,FALSE,97.96
2338,"def get_data ( self , path ) : """"""Gross hack to contort loader to deal w/ load_*()'s bad API."""""" if self . file and path == self . path : <MASK> file = self . file else : self . file = file = open ( self . path , "" r "" ) with file : # Technically should be returning bytes, but # SourceLoader.get_code() just passed what is returned to # compile() which can handle str. And converting to bytes would # require figuring out the encoding to decode to and # tokenize.detect_encoding() only accepts bytes. return file . read ( ) else : return super ( ) . get_data ( path )",if not self . file . closed :,"if isinstance ( self . file , str ) :",88.93818748,FALSE,94.9
2339,"def handle_read ( self ) : """"""Called when there is data waiting to be read."""""" try : chunk = self . recv ( self . ac_in_buffer_size ) except RetryError : pass except socket . error : self . handle_error ( ) else : self . tot_bytes_received + = len ( chunk ) <MASK> self . transfer_finished = True # self.close()  # <-- asyncore.recv() already do that... return if self . _data_wrapper is not None : chunk = self . _data_wrapper ( chunk ) try : self . file_obj . write ( chunk ) except OSError as err : raise _FileReadWriteError ( err )",if not chunk :,if self . tot_bytes_received >= self . transfer_size :,73.17841892,FALSE,90.33
2340,"def _swig_extract_dependency_files ( self , src ) : dep = [ ] for line in open ( src ) : <MASK> line = line . split ( ""   "" ) [ 1 ] . strip ( """""" ' "" \r \n """""" ) if not ( "" < "" in line or line in dep ) : dep . append ( line ) return [ i for i in dep if os . path . exists ( i ) ]","if line . startswith ( ""#include"" ) or line . startswith ( ""%include"" ) :","if line . startswith ( ""dependency"" ) :",92.6116131,FALSE,87.53
2341,"def buffer ( self , lines , scroll_end = True , scroll_if_editing = False ) : "" Add data to be displayed in the buffer. "" self . values . extend ( lines ) if scroll_end : if not self . editing : self . start_display_at = len ( self . values ) - len ( self . _my_widgets ) <MASK> self . start_display_at = len ( self . values ) - len ( self . _my_widgets )",elif scroll_if_editing :,elif self . editing and self . start_display_at > 0 :,89.70206812,FALSE,87.84
2342,"def test_getline ( self ) : with tokenize . open ( self . file_name ) as fp : for index , line in enumerate ( fp ) : <MASK> line + = "" \n "" cached_line = linecache . getline ( self . file_name , index + 1 ) self . assertEqual ( line , cached_line )","if not line . endswith ( ""\n"" ) :",if index + 1 < len ( fp ) - 1 :,87.40436291,FALSE,85.73
2343,"def selectRow ( self , rowNumber , highlight = None ) : if rowNumber == "" h "" : rowNumber = 0 else : rowNumber = int ( rowNumber ) + 1 if 1 > rowNumber > = len ( self . cells ) + 1 : raise Exception ( "" Invalid row number. "" ) else : selected = self . cells [ rowNumber ] [ 0 ] . selected for cell in self . cells [ rowNumber ] : <MASK> if selected : cell . deselect ( ) else : cell . select ( ) else : if highlight : cell . mouseEnter ( ) else : cell . mouseLeave ( )",if highlight is None :,if cell . selected :,95.7165635,FALSE,96.19
2344,"def put ( self , session ) : with sess_lock : self . parent . put ( session ) # Do not store the session if skip paths for sp in self . skip_paths : if request . path . startswith ( sp ) : return <MASK> try : del self . _cache [ session . sid ] except Exception : pass self . _cache [ session . sid ] = session self . _normalize ( )",if session . sid in self . _cache :,if session . sid in self . _cache :,75,TRUE,100
2345,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 10 : length = d . getVarInt32 ( ) tmp = ProtocolBuffer . Decoder ( d . buffer ( ) , d . pos ( ) , d . pos ( ) + length ) d . skip ( length ) self . add_status ( ) . TryMerge ( tmp ) continue <MASK> self . add_doc_id ( d . getPrefixedString ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 18 :,if tt == 18 :,100,TRUE,100
2346,"def extract ( self , zip ) : max_nb = maxNbFile ( self ) for index , field in enumerate ( zip . array ( "" file "" ) ) : <MASK> self . warning ( "" ZIP archive contains many files, but only first  %s  files are processed "" % max_nb ) break self . processFile ( field )",if max_nb is not None and max_nb <= index :,if index > max_nb :,87.08846839,FALSE,83.33
2347,"def get_norm ( norm , out_channels ) : if isinstance ( norm , str ) : <MASK> return None norm = { "" BN "" : BatchNorm2d , "" GN "" : lambda channels : nn . GroupNorm ( 32 , channels ) , "" nnSyncBN "" : nn . SyncBatchNorm , # keep for debugging "" "" : lambda x : x , } [ norm ] return norm ( out_channels )",if len ( norm ) == 0 :,"if norm . startswith ( ""bn"" ) :",90.72512496,FALSE,89.81
2348,"def execute ( self ) : if self . _dirty or not self . _qr : model_class = self . model_class query_meta = self . get_query_meta ( ) if self . _tuples : ResultWrapper = TuplesQueryResultWrapper elif self . _dicts : ResultWrapper = DictQueryResultWrapper elif self . _naive or not self . _joins or self . verify_naive ( ) : ResultWrapper = NaiveQueryResultWrapper <MASK> ResultWrapper = AggregateQueryResultWrapper else : ResultWrapper = ModelQueryResultWrapper self . _qr = ResultWrapper ( model_class , self . _execute ( ) , query_meta ) self . _dirty = False return self . _qr else : return self . _qr",elif self . _aggregate_rows :,elif self . _aggregate :,98.82791284,FALSE,97.41
2349,"def emitIpToDomainsData ( self , data , event ) : self . emitRawRirData ( data , event ) domains = data . get ( "" domains "" ) if isinstance ( domains , list ) : for domain in domains : if self . checkForStop ( ) : return None domain = domain . strip ( ) <MASK> self . emitHostname ( domain , event )",if domain :,if domain :,100,TRUE,100
2350,"def delete ( self ) : from weblate . trans . models import Change , Suggestion , Vote fast_deletes = [ ] for item in self . fast_deletes : <MASK> fast_deletes . append ( Vote . objects . filter ( suggestion__in = item ) ) fast_deletes . append ( Change . objects . filter ( suggestion__in = item ) ) fast_deletes . append ( item ) self . fast_deletes = fast_deletes return super ( ) . delete ( )",if item . model is Suggestion :,"if isinstance ( item , Suggestion ) :",92.98844024,FALSE,93.26
2351,"def token ( self ) : if not self . _token : try : cookie_token = self . state [ "" request "" ] . headers . cookie [ CSRF_TOKEN ] . value except KeyError : cookie_token = "" "" <MASK> self . _token = cookie_token else : self . _token = get_random_string ( TOKEN_LENGTH ) return self . _token",if len ( cookie_token ) == TOKEN_LENGTH :,if cookie_token :,70.56244362,FALSE,87.25
2352,"def get_logs ( last_file = None , last_time = None ) : try : response = client . get_logs ( last_file = last_file , last_time = last_time ) get_logs_streamer ( show_timestamp = not hide_time , all_containers = all_containers , all_info = all_info , ) ( response ) return response except ( ApiException , HTTPError ) as e : <MASK> handle_cli_error ( e , message = "" Could not get logs for run ` {} `. "" . format ( client . run_uuid ) , ) sys . exit ( 1 )",if not follow :,"if client . run_uuid not in [ ""default"" , ""no_logs"" ]",93.14488776,FALSE,86.93
2353,"def update ( self , targets ) : Section . update ( self , targets ) outputNames = set ( ) for target in targets : g = target . globals ( ) outputNames . update ( [ k for k in g . keys ( ) if k . startswith ( "" output: "" ) ] ) rows = [ ] outputNames = sorted ( outputNames ) for outputName in outputNames : row = self . __rows . get ( outputName ) <MASK> row = _OutputRow ( outputName ) self . __rows [ outputName ] = row row . update ( targets ) row . setAlternate ( len ( rows ) % 2 ) rows . append ( row ) self . _mainColumn ( ) [ : ] = rows",if row is None :,if row is None :,100,TRUE,100
2354,"def getBranches ( self ) : returned = [ ] for git_branch_line in self . _executeGitCommandAssertSuccess ( "" branch "" ) . stdout : if git_branch_line . startswith ( "" * "" ) : git_branch_line = git_branch_line [ 1 : ] git_branch_line = git_branch_line . strip ( ) <MASK> alias_name , aliased = git_branch_line . split ( BRANCH_ALIAS_MARKER ) returned . append ( branch . LocalBranchAlias ( self , alias_name , aliased ) ) else : returned . append ( branch . LocalBranch ( self , git_branch_line ) ) return returned",if BRANCH_ALIAS_MARKER in git_branch_line :,if BRANCH_ALIAS_MARKER in git_branch_line :,100,TRUE,100
2355,"def has_bad_headers ( self ) : headers = [ self . sender , self . reply_to ] + self . recipients for header in headers : if _has_newline ( header ) : return True if self . subject : if _has_newline ( self . subject ) : for linenum , line in enumerate ( self . subject . split ( "" \r \n "" ) ) : if not line : return True <MASK> return True if _has_newline ( line ) : return True if len ( line . strip ( ) ) == 0 : return True return False","if linenum > 0 and line [ 0 ] not in ""\t "" :",if linenum == 0 :,91.0546801,FALSE,88.95
2356,"def resolve_references ( self , note , reflist ) : assert len ( note [ "" ids "" ] ) == 1 id = note [ "" ids "" ] [ 0 ] for ref in reflist : <MASK> continue ref . delattr ( "" refname "" ) ref [ "" refid "" ] = id assert len ( ref [ "" ids "" ] ) == 1 note . add_backref ( ref [ "" ids "" ] [ 0 ] ) ref . resolved = 1 note . resolved = 1",if ref . resolved :,"if ref . get ( ""refname"" ) == id :",92.19222199,FALSE,90.3
2357,"def pickPath ( self , color ) : self . path [ color ] = ( ) currentPos = self . starts [ color ] while True : minDist = None minGuide = None for guide in self . guides [ color ] : guideDist = dist ( currentPos , guide ) if minDist == None or guideDist < minDist : minDist = guideDist minGuide = guide <MASK> return if minGuide == None : return self . path [ color ] = self . path [ color ] + ( minGuide , ) currentPos = minGuide self . guides [ color ] . remove ( minGuide )","if dist ( currentPos , self . ends [ color ] ) == 1 :",if guideDist == 0 :,89.72090948,FALSE,87.7
2358,"def __hierarchyViewKeyPress ( hierarchyView , event ) : if event == __editSourceKeyPress : selectedPath = __hierarchyViewSelectedPath ( hierarchyView ) <MASK> __editSourceNode ( hierarchyView . getContext ( ) , hierarchyView . scene ( ) , selectedPath ) return True elif event == __editTweaksKeyPress : selectedPath = __hierarchyViewSelectedPath ( hierarchyView ) <MASK> __editTweaksNode ( hierarchyView . getContext ( ) , hierarchyView . scene ( ) , selectedPath ) return True",if selectedPath is not None :,if selectedPath is not None :,100,TRUE,100
2359,"def getSubsegments ( self ) : for num , localdata in self . lfh . LocalData : for bucket , seginfo in localdata . SegmentInfo : <MASK> continue yield Win32Subsegment ( self . trace , self . heap , seginfo . ActiveSubsegment )",if seginfo . ActiveSubsegment == 0 :,if seginfo . IsSubsegment :,91.46343435,FALSE,88.57
2360,"def test_full_hd_bluray ( self ) : cur_test = "" full_hd_bluray "" cur_qual = common . Quality . FULLHDBLURAY for name , tests in iteritems ( self . test_cases ) : for test in tests : <MASK> self . assertEqual ( cur_qual , common . Quality . name_quality ( test ) ) else : self . assertNotEqual ( cur_qual , common . Quality . name_quality ( test ) )",if name == cur_test :,if name == cur_test :,100,TRUE,100
2361,"def calc ( self , arg ) : op = arg [ "" op "" ] if op == "" C "" : self . clear ( ) return str ( self . current ) num = decimal . Decimal ( arg [ "" num "" ] ) if self . op : if self . op == "" + "" : self . current + = num elif self . op == "" - "" : self . current - = num elif self . op == "" * "" : self . current * = num <MASK> self . current / = num self . op = op else : self . op = op self . current = num res = str ( self . current ) if op == "" = "" : self . clear ( ) return res","elif self . op == ""/"" :","elif self . op == ""/"" :",100,TRUE,100
2362,"def strip_export_type ( path ) : matched = re . search ( r "" #([a-zA-Z0-9 \ -]+ \\ +[a-zA-Z0-9 \ -]+)?$ "" , path . encode ( "" utf-8 "" ) ) mime_type = None if matched : fragment = matched . group ( 0 ) mime_type = matched . group ( 1 ) <MASK> mime_type = mime_type . replace ( "" + "" , "" / "" ) path = path [ : - len ( fragment ) ] return ( path , mime_type )",if mime_type is not None :,if mime_type :,94.20483607,FALSE,96.31
2363,"def _save_as_module ( file , data , binary = False ) : if not data : return with open ( file , "" w "" ) as f : f . write ( "" DATA= "" ) <MASK> f . write ( ' "" ' ) f . write ( base64 . b64encode ( data ) . decode ( "" ascii "" ) ) f . write ( ' "" ' ) else : f . write ( str ( data ) . replace ( "" \\ \\ "" , "" \\ "" ) ) f . flush ( )",if binary :,if binary :,100,TRUE,100
2364,"def ProcessStringLiteral ( self ) : if self . _lastToken == None or self . _lastToken . type == self . OpenBrace : text = super ( JavaScriptBaseLexer , self ) . text <MASK> if len ( self . _scopeStrictModes ) > 0 : self . _scopeStrictModes . pop ( ) self . _useStrictCurrent = True self . _scopeStrictModes . append ( self . _useStrictCurrent )","if text == '""use strict""' or text == ""'use strict'"" :","if text . startswith ( ""Strict Strict"" ) :",89.0028582,FALSE,81.15
2365,"def run ( self , ttl = None ) : self . zeroconf = zeroconf . Zeroconf ( ) zeroconf . ServiceBrowser ( self . zeroconf , self . domain , MDNSHandler ( self ) ) if ttl : gobject . timeout_add ( ttl * 1000 , self . shutdown ) self . __running = True self . __mainloop = gobject . MainLoop ( ) context = self . __mainloop . get_context ( ) while self . __running : try : <MASK> context . iteration ( True ) else : time . sleep ( 0.1 ) except KeyboardInterrupt : break self . zeroconf . close ( ) logger . debug ( "" MDNSListener.run() quit "" )",if context . pending ( ) :,if context . is_running ( ) :,98.96936433,FALSE,96.74
2366,"def topology_change_notify ( self , port_state ) : notice = False if port_state is PORT_STATE_FORWARD : for port in self . ports . values ( ) : if port . role is DESIGNATED_PORT : notice = True break else : notice = True if notice : self . send_event ( EventTopologyChange ( self . dp ) ) <MASK> self . _transmit_tc_bpdu ( ) else : self . _transmit_tcn_bpdu ( )",if self . is_root_bridge :,"if self . dp . type == ""tcn"" :",94.09079727,FALSE,90.98
2367,def close_open_fds ( keep = None ) : # noqa keep = [ maybe_fileno ( f ) for f in ( keep or [ ] ) if maybe_fileno ( f ) is not None ] for fd in reversed ( range ( get_fdmax ( default = 2048 ) ) ) : if fd not in keep : try : os . close ( fd ) except OSError as exc : <MASK> raise,if exc . errno != errno . EBADF :,if exc . errno != errno . EINVAL :,98.59381326,FALSE,97.47
2368,"def collect_attributes ( options , node , master_list ) : """"""Collect all attributes"""""" for ii in node . instructions : if field_check ( ii , "" attributes "" ) : s = getattr ( ii , "" attributes "" ) if isinstance ( s , list ) : for x in s : if x not in master_list : master_list . append ( x ) <MASK> master_list . append ( s ) for nxt in node . next . values ( ) : collect_attributes ( options , nxt , master_list )",elif s != None and s not in master_list :,"elif isinstance ( s , dict ) :",89.30986537,FALSE,90.03
2369,"def remove_test_run_directories ( expiry_time : int = 60 * 60 ) - > int : removed = 0 directories = glob . glob ( os . path . join ( UUID_VAR_DIR , "" test-backend "" , "" run_* "" ) ) for test_run in directories : <MASK> try : shutil . rmtree ( test_run ) removed + = 1 except FileNotFoundError : pass return removed",if round ( time . time ( ) ) - os . path . getmtime ( test_run ) > expiry_time :,if os . path . exists ( test_run ) and time ( ) - expiry_time,83.18036899,FALSE,84.5
2370,"def read_work_titles ( fields ) : found = [ ] if "" 240 "" in fields : for line in fields [ "" 240 "" ] : title = join_subfield_values ( line , [ "" a "" , "" m "" , "" n "" , "" p "" , "" r "" ] ) <MASK> found . append ( title ) if "" 130 "" in fields : for line in fields [ "" 130 "" ] : title = ""   "" . join ( get_lower_subfields ( line ) ) <MASK> found . append ( title ) return { "" work_titles "" : found } if found else { }",if title not in found :,if title :,90.45385565,FALSE,93.18
2371,"def _process_v1_msg ( prot , msg ) : header = None body = msg [ 1 ] if not isinstance ( body , ( binary_type , mmap , memoryview ) ) : raise ValidationError ( body , "" Body must be a bytestream. "" ) if len ( msg ) > 2 : header = msg [ 2 ] <MASK> raise ValidationError ( header , "" Header must be a dict. "" ) for k , v in header . items ( ) : header [ k ] = msgpack . unpackb ( v ) ctx = MessagePackMethodContext ( prot , MessagePackMethodContext . SERVER ) ctx . in_string = [ body ] ctx . transport . in_header = header return ctx","if not isinstance ( header , dict ) :","if not isinstance ( header , dict ) :",100,TRUE,100
2372,"def find ( self , node ) : typename = type ( node ) . __name__ method = getattr ( self , "" find_ {} "" . format ( typename ) , None ) if method is None : fields = getattr ( node , "" _fields "" , None ) <MASK> return for field in fields : value = getattr ( node , field ) for result in self . find ( value ) : yield result else : for result in method ( node ) : yield result",if fields is None :,if not fields :,66.72741561,FALSE,95.71
2373,"def _str_param_list ( self , name ) : out = [ ] if self [ name ] : out + = self . _str_header ( name ) for param in self [ name ] : parts = [ ] <MASK> parts . append ( param . name ) if param . type : parts . append ( param . type ) out + = [ ""  :  "" . join ( parts ) ] if param . desc and "" "" . join ( param . desc ) . strip ( ) : out + = self . _str_indent ( param . desc ) out + = [ "" "" ] return out",if param . name :,if param . name :,100,TRUE,100
2374,"def _get_image ( self , image_list , source ) : if source . startswith ( "" wx "" ) : img = wx . ArtProvider_GetBitmap ( source , wx . ART_OTHER , _SIZE ) else : path = os . path . join ( _BASE , source ) <MASK> img = wx . Image ( path , wx . BITMAP_TYPE_GIF ) . ConvertToBitmap ( ) else : img = wx . Image ( path , wx . BITMAP_TYPE_PNG ) . ConvertToBitmap ( ) return image_list . Add ( img )","if source . endswith ( ""gif"" ) :","if os . name == ""nt"" :",85.18072647,FALSE,92.58
2375,"def change_opacity_function ( self , new_f ) : self . opacity_function = new_f dr = self . radius / self . num_levels sectors = [ ] for submob in self . submobjects : <MASK> sectors . append ( submob ) for ( r , submob ) in zip ( np . arange ( 0 , self . radius , dr ) , sectors ) : if type ( submob ) != AnnularSector : # it's the shadow, don't dim it continue alpha = self . opacity_function ( r ) submob . set_fill ( opacity = alpha )",if type ( submob ) == AnnularSector :,if submob . is_sector ( ) :,94.27390285,FALSE,93.59
2376,"def _sqlite_post_configure_engine ( url , engine , follower_ident ) : from sqlalchemy import event @event . listens_for ( engine , "" connect "" ) def connect ( dbapi_connection , connection_record ) : # use file DBs in all cases, memory acts kind of strangely # as an attached <MASK> dbapi_connection . execute ( ' ATTACH DATABASE  "" test_schema.db ""  AS test_schema ' ) else : dbapi_connection . execute ( ' ATTACH DATABASE  "" %s _test_schema.db ""  AS test_schema ' % follower_ident )",if not follower_ident :,"if follower_ident == ""file"" :",97.56746026,FALSE,93.64
2377,"def apply_conf_file ( fn , conf_filename ) : for env in LSF_CONF_ENV : conf_file = get_conf_file ( conf_filename , env ) if conf_file : with open ( conf_file ) as conf_handle : value = fn ( conf_handle ) <MASK> return value return None",if value :,if value :,100,TRUE,100
2378,"def test_call_extern_c_fn ( self ) : global memcmp memcmp = cffi_support . ExternCFunction ( "" memcmp "" , ( "" int memcmp ( const uint8_t * ptr1,  "" "" const uint8_t * ptr2, size_t num ) "" ) , ) @udf ( BooleanVal ( FunctionContext , StringVal , StringVal ) ) def fn ( context , a , b ) : if a . is_null != b . is_null : return False if a is None : return True <MASK> return False if a . ptr == b . ptr : return True return memcmp ( a . ptr , b . ptr , a . len ) == 0",if len ( a ) != b . len :,if b is None :,89.73128029,FALSE,92.85
2379,"def _get_initialized_app ( app ) : """"""Returns a reference to an initialized App instance."""""" if app is None : return firebase_admin . get_app ( ) if isinstance ( app , firebase_admin . App ) : initialized_app = firebase_admin . get_app ( app . name ) <MASK> raise ValueError ( "" Illegal app argument. App instance not  "" "" initialized via the firebase module. "" ) return app raise ValueError ( "" Illegal app argument. Argument must be of type  "" '  firebase_admin.App, but given  "" {0} "" . ' . format ( type ( app ) ) )",if app is not initialized_app :,if initialized_app is not None :,93.27309602,FALSE,96.5
2380,def compiled_query ( self ) : <MASK> self . lazy_init_lock_ . acquire ( ) try : <MASK> self . compiled_query_ = CompiledQuery ( ) finally : self . lazy_init_lock_ . release ( ) return self . compiled_query_,if self . compiled_query_ is None :,if self . compiled_query_ is None :,100,TRUE,100
2381,"def clean_subevent ( event , subevent ) : if event . has_subevents : <MASK> raise ValidationError ( _ ( "" Subevent cannot be null for event series. "" ) ) if event != subevent . event : raise ValidationError ( _ ( "" The subevent does not belong to this event. "" ) ) else : if subevent : raise ValidationError ( _ ( "" The subevent does not belong to this event. "" ) )",if not subevent :,if not subevent :,100,TRUE,100
2382,"def get_blob_type_declaration_sql ( self , column ) : length = column . get ( "" length "" ) if length : if length < = self . LENGTH_LIMIT_TINYBLOB : return "" TINYBLOB "" if length < = self . LENGTH_LIMIT_BLOB : return "" BLOB "" <MASK> return "" MEDIUMBLOB "" return "" LONGBLOB """,if length <= self . LENGTH_LIMIT_MEDIUMBLOB :,if length <= self . LENGTH_LIMIT_MEDIUMBLOB :,100,TRUE,100
2383,"def decompress ( self , data ) : if not data : return data if not self . _first_try : return self . _obj . decompress ( data ) self . _data + = data try : decompressed = self . _obj . decompress ( data ) <MASK> self . _first_try = False self . _data = None return decompressed except zlib . error : self . _first_try = False self . _obj = zlib . decompressobj ( - zlib . MAX_WBITS ) try : return self . decompress ( self . _data ) finally : self . _data = None",if decompressed :,if decompressed is None :,81.27446629,FALSE,97.11
2384,"def _record_event ( self , path , fsevent_handle , filename , events , error ) : with self . lock : self . events [ path ] . append ( events ) if events | pyuv . fs . UV_RENAME : <MASK> self . watches . pop ( path ) . close ( )",if not os . path . exists ( path ) :,if path in self . watches :,86.30773434,FALSE,85.48
2385,"def __init__ ( self , duration , batch_shape , event_shape , validate_args = None ) : if duration is None : <MASK> # Infer duration from event_shape. duration = event_shape [ 0 ] elif duration != event_shape [ 0 ] : <MASK> raise ValueError ( "" duration, event_shape mismatch:  {}  vs  {} "" . format ( duration , event_shape ) ) # Infer event_shape from duration. event_shape = torch . Size ( ( duration , ) + event_shape [ 1 : ] ) self . _duration = duration super ( ) . __init__ ( batch_shape , event_shape , validate_args )",if event_shape [ 0 ] != 1 :,if len ( event_shape ) == 1 :,86.19267599,FALSE,91.32
2386,"def _CheckPrerequisites ( self ) : """"""Exits if any of the prerequisites is not met."""""" if not FLAGS . kubectl : raise Exception ( "" Please provide path to kubectl tool using --kubectl  "" "" flag. Exiting. "" ) if not FLAGS . kubeconfig : raise Exception ( "" Please provide path to kubeconfig using --kubeconfig  "" "" flag. Exiting. "" ) if self . disk_specs and self . disk_specs [ 0 ] . disk_type == disk . STANDARD : <MASK> raise Exception ( "" Please provide a list of Ceph Monitors using  "" "" --ceph_monitors flag. "" )",if not FLAGS . ceph_monitors :,if not FLAGS . ceph_monitors :,100,TRUE,100
2387,"def invalidateDependentSlices ( self , iFirstCurve ) : # only user defined curve can have slice dependency relationships if self . isSystemCurveIndex ( iFirstCurve ) : return nCurves = self . getNCurves ( ) for i in range ( iFirstCurve , nCurves ) : c = self . getSystemCurve ( i ) <MASK> c . invalidate ( ) elif i == iFirstCurve : # if first curve isn't a slice, break # there are no dependent slices","if isinstance ( c . getSymbol ( ) . getSymbolType ( ) , SymbolType . PieSliceSymbolType ) :",if c . isDependent ( ) :,66.80272022,FALSE,83.32
2388,"def find_backwards ( self , offset ) : try : for _ , token_type , token_value in reversed ( self . tokens [ self . offset : offset ] ) : if token_type in ( "" comment "" , "" linecomment "" ) : try : prefix , comment = token_value . split ( None , 1 ) except ValueError : continue <MASK> return [ comment . rstrip ( ) ] return [ ] finally : self . offset = offset",if prefix in self . comment_tags :,"if prefix . startswith ( ""comment"" ) :",93.55819349,FALSE,91.84
2389,"def parse_column_definitions ( self , elem ) : for column_elem in elem . findall ( "" column "" ) : name = column_elem . get ( "" name "" , None ) assert name is not None , "" Required  ' name '  attribute missing from column def "" index = column_elem . get ( "" index "" , None ) assert index is not None , "" Required  ' index '  attribute missing from column def "" index = int ( index ) self . columns [ name ] = index <MASK> self . largest_index = index assert "" value "" in self . columns , "" Required  ' value '  column missing from column def "" if "" name "" not in self . columns : self . columns [ "" name "" ] = self . columns [ "" value "" ]",if index > self . largest_index :,if index < self . largest_index :,99.1758707,FALSE,98.46
2390,"def __find_smallest ( self ) : """"""Find the smallest uncovered value in the matrix."""""" minval = sys . maxsize for i in range ( self . n ) : for j in range ( self . n ) : if ( not self . row_covered [ i ] ) and ( not self . col_covered [ j ] ) : <MASK> minval = self . C [ i ] [ j ] return minval",if minval > self . C [ i ] [ j ] :,if self . C [ i ] [ j ] < minval :,93.52185847,FALSE,95.16
2391,"def includes_tools_for_display_in_tool_panel ( self ) : if self . includes_tools : tool_dicts = self . metadata [ "" tools "" ] for tool_dict in tool_dicts : <MASK> return True return False","if tool_dict . get ( ""add_to_tool_panel"" , True ) :","if ""toolpanel"" in tool_dict and tool_dict [ ""toolpanel"" ]",79.94685104,FALSE,73.76
2392,"def commit ( self , notify = False ) : if self . editing : text = self . _text if text : try : value = self . type ( text ) except ValueError : return value = self . clamp_value ( value ) else : value = self . empty if value is NotImplemented : return self . value = value self . insertion_point = None <MASK> self . change_text ( unicode ( value ) ) else : self . _text = unicode ( value ) self . editing = False else : self . insertion_point = None",if notify :,if notify :,100,TRUE,100
2393,"def GeneratePageMetatadata ( self , task ) : address_space = self . session . GetParameter ( "" default_address_space "" ) for vma in task . mm . mmap . walk_list ( "" vm_next "" ) : start = vma . vm_start end = vma . vm_end # Skip the entire region. if end < self . plugin_args . start : continue # Done. <MASK> break for vaddr in utils . xrange ( start , end , 0x1000 ) : if self . plugin_args . start < = vaddr < = self . plugin_args . end : yield vaddr , self . _CreateMetadata ( address_space . describe_vtop ( vaddr ) )",if start > self . plugin_args . end :,if start >= self . plugin_args . end :,98.93406054,FALSE,98.28
2394,"def _check_for_duplicate_host_entries ( self , task_entries ) : non_host_statuses = ( models . HostQueueEntry . Status . PARSING , models . HostQueueEntry . Status . ARCHIVING , ) for task_entry in task_entries : using_host = ( task_entry . host is not None and task_entry . status not in non_host_statuses ) <MASK> self . _assert_host_has_no_agent ( task_entry )",if using_host :,if using_host :,100,TRUE,100
2395,"def get_biggest_wall_time ( jsons ) : lowest_wall = None for j in jsons : <MASK> lowest_wall = j [ "" wall_time "" ] if lowest_wall < j [ "" wall_time "" ] : lowest_wall = j [ "" wall_time "" ] return lowest_wall",if lowest_wall is None :,if lowest_wall is None :,100,TRUE,100
2396,"def log_change_report ( self , old_value , new_value , include_details = False ) : from octoprint . util import map_boolean with self . _check_mutex : self . _logger . info ( "" Connectivity changed from  {}  to  {} "" . format ( map_boolean ( old_value , "" online "" , "" offline "" ) , map_boolean ( new_value , "" online "" , "" offline "" ) , ) ) <MASK> self . log_details ( )",if include_details :,if include_details :,100,TRUE,100
2397,"def _include_block ( self , value , context = None ) : if hasattr ( value , "" render_as_block "" ) : <MASK> new_context = context . get_all ( ) else : new_context = { } return jinja2 . Markup ( value . render_as_block ( context = new_context ) ) return jinja2 . Markup ( value )",if context :,if context :,100,TRUE,100
2398,"def __lt__ ( self , other ) : # 0: clock 1: timestamp 3: process id try : A , B = self [ 0 ] , other [ 0 ] # uses logical clock value first if A and B : # use logical clock if available <MASK> # equal clocks use lower process id return self [ 2 ] < other [ 2 ] return A < B return self [ 1 ] < other [ 1 ] # ... or use timestamp except IndexError : return NotImplemented",if A == B :,if A == B :,75,TRUE,100
2399,"def _get_port ( ) : while True : port = 20000 + random . randint ( 1 , 9999 ) for i in range ( 5 ) : sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) result = sock . connect_ex ( ( "" 127.0.0.1 "" , port ) ) <MASK> continue else : return port",if result == 0 :,if not result :,91.29161466,FALSE,92.92
2400,"def fetch_all ( self , api_client , fetchstatuslogger , q , targets ) : self . fetchstatuslogger = fetchstatuslogger if targets != None : # Ensure targets is a tuple if type ( targets ) != list and type ( targets ) != tuple : targets = tuple ( targets , ) <MASK> targets = tuple ( targets ) for target in targets : self . _fetch_targets ( api_client , q , target )",elif type ( targets ) != tuple :,elif type ( targets ) != list and type ( targets ) != list :,96.47606268,FALSE,89.53
2401,"def migrate_node_facts ( facts ) : """"""Migrate facts from various roles into node"""""" params = { "" common "" : ( "" dns_ip "" ) , } if "" node "" not in facts : facts [ "" node "" ] = { } # pylint: disable=consider-iterating-dictionary for role in params . keys ( ) : <MASK> for param in params [ role ] : if param in facts [ role ] : facts [ "" node "" ] [ param ] = facts [ role ] . pop ( param ) return facts",if role in facts :,"if isinstance ( params [ role ] , list ) :",96.26981511,FALSE,91.25
2402,"def build_dimension_param ( self , dimension , params ) : prefix = "" Dimensions.member "" i = 0 for dim_name in dimension : dim_value = dimension [ dim_name ] <MASK> if isinstance ( dim_value , six . string_types ) : dim_value = [ dim_value ] for value in dim_value : params [ "" %s . %d .Name "" % ( prefix , i + 1 ) ] = dim_name params [ "" %s . %d .Value "" % ( prefix , i + 1 ) ] = value i + = 1 else : params [ "" %s . %d .Name "" % ( prefix , i + 1 ) ] = dim_name i + = 1",if dim_value :,"if dim_name == ""value"" :",92.03062448,FALSE,95.44
2403,"def add_if_unique ( self , issuer , use , keys ) : if use in self . issuer_keys [ issuer ] and self . issuer_keys [ issuer ] [ use ] : for typ , key in keys : flag = 1 for _typ , _key in self . issuer_keys [ issuer ] [ use ] : <MASK> flag = 0 break if flag : self . issuer_keys [ issuer ] [ use ] . append ( ( typ , key ) ) else : self . issuer_keys [ issuer ] [ use ] = keys",if _typ == typ and key is _key :,if typ == _key :,93.26699216,FALSE,93.35
2404,"def run ( self ) : while True : message = self . in_queue . get ( ) <MASK> self . reset ( ) elif message == EXIT : return else : index , transaction = message self . results_queue . put ( ( index , self . validate ( transaction ) ) )",if message == RESET :,if message == RESET :,100,TRUE,100
2405,"def __run ( self ) : threads = self . parameters ( ) [ "" threads "" ] . getTypedValue ( ) with IECore . tbb_global_control ( IECore . tbb_global_control . parameter . max_allowed_parallelism , IECore . hardwareConcurrency ( ) if threads == 0 else threads , ) : self . _executeStartupFiles ( self . root ( ) . getName ( ) ) # Append DEBUG message with process information to all messages defaultMessageHandler = IECore . MessageHandler . getDefaultHandler ( ) <MASK> IECore . MessageHandler . setDefaultHandler ( Gaffer . ProcessMessageHandler ( defaultMessageHandler ) ) return self . _run ( self . parameters ( ) . getValidatedValue ( ) )","if not isinstance ( defaultMessageHandler , Gaffer . ProcessMessageHandler ) :",if defaultMessageHandler is not None :,95.86628724,FALSE,92.63
2406,"def adjust_uri ( self , uri , relativeto ) : """"""Adjust the given ``uri`` based on the given relative URI."""""" key = ( uri , relativeto ) if key in self . _uri_cache : return self . _uri_cache [ key ] if uri [ 0 ] != "" / "" : <MASK> v = self . _uri_cache [ key ] = posixpath . join ( posixpath . dirname ( relativeto ) , uri ) else : v = self . _uri_cache [ key ] = "" / "" + uri else : v = self . _uri_cache [ key ] = uri return v",if relativeto is not None :,if relativeto :,92.06276392,FALSE,96.69
2407,"def decoder ( s ) : r = [ ] decode = [ ] for c in s : <MASK> decode . append ( "" & "" ) elif c == "" - "" and decode : if len ( decode ) == 1 : r . append ( "" & "" ) else : r . append ( modified_unbase64 ( "" "" . join ( decode [ 1 : ] ) ) ) decode = [ ] elif decode : decode . append ( c ) else : r . append ( c ) if decode : r . append ( modified_unbase64 ( "" "" . join ( decode [ 1 : ] ) ) ) bin_str = "" "" . join ( r ) return ( bin_str , len ( s ) )","if c == ""&"" and not decode :","if c == ""&"" :",96.02745574,FALSE,97.06
2408,"def _process_file ( self , content ) : args = [ ] for line in content . splitlines ( ) : line = line . strip ( ) if line . startswith ( "" - "" ) : args . extend ( self . _split_option ( line ) ) <MASK> args . append ( line ) return args","elif line and not line . startswith ( ""#"" ) :","elif line . startswith ( ""file:"" ) :",90.27889394,FALSE,91.55
2409,"def _method_events_callback ( self , values ) : try : previous_echoed = ( values [ "" child_result_list "" ] [ - 1 ] . decode ( ) . split ( "" \n "" ) [ - 2 ] . strip ( ) ) if previous_echoed . endswith ( "" foo1 "" ) : return "" echo foo2 \n "" elif previous_echoed . endswith ( "" foo2 "" ) : return "" echo foo3 \n "" <MASK> return "" exit \n "" else : raise Exception ( "" Unexpected output  {0!r} "" . format ( previous_echoed ) ) except IndexError : return "" echo foo1 \n ""","elif previous_echoed . endswith ( ""foo3"" ) :","elif previous_echoed . endswith ( ""foo3"" ) :",100,TRUE,100
2410,"def __delete_hook ( self , rpc ) : try : rpc . check_success ( ) except apiproxy_errors . Error : return None result = [ ] for status in rpc . response . delete_status_list ( ) : <MASK> result . append ( DELETE_SUCCESSFUL ) elif status == MemcacheDeleteResponse . NOT_FOUND : result . append ( DELETE_ITEM_MISSING ) else : result . append ( DELETE_NETWORK_FAILURE ) return result",if status == MemcacheDeleteResponse . DELETED :,if status == MemcacheDeleteResponse . SUCCESSFUL :,98.26372277,FALSE,97.42
2411,"def __createRandom ( plug ) : node = plug . node ( ) parentNode = node . ancestor ( Gaffer . Node ) with Gaffer . UndoScope ( node . scriptNode ( ) ) : randomNode = Gaffer . Random ( ) parentNode . addChild ( randomNode ) <MASK> plug . setInput ( randomNode [ "" outFloat "" ] ) elif isinstance ( plug , Gaffer . Color3fPlug ) : plug . setInput ( randomNode [ "" outColor "" ] ) GafferUI . NodeEditor . acquire ( randomNode )","if isinstance ( plug , ( Gaffer . FloatPlug , Gaffer . IntPlug ) ) :","if isinstance ( plug , Gaffer . Float3fPlug ) :",92.24778492,FALSE,91.03
2412,"def escapeentities ( self , line ) : "" Escape all Unicode characters to HTML entities. "" result = "" "" pos = TextPosition ( line ) while not pos . finished ( ) : if ord ( pos . current ( ) ) > 128 : codepoint = hex ( ord ( pos . current ( ) ) ) <MASK> codepoint = hex ( ord ( pos . next ( ) ) + 0xF800 ) result + = "" &# "" + codepoint [ 1 : ] + "" ; "" else : result + = pos . current ( ) pos . skipcurrent ( ) return result","if codepoint == ""0xd835"" :",elif ord ( pos . next ( ) ) > 0x10000 :,87.49581927,FALSE,89.61
2413,def get_and_set_all_aliases ( self ) : all_aliases = [ ] for page in self . pages : <MASK> all_aliases . extend ( page . relations . aliases_norm ) if page . relations . aliases is not None : all_aliases . extend ( page . relations . aliases ) return set ( all_aliases ),if page . relations . aliases_norm is not None :,if page . relations . aliases_norm is not None :,100,TRUE,100
2414,"def _list_cases ( suite ) : for test in suite : <MASK> _list_cases ( test ) elif isinstance ( test , unittest . TestCase ) : if support . match_test ( test ) : print ( test . id ( ) )","if isinstance ( test , unittest . TestSuite ) :","if isinstance ( test , unittest . TestCase ) :",97.47090747,FALSE,95.59
2415,"def get_next_requests ( self , max_n_requests , * * kwargs ) : next_pages = [ ] partitions = set ( kwargs . pop ( "" partitions "" , [ ] ) ) for partition_id in range ( 0 , self . queue_partitions ) : <MASK> continue results = self . queue . get_next_requests ( max_n_requests , partition_id ) next_pages . extend ( results ) self . logger . debug ( "" Got  %d  requests for partition id  %d "" , len ( results ) , partition_id ) return next_pages",if partition_id not in partitions :,if partition_id not in partitions :,100,TRUE,100
2416,"def __iter__ ( self ) : if ( self . query is not None ) and sqlite . is_read_only_query ( self . query ) : cur = self . connection . cursor ( ) results = cur . execute ( self . query ) <MASK> yield [ col [ 0 ] for col in cur . description ] for i , row in enumerate ( results ) : if i > = self . limit : break yield [ val for val in row ] else : yield",if self . headers :,if results :,92.73019994,FALSE,95.51
2417,"def rollback ( self ) : for operation , values in self . current_transaction_state [ : : - 1 ] : if operation == "" insert "" : values . remove ( ) <MASK> old_value , new_value = values if new_value . full_filename != old_value . full_filename : os . unlink ( new_value . full_filename ) old_value . write ( ) self . _post_xact_cleanup ( )","elif operation == ""update"" :","elif operation == ""delete"" :",98.17617264,FALSE,97.49
2418,"def index ( self , value ) : if self . _growing : if self . _start < = value < self . _stop : q , r = divmod ( value - self . _start , self . _step ) <MASK> return int ( q ) else : if self . _start > = value > self . _stop : q , r = divmod ( self . _start - value , - self . _step ) <MASK> return int ( q ) raise ValueError ( "" {}  is not in numeric range "" . format ( value ) )",if r == self . _zero :,if r == 0 :,92.06226625,FALSE,91.15
2419,"def validate_name_and_description ( body , check_length = True ) : for attribute in [ "" name "" , "" description "" , "" display_name "" , "" display_description "" ] : value = body . get ( attribute ) <MASK> if isinstance ( value , six . string_types ) : body [ attribute ] = value . strip ( ) if check_length : try : utils . check_string_length ( body [ attribute ] , attribute , min_length = 0 , max_length = 255 ) except exception . InvalidInput as error : raise webob . exc . HTTPBadRequest ( explanation = error . msg )",if value is not None :,if value :,95.62322551,FALSE,96.57
2420,"def printWiki ( ) : firstHeading = False for m in protocol : if m [ 0 ] == "" "" : <MASK> output ( "" |} "" ) __printWikiHeader ( m [ 1 ] , m [ 2 ] ) firstHeading = True else : output ( "" |- "" ) output ( ' | <span style= "" white-space:nowrap; "" ><tt> ' + m [ 0 ] + "" </tt></span> || ||  "" + m [ 1 ] ) output ( "" |} "" )",if firstHeading :,if firstHeading :,100,TRUE,100
2421,"def _get_platforms ( data ) : platform_list = [ ] for item in data : if item . startswith ( "" PlatformEdit.html? "" ) : parameter_list = item . split ( "" PlatformEdit.html? "" , 1 ) [ 1 ] . split ( "" & "" ) for parameter in parameter_list : <MASK> platform_list . append ( parameter . split ( "" = "" ) [ 1 ] ) return platform_list","if parameter . startswith ( ""platformName"" ) :","if parameter . startswith ( ""platform="" ) :",98.37188563,FALSE,96.43
2422,"def find_scintilla_constants ( f ) : lexers = [ ] states = [ ] for name in f . order : v = f . features [ name ] if v [ "" Category "" ] != "" Deprecated "" : if v [ "" FeatureType "" ] == "" val "" : <MASK> states . append ( ( name , v [ "" Value "" ] ) ) elif name . startswith ( "" SCLEX_ "" ) : lexers . append ( ( name , v [ "" Value "" ] ) ) return ( lexers , states )","if name . startswith ( ""SCE_"" ) :","if name . startswith ( ""SCINTO_"" ) :",98.81963958,FALSE,97.82
2423,"def get_operation_ast ( document_ast , operation_name = None ) : operation = None for definition in document_ast . definitions : if isinstance ( definition , ast . OperationDefinition ) : <MASK> # If no operation name is provided, only return an Operation if it is the only one present in the # document. This means that if we've encountered a second operation as we were iterating over the # definitions in the document, there are more than one Operation defined, and we should return None. if operation : return None operation = definition elif definition . name and definition . name . value == operation_name : return definition return operation",if not operation_name :,if operation_name is None :,88.77575098,FALSE,96.2
2424,"def _insertNewItemAtParent ( self , targetIndex ) : if not self . isContainer ( targetIndex ) : return elif not self . isContainerOpen ( targetIndex ) : uri = self . _rows [ targetIndex ] . uri modelNode = self . getNodeForURI ( uri ) <MASK> modelNode . markForRefreshing ( ) return self . refreshView ( targetIndex )",if modelNode :,if modelNode :,100,TRUE,100
2425,"def _get_trace ( self , model , guide , args , kwargs ) : model_trace , guide_trace = super ( ) . _get_trace ( model , guide , args , kwargs ) # Mark all sample sites with require_backward to gather enumerated # sites and adjust cond_indep_stack of all sample sites. for node in model_trace . nodes . values ( ) : <MASK> log_prob = node [ "" packed "" ] [ "" unscaled_log_prob "" ] require_backward ( log_prob ) self . _saved_state = model , model_trace , guide_trace , args , kwargs return model_trace , guide_trace","if node [ ""type"" ] == ""sample"" and not node [ ""is_observed"" ] :","if ""packed"" in node and ""unscaled_log_prob"" in node [",93.80961567,FALSE,86.56
2426,"def _url_encode_impl ( obj , charset , encode_keys , sort , key ) : from . datastructures import iter_multi_items iterable = iter_multi_items ( obj ) if sort : iterable = sorted ( iterable , key = key ) for key , value in iterable : if value is None : continue if not isinstance ( key , bytes ) : key = text_type ( key ) . encode ( charset ) <MASK> value = text_type ( value ) . encode ( charset ) yield _fast_url_quote_plus ( key ) + "" = "" + _fast_url_quote_plus ( value )","if not isinstance ( value , bytes ) :","if not isinstance ( value , bytes ) :",100,TRUE,100
2427,"def handle_parse_result ( self , ctx , opts , args ) : with augment_usage_errors ( ctx , param = self ) : value = self . consume_value ( ctx , opts ) try : value = self . full_process_value ( ctx , value ) except Exception : <MASK> raise value = None if self . callback is not None : try : value = invoke_param_callback ( self . callback , ctx , self , value ) except Exception : <MASK> raise if self . expose_value : ctx . params [ self . name ] = value return value , args",if not ctx . resilient_parsing :,if not self . allow_empty :,81.36317385,FALSE,90.87
2428,"def word_pattern ( pattern , str ) : dict = { } set_value = set ( ) list_str = str . split ( ) if len ( list_str ) != len ( pattern ) : return False for i in range ( len ( pattern ) ) : if pattern [ i ] not in dict : <MASK> return False dict [ pattern [ i ] ] = list_str [ i ] set_value . add ( list_str [ i ] ) else : if dict [ pattern [ i ] ] != list_str [ i ] : return False return True",if list_str [ i ] in set_value :,if set_value . add ( list_str [ i ] ) :,94.80449336,FALSE,93.48
2429,"def create ( self , path , wipe = False ) : # type: (Text, bool) -> bool _path = self . validatepath ( path ) with ftp_errors ( self , path ) : <MASK> empty_file = io . BytesIO ( ) self . ftp . storbinary ( str ( "" STOR  "" ) + _encode ( _path , self . ftp . encoding ) , empty_file ) return True return False",if wipe or not self . isfile ( path ) :,if not wipe :,93.67498789,FALSE,89.22
2430,"def build_output_for_item ( self , item ) : output = [ ] for field in self . fields : values = self . _get_item ( item , field ) <MASK> values = [ values ] for value in values : if value : output . append ( self . build_output_for_single_value ( value ) ) return "" "" . join ( output )","if not isinstance ( values , list ) :","if not isinstance ( values , list ) :",100,TRUE,100
2431,"def get_resource_public_actions ( resource_class ) : resource_class_members = inspect . getmembers ( resource_class ) resource_methods = { } for name , member in resource_class_members : if not name . startswith ( "" _ "" ) : if not name [ 0 ] . isupper ( ) : <MASK> if is_resource_action ( member ) : resource_methods [ name ] = member return resource_methods","if not name . startswith ( ""wait_until"" ) :",if inspect . isclass ( member ) and inspect . isfunction ( member ) :,89.31521081,FALSE,87.27
2432,"def get_command ( cls ) : ifconfig_cmd = "" ifconfig "" for path in [ "" /sbin "" , "" /usr/sbin "" , "" /bin "" , "" /usr/bin "" ] : <MASK> ifconfig_cmd = os . path . join ( path , ifconfig_cmd ) break ifconfig_cmd = ifconfig_cmd + ""  -a "" return ifconfig_cmd","if os . path . exists ( os . path . join ( path , ifconfig_cmd ) ) :",ifconfig_cmd . startswith ( path ) :,80.48195222,FALSE,80.47
2433,"def main ( ) : base_dir = os . path . join ( os . path . split ( __file__ ) [ 0 ] , "" .. "" , "" .. "" ) for path in PATHS : path = os . path . join ( base_dir , path ) for root , _ , files in os . walk ( path ) : for file in files : extension = os . path . splitext ( file ) [ 1 ] <MASK> path = os . path . join ( root , file ) validate_header ( path )",if extension in EXTENSIONS :,"if extension == "".py"" :",93.98038847,FALSE,93.6
2434,"def auth_login ( request ) : form = RegistrationForm ( request . POST or None ) if form . is_valid ( ) : authed_user = authenticate ( username = form . cleaned_data [ "" username "" ] , password = form . cleaned_data [ "" password "" ] , ) <MASK> login ( request , authed_user ) return HttpResponse ( "" Success "" ) raise Http404",if authed_user :,if authed_user :,100,TRUE,100
2435,"def set ( self , _key , _new_login = True ) : with self . lock : user = self . users . get ( current_user . id , None ) if user is None : self . users [ current_user . id ] = dict ( session_count = 1 , key = _key ) else : <MASK> user [ "" session_count "" ] + = 1 user [ "" key "" ] = _key",if _new_login :,if _new_login :,100,TRUE,100
2436,"def fetch ( self , fingerprints ) : to_fetch = [ f for f in fingerprints if f not in self . _cache ] self . _logger . debug ( "" cache size  %s "" % len ( self . _cache ) ) self . _logger . debug ( "" to fetch  %d  from  %d "" % ( len ( to_fetch ) , len ( fingerprints ) ) ) [ self . _redis_pipeline . hgetall ( key ) for key in to_fetch ] responses = self . _redis_pipeline . execute ( ) for index , key in enumerate ( to_fetch ) : response = responses [ index ] <MASK> self . _cache [ key ] = response [ FIELD_STATE ] else : self . _cache [ key ] = self . NOT_CRAWLED",if len ( response ) > 0 and FIELD_STATE in response :,if response [ FIELD_STATE ] :,91.57677807,FALSE,93.06
2437,"def _append_to_io_queue ( self , data , stream_name ) : # Make sure ANSI CSI codes and object links are stored as separate events # TODO: try to complete previously submitted incomplete code parts = re . split ( OUTPUT_SPLIT_REGEX , data ) for part in parts : if part : # split may produce empty string in the beginning or start # split the data so that very long lines separated for block in re . split ( "" (. { %d ,}) "" % ( self . _get_squeeze_threshold ( ) + 1 ) , part ) : <MASK> self . _queued_io_events . append ( ( block , stream_name ) )",if block :,if block not in self . _queued_io_events :,73.27617565,FALSE,92.45
2438,"def find_file_at_path_with_indexes ( self , path , url ) : if url . endswith ( "" / "" ) : path = os . path . join ( path , self . index_file ) return self . get_static_file ( path , url ) elif url . endswith ( "" / "" + self . index_file ) : <MASK> return self . redirect ( url , url [ : - len ( self . index_file ) ] ) else : try : return self . get_static_file ( path , url ) except IsDirectoryError : if os . path . isfile ( os . path . join ( path , self . index_file ) ) : return self . redirect ( url , url + "" / "" ) raise MissingFileError ( path )",if os . path . isfile ( path ) :,if url . endswith ( self . index_file ) :,93.74238907,FALSE,94.16
2439,"def module_list ( target , fast ) : """"""Find the list of modules to be compiled"""""" modules = [ ] native = native_modules ( target ) basedir = os . path . join ( ouroboros_repo_folder ( ) , "" ouroboros "" ) for name in os . listdir ( basedir ) : module_name , ext = os . path . splitext ( name ) if ext == "" .py "" or ext == "" "" and os . path . isdir ( os . path . join ( basedir , name ) ) : if module_name not in IGNORE_MODULES and module_name not in native : <MASK> modules . append ( module_name ) return set ( modules )",if not ( fast and module_name in KNOWN_PROBLEM_MODULES ) :,if fast :,92.93535199,FALSE,90.04
2440,"def housenumber ( self ) : if self . address : expression = r "" \ d+ "" pattern = re . compile ( expression ) match = pattern . search ( self . address ) <MASK> return int ( match . group ( 0 ) )",if match :,if match :,100,TRUE,100
2441,"def get_pip_version ( import_path = BASE_IMPORT_PATH ) : try : pip = importlib . import_module ( import_path ) except ImportError : <MASK> return get_pip_version ( import_path = "" pip "" ) else : import subprocess version = subprocess . check_output ( [ "" pip "" , "" --version "" ] ) if version : version = version . decode ( "" utf-8 "" ) . split ( ) [ 1 ] return version return "" 0.0.0 "" version = getattr ( pip , "" __version__ "" , None ) return version","if import_path != ""pip"" :",if sys . version_info . major < 3 :,94.74754592,FALSE,91.81
2442,"def __animate_progress ( self ) : """"""Change the status message, mostly used to animate progress."""""" while True : sleep_time = ThreadPool . PROGRESS_IDLE_DELAY with self . __progress_lock : if not self . __progress_status : sleep_time = ThreadPool . PROGRESS_IDLE_DELAY <MASK> self . __progress_status . update_progress ( self . __current_operation_name ) sleep_time = ThreadPool . PROGRESS_UPDATE_DELAY else : self . __progress_status . show_as_ready ( ) sleep_time = ThreadPool . PROGRESS_IDLE_DELAY # Allow some time for progress status to be updated. time . sleep ( sleep_time )",elif self . __show_animation :,elif self . __current_operation_name :,91.76023827,FALSE,96.09
2443,"def range_key_names ( self ) : keys = [ self . range_key_attr ] for index in self . global_indexes : range_key = None for key in index . schema : <MASK> range_key = keys . append ( key [ "" AttributeName "" ] ) keys . append ( range_key ) return keys","if key [ ""KeyType"" ] == ""RANGE"" :","if key [ ""Type"" ] == ""RangeKey"" :",95.87322063,FALSE,93.54
2444,"def run ( self ) : dist = self . distribution commands = dist . command_options . keys ( ) settings = { } for cmd in commands : if cmd == "" saveopts "" : continue # don't save our own options! for opt , ( src , val ) in dist . get_option_dict ( cmd ) . items ( ) : <MASK> settings . setdefault ( cmd , { } ) [ opt ] = val edit_config ( self . filename , settings , self . dry_run )","if src == ""command line"" :","if src == ""default"" :",98.15224019,FALSE,96.77
2445,"def parse_move ( self , node ) : old , new = "" "" , "" "" for child in node : tag , text = child . tag , child . text text = text . strip ( ) if text else None if tag == "" Old "" and text : old = text <MASK> new = text return Move ( old , new )","elif tag == ""New"" and text :","elif tag == ""New"" and text :",100,TRUE,100
2446,"def __codeanalysis_settings_changed ( self , current_finfo ) : if self . data : run_pyflakes , run_pep8 = self . pyflakes_enabled , self . pep8_enabled for finfo in self . data : self . __update_editor_margins ( finfo . editor ) finfo . cleanup_analysis_results ( ) if ( run_pyflakes or run_pep8 ) and current_finfo is not None : <MASK> finfo . run_code_analysis ( run_pyflakes , run_pep8 )",if current_finfo is not finfo :,if finfo . editor . settings . codeanalysis_enabled :,90.96292154,FALSE,91.16
2447,"def tchg ( var , width ) : "" Convert time string to given length "" ret = "" %2d h %02d "" % ( var / 60 , var % 60 ) <MASK> ret = "" %2d h "" % ( var / 60 ) <MASK> ret = "" %2d d "" % ( var / 60 / 24 ) <MASK> ret = "" %2d w "" % ( var / 60 / 24 / 7 ) return ret",if len ( ret ) > width :,if width :,62.94901752,FALSE,80.43
2448,"def spider_log_activity ( self , messages ) : for i in range ( 0 , messages ) : <MASK> self . sp_sl_p . send ( sha1 ( str ( randint ( 1 , 1000 ) ) ) , b "" http://helloworld.com/way/to/the/sun/ "" + b "" 0 "" , ) else : self . sp_sl_p . send ( sha1 ( str ( randint ( 1 , 1000 ) ) ) , b "" http://way.to.the.sun "" + b "" 0 "" ) self . sp_sl_p . flush ( )",if i % 2 == 0 :,if i == 0 :,70.21723961,FALSE,97.37
2449,"def decode_serial ( self , offset ) : serialnum = ( ( self . cache [ offset + 3 ] << 24 ) + ( self . cache [ offset + 2 ] << 16 ) + ( self . cache [ offset + 1 ] << 8 ) + self . cache [ offset ] ) serialstr = "" "" is_alnum = True for i in range ( 4 ) : <MASK> is_alnum = False break serialstr + = chr ( self . cache [ offset + 3 - i ] ) serial = serialstr if is_alnum else str ( serialnum ) self . ann_field ( offset , offset + 3 , "" Serial  "" + serial )",if not chr ( self . cache [ offset + 3 - i ] ) . isalnum ( ) :,if self . cache [ offset + 3 ] & ( 1 << i ) :,87.17910055,FALSE,91.76
2450,def gettext ( rv ) : for child in rv . childNodes : if child . nodeType == child . TEXT_NODE : yield child . nodeValue <MASK> for item in gettext ( child ) : yield item,if child . nodeType == child . ELEMENT_NODE :,elif child . nodeType == child . ELEMENT_NODE :,67.83035841,FALSE,94.87
2451,"def determine_block_hints ( self , text ) : hints = "" "" if text : if text [ 0 ] in ""   \n \x85 \u2028 \u2029 "" : hints + = str ( self . best_indent ) if text [ - 1 ] not in "" \n \x85 \u2028 \u2029 "" : hints + = "" - "" <MASK> hints + = "" + "" return hints","elif len ( text ) == 1 or text [ - 2 ] in ""\n\x85\u2028\u2029"" :","if text [ - 2 ] in ""\n\x85\u2028\u202",79.74613422,FALSE,85.29
2452,"def _infer_return_type ( * args ) : """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args : if arg is None : continue <MASK> if return_type is str : raise TypeError ( "" Can ' t mix bytes and non-bytes in  "" "" path components. "" ) return_type = bytes else : if return_type is bytes : raise TypeError ( "" Can ' t mix bytes and non-bytes in  "" "" path components. "" ) return_type = str if return_type is None : return str # tempfile APIs return a str by default. return return_type","if isinstance ( arg , bytes ) :",if arg is not None :,93.30091462,FALSE,94.83
2453,"def as_iconbitmap ( cls , rkey ) : """"""Get image path for use in iconbitmap property"""""" img = None if rkey in cls . _stock : data = cls . _stock [ rkey ] <MASK> fpath = data [ "" filename "" ] fname = os . path . basename ( fpath ) name , file_ext = os . path . splitext ( fname ) file_ext = str ( file_ext ) . lower ( ) if file_ext in TK_BITMAP_FORMATS : img = BITMAP_TEMPLATE . format ( fpath ) return img","if data [ ""type"" ] not in ( ""stock"" , ""data"" , ""image"" ) :","if ""filename"" in data :",66.26584696,FALSE,83.41
2454,"def anonymize_ip ( ip ) : if ip : match = RE_FIRST_THREE_OCTETS_OF_IP . findall ( str ( ip ) ) <MASK> return "" %s %s "" % ( match [ 0 ] [ 0 ] , "" 0 "" ) return "" """,if match :,if match :,100,TRUE,100
2455,"def serialize_tail ( self ) : msg = bytearray ( ) for v in self . info : <MASK> value = v [ "" value "" ] . encode ( "" utf-8 "" ) elif v [ "" type "" ] == BMP_TERM_TYPE_REASON : value = struct . pack ( "" !H "" , v [ "" value "" ] ) v [ "" len "" ] = len ( value ) msg + = struct . pack ( self . _TLV_PACK_STR , v [ "" type "" ] , v [ "" len "" ] ) msg + = value return msg","if v [ ""type"" ] == BMP_TERM_TYPE_STRING :","if v [ ""type"" ] == BMP_TERM_TYPE_STRING :",100,TRUE,100
2456,"def get_django_comment ( text : str , i : int ) - > str : end = i + 4 unclosed_end = 0 while end < = len ( text ) : <MASK> return text [ i : end ] if not unclosed_end and text [ end ] == "" < "" : unclosed_end = end end + = 1 raise TokenizationException ( "" Unclosed comment "" , text [ i : unclosed_end ] )","if text [ end - 2 : end ] == ""#}"" :","if text [ end ] == ""<"" :",90.87843892,FALSE,91.78
2457,"def ComboBoxDroppedHeightTest ( windows ) : "" Check if each combobox height is the same as the reference "" bugs = [ ] for win in windows : if not win . ref : continue if win . Class ( ) != "" ComboBox "" or win . ref . Class ( ) != "" ComboBox "" : continue <MASK> bugs . append ( ( [ win , ] , { } , testname , 0 , ) ) return bugs",if win . DroppedRect ( ) . height ( ) != win . ref . DroppedRect ( ) . height ( ) :,if win . ref . GetDroppedHeight ( ) != win . ref . GetDroppedHeight (,87.13799638,FALSE,84.92
2458,"def testBadModeArgument ( self ) : # verify that we get a sensible error message for bad mode argument bad_mode = "" qwerty "" try : f = self . open ( TESTFN , bad_mode ) except ValueError as msg : <MASK> s = str ( msg ) if TESTFN in s or bad_mode not in s : self . fail ( "" bad error message for invalid mode:  %s "" % s ) # if msg.args[0] == 0, we're probably on Windows where there may be # no obvious way to discover why open() failed. else : f . close ( ) self . fail ( "" no error for invalid mode:  %s "" % bad_mode )",if msg . args [ 0 ] != 0 :,if msg . args [ 0 ] == 0 :,74.01341085,FALSE,98.31
2459,"def command_group_expired ( self , command_group_name ) : try : deprecate_info = self . _command_loader . command_group_table [ command_group_name ] . group_kwargs . get ( "" deprecate_info "" , None ) <MASK> return deprecate_info . expired ( ) except AttributeError : # Items with only token presence in the command table will not have any data. They can't be expired. pass return False",if deprecate_info :,if deprecate_info :,100,TRUE,100
2460,"def test_non_uniform_probabilities_over_elements ( self ) : param = iap . Choice ( [ 0 , 1 ] , p = [ 0.25 , 0.75 ] ) samples = param . draw_samples ( ( 10000 , ) ) unique , counts = np . unique ( samples , return_counts = True ) assert len ( unique ) == 2 for val , count in zip ( unique , counts ) : if val == 0 : assert 2500 - 500 < count < 2500 + 500 <MASK> assert 7500 - 500 < count < 7500 + 500 else : assert False",elif val == 1 :,elif val == 1 :,100,TRUE,100
2461,"def get_labels ( directory ) : cache = get_labels . __cache if directory not in cache : l = { } for t in get_visual_configs ( directory ) [ 0 ] [ LABEL_SECTION ] : <MASK> Messager . warning ( "" In configuration, labels for  ' %s '  defined more than once. Only using the last set. "" % t . storage_form ( ) , - 1 , ) # first is storage for, rest are labels. l [ t . storage_form ( ) ] = t . terms [ 1 : ] cache [ directory ] = l return cache [ directory ]",if t . storage_form ( ) in l :,if t . storage_form ( ) in l :,100,TRUE,100
2462,"def try_split ( self , split_text : List [ str ] ) : ret = [ ] for i in split_text : if len ( i ) == 0 : continue val = int ( i , 2 ) <MASK> return None ret . append ( val ) if len ( ret ) != 0 : ret = bytes ( ret ) logger . debug ( f "" binary successful, returning  { ret . __repr__ ( ) } "" ) return ret",if val > 255 or val < 0 :,if val == 0 :,92.38526291,FALSE,93.58
2463,"def setCellValue ( self , row_idx , col , value ) : assert col . id == "" repls-marked "" with self . _lock : rgroup = self . events [ row_idx ] <MASK> return rgroup . _marked = value == "" true "" and True or False if self . _tree : self . _tree . invalidateCell ( row_idx , col )","if not isinstance ( rgroup , findlib2 . ReplaceHitGroup ) :",if rgroup . _marked == value :,87.96594075,FALSE,87.97
2464,"def create ( cls , settlement_manager , resource_id ) : """"""Create a production chain that can produce the given resource."""""" resource_producer = { } for abstract_building in AbstractBuilding . buildings . values ( ) : for resource , production_line in abstract_building . lines . items ( ) : <MASK> resource_producer [ resource ] = [ ] resource_producer [ resource ] . append ( ( production_line , abstract_building ) ) return ProductionChain ( settlement_manager , resource_id , resource_producer )",if resource not in resource_producer :,if resource not in resource_producer :,100,TRUE,100
2465,def get_all_partition_sets ( self ) : partition_sets = [ ] if self . partitions_handle : partition_sets . extend ( self . partitions_handle . get_partition_sets ( ) ) if self . scheduler_handle : partition_sets . extend ( [ schedule_def . get_partition_set ( ) for schedule_def in self . scheduler_handle . all_schedule_defs ( ) <MASK> ] ) return partition_sets,"if isinstance ( schedule_def , PartitionScheduleDefinition )","if isinstance ( schedule_def , PartitionScheduleDefinition )",100,TRUE,100
2466,"def _sendDatapointsNow ( self , datapoints ) : metrics = { } payload_pb = Payload ( ) for metric , datapoint in datapoints : <MASK> metric_pb = payload_pb . metrics . add ( ) metric_pb . metric = metric metrics [ metric ] = metric_pb else : metric_pb = metrics [ metric ] point_pb = metric_pb . points . add ( ) point_pb . timestamp = int ( datapoint [ 0 ] ) point_pb . value = datapoint [ 1 ] self . sendString ( payload_pb . SerializeToString ( ) )",if metric not in metrics :,if metric not in metrics :,100,TRUE,100
2467,"def execute ( self ) : if self . _dirty or not self . _qr : model_class = self . model_class query_meta = self . get_query_meta ( ) if self . _tuples : ResultWrapper = TuplesQueryResultWrapper <MASK> ResultWrapper = DictQueryResultWrapper elif self . _naive or not self . _joins or self . verify_naive ( ) : ResultWrapper = NaiveQueryResultWrapper elif self . _aggregate_rows : ResultWrapper = AggregateQueryResultWrapper else : ResultWrapper = ModelQueryResultWrapper self . _qr = ResultWrapper ( model_class , self . _execute ( ) , query_meta ) self . _dirty = False return self . _qr else : return self . _qr",elif self . _dicts :,elif self . _dicts :,100,TRUE,100
2468,"def get_metrics ( ) : classifier , feature_labels = load_classifier ( ) available_metrics = ImgageMetrics . get_metric_classes ( ) # todo review: DONE IN DOCS #  effective_metrics isn't used after filling it with values #  in the loops below effective_metrics = [ ] for metric in available_metrics : for label in feature_labels : for label_part in metric . get_labels ( ) : <MASK> effective_metrics . append ( metric ) return ( classifier , feature_labels , available_metrics )",if label_part == label and metric not in effective_metrics :,if classifier . is_metric_part_effective ( label_part ) :,94.18324839,FALSE,89.02
2469,"def test_nic_names ( self ) : p = subprocess . Popen ( [ "" ipconfig "" , "" /all "" ] , stdout = subprocess . PIPE ) out = p . communicate ( ) [ 0 ] if PY3 : out = str ( out , sys . stdout . encoding ) nics = psutil . net_io_counters ( pernic = True ) . keys ( ) for nic in nics : if "" pseudo-interface "" in nic . replace ( ""   "" , "" - "" ) . lower ( ) : continue <MASK> self . fail ( "" %r  nic wasn ' t found in  ' ipconfig /all '  output "" % nic )",if nic not in out :,"if out . lower ( ) not in ( ""t"" , ""t"" ) . lower",92.8313361,FALSE,87.08
2470,"def convert_with_key ( self , key , value , replace = True ) : result = self . configurator . convert ( value ) # If the converted value is different, save for next time if value is not result : if replace : self [ key ] = result <MASK> result . parent = self result . key = key return result","if type ( result ) in ( ConvertingDict , ConvertingList , ConvertingTuple ) :",elif self . parent :,90.10970792,FALSE,81.33
2471,"def _EvaluateFile ( self , test_list , file ) : ( name , ext ) = os . path . splitext ( file ) if ext == "" .cc "" or ext == "" .cpp "" or ext == "" .c "" : <MASK> logger . SilentLog ( "" Found native test file  %s "" % file ) test_list . append ( name )","if re . search ( ""_test$|_test_$|_unittest$|_unittest_$|^test_|Tests$"" , name ) :",if name in test_list :,89.56809343,FALSE,66.16
2472,"def leading_whitespace ( self , inputstring ) : """"""Get leading whitespace."""""" leading_ws = [ ] for i , c in enumerate ( inputstring ) : <MASK> leading_ws . append ( c ) else : break if self . indchar is None : self . indchar = c elif c != self . indchar : self . strict_err_or_warn ( "" found mixing of tabs and spaces "" , inputstring , i ) return "" "" . join ( leading_ws )",if c in legal_indent_chars :,if c in self . leading_ws :,93.73276497,FALSE,94.16
2473,"def ident_values ( self ) : value = self . _ident_values if value is False : value = None # XXX: how will this interact with orig_prefix ? #      not exposing attrs for now if orig_prefix is set. <MASK> wrapped = self . wrapped idents = getattr ( wrapped , "" ident_values "" , None ) if idents : value = [ self . _wrap_hash ( ident ) for ident in idents ] ##else: ##    ident = self.ident ##    if ident is not None: ##        value = [ident] self . _ident_values = value return value",if not self . orig_prefix :,if self . _prefix is not None :,97.24014144,FALSE,94.82
2474,"def _available_symbols ( self , scoperef , expr ) : cplns = [ ] found_names = set ( ) while scoperef : elem = self . _elem_from_scoperef ( scoperef ) for child in elem : name = child . get ( "" name "" , "" "" ) <MASK> if name not in found_names : found_names . add ( name ) ilk = child . get ( "" ilk "" ) or child . tag cplns . append ( ( ilk , name ) ) scoperef = self . parent_scoperef_from_scoperef ( scoperef ) if not scoperef : break return sorted ( cplns , key = operator . itemgetter ( 1 ) )",if name . startswith ( expr ) :,if name :,86.95611994,FALSE,95.24
2475,"def pid_from_name ( name ) : # quick and dirty, works with all linux not depending on ps output for pid in os . listdir ( "" /proc "" ) : try : int ( pid ) except : continue pname = "" "" with open ( "" /proc/ %s /cmdline "" % pid , "" r "" ) as f : pname = f . read ( ) <MASK> return int ( pid ) raise ProcessException ( "" No process with such name:  %s "" % name )",if name in pname :,if pname == name :,72.66228427,FALSE,95.18
2476,"def touch ( self ) : if not self . exists ( ) : try : self . parent ( ) . touch ( ) except ValueError : pass node = self . _fs . touch ( self . pathnames , { } ) <MASK> raise AssertionError ( "" Not a folder:  %s "" % self . path ) if self . watcher : self . watcher . emit ( "" created "" , self )",if not node . isdir :,if not node . exists ( ) :,95.40384852,FALSE,94.65
2477,"def setUp ( self ) : BaseTestCase . setUp ( self ) self . rawData = [ ] self . dataByKey = { } for i in range ( 1 , 11 ) : stringCol = "" String  %d "" % i fixedCharCol = ( "" Fixed Char  %d "" % i ) . ljust ( 40 ) rawCol = "" Raw  %d "" % i <MASK> nullableCol = "" Nullable  %d "" % i else : nullableCol = None dataTuple = ( i , stringCol , rawCol , fixedCharCol , nullableCol ) self . rawData . append ( dataTuple ) self . dataByKey [ i ] = dataTuple",if i % 2 :,if i % 2 == 0 :,84.62449962,FALSE,96.19
2478,"def GenerateVector ( self , hits , vector , level ) : """"""Generate possible hit vectors which match the rules."""""" for item in hits . get ( level , [ ] ) : if vector : if item < vector [ - 1 ] : continue if item > self . max_separation + vector [ - 1 ] : break new_vector = vector + [ item ] <MASK> yield new_vector elif level + 1 < len ( hits ) : for result in self . GenerateVector ( hits , new_vector , level + 1 ) : yield result",if level + 1 == len ( hits ) :,if level + 1 == len ( hits ) :,100,TRUE,100
2479,"def __repr__ ( self ) : attrs = [ ] for k in self . keydata : <MASK> attrs . append ( "" p( %d ) "" % ( self . size ( ) + 1 , ) ) elif hasattr ( self . key , k ) : attrs . append ( k ) if self . has_private ( ) : attrs . append ( "" private "" ) return "" < %s  @0x %x   %s > "" % ( self . __class__ . __name__ , id ( self ) , "" , "" . join ( attrs ) )","if k == ""p"" :","if k == ""p"" :",100,TRUE,100
2480,"def autoload ( self ) : if self . _app . config . THEME == "" auto "" : <MASK> if get_osx_theme ( ) == 1 : theme = DARK else : theme = LIGHT else : theme = self . guess_system_theme ( ) if theme == Dark : theme = MacOSDark else : # user settings have highest priority theme = self . _app . config . THEME self . load_theme ( theme )","if sys . platform == ""darwin"" :",if self . _app . config . USE_DARK :,92.51551769,FALSE,88.66
2481,"def _get_matching_bracket ( self , s , pos ) : if s [ pos ] != "" { "" : return None end = len ( s ) depth = 1 pos + = 1 while pos != end : c = s [ pos ] if c == "" { "" : depth + = 1 elif c == "" } "" : depth - = 1 <MASK> break pos + = 1 if pos < end and s [ pos ] == "" } "" : return pos return None",if depth == 0 :,if depth == 0 :,100,TRUE,100
2482,"def update_meter ( self , output , target , meters = { "" accuracy "" } ) : output = self . __to_tensor ( output ) target = self . __to_tensor ( target ) for meter in meters : <MASK> self . __addmeter ( meter ) if meter in [ "" ap "" , "" map "" , "" confusion "" ] : target_th = self . _ver2tensor ( target ) self . meter [ meter ] . add ( output , target_th ) else : self . meter [ meter ] . add ( output , target )",if meter not in self . meter . keys ( ) :,if not self . meter_has_metric ( meter ) :,91.46811982,FALSE,92.63
2483,"def _reinit_optimizers_with_oss ( self ) : optimizers = self . lightning_module . trainer . optimizers for x , optimizer in enumerate ( optimizers ) : if is_lightning_optimizer ( optimizer ) : optimizer = optimizer . _optimizer <MASK> optim_class = type ( optimizer ) zero_optimizer = OSS ( params = optimizer . param_groups , optim = optim_class , * * optimizer . defaults ) optimizers [ x ] = zero_optimizer del optimizer trainer = self . lightning_module . trainer trainer . optimizers = optimizers trainer . convert_to_lightning_optimizers ( )","if not isinstance ( optimizer , OSS ) :","if isinstance ( optimizer , OSS ) :",95.49799973,FALSE,98.02
2484,"def OnSelChanged ( self , event ) : self . item = event . GetItem ( ) if self . item : self . log . write ( "" OnSelChanged:  %s "" % self . GetItemText ( self . item ) ) <MASK> self . log . write ( "" , BoundingRect:  %s \n "" % self . GetBoundingRect ( self . item , True ) ) else : self . log . write ( "" \n "" ) event . Skip ( )","if wx . Platform == ""__WXMSW__"" :",if self . GetBoundingRect :,93.66093235,FALSE,86.66
2485,"def parse_batch ( args ) : errmsg = "" Invalid batch definition: batch entry has to be defined as RULE=BATCH/BATCHES (with integers BATCH <= BATCHES, BATCH >= 1). "" if args . batch is not None : rule , batchdef = parse_key_value_arg ( args . batch , errmsg = errmsg ) try : batch , batches = batchdef . split ( "" / "" ) batch = int ( batch ) batches = int ( batches ) except ValueError : raise ValueError ( errmsg ) <MASK> raise ValueError ( errmsg ) return Batch ( rule , batch , batches ) return None",if batch > batches or batch < 1 :,if batch < 1 :,83.10239583,FALSE,96.25
2486,"def get_foreign_key_columns ( self , engine , table_name ) : foreign_keys = set ( ) table = db_utils . get_table ( engine , table_name ) inspector = reflection . Inspector . from_engine ( engine ) for column_dict in inspector . get_columns ( table_name ) : column_name = column_dict [ "" name "" ] column = getattr ( table . c , column_name ) <MASK> foreign_keys . add ( column_name ) return foreign_keys",if column . foreign_keys :,if column . foreign_key :,97.42816918,FALSE,97.76
2487,"def update ( self , t ) : l = int ( t * self . nr_of_tiles ) for i in range ( self . nr_of_tiles ) : t = self . tiles_order [ i ] <MASK> self . turn_off_tile ( t ) else : self . turn_on_tile ( t )",if i < l :,if l < t :,95.13191128,FALSE,94.33
2488,"def read ( self , amt = None ) : # the _rbuf test is only in this first if for speed.  It's not # logically necessary if self . _rbuf and not amt is None : L = len ( self . _rbuf ) <MASK> amt - = L else : s = self . _rbuf [ : amt ] self . _rbuf = self . _rbuf [ amt : ] return s s = self . _rbuf + self . _raw_read ( amt ) self . _rbuf = b "" "" return s",if amt > L :,if amt > L :,100,TRUE,100
2489,"def draw_menu_button ( self , context , layout , node , text ) : if ( hasattr ( node . id_data , "" sv_show_socket_menus "" ) and node . id_data . sv_show_socket_menus ) : <MASK> layout . menu ( "" SV_MT_SocketOptionsMenu "" , text = "" "" , icon = "" TRIA_DOWN "" )",if self . is_output or self . is_linked or not self . use_prop :,if not self . is_socket_visible ( context ) :,56.9011723,FALSE,83.36
2490,"def __enter__ ( self ) : with DB . connection_context ( ) : session_record = SessionRecord ( ) session_record . f_session_id = self . _session_id session_record . f_engine_name = self . _engine_name session_record . f_engine_type = EngineType . STORAGE # TODO: engine address session_record . f_engine_address = { } session_record . f_create_time = current_timestamp ( ) rows = session_record . save ( force_insert = True ) <MASK> raise Exception ( f "" create session record  { self . _session_id }  failed "" ) LOGGER . debug ( f "" save session  { self . _session_id }  record "" ) self . create ( ) return self",if rows != 1 :,if not rows :,98.05125137,FALSE,96.84
2491,"def tearDown ( self ) : """"""Shutdown the server."""""" try : if self . server : self . server . stop ( 2.0 ) <MASK> self . root_logger . removeHandler ( self . sl_hdlr ) self . sl_hdlr . close ( ) finally : BaseTest . tearDown ( self )",if self . sl_hdlr :,if self . sl_hdlr :,100,TRUE,100
2492,"def _dec_device ( self , srcdev , dstdev ) : if srcdev : self . srcdevs [ srcdev ] - = 1 <MASK> del self . srcdevs [ srcdev ] self . _set_limits ( "" read "" , self . srcdevs ) if dstdev : self . dstdevs [ dstdev ] - = 1 if self . dstdevs [ dstdev ] == 0 : del self . dstdevs [ dstdev ] self . _set_limits ( "" write "" , self . dstdevs )",if self . srcdevs [ srcdev ] == 0 :,if self . srcdevs [ srcdev ] == 0 :,100,TRUE,100
2493,"def array_for ( self , i ) : if 0 < = i < self . _cnt : <MASK> return self . _tail node = self . _root level = self . _shift while level > 0 : assert isinstance ( node , Node ) node = node . _array [ ( i >> level ) & 0x01F ] level - = 5 return node . _array affirm ( False , u "" Index out of Range "" )",if i >= self . tailoff ( ) :,if self . _array [ ( i >> 0x01F ) & 0x01F,59.56476869,FALSE,87.32
2494,"def convert_tensor ( self , offsets , sizes ) : results = [ ] for b , batch in enumerate ( offsets ) : utterances = [ ] for p , utt in enumerate ( batch ) : size = sizes [ b ] [ p ] <MASK> utterances . append ( utt [ 0 : size ] ) else : utterances . append ( torch . tensor ( [ ] , dtype = torch . int ) ) results . append ( utterances ) return results",if sizes [ b ] [ p ] > 0 :,if size > 0 :,88.96616103,FALSE,91.17
2495,"def _predict_proba ( self , X , preprocess = True ) : if preprocess : X = self . preprocess ( X ) if self . problem_type == REGRESSION : return self . model . predict ( X ) y_pred_proba = self . model . predict_proba ( X ) if self . problem_type == BINARY : if len ( y_pred_proba . shape ) == 1 : return y_pred_proba <MASK> return y_pred_proba [ : , 1 ] else : return y_pred_proba elif y_pred_proba . shape [ 1 ] > 2 : return y_pred_proba else : return y_pred_proba [ : , 1 ]",elif y_pred_proba . shape [ 1 ] > 1 :,elif y_pred_proba . shape [ 1 ] == 2 :,98.41977652,FALSE,97.13
2496,def timeout ( self ) : now = ptime . time ( ) dt = now - self . lastPlayTime if dt < 0 : return n = int ( self . playRate * dt ) if n != 0 : self . lastPlayTime + = float ( n ) / self . playRate <MASK> self . play ( 0 ) self . jumpFrames ( n ),"if self . currentIndex + n > self . image . shape [ self . axes [ ""t"" ] ] :",if self . lastPlayTime >= self . maxplayRate :,82.7562766,FALSE,77.11
2497,"def __init__ ( self , data , weights = None , ddof = 0 ) : self . data = np . asarray ( data ) if weights is None : self . weights = np . ones ( self . data . shape [ 0 ] ) else : self . weights = np . asarray ( weights ) . astype ( float ) # TODO: why squeeze? <MASK> self . weights = self . weights . squeeze ( ) self . ddof = ddof",if len ( self . weights . shape ) > 1 and len ( self . weights ) > 1 :,if self . data . ndim == 1 :,91.0256853,FALSE,82.87
2498,"def writerow ( self , row ) : unicode_row = [ ] for col in row : <MASK> unicode_row . append ( col . encode ( "" utf-8 "" ) . strip ( ) ) else : unicode_row . append ( col ) self . writer . writerow ( unicode_row ) # Fetch UTF-8 output from the queue ... data = self . queue . getvalue ( ) data = data . decode ( "" utf-8 "" ) # ... and reencode it into the target encoding data = self . encoder . encode ( data ) # write to the target stream self . stream . write ( data ) # empty queue self . queue . truncate ( 0 )",if type ( col ) == str or type ( col ) == unicode :,"if isinstance ( col , unicode ) :",89.11713539,FALSE,89.58
2499,"def __init__ ( self , choices , allow_blank = False , * * kwargs ) : self . choiceset = choices self . allow_blank = allow_blank self . _choices = dict ( ) # Unpack grouped choices for k , v in choices : <MASK> for k2 , v2 in v : self . _choices [ k2 ] = v2 else : self . _choices [ k ] = v super ( ) . __init__ ( * * kwargs )","if type ( v ) in [ list , tuple ] :","if isinstance ( v , ( list , tuple ) ) :",94.79325751,FALSE,91.46
2500,"def simp_ext ( _ , expr ) : if expr . op . startswith ( "" zeroExt_ "" ) : arg = expr . args [ 0 ] <MASK> return arg return ExprCompose ( arg , ExprInt ( 0 , expr . size - arg . size ) ) if expr . op . startswith ( "" signExt_ "" ) : arg = expr . args [ 0 ] add_size = expr . size - arg . size new_expr = ExprCompose ( arg , ExprCond ( arg . msb ( ) , ExprInt ( size2mask ( add_size ) , add_size ) , ExprInt ( 0 , add_size ) ) , ) return new_expr return expr",if expr . size == arg . size :,if arg . msb ( ) == 0 :,91.84591729,FALSE,94.19
2501,"def mark_differences ( value : str , compare_against : str ) : result = [ ] for i , char in enumerate ( value ) : try : <MASK> result . append ( ' <font color= "" red "" > {} </font> ' . format ( char ) ) else : result . append ( char ) except IndexError : result . append ( char ) return "" "" . join ( result )",if char != compare_against [ i ] :,if compare_against and ( i == 0 and compare_against in char ) :,91.5907999,FALSE,84.56
2502,"def run_query ( self , query , user ) : url = "" %s %s "" % ( self . base_url , "" & "" . join ( query . split ( "" \n "" ) ) ) error = None data = None try : response = requests . get ( url , auth = self . auth , verify = self . verify ) <MASK> data = _transform_result ( response ) else : error = "" Failed getting results ( %d ) "" % response . status_code except Exception as ex : data = None error = str ( ex ) return data , error",if response . status_code == 200 :,if response . status_code == 200 :,100,TRUE,100
2503,"def on_enter ( self ) : """"""Fired when mouse enter the bbox of the widget."""""" if hasattr ( self , "" md_bg_color "" ) and self . focus_behavior : <MASK> self . md_bg_color = self . theme_cls . bg_normal else : if not self . focus_color : self . md_bg_color = App . get_running_app ( ) . theme_cls . bg_normal else : self . md_bg_color = self . focus_color","if hasattr ( self , ""theme_cls"" ) and not self . focus_color :","if self . focus_color == ""normal"" :",87.56664716,FALSE,88.58
2504,"def tearDown ( self ) : if not self . is_playback ( ) : try : <MASK> self . sms . delete_hosted_service ( self . hosted_service_name ) except : pass try : if self . storage_account_name is not None : self . sms . delete_storage_account ( self . storage_account_name ) except : pass try : self . sms . delete_affinity_group ( self . affinity_group_name ) except : pass return super ( LegacyMgmtAffinityGroupTest , self ) . tearDown ( )",if self . hosted_service_name is not None :,if self . hosted_service_name is not None :,100,TRUE,100
2505,"def name2cp ( k ) : if k == "" apos "" : return ord ( "" ' "" ) if hasattr ( htmlentitydefs , "" name2codepoint "" ) : # requires Python 2.3 return htmlentitydefs . name2codepoint [ k ] else : k = htmlentitydefs . entitydefs [ k ] <MASK> return int ( k [ 2 : - 1 ] ) # not in latin-1 return ord ( codecs . latin_1_decode ( k ) [ 0 ] )","if k . startswith ( ""&#"" ) and k . endswith ( "";"" ) :","if k . startswith ( ""x"" ) and k . endswith ( ""x"" ) :",97.30926925,FALSE,94.1
2506,"def _para_set ( self , params , part ) : if len ( params ) == 0 : result = suggest ( [ i . get_name ( ) for i in self . _options ] , part ) return result elif len ( params ) == 1 : paramName = params [ 0 ] if paramName not in self . _options : return [ ] opt = self . _options [ paramName ] paramType = opt . get_type ( ) <MASK> values = [ opt . get_default_value ( ) == "" True "" and "" False "" or "" True "" ] else : values = self . _memory [ paramName ] return suggest ( values , part ) else : return [ ]","if paramType == ""boolean"" :","if paramType == ""bool"" :",99.02433197,FALSE,98.3
2507,"def hexcmp ( x , y ) : try : a = int ( x , 16 ) b = int ( y , 16 ) <MASK> return - 1 if a > b : return 1 return 0 except : return cmp ( x , y )",if a < b :,if a < b :,100,TRUE,100
2508,"def execute ( self , statement , arguments = None ) : while True : try : if arguments : self . cursor . execute ( statement , arguments ) else : self . cursor . execute ( statement ) except sqlite3 . OperationalError as ex : <MASK> raise else : break if statement . lstrip ( ) . upper ( ) . startswith ( "" SELECT "" ) : return self . cursor . fetchall ( )","if ""locked"" not in getSafeExString ( ex ) :",if ex . errno != sqlite3 . EINTR :,92.83958641,FALSE,88.12
2509,"def _test_forever ( self , tests ) : while True : for test_name in tests : yield test_name <MASK> return if self . ns . fail_env_changed and self . environment_changed : return",if self . bad :,if self . ns . fail_test_changed and self . test_name_changed :,63.40097235,FALSE,73.09
2510,"def removeUser ( self , username ) : hideFromOSD = not constants . SHOW_DIFFERENT_ROOM_OSD if username in self . _users : user = self . _users [ username ] if user . room : <MASK> hideFromOSD = not constants . SHOW_SAME_ROOM_OSD if username in self . _users : self . _users . pop ( username ) message = getMessage ( "" left-notification "" ) . format ( username ) self . ui . showMessage ( message , hideFromOSD ) self . _client . lastLeftTime = time . time ( ) self . _client . lastLeftUser = username self . userListChange ( )",if self . isRoomSame ( user . room ) :,if user . room . owner != self . _client . owner :,93.98778755,FALSE,90.7
2511,"def AutoTest ( ) : with open ( sys . argv [ 1 ] , "" rb "" ) as f : for line in f . read ( ) . split ( b "" \n "" ) : line = BYTES2SYSTEMSTR ( line . strip ( ) ) if not line : continue elif line . startswith ( "" # "" ) : print ( line ) else : print ( "" >>>  "" + line ) os . system ( line ) sys . stdout . write ( "" \n press enter to continue... "" ) <MASK> input ( ) else : raw_input ( ) sys . stdout . write ( "" \n "" )",if PY3 :,"if sys . platform == ""win32"" :",94.12067244,FALSE,92.93
2512,"def get_first_field ( layout , clz ) : for layout_object in layout . fields : <MASK> return layout_object elif hasattr ( layout_object , "" get_field_names "" ) : gf = get_first_field ( layout_object , clz ) if gf : return gf","if issubclass ( layout_object . __class__ , clz ) :","if issubclass ( layout_object , clz ) :",85.02878605,FALSE,89.74
2513,"def sanitize_event_keys ( kwargs , valid_keys ) : # Sanity check: Don't honor keys that we don't recognize. for key in list ( kwargs . keys ( ) ) : if key not in valid_keys : kwargs . pop ( key ) # Truncate certain values over 1k for key in [ "" play "" , "" role "" , "" task "" , "" playbook "" ] : if isinstance ( kwargs . get ( "" event_data "" , { } ) . get ( key ) , str ) : <MASK> kwargs [ "" event_data "" ] [ key ] = Truncator ( kwargs [ "" event_data "" ] [ key ] ) . chars ( 1024 )","if len ( kwargs [ ""event_data"" ] [ key ] ) > 1024 :","if kwargs [ ""event_data"" ] [ key ] :",71.98844062,FALSE,94.68
2514,"def visit_productionlist ( self , node ) : self . new_state ( ) names = [ ] for production in node : names . append ( production [ "" tokenname "" ] ) maxlen = max ( len ( name ) for name in names ) for production in node : <MASK> self . add_text ( production [ "" tokenname "" ] . ljust ( maxlen ) + ""  ::= "" ) lastname = production [ "" tokenname "" ] else : self . add_text ( "" %s      "" % ( ""   "" * len ( lastname ) ) ) self . add_text ( production . astext ( ) + self . nl ) self . end_state ( wrap = False ) raise nodes . SkipNode","if production [ ""tokenname"" ] :","if production [ ""tokenname"" ] :",100,TRUE,100
2515,"def uuid ( self ) : if not getattr ( self , "" _uuid "" , None ) : <MASK> self . _uuid = self . repository . _kp_uuid ( self . path ) # Use repository UUID (even if None) else : self . _uuid = str ( uuid . uuid4 ( ) ) return self . _uuid",if self . repository is not None :,if self . repository :,88.13471981,FALSE,93.84
2516,"def remove ( self , values ) : if not isinstance ( values , ( list , tuple , set ) ) : values = [ values ] for v in values : v = str ( v ) <MASK> self . _definition . pop ( v , None ) elif self . _definition == "" ANY "" : if v == "" ANY "" : self . _definition = [ ] elif v in self . _definition : self . _definition . remove ( v ) if ( self . _value is not None and self . _value not in self . _definition and self . _not_any ( ) ) : raise ConanException ( bad_value_msg ( self . _name , self . _value , self . values_range ) )","if isinstance ( self . _definition , dict ) :",if self . _definition == v :,94.5451563,FALSE,94.97
2517,"def make ( self ) : pygments_dir = join ( self . dir , "" externals "" , "" pygments "" ) if exists ( pygments_dir ) : run_in_dir ( "" hg pull "" , pygments_dir , self . log . info ) run_in_dir ( "" hg update "" , pygments_dir , self . log . info ) else : <MASK> os . makedirs ( dirname ( pygments_dir ) ) run_in_dir ( "" hg clone http://dev.pocoo.org/hg/pygments-main  %s "" % basename ( pygments_dir ) , dirname ( pygments_dir ) , self . log . info , )",if not exists ( dirname ( pygments_dir ) ) :,if not exists ( dirname ( pygments_dir ) ) :,100,TRUE,100
2518,def set_field ( self ) : i = 0 for string in self . display_string : <MASK> self . config [ self . field + str ( i ) ] = self . conversion_fn ( self . str [ i ] ) else : self . config [ self . field + str ( i ) ] = self . str [ i ] i = i + 1,if self . conversion_fn :,if self . conversion_fn :,100,TRUE,100
2519,"def cleanup ( self ) : with self . lock : for proc in self . processes : <MASK> continue proc . join ( ) self . processes . remove ( proc ) log . debug ( "" Subprocess  %s  cleaned up "" , proc . name )",if proc . is_alive ( ) :,if not proc . is_alive ( ) :,63.87136952,FALSE,95.51
2520,"def setup ( self , gen ) : Node . setup ( self , gen ) for c in self . children : c . setup ( gen ) if not self . accepts_epsilon : # If it's not already accepting epsilon, it might now do so. for c in self . children : # any non-epsilon means all is non-epsilon <MASK> break else : self . accepts_epsilon = 1 gen . changed ( )",if not c . accepts_epsilon :,if c . accept_epsilon :,96.81957585,FALSE,94.46
2521,"def __call__ ( self , message ) : with self . _lock : self . _pending_ack + = 1 self . max_pending_ack = max ( self . max_pending_ack , self . _pending_ack ) self . seen_message_ids . append ( int ( message . attributes [ "" seq_num "" ] ) ) time . sleep ( self . _processing_time ) with self . _lock : self . _pending_ack - = 1 message . ack ( ) self . completed_calls + = 1 if self . completed_calls > = self . _resolve_at_msg_count : <MASK> self . done_future . set_result ( None )",if not self . done_future . done ( ) :,if self . done_future :,94.17081562,FALSE,94.85
2522,"def build_canned_image_list ( path ) : layers_path = get_bitbake_var ( "" BBLAYERS "" ) canned_wks_layer_dirs = [ ] if layers_path is not None : for layer_path in layers_path . split ( ) : for wks_path in ( WIC_DIR , SCRIPTS_CANNED_IMAGE_DIR ) : cpath = os . path . join ( layer_path , wks_path ) <MASK> canned_wks_layer_dirs . append ( cpath ) cpath = os . path . join ( path , CANNED_IMAGE_DIR ) canned_wks_layer_dirs . append ( cpath ) return canned_wks_layer_dirs",if os . path . isdir ( cpath ) :,if os . path . exists ( cpath ) :,98.59738801,FALSE,98.31
2523,"def _recv_loop ( self ) - > None : async with self . _ws as connection : self . _connected = True self . connection = connection while self . _connected : try : resp = await self . connection . recv ( ) <MASK> await self . _on_message ( resp ) except ( websockets . ConnectionClosed , ConnectionResetError ) : logger . info ( "" connection closed "" ) break await asyncio . sleep ( 0 ) if self . _connected : self . _loop . create_task ( self . dispose ( ) )",if resp :,if resp :,100,TRUE,100
2524,"def _get_between ( content , start , end = None ) : should_yield = False for line in content . split ( "" \n "" ) : if start in line : should_yield = True continue <MASK> return if should_yield and line : yield line . strip ( ) . split ( ""   "" ) [ 0 ]",if end and end in line :,if end and end in line :,100,TRUE,100
2525,"def handle_parse_result ( self , ctx , opts , args ) : if self . name in opts : if self . mutually_exclusive . intersection ( opts ) : self . _raise_exclusive_error ( ) <MASK> self . _raise_exclusive_error ( ) return super ( MutuallyExclusiveOption , self ) . handle_parse_result ( ctx , opts , args )",if self . multiple and len ( set ( opts [ self . name ] ) ) > 1 :,if self . mutually_exclusive . intersection ( opts ) :,80.85309972,FALSE,82.32
2526,"def write ( self , s ) : if self . interactive : <MASK> self . active_mode . write ( s ) else : component . get ( "" CmdLine "" ) . add_line ( s , False ) self . events . append ( s ) else : print ( colors . strip_colors ( s ) )","if isinstance ( self . active_mode , deluge . ui . console . modes . cmdline . CmdLine ) :",if self . active_mode :,58.49088928,FALSE,77.09
2527,"def findfiles ( path ) : files = [ ] for name in os . listdir ( path ) : # ignore hidden files/dirs and other unwanted files if name . startswith ( "" . "" ) or name == "" lastsnap.jpg "" : continue pathname = os . path . join ( path , name ) st = os . lstat ( pathname ) mode = st . st_mode <MASK> files . extend ( findfiles ( pathname ) ) elif stat . S_ISREG ( mode ) : files . append ( ( pathname , name , st ) ) return files",if stat . S_ISDIR ( mode ) :,if stat . S_ISDIR ( mode ) :,100,TRUE,100
2528,"def _get_documented_completions ( self , table , startswith = None ) : names = [ ] for key , command in table . items ( ) : if getattr ( command , "" _UNDOCUMENTED "" , False ) : # Don't tab complete undocumented commands/params continue if startswith is not None and not key . startswith ( startswith ) : continue <MASK> continue names . append ( key ) return names","if getattr ( command , ""positional_arg"" , False ) :",if key . startswith ( self . _prefix ) :,94.77505256,FALSE,86.69
2529,"def fix_newlines ( lines ) : """"""Convert newlines to unix."""""" for i , line in enumerate ( lines ) : <MASK> lines [ i ] = line [ : - 2 ] + "" \n "" elif line . endswith ( "" \r "" ) : lines [ i ] = line [ : - 1 ] + "" \n ""","if line . endswith ( ""\r\n"" ) :","if line . endswith ( ""\n"" ) :",92.76093423,FALSE,96.7
2530,"def GeneratePageMetatadata ( self , task ) : address_space = self . session . GetParameter ( "" default_address_space "" ) for vma in task . mm . mmap . walk_list ( "" vm_next "" ) : start = vma . vm_start end = vma . vm_end # Skip the entire region. if end < self . plugin_args . start : continue # Done. if start > self . plugin_args . end : break for vaddr in utils . xrange ( start , end , 0x1000 ) : <MASK> yield vaddr , self . _CreateMetadata ( address_space . describe_vtop ( vaddr ) )",if self . plugin_args . start <= vaddr <= self . plugin_args . end :,if self . plugin_args . start <= vaddr <= self . plugin_args . end,98.91736986,FALSE,98.28
2531,"def get_shape_at_node ( self , node , assumptions ) : for k , v in assumptions . items ( ) : <MASK> return v if node . inputs : return node . container . shape ( input_shapes = [ self . get_shape_at_node ( input_node , assumptions ) for input_node in node . inputs ] ) else : return node . container . shape ( None )",if k in node . names :,if k in self . _shape_map :,72.02613152,FALSE,92.04
2532,"def fix_doc ( self , doc ) : type = doc . get ( "" type "" , { } ) . get ( "" key "" ) if type == "" /type/work "" : <MASK> # some record got empty author records because of an error # temporary hack to fix doc [ "" authors "" ] = [ a for a in doc [ "" authors "" ] if "" author "" in a and "" key "" in a [ "" author "" ] ] elif type == "" /type/edition "" : # get rid of title_prefix. if "" title_prefix "" in doc : title = doc [ "" title_prefix "" ] . strip ( ) + ""   "" + doc . get ( "" title "" , "" "" ) doc [ "" title "" ] = title . strip ( ) del doc [ "" title_prefix "" ] return doc","if doc . get ( ""authors"" ) :","if ""authors"" in doc :",91.1540225,FALSE,95.71
2533,"def modify_column ( self , column : List [ Optional [ "" Cell "" ] ] ) : for i in range ( len ( column ) ) : gate = column [ i ] <MASK> continue elif isinstance ( gate , ParityControlCell ) : # The first parity control to modify the column must merge all # of the other parity controls into itself. column [ i ] = None self . _basis_change + = gate . _basis_change self . qubits + = gate . qubits elif gate is not None : column [ i ] = gate . controlled_by ( self . qubits [ 0 ] )",if gate is self :,if gate is None :,84.34188391,FALSE,97.95
2534,"def onSync ( self , auto = False , reload = True ) : if not auto or ( self . pm . profile [ "" syncKey "" ] and self . pm . profile [ "" autoSync "" ] and not self . safeMode ) : from aqt . sync import SyncManager if not self . unloadCollection ( ) : return # set a sync state so the refresh timer doesn't fire while deck # unloaded self . state = "" sync "" self . syncer = SyncManager ( self , self . pm ) self . syncer . sync ( ) if reload : <MASK> self . loadCollection ( )",if not self . col :,if self . safeMode :,97.84007277,FALSE,95.86
2535,"def _has_url_match ( self , match , request_url ) : url = match [ "" url "" ] if _is_string ( url ) : <MASK> return self . _has_strict_url_match ( url , request_url ) else : url_without_qs = request_url . split ( "" ? "" , 1 ) [ 0 ] return url == url_without_qs elif isinstance ( url , re . _pattern_type ) and url . match ( request_url ) : return True else : return False","if match [ ""match_querystring"" ] :","if url . startswith ( ""https?://"" ) :",64.28946565,FALSE,89.53
2536,"def pool_image ( self , image ) : if self . count < self . pool_size : self . pool . append ( image ) self . count + = 1 return image else : p = random . random ( ) <MASK> random_id = random . randint ( 0 , self . pool_size - 1 ) temp = self . pool [ random_id ] self . pool [ random_id ] = image return temp else : return image",if p > 0.5 :,if p < self . pool_size :,84.71935265,FALSE,92.29
2537,"def get_target_dimensions ( self ) : width , height = self . engine . size for operation in self . operations : <MASK> width = operation [ "" right "" ] - operation [ "" left "" ] height = operation [ "" bottom "" ] - operation [ "" top "" ] if operation [ "" type "" ] == "" resize "" : width = operation [ "" width "" ] height = operation [ "" height "" ] return ( width , height )","if operation [ ""type"" ] == ""crop"" :","if operation [ ""type"" ] == ""resize"" :",98.65046662,FALSE,97.54
2538,"def validate_matrix ( matrix ) : if not matrix : return None for key , value in matrix . items ( ) : <MASK> raise ValidationError ( "" ` {} ` defines a non uniform distribution,  "" "" and it cannot be used with bayesian optimization. "" . format ( key ) ) return matrix",if value . is_distribution and not value . is_uniform :,"if not ( key in ( ""uniform"" , ""uniform"" ) ) :",79.20427284,FALSE,79.5
2539,"def scm_to_conandata ( self ) : try : scm_to_conandata = get_env ( "" CONAN_SCM_TO_CONANDATA "" ) <MASK> scm_to_conandata = self . get_item ( "" general.scm_to_conandata "" ) return scm_to_conandata . lower ( ) in ( "" 1 "" , "" true "" ) except ConanException : return False",if scm_to_conandata is None :,if not scm_to_conandata :,78.1402131,FALSE,94.16
2540,"def _link_vrf_table ( self , vrf_table , rt_list ) : route_family = vrf_table . route_family for rt in rt_list : rt_rf_id = rt + "" : "" + str ( route_family ) table_set = self . _tables_for_rt . get ( rt_rf_id ) <MASK> table_set = set ( ) self . _tables_for_rt [ rt_rf_id ] = table_set table_set . add ( vrf_table ) LOG . debug ( "" Added VrfTable  %s  to import RT table list:  %s "" , vrf_table , rt )",if table_set is None :,if table_set is None :,100,TRUE,100
2541,"def add_tags ( self , cve_results : Dict [ str , Dict [ str , Dict [ str , str ] ] ] , file_object : FileObject ) : # results structure: {'component': {'cve_id': {'score2': '6.4', 'score3': 'N/A'}}} for component in cve_results : for cve_id in cve_results [ component ] : entry = cve_results [ component ] [ cve_id ] <MASK> self . add_analysis_tag ( file_object , "" CVE "" , "" critical CVE "" , TagColor . RED , True ) return",if self . _entry_has_critical_rating ( entry ) :,"if entry [ ""score2"" ] == ""6.4"" :",95.97245891,FALSE,89.31
2542,"def _validate ( self ) : try : super ( CustomClassifier , self ) . _validate ( ) except UnsupportedDataType : if self . dtype in FACTOR_DTYPES : raise UnsupportedDataType ( typename = type ( self ) . __name__ , dtype = self . dtype , hint = "" Did you mean to create a CustomFactor? "" , ) <MASK> raise UnsupportedDataType ( typename = type ( self ) . __name__ , dtype = self . dtype , hint = "" Did you mean to create a CustomFilter? "" , ) raise",elif self . dtype in FILTER_DTYPES :,if self . dtype in FILTER_DTYPES :,95.90090658,FALSE,97.76
2543,"def formatMessage ( self , record ) : recordcopy = copy ( record ) levelname = recordcopy . levelname seperator = ""   "" * ( 8 - len ( recordcopy . levelname ) ) if self . use_colors : levelname = self . color_level_name ( levelname , recordcopy . levelno ) <MASK> recordcopy . msg = recordcopy . __dict__ [ "" color_message "" ] recordcopy . __dict__ [ "" message "" ] = recordcopy . getMessage ( ) recordcopy . __dict__ [ "" levelprefix "" ] = levelname + "" : "" + seperator return super ( ) . formatMessage ( recordcopy )","if ""color_message"" in recordcopy . __dict__ :",if recordcopy . msg is None :,83.28377398,FALSE,89.72
2544,"def dumpregs ( self ) : for reg in ( list ( self . regs . retaddr ) + list ( self . regs . misc ) + list ( self . regs . common ) + list ( self . regs . flags ) ) : enum = self . get_reg_enum ( reg ) <MASK> debug ( "" # Could not dump register  %r "" % reg ) continue name = "" U.x86_const.UC_X86_REG_ %s "" % reg . upper ( ) value = self . uc . reg_read ( enum ) debug ( "" uc.reg_read( %(name)s ) ==>  %(value)x "" % locals ( ) )",if not reg or enum is None :,if not enum :,93.01957893,FALSE,95.68
2545,"def filter ( self , lexer , stream ) : current_type = None current_value = None for ttype , value in stream : if ttype is current_type : current_value + = value else : <MASK> yield current_type , current_value current_type = ttype current_value = value <MASK> yield current_type , current_value",if current_type is not None :,if current_type is not None :,100,TRUE,100
2546,"def _get_between ( content , start , end = None ) : should_yield = False for line in content . split ( "" \n "" ) : <MASK> should_yield = True continue if end and end in line : return if should_yield and line : yield line . strip ( ) . split ( ""   "" ) [ 0 ]",if start in line :,if start and start in line :,95.28465741,FALSE,96.28
2547,"def parse_git_config ( path ) : """"""Parse git config file."""""" config = dict ( ) section = None with open ( os . path . join ( path , "" config "" ) , "" r "" ) as f : for line in f : line = line . strip ( ) if line . startswith ( "" [ "" ) : section = line [ 1 : - 1 ] . strip ( ) config [ section ] = dict ( ) <MASK> key , value = line . replace ( ""   "" , "" "" ) . split ( "" = "" ) config [ section ] [ key ] = value return config",elif section :,"elif line . startswith ( "" "" ) :",84.54279329,FALSE,93.67
2548,"def test_has_arg ( fn , name , accept_all , expected ) : if isinstance ( fn , str ) : context = dict ( ) try : exec ( "" def  {} : pass "" . format ( fn ) , context ) except SyntaxError : <MASK> raise pytest . skip ( "" Function is not compatible with Python 2 "" ) # Sometimes exec adds builtins to the context context . pop ( "" __builtins__ "" , None ) ( fn , ) = context . values ( ) assert has_arg ( fn , name , accept_all ) is expected","if sys . version_info >= ( 3 , ) :","if sys . version_info < ( 2 , ) :",98.06671655,FALSE,95.84
2549,"def ObjectExpression ( self , properties , * * kwargs ) : data = [ ] for prop in properties : self . emit ( prop [ "" value "" ] ) <MASK> raise NotImplementedError ( "" ECMA 5.1 does not support computed object properties! "" ) data . append ( ( to_key ( prop [ "" key "" ] ) , prop [ "" kind "" ] [ 0 ] ) ) self . emit ( "" LOAD_OBJECT "" , tuple ( data ) )","if prop [ ""computed"" ] :","if prop [ ""kind"" ] [ 0 ] == ""computed"" :",95.23612311,FALSE,89.66
2550,"def run ( self ) : for domain , locale , po in self . locales : <MASK> path = os . path . join ( "" locale "" , locale , "" LC_MESSAGES "" ) else : path = os . path . join ( self . build_dir , locale , "" LC_MESSAGES "" ) mo = os . path . join ( path , "" %s .mo "" % domain ) self . mkpath ( path ) self . spawn ( [ "" msgfmt "" , "" -o "" , mo , po ] )",if self . inplace :,"if domain == ""en"" :",93.56300713,FALSE,93.2
2551,"def _compute_map ( self , first_byte , second_byte = None ) : if first_byte != 0x0F : return "" XED_ILD_MAP0 "" else : if second_byte == None : return "" XED_ILD_MAP1 "" if second_byte == 0x38 : return "" XED_ILD_MAP2 "" <MASK> return "" XED_ILD_MAP3 "" if second_byte == 0x0F and self . amd_enabled : return "" XED_ILD_MAPAMD "" die ( "" Unhandled escape  {}  / map  {}  bytes "" . format ( first_byte , second_byte ) )",if second_byte == 0x3A :,if second_byte == 0x3E :,98.46347861,FALSE,98.07
2552,"def parse_tag ( self ) : buf = [ ] escaped = False for c in self . get_next_chars ( ) : <MASK> buf . append ( c ) elif c == "" \\ "" : escaped = True elif c == "" > "" : return "" "" . join ( buf ) else : buf . append ( c ) raise Exception ( "" Unclosed tag  "" + "" "" . join ( buf ) )",if escaped :,if escaped :,100,TRUE,100
2553,"def print_pairs ( attrs = None , offset_y = 0 ) : fmt = ""  ( {0} : {1} )  "" fmt_len = len ( fmt ) for bg , fg in get_fg_bg ( ) : try : color = curses . color_pair ( pair_number ( fg , bg ) ) <MASK> for attr in attrs : color | = attr screen . addstr ( offset_y + bg , fg * fmt_len , fmt . format ( fg , bg ) , color ) pass except curses . error : pass",if not attrs is None :,if attrs :,89.95899939,FALSE,95.47
2554,"def _impl ( inputs , input_types ) : data = inputs [ 0 ] axis = None keepdims = False if len ( inputs ) > 2 : # default, torch have only data, axis=None, keepdims=False <MASK> axis = int ( inputs [ 1 ] ) elif _is_int_seq ( inputs [ 1 ] ) : axis = inputs [ 1 ] else : axis = list ( _infer_shape ( inputs [ 1 ] ) ) keepdims = bool ( inputs [ 2 ] ) return get_relay_op ( name ) ( data , axis = axis , keepdims = keepdims )","if isinstance ( inputs [ 1 ] , int ) :",if _is_int_seq ( inputs [ 1 ] ) :,97.2515179,FALSE,93.41
2555,"def run ( self , args , * * kwargs ) : # Filtering options if args . trace_tag : kwargs [ "" trace_tag "" ] = args . trace_tag if args . trigger_instance : kwargs [ "" trigger_instance "" ] = args . trigger_instance if args . execution : kwargs [ "" execution "" ] = args . execution if args . rule : kwargs [ "" rule "" ] = args . rule if args . sort_order : <MASK> kwargs [ "" sort_asc "" ] = True elif args . sort_order in [ "" desc "" , "" descending "" ] : kwargs [ "" sort_desc "" ] = True return self . manager . query_with_count ( limit = args . last , * * kwargs )","if args . sort_order in [ ""asc"" , ""ascending"" ] :","if args . sort_order in [ ""asc"" , ""asc"" ] :",74.09789259,FALSE,98.5
2556,def retaddr ( ) : sp = pwndbg . regs . sp stack = pwndbg . vmmap . find ( sp ) # Enumerate all return addresses frame = gdb . newest_frame ( ) addresses = [ ] while frame : addresses . append ( frame . pc ( ) ) frame = frame . older ( ) # Find all of them on the stack start = stack . vaddr stop = start + stack . memsz while addresses and start < sp < stop : value = pwndbg . memory . u ( sp ) <MASK> index = addresses . index ( value ) del addresses [ : index ] print ( pwndbg . chain . format ( sp ) ) sp + = pwndbg . arch . ptrsize,if value in addresses :,if value in addresses :,100,TRUE,100
2557,"def update_from_dictio ( self , dictio_item ) : for index , dictio_payload in enumerate ( dictio_item , 1 ) : fuzz_payload = None for fuzz_payload in self . payloads [ index ] : fuzz_payload . content = dictio_payload . content fuzz_payload . type = dictio_payload . type # payload generated not used in seed but in filters <MASK> self . add ( { "" full_marker "" : None , "" word "" : None , "" index "" : index , "" field "" : None } , dictio_item [ index - 1 ] , )",if fuzz_payload is None :,"if fuzz_payload . type == ""seed"" :",97.46758102,FALSE,93.52
2558,"def check_expected ( result , expected , contains = False ) : if sys . version_info [ 0 ] > = 3 : <MASK> result = result . encode ( "" ascii "" ) if isinstance ( expected , str ) : expected = expected . encode ( "" ascii "" ) resultlines = result . splitlines ( ) expectedlines = expected . splitlines ( ) if len ( resultlines ) != len ( expectedlines ) : return False for rline , eline in zip ( resultlines , expectedlines ) : if contains : if eline not in rline : return False else : if not rline . endswith ( eline ) : return False return True","if isinstance ( result , str ) :","if isinstance ( result , str ) :",100,TRUE,100
2559,"def execute_sql ( self , sql , params = None , commit = True ) : try : cursor = super ( RetryOperationalError , self ) . execute_sql ( sql , params , commit ) except OperationalError : if not self . is_closed ( ) : self . close ( ) with __exception_wrapper__ : cursor = self . cursor ( ) cursor . execute ( sql , params or ( ) ) <MASK> self . commit ( ) return cursor",if commit and not self . in_transaction ( ) :,if commit :,90.86385315,FALSE,89.38
2560,"def get_operation_ast ( document_ast , operation_name = None ) : operation = None for definition in document_ast . definitions : if isinstance ( definition , ast . OperationDefinition ) : if not operation_name : # If no operation name is provided, only return an Operation if it is the only one present in the # document. This means that if we've encountered a second operation as we were iterating over the # definitions in the document, there are more than one Operation defined, and we should return None. <MASK> return None operation = definition elif definition . name and definition . name . value == operation_name : return definition return operation",if operation :,if not definition . name :,98.25636585,FALSE,95.9
2561,"def removeTrailingWs ( self , aList ) : i = 0 while i < len ( aList ) : if self . is_ws ( aList [ i ] ) : j = i i = self . skip_ws ( aList , i ) assert j < i <MASK> # print ""removing trailing ws:"", `i-j` del aList [ j : i ] i = j else : i + = 1","if i >= len ( aList ) or aList [ i ] == ""\n"" :","if aList [ j ] == ""ws"" :",84.43254198,FALSE,83.99
2562,"def _process_filter ( self , query , host_state ) : """"""Recursively parse the query structure."""""" if not query : return True cmd = query [ 0 ] method = self . commands [ cmd ] cooked_args = [ ] for arg in query [ 1 : ] : <MASK> arg = self . _process_filter ( arg , host_state ) elif isinstance ( arg , basestring ) : arg = self . _parse_string ( arg , host_state ) if arg is not None : cooked_args . append ( arg ) result = method ( self , cooked_args ) return result","if isinstance ( arg , list ) :","if isinstance ( arg , ( list , tuple ) ) :",96.98857125,FALSE,95.41
2563,"def handle_sent ( self , elt ) : sent = [ ] for child in elt : if child . tag in ( "" mw "" , "" hi "" , "" corr "" , "" trunc "" ) : sent + = [ self . handle_word ( w ) for w in child ] elif child . tag in ( "" w "" , "" c "" ) : sent . append ( self . handle_word ( child ) ) <MASK> raise ValueError ( "" Unexpected element  %s "" % child . tag ) return BNCSentence ( elt . attrib [ "" n "" ] , sent )",elif child . tag not in self . tags_to_ignore :,"elif child . tag != ""n"" :",93.92405473,FALSE,91.74
2564,"def get_display_price ( base : Union [ TaxedMoney , TaxedMoneyRange ] , display_gross : bool = False ) - > Money : """"""Return the price amount that should be displayed based on settings."""""" if not display_gross : display_gross = display_gross_prices ( ) if isinstance ( base , TaxedMoneyRange ) : <MASK> base = MoneyRange ( start = base . start . gross , stop = base . stop . gross ) else : base = MoneyRange ( start = base . start . net , stop = base . stop . net ) if isinstance ( base , TaxedMoney ) : base = base . gross if display_gross else base . net return base",if display_gross :,if display_gross :,100,TRUE,100
2565,"def check_classes ( self , node ) : if isinstance ( node , nodes . Element ) : for class_value in node [ "" classes "" ] [ : ] : if class_value in self . strip_classes : node [ "" classes "" ] . remove ( class_value ) <MASK> return 1",if class_value in self . strip_elements :,"if node [ ""classes"" ] [ - 1 ] == self . strip_classes [ -",86.66555824,FALSE,77.59
2566,"def validate ( outfile = sys . stdout , silent_success = False ) : "" Validates all installed models. "" try : num_errors = get_validation_errors ( outfile ) <MASK> return outfile . write ( "" %s  error %s  found. \n "" % ( num_errors , num_errors != 1 and "" s "" or "" "" ) ) except ImproperlyConfigured : outfile . write ( "" Skipping validation because things aren ' t configured properly. "" )",if silent_success and num_errors == 0 :,if silent_success :,91.44327071,FALSE,91.72
2567,"def check_basename_conflicts ( self , targets ) : """"""Apps' basenames are used as bundle directory names. Ensure they are all unique."""""" basename_seen = { } for target in targets : <MASK> raise self . BasenameConflictError ( "" Basename must be unique, found two targets use  "" "" the same basename:  {} ' \n \t {}  and  \n \t {} "" . format ( target . basename , basename_seen [ target . basename ] . address . spec , target . address . spec , ) ) basename_seen [ target . basename ] = target",if target . basename in basename_seen :,if target . basename in basename_seen :,100,TRUE,100
2568,"def __init__ ( self , api_version_str ) : try : self . latest = self . preview = False self . yyyy = self . mm = self . dd = None <MASK> self . latest = True else : if "" preview "" in api_version_str : self . preview = True parts = api_version_str . split ( "" - "" ) self . yyyy = int ( parts [ 0 ] ) self . mm = int ( parts [ 1 ] ) self . dd = int ( parts [ 2 ] ) except ( ValueError , TypeError ) : raise ValueError ( "" The API version  {}  is not in a  "" "" supported format "" . format ( api_version_str ) )","if api_version_str == ""latest"" :","if api_version_str == ""1.0.0.0"" :",99.02882886,FALSE,98.38
2569,"def _osp2ec ( self , bytes ) : compressed = self . _from_bytes ( bytes ) y = compressed >> self . _bits x = compressed & ( 1 << self . _bits ) - 1 if x == 0 : y = self . _curve . b else : result = self . sqrtp ( x * * 3 + self . _curve . a * x + self . _curve . b , self . _curve . field . p ) <MASK> y = result [ 0 ] elif len ( result ) == 2 : y1 , y2 = result y = y1 if ( y1 & 1 == y ) else y2 else : return None return ec . Point ( self . _curve , x , y )",if len ( result ) == 1 :,if len ( result ) == 1 :,100,TRUE,100
2570,"def _visit_import_alike ( self , node : Union [ cst . Import , cst . ImportFrom ] ) - > bool : names = node . names if isinstance ( names , cst . ImportStar ) : return False # make sure node.names is Sequence[ImportAlias] for name in names : self . provider . set_metadata ( name , self . scope ) asname = name . asname <MASK> name_values = _gen_dotted_names ( cst . ensure_type ( asname . name , cst . Name ) ) else : name_values = _gen_dotted_names ( name . name ) for name_value , _ in name_values : self . scope . record_assignment ( name_value , node ) return False",if asname is not None :,if asname :,97.72013165,FALSE,97.06
2571,"def test_sanity_no_unmatched_parentheses ( CorpusType : Type [ ColumnCorpus ] ) : corpus = CorpusType ( ) unbalanced_entities = [ ] for sentence in corpus . get_all_sentences ( ) : entities = sentence . get_spans ( "" ner "" ) for entity in entities : entity_text = "" "" . join ( t . text for t in entity . tokens ) <MASK> unbalanced_entities . append ( entity_text ) assert unbalanced_entities == [ ]",if not has_balanced_parantheses ( entity_text ) :,if entity_text not in unbalanced_entities :,93.85994527,FALSE,90.25
2572,"def _learn_rate_adjust ( self ) : if self . learn_rate_decays == 1.0 : return learn_rate_decays = self . _vp ( self . learn_rate_decays ) learn_rate_minimums = self . _vp ( self . learn_rate_minimums ) for index , decay in enumerate ( learn_rate_decays ) : new_learn_rate = self . net_ . learnRates [ index ] * decay <MASK> self . net_ . learnRates [ index ] = new_learn_rate if self . verbose > = 2 : print ( "" Learn rates:  {} "" . format ( self . net_ . learnRates ) )",if new_learn_rate >= learn_rate_minimums [ index ] :,if new_learn_rate < learn_rate_minimums :,93.20445343,FALSE,94.69
2573,"def set_attr_from_xmp_tag ( self , attr , xmp_tags , tags , cast = None ) : v = self . get_xmp_tag ( xmp_tags , tags ) if v is not None : <MASK> setattr ( self , attr , v ) else : # Handle fractions if ( cast == float or cast == int ) and "" / "" in v : v = self . try_parse_fraction ( v ) setattr ( self , attr , cast ( v ) )",if cast is None :,if cast is None :,100,TRUE,100
2574,"def _merge_scientific_float_tokens ( tokens : Iterable [ str ] ) - > List [ str ] : tokens = list ( tokens ) i = 0 while "" e "" in tokens [ i + 1 : ] : i = tokens . index ( "" e "" , i + 1 ) s = i - 1 e = i + 1 <MASK> continue if re . match ( "" [+-] "" , str ( tokens [ e ] ) ) : e + = 1 if re . match ( "" [0-9] "" , str ( tokens [ e ] ) ) : e + = 1 tokens [ s : e ] = [ "" "" . join ( tokens [ s : e ] ) ] i - = 1 return tokens","if not re . match ( ""[0-9]"" , str ( tokens [ s ] ) ) :",if i == 0 :,90.30192389,FALSE,86.54
2575,"def anypython ( request ) : name = request . param executable = getexecutable ( name ) if executable is None : <MASK> executable = winpymap . get ( name , None ) if executable : executable = py . path . local ( executable ) if executable . check ( ) : return executable pytest . skip ( "" no suitable  %s  found "" % ( name , ) ) return executable","if sys . platform == ""win32"" :",if name in winpymap :,93.21873297,FALSE,88.24
2576,"def set_meta ( self , dataset , overwrite = True , * * kwd ) : super ( ) . set_meta ( dataset , overwrite = overwrite , * * kwd ) try : <MASK> with tarfile . open ( dataset . file_name , "" r "" ) as temptar : dataset . metadata . fast5_count = sum ( 1 for f in temptar if f . name . endswith ( "" .fast5 "" ) ) except Exception as e : log . warning ( "" %s , set_meta Exception:  %s "" , self , e )",if dataset and tarfile . is_tarfile ( dataset . file_name ) :,if dataset . metadata . fast5_count is None :,88.61503251,FALSE,89.28
2577,"def run ( self ) : for k in list ( iterkeys ( self . objs ) ) : <MASK> continue v = self . objs [ k ] if v [ "" _class "" ] == "" User "" : self . split_user ( k , v ) elif v [ "" _class "" ] in [ "" Message "" , "" PrintJob "" , "" Question "" , "" Submission "" , "" UserTest "" , ] : v [ "" participation "" ] = v [ "" user "" ] del v [ "" user "" ] return self . objs","if k . startswith ( ""_"" ) :","if k . startswith ( ""_"" ) :",100,TRUE,100
2578,"def _findInTree ( t , n ) : ret = [ ] if type ( t ) is dict : <MASK> ret . append ( t ) for k , v in t . items ( ) : ret + = _findInTree ( v , n ) if type ( t ) is list : for v in t : ret + = _findInTree ( v , n ) return ret","if ""_name"" in t and t [ ""_name"" ] == n :",if n :,85.0800738,FALSE,79.36
2579,"def parseArrayPattern ( self ) : node = Node ( ) elements = [ ] self . expect ( "" [ "" ) while not self . match ( "" ] "" ) : <MASK> self . lex ( ) elements . append ( null ) else : if self . match ( "" ... "" ) : restNode = Node ( ) self . lex ( ) rest = self . parseVariableIdentifier ( ) elements . append ( restNode . finishRestElement ( rest ) ) break else : elements . append ( self . parsePatternWithDefault ( ) ) if not self . match ( "" ] "" ) : self . expect ( "" , "" ) self . expect ( "" ] "" ) return node . finishArrayPattern ( elements )","if self . match ( "","" ) :","if self . match ( ""]"" ) :",99.10298557,FALSE,98.24
2580,"def _set_log_writer ( self ) : if self . config [ "" logging "" ] : config = self . config [ "" log_writer_config "" ] <MASK> self . log_writer = LogWriter ( * * config ) elif config [ "" writer "" ] == "" tensorboard "" : self . log_writer = TensorBoardWriter ( * * config ) else : raise ValueError ( f "" Unrecognized writer option:  { config [ ' writer ' ] } "" ) else : self . log_writer = None","if config [ ""writer"" ] == ""json"" :","if config [ ""writer"" ] == ""log"" :",98.63904904,FALSE,97.78
2581,"def _parse ( self , contents ) : entries = [ ] hostnames_found = set ( ) for line in contents . splitlines ( ) : <MASK> entries . append ( ( "" blank "" , [ line ] ) ) continue ( head , tail ) = chop_comment ( line . strip ( ) , "" # "" ) if not len ( head ) : entries . append ( ( "" all_comment "" , [ line ] ) ) continue entries . append ( ( "" hostname "" , [ head , tail ] ) ) hostnames_found . add ( head ) if len ( hostnames_found ) > 1 : raise IOError ( "" Multiple hostnames ( %s ) found! "" % ( hostnames_found ) ) return entries",if not len ( line . strip ( ) ) :,if not line :,86.74026404,FALSE,93.9
2582,"def get_all_values ( self , project ) : if isinstance ( project , models . Model ) : project_id = project . id else : project_id = project if project_id not in self . __cache : cache_key = self . _make_key ( project_id ) result = cache . get ( cache_key ) <MASK> result = self . reload_cache ( project_id ) else : self . __cache [ project_id ] = result return self . __cache . get ( project_id , { } )",if result is None :,if result is None :,100,TRUE,100
2583,"def needed_libraries ( self ) : for cmd in self . load_commands_of_type ( 0xC ) : # LC_LOAD_DYLIB tname = self . _get_typename ( "" dylib_command "" ) dylib_command = cmd . cast ( tname ) name_addr = cmd . obj_offset + dylib_command . name dylib_name = self . obj_vm . read ( name_addr , 256 ) <MASK> idx = dylib_name . find ( "" \x00 "" ) if idx != - 1 : dylib_name = dylib_name [ : idx ] yield dylib_name",if dylib_name :,if dylib_name :,100,TRUE,100
2584,"def compress ( self , data_list ) : warn_untested ( ) if data_list : <MASK> error = self . error_messages [ "" invalid_year "" ] raise forms . ValidationError ( error ) if data_list [ 0 ] in forms . fields . EMPTY_VALUES : error = self . error_messages [ "" invalid_month "" ] raise forms . ValidationError ( error ) year = int ( data_list [ 1 ] ) month = int ( data_list [ 0 ] ) # find last day of the month day = monthrange ( year , month ) [ 1 ] return date ( year , month , day ) return None",if data_list [ 1 ] in forms . fields . EMPTY_VALUES :,if data_list [ 1 ] in forms . fields . EMPTY_VALUES :,100,TRUE,100
2585,"def put ( self , obj , block = True , timeout = None ) : assert not self . _closed if not self . _sem . acquire ( block , timeout ) : raise Full with self . _notempty : with self . _cond : <MASK> self . _start_thread ( ) self . _buffer . append ( obj ) self . _unfinished_tasks . release ( ) self . _notempty . notify ( )",if self . _thread is None :,if self . _thread is None :,100,TRUE,100
2586,"def has_module ( self , module , version ) : has_module = False for directory in self . directories : module_directory = join ( directory , module ) has_module_directory = isdir ( module_directory ) if not version : has_module = has_module_directory or exists ( module_directory ) # could be a bare modulefile else : modulefile = join ( module_directory , version ) has_modulefile = exists ( modulefile ) has_module = has_module_directory and has_modulefile <MASK> break return has_module",if has_module :,if has_module :,100,TRUE,100
2587,"def expanduser ( path ) : if path [ : 1 ] == "" ~ "" : c = path [ 1 : 2 ] <MASK> return gethome ( ) if c == os . sep : return asPyString ( File ( gethome ( ) , path [ 2 : ] ) . getPath ( ) ) return path",if not c :,if c == os . path . sep :,91.91009242,FALSE,86.43
2588,"def mock_touch ( self , bearer , version = None , revision = None , * * kwargs ) : if version : <MASK> try : return self . versions [ int ( version ) - 1 ] except ( IndexError , ValueError ) : return None else : return None return file_models . FileVersion ( )",if self . versions :,if version in self . versions :,94.79451622,FALSE,94.61
2589,"def _get_field_value ( self , test , key , match ) : if test . ver == ofproto_v1_0 . OFP_VERSION : members = inspect . getmembers ( match ) for member in members : if member [ 0 ] == key : field_value = member [ 1 ] <MASK> wildcards = member [ 1 ] if key == "" nw_src "" : field_value = test . nw_src_to_str ( wildcards , field_value ) elif key == "" nw_dst "" : field_value = test . nw_dst_to_str ( wildcards , field_value ) else : field_value = match [ key ] return field_value","elif member [ 0 ] == ""wildcards"" :","elif member [ 0 ] == ""wildcard"" :",84.29466048,FALSE,98.35
2590,"def check_expected ( result , expected , contains = False ) : if sys . version_info [ 0 ] > = 3 : if isinstance ( result , str ) : result = result . encode ( "" ascii "" ) if isinstance ( expected , str ) : expected = expected . encode ( "" ascii "" ) resultlines = result . splitlines ( ) expectedlines = expected . splitlines ( ) if len ( resultlines ) != len ( expectedlines ) : return False for rline , eline in zip ( resultlines , expectedlines ) : <MASK> if eline not in rline : return False else : if not rline . endswith ( eline ) : return False return True",if contains :,if contains :,100,TRUE,100
2591,"def OnKeyUp ( self , event ) : if self . _properties . modifiable : if event . GetKeyCode ( ) == wx . WXK_ESCAPE : self . _cancel_editing ( ) <MASK> self . _update_value ( ) elif event . GetKeyCode ( ) == wx . WXK_DELETE : self . SetValue ( "" "" ) if event . GetKeyCode ( ) != wx . WXK_RETURN : # Don't send skip event if enter key is pressed # On some platforms this event is sent too late and causes crash event . Skip ( )",elif event . GetKeyCode ( ) == wx . WXK_RETURN :,elif event . GetKeyCode ( ) == wx . WXK_UPDATE :,96.15603638,FALSE,97.93
2592,"def load_modules ( to_load , load , attr , modules_dict , excluded_aliases , loading_message = None ) : if loading_message : print ( loading_message ) for name in to_load : module = load ( name ) <MASK> continue cls = getattr ( module , attr ) if hasattr ( cls , "" initialize "" ) and not cls . initialize ( ) : continue if hasattr ( module , "" aliases "" ) : for alias in module . aliases ( ) : if alias not in excluded_aliases : modules_dict [ alias ] = module else : modules_dict [ name ] = module if loading_message : print ( )","if module is None or not hasattr ( module , attr ) :",if module is None :,91.86158101,FALSE,93.27
2593,def eventIterator ( ) : while True : yield eventmodule . wait ( ) while True : event = eventmodule . poll ( ) <MASK> break else : yield event,if event . type == NOEVENT :,if event is None :,59.52380431,FALSE,81.5
2594,"def _get_state_without_padding ( self , state_with_padding , padding ) : lean_state = { } for key , value in state_with_padding . items ( ) : <MASK> lean_length = value . numel ( ) - padding lean_state [ key ] = value [ : lean_length ] else : lean_state [ key ] = value return lean_state",if torch . is_tensor ( value ) :,if padding :,83.69397365,FALSE,89.19
2595,"def _get_validate ( data ) : """"""Retrieve items to validate, from single samples or from combined joint calls."""""" if data . get ( "" vrn_file "" ) and tz . get_in ( [ "" config "" , "" algorithm "" , "" validate "" ] , data ) : return utils . deepish_copy ( data ) elif "" group_orig "" in data : for sub in multi . get_orig_items ( data ) : <MASK> sub_val = utils . deepish_copy ( sub ) sub_val [ "" vrn_file "" ] = data [ "" vrn_file "" ] return sub_val return None","if ""validate"" in sub [ ""config"" ] [ ""algorithm"" ] :","if sub . get ( ""vrn_file"" ) :",90.72436462,FALSE,89.35
2596,"def OnPopup ( self , form , popup_handle ) : for num , action_name , menu_name , shortcut in self . actions : <MASK> ida_kernwin . attach_action_to_popup ( form , popup_handle , None ) else : handler = command_handler_t ( self , num , 2 ) desc = ida_kernwin . action_desc_t ( action_name , menu_name , handler , shortcut ) ida_kernwin . attach_dynamic_action_to_popup ( form , popup_handle , desc )",if menu_name is None :,if num == 0 :,93.00842932,FALSE,94.41
2597,"def show ( self , indent = 0 ) : """"""Pretty print this structure."""""" if indent == 0 : print ( "" struct  {} "" . format ( self . name ) ) for field in self . fields : <MASK> offset = "" 0x?? "" else : offset = "" 0x {:02x} "" . format ( field . offset ) print ( "" {} + {}   {}   {} "" . format ( ""   "" * indent , offset , field . name , field . type ) ) if isinstance ( field . type , Structure ) : field . type . show ( indent + 1 )",if field . offset is None :,if field . offset is None :,100,TRUE,100
2598,"def get_operation_ast ( document_ast , operation_name = None ) : operation = None for definition in document_ast . definitions : <MASK> if not operation_name : # If no operation name is provided, only return an Operation if it is the only one present in the # document. This means that if we've encountered a second operation as we were iterating over the # definitions in the document, there are more than one Operation defined, and we should return None. if operation : return None operation = definition elif definition . name and definition . name . value == operation_name : return definition return operation","if isinstance ( definition , ast . OperationDefinition ) :","if isinstance ( definition , Operation ) :",91.47347643,FALSE,96.57
2599,"def getSubMenu ( self , callingWindow , context , mainItem , selection , rootMenu , i , pitem ) : msw = True if "" wxMSW "" in wx . PlatformInfo else False self . context = context self . abilityIds = { } sub = wx . Menu ( ) for ability in self . fighter . abilities : <MASK> continue menuItem = self . addAbility ( rootMenu if msw else sub , ability ) sub . Append ( menuItem ) menuItem . Check ( ability . active ) return sub",if not ability . effect . isImplemented :,"if ability . name != ""Menu"" :",65.56756324,FALSE,91.16
2600,"def consume ( self , event : Dict [ str , Any ] ) - > None : with self . lock : logging . debug ( "" Received missedmessage_emails event:  %s "" , event ) # When we process an event, just put it into the queue and ensure we have a timer going. user_profile_id = event [ "" user_profile_id "" ] <MASK> self . batch_start_by_recipient [ user_profile_id ] = time . time ( ) self . events_by_recipient [ user_profile_id ] . append ( event ) self . ensure_timer ( )",if user_profile_id not in self . batch_start_by_recipient :,if user_profile_id not in self . events_by_recipient :,73.72133754,FALSE,96.81
2601,"def __init__ ( self , start_enabled = False , use_hardware = True ) : self . _use_hardware = use_hardware if use_hardware : self . _button = Button ( BUTTON_GPIO_PIN ) self . _enabled = start_enabled <MASK> self . _button . when_pressed = self . _enable",if not start_enabled :,if self . _enabled :,75.44562949,FALSE,95.33
2602,"def execute ( cls , ctx , op : "" DataFrameGroupByAgg "" ) : try : pd . set_option ( "" mode.use_inf_as_na "" , op . use_inf_as_na ) <MASK> cls . _execute_map ( ctx , op ) elif op . stage == OperandStage . combine : cls . _execute_combine ( ctx , op ) elif op . stage == OperandStage . agg : cls . _execute_agg ( ctx , op ) else : # pragma: no cover raise ValueError ( "" Aggregation operand not executable "" ) finally : pd . reset_option ( "" mode.use_inf_as_na "" )",if op . stage == OperandStage . map :,if op . stage == OperandStage . map :,100,TRUE,100
2603,"def load_package ( name , path ) : if os . path . isdir ( path ) : extensions = machinery . SOURCE_SUFFIXES [ : ] + machinery . BYTECODE_SUFFIXES [ : ] for extension in extensions : init_path = os . path . join ( path , "" __init__ "" + extension ) <MASK> path = init_path break else : raise ValueError ( "" {!r}  is not a package "" . format ( path ) ) spec = util . spec_from_file_location ( name , path , submodule_search_locations = [ ] ) if name in sys . modules : return _exec ( spec , sys . modules [ name ] ) else : return _load ( spec )",if os . path . exists ( init_path ) :,if os . path . exists ( init_path ) :,100,TRUE,100
2604,def setup ( level = None ) : from pipeline . logging import pipeline_logger as logger from pipeline . log . handlers import EngineLogHandler if level in set ( logging . _levelToName . values ( ) ) : logger . setLevel ( level ) logging . _acquireLock ( ) try : for hdl in logger . handlers : <MASK> break else : hdl = EngineLogHandler ( ) hdl . setLevel ( logger . level ) logger . addHandler ( hdl ) finally : logging . _releaseLock ( ),"if isinstance ( hdl , EngineLogHandler ) :",if hdl . setLevel ( level ) == logging . _levelToName . get ( level,86.69020462,FALSE,84.37
2605,"def find_approximant ( x ) : c = 1e-4 it = sympy . ntheory . continued_fraction_convergents ( sympy . ntheory . continued_fraction_iterator ( x ) ) for i in it : p , q = i . as_numer_denom ( ) tol = c / q * * 2 if abs ( i - x ) < = tol : return i <MASK> break return x",if tol < machine_epsilon :,if p == 0 :,93.94145335,FALSE,92.2
2606,"def resolve ( self , debug : bool = False , silent : bool = False , level : Optional [ int ] = None ) - > bool : if silent : spinner = nullcontext ( type ( "" Mock "" , ( ) , { } ) ) else : spinner = yaspin ( text = "" resolving... "" ) with spinner as spinner : while True : resolved = self . _resolve ( debug = debug , silent = silent , level = level , spinner = spinner ) <MASK> continue self . graph . clear ( ) # remove unused deps from graph return resolved",if resolved is None :,if resolved is None :,100,TRUE,100
2607,"def canonicalize_instruction_name ( instr ) : name = instr . insn_name ( ) . upper ( ) # XXX bypass a capstone bug that incorrectly labels some insns as mov if name == "" MOV "" : <MASK> return "" LSR "" elif instr . mnemonic . startswith ( "" lsl "" ) : return "" LSL "" elif instr . mnemonic . startswith ( "" asr "" ) : return "" ASR "" return OP_NAME_MAP . get ( name , name )","if instr . mnemonic . startswith ( ""lsr"" ) :","if instr . mnemonic . startswith ( ""ls"" ) :",98.62700002,FALSE,97.57
2608,"def run_all ( rule_list , defined_variables , defined_actions , stop_on_first_trigger = False ) : rule_was_triggered = False for rule in rule_list : result = run ( rule , defined_variables , defined_actions ) if result : rule_was_triggered = True <MASK> return True return rule_was_triggered",if stop_on_first_trigger :,if stop_on_first_trigger :,100,TRUE,100
2609,"def get_filters ( self , request ) : filter_specs = [ ] if self . lookup_opts . admin . list_filter and not self . opts . one_to_one_field : filter_fields = [ self . lookup_opts . get_field ( field_name ) for field_name in self . lookup_opts . admin . list_filter ] for f in filter_fields : spec = FilterSpec . create ( f , request , self . params , self . model ) <MASK> filter_specs . append ( spec ) return filter_specs , bool ( filter_specs )",if spec and spec . has_output ( ) :,if spec . has_spec ( request ) :,92.68589359,FALSE,95.6
2610,"def get_type ( type_ref ) : kind = type_ref . get ( "" kind "" ) if kind == TypeKind . LIST : item_ref = type_ref . get ( "" ofType "" ) <MASK> raise Exception ( "" Decorated type deeper than introspection query. "" ) return GraphQLList ( get_type ( item_ref ) ) elif kind == TypeKind . NON_NULL : nullable_ref = type_ref . get ( "" ofType "" ) if not nullable_ref : raise Exception ( "" Decorated type deeper than introspection query. "" ) return GraphQLNonNull ( get_type ( nullable_ref ) ) return get_named_type ( type_ref [ "" name "" ] )",if not item_ref :,if not item_ref :,100,TRUE,100
2611,"def _1_0_cloud_ips_cip_jsjc5_map ( self , method , url , body , headers ) : if method == "" POST "" : body = json . loads ( body ) <MASK> return self . test_response ( httplib . ACCEPTED , "" "" ) else : data = ' { "" error_name "" : "" bad destination "" ,  "" errors "" : [ "" Bad destination "" ]} ' return self . test_response ( httplib . BAD_REQUEST , data )","if ""destination"" in body :","if ""cloud-ips-cip-jsjc5-map"" in body :",98.47000344,FALSE,95.84
2612,"def _get_prefixed_values ( data , prefix ) : """"""Collect lines which start with prefix; with trimming"""""" matches = [ ] for line in data . splitlines ( ) : line = line . strip ( ) <MASK> match = line [ len ( prefix ) : ] match = match . strip ( ) matches . append ( match ) return matches",if line . startswith ( prefix ) :,if line . startswith ( prefix ) :,100,TRUE,100
2613,"def _power_exact ( y , xc , yc , xe ) : yc , ye = y . int , y . exp while yc % 10 == 0 : yc / / = 10 ye + = 1 if xc == 1 : xe * = yc while xe % 10 == 0 : xe / / = 10 ye + = 1 <MASK> return None exponent = xe * 10 * * ye if y and xe : xc = exponent else : xc = 0 return 5",if ye < 0 :,if ye == 1 :,98.13799604,FALSE,95.46
2614,"def init ( self , view , items = None ) : selections = [ ] if view . sel ( ) : for region in view . sel ( ) : selections . append ( view . substr ( region ) ) values = [ ] for idx , index in enumerate ( map ( int , items ) ) : if idx > = len ( selections ) : break i = index - 1 if i > = 0 and i < len ( selections ) : values . append ( selections [ i ] ) else : values . append ( None ) # fill up for idx , value in enumerate ( selections ) : <MASK> values . append ( value ) self . stack = values",if len ( values ) + 1 < idx :,if idx >= 0 and idx < len ( selections ) :,96.81780069,FALSE,92.24
2615,"def toggleFactorReload ( self , value = None ) : self . serviceFittingOptions [ "" useGlobalForceReload "" ] = ( value if value is not None else not self . serviceFittingOptions [ "" useGlobalForceReload "" ] ) fitIDs = set ( ) for fit in set ( self . _loadedFits ) : if fit is None : continue <MASK> fit . factorReload = self . serviceFittingOptions [ "" useGlobalForceReload "" ] fit . clearFactorReloadDependentData ( ) fitIDs . add ( fit . ID ) return fitIDs",if fit . calculated :,if fit . isInScope ( ) :,96.02568058,FALSE,95.06
2616,"def init_weights ( self ) : """"""Initialize model weights."""""" for m in self . predict_layers . modules ( ) : if isinstance ( m , nn . Conv2d ) : kaiming_init ( m ) <MASK> constant_init ( m , 1 ) elif isinstance ( m , nn . Linear ) : normal_init ( m , std = 0.01 )","elif isinstance ( m , nn . BatchNorm2d ) :","elif isinstance ( m , nn . BatchNorm2d ) :",100,TRUE,100
2617,"def _unzip_file ( self , filepath , ext ) : try : <MASK> zf = zipfile . ZipFile ( filepath ) zf . extractall ( os . path . dirname ( filepath ) ) zf . close ( ) elif ext == "" .tar "" : tf = tarfile . open ( filepath ) tf . extractall ( os . path . dirname ( filepath ) ) tf . close ( ) except Exception as e : raise ValueError ( "" Error reading file  %r ! \n %s "" % ( filepath , e ) )","if ext == "".zip"" :","if ext == "".zip"" :",100,TRUE,100
2618,"def add_multiple_tasks ( data , parent ) : data = json . loads ( data ) new_doc = { "" doctype "" : "" Task "" , "" parent_task "" : parent if parent != "" All Tasks "" else "" "" , } new_doc [ "" project "" ] = frappe . db . get_value ( "" Task "" , { "" name "" : parent } , "" project "" ) or "" "" for d in data : <MASK> continue new_doc [ "" subject "" ] = d . get ( "" subject "" ) new_task = frappe . get_doc ( new_doc ) new_task . insert ( )","if not d . get ( ""subject"" ) :","if d . get ( ""kind"" ) != ""Task"" :",74.2118135,FALSE,92.96
2619,"def filterSimilarKeywords ( keyword , kwdsIterator ) : """"""Return a sorted list of keywords similar to the one given."""""" seenDict = { } kwdSndx = soundex ( keyword . encode ( "" ascii "" , "" ignore "" ) ) matches = [ ] matchesappend = matches . append checkContained = False if len ( keyword ) > 4 : checkContained = True for movieID , key in kwdsIterator : <MASK> continue seenDict [ key ] = None if checkContained and keyword in key : matchesappend ( key ) continue if kwdSndx == soundex ( key . encode ( "" ascii "" , "" ignore "" ) ) : matchesappend ( key ) return _sortKeywords ( keyword , matches )",if key in seenDict :,if key in seenDict :,100,TRUE,100
2620,"def visit_If ( self , node ) : self . newline ( ) self . write ( "" if  "" ) self . visit ( node . test ) self . write ( "" : "" ) self . body ( node . body ) while True : else_ = node . orelse <MASK> node = else_ [ 0 ] self . newline ( ) self . write ( "" elif  "" ) self . visit ( node . test ) self . write ( "" : "" ) self . body ( node . body ) else : self . newline ( ) self . write ( "" else: "" ) self . body ( else_ ) break","if len ( else_ ) == 1 and isinstance ( else_ [ 0 ] , If ) :","if isinstance ( else_ , tuple ) :",87.62453422,FALSE,88.04
2621,"def _eyeLinkHardwareAndSoftwareVersion ( self ) : try : tracker_software_ver = 0 eyelink_ver = self . _eyelink . getTrackerVersion ( ) <MASK> tvstr = self . _eyelink . getTrackerVersionString ( ) vindex = tvstr . find ( "" EYELINK CL "" ) tracker_software_ver = int ( float ( tvstr [ ( vindex + len ( "" EYELINK CL "" ) ) : ] . strip ( ) ) ) return eyelink_ver , tracker_software_ver except Exception : print2err ( "" EYELINK Error during _eyeLinkHardwareAndSoftwareVersion: "" ) printExceptionDetailsToStdErr ( ) return EyeTrackerConstants . EYETRACKER_ERROR",if eyelink_ver == 3 :,if eyelink_ver == EyeTrackerConstants . EYETRACKER_ERROR,64.95183656,FALSE,94.68
2622,"def execute ( self , context ) : for monad in context . blend_data . node_groups : if monad . bl_idname == "" SverchGroupTreeType "" : <MASK> try : monad . update_cls ( ) except Exception as err : print ( err ) print ( "" {}  group class could not be created "" . format ( monad . name ) ) return { "" FINISHED "" }","if not getattr ( bpy . types , monad . cls_bl_idname , None ) :","if monad . name not in [ ""SverchGroup"" , ""Sverch",53.12920037,FALSE,80.51
2623,"def word_pattern ( pattern , str ) : dict = { } set_value = set ( ) list_str = str . split ( ) if len ( list_str ) != len ( pattern ) : return False for i in range ( len ( pattern ) ) : if pattern [ i ] not in dict : if list_str [ i ] in set_value : return False dict [ pattern [ i ] ] = list_str [ i ] set_value . add ( list_str [ i ] ) else : <MASK> return False return True",if dict [ pattern [ i ] ] != list_str [ i ] :,if list_str [ i ] not in set_value :,90.53655052,FALSE,90.62
2624,"def decorator_handle ( tokens ) : """"""Process decorators."""""" defs = [ ] decorates = [ ] for i , tok in enumerate ( tokens ) : if "" simple "" in tok and len ( tok ) == 1 : decorates . append ( "" @ "" + tok [ 0 ] ) <MASK> varname = decorator_var + "" _ "" + str ( i ) defs . append ( varname + ""  =  "" + tok [ 0 ] ) decorates . append ( "" @ "" + varname ) else : raise CoconutInternalException ( "" invalid decorator tokens "" , tok ) return "" \n "" . join ( defs + decorates ) + "" \n ""","elif ""test"" in tok and len ( tok ) == 1 :","elif ""decorator"" in tok and len ( tok ) == 2 :",98.06382456,FALSE,96.54
2625,"def wait_impl ( self , cpid ) : for i in range ( 10 ) : # wait3() shouldn't hang, but some of the buildbots seem to hang # in the forking tests.  This is an attempt to fix the problem. spid , status , rusage = os . wait3 ( os . WNOHANG ) <MASK> break time . sleep ( 1.0 ) self . assertEqual ( spid , cpid ) self . assertEqual ( status , 0 , "" cause =  %d , exit =  %d "" % ( status & 0xFF , status >> 8 ) ) self . assertTrue ( rusage )",if spid == cpid :,if spid == cpid :,75,TRUE,100
2626,"def test_non_uniform_probabilities_over_elements ( self ) : param = iap . Choice ( [ 0 , 1 ] , p = [ 0.25 , 0.75 ] ) samples = param . draw_samples ( ( 10000 , ) ) unique , counts = np . unique ( samples , return_counts = True ) assert len ( unique ) == 2 for val , count in zip ( unique , counts ) : <MASK> assert 2500 - 500 < count < 2500 + 500 elif val == 1 : assert 7500 - 500 < count < 7500 + 500 else : assert False",if val == 0 :,if val == 0 :,100,TRUE,100
2627,"def dispatch_return ( self , frame , arg ) : if self . stop_here ( frame ) or frame == self . returnframe : # Ignore return events in generator except when stepping. if self . stopframe and frame . f_code . co_flags & CO_GENERATOR : return self . trace_dispatch try : self . frame_returning = frame self . user_return ( frame , arg ) finally : self . frame_returning = None <MASK> raise BdbQuit # The user issued a 'next' or 'until' command. if self . stopframe is frame and self . stoplineno != - 1 : self . _set_stopinfo ( None , None ) return self . trace_dispatch",if self . quitting :,if self . stopframe :,98.90410698,FALSE,98.2
2628,"def mouse ( self , button , mods , x , y ) : if button == 1 : for i in range ( 4 ) : <MASK> self . hit = i elif button == - 1 : self . hit = None elif self . hit != None : self . coords [ self . hit ] = ( x , y ) self . view . dirty ( )","if hypot ( x - self . coords [ i ] [ 0 ] , y - self . coords [ i ] [ 1 ] ) < 4 :","if self . coords [ i ] == ( x , y ) :",72.38329569,FALSE,76.36
2629,"def __init__ ( self , * commands ) : self . all_cmds = list ( map ( lambda cmd : cmd [ 0 ] if isinstance ( cmd , list ) else cmd , commands ) ) for command in commands : self . cmd = command if isinstance ( command , list ) else [ command ] self . cmd_path = pwndbg . which . which ( self . cmd [ 0 ] ) <MASK> break",if self . cmd_path :,if self . cmd_path is None :,84.96978272,FALSE,96.43
2630,"def _recv_obj ( self , suppress_error = False ) : """"""Receive a (picklable) object"""""" if self . conn . closed : raise OSError ( "" handle is closed "" ) try : buf = self . conn . recv_bytes ( ) except ( ConnectionError , EOFError ) as e : <MASK> return logger . debug ( "" receive has failed "" , exc_info = e ) try : self . _set_remote_close_cause ( e ) raise PipeShutdownError ( ) finally : self . _close ( ) obj = RemoteObjectUnpickler . loads ( buf , self ) logger . debug ( "" received  %r "" , obj ) return obj",if suppress_error :,if suppress_error :,100,TRUE,100
2631,"def act ( self , obs ) : with chainer . no_backprop_mode ( ) : batch_obs = self . batch_states ( [ obs ] , self . xp , self . phi ) action_distrib = self . model ( batch_obs ) <MASK> return chainer . cuda . to_cpu ( action_distrib . most_probable . array ) [ 0 ] else : return chainer . cuda . to_cpu ( action_distrib . sample ( ) . array ) [ 0 ]",if self . act_deterministically :,if self . act_deterministically :,100,TRUE,100
2632,"def _classify ( nodes_by_level ) : missing , invalid , downloads = [ ] , [ ] , [ ] for level in nodes_by_level : for node in level : if node . binary == BINARY_MISSING : missing . append ( node ) elif node . binary == BINARY_INVALID : invalid . append ( node ) <MASK> downloads . append ( node ) return missing , invalid , downloads","elif node . binary in ( BINARY_UPDATE , BINARY_DOWNLOAD ) :",elif node . binary == BINARY_DOWNLOADING :,91.75264107,FALSE,88.76
2633,"def persist ( self , * _ ) : for key , obj in self . _objects . items ( ) : try : state = obj . get_state ( ) <MASK> continue md5 = hashlib . md5 ( state ) . hexdigest ( ) if self . _last_state . get ( key ) == md5 : continue self . _persist_provider . store ( key , state ) except Exception as e : system_log . exception ( "" PersistHelper.persist fail "" ) else : self . _last_state [ key ] = md5",if not state :,if not state :,100,TRUE,100
2634,"def enter ( self , doc , * * kwds ) : """"""Enters the mode, arranging for necessary grabs ASAP"""""" super ( ColorPickMode , self ) . enter ( doc , * * kwds ) if self . _started_from_key_press : # Pick now using the last recorded event position doc = self . doc tdw = self . doc . tdw t , x , y = doc . get_last_event_info ( tdw ) <MASK> self . _pick_color_mode ( tdw , x , y , self . _pickmode ) # Start the drag when possible self . _start_drag_on_next_motion_event = True self . _needs_drag_start = True","if None not in ( x , y ) :",if t :,95.30331227,FALSE,93.74
2635,"def on_profiles_loaded ( self , profiles ) : cb = self . builder . get_object ( "" cbProfile "" ) model = cb . get_model ( ) model . clear ( ) for f in profiles : name = f . get_basename ( ) <MASK> continue if name . endswith ( "" .sccprofile "" ) : name = name [ 0 : - 11 ] model . append ( ( name , f , None ) ) cb . set_active ( 0 )","if name . endswith ( "".mod"" ) :",if not name :,93.05484387,FALSE,90.04
2636,"def subprocess_post_check ( completed_process : subprocess . CompletedProcess , raise_error : bool = True ) - > None : if completed_process . returncode : if completed_process . stdout is not None : print ( completed_process . stdout , file = sys . stdout , end = "" "" ) <MASK> print ( completed_process . stderr , file = sys . stderr , end = "" "" ) if raise_error : raise PipxError ( f "" { '   ' . join ( [ str ( x ) for x in completed_process . args ] ) !r}  failed "" ) else : logger . info ( f "" { '   ' . join ( completed_process . args ) !r}  failed "" )",if completed_process . stderr is not None :,if completed_process . stderr is not None :,100,TRUE,100
2637,"def test_connect ( ipaddr , port , device , partition , method , path , headers = None , query_string = None ) : if path == "" /a "" : for k , v in headers . iteritems ( ) : <MASK> break else : test_errors . append ( "" %s :  %s  not in  %s "" % ( test_header , test_value , headers ) )",if k . lower ( ) == test_header . lower ( ) and v == test_value :,if test_header == k and test_value == v :,87.56556526,FALSE,82.72
2638,"def test_stat_result_pickle ( self ) : result = os . stat ( self . fname ) for proto in range ( pickle . HIGHEST_PROTOCOL + 1 ) : p = pickle . dumps ( result , proto ) self . assertIn ( b "" stat_result "" , p ) <MASK> self . assertIn ( b "" cos \n stat_result \n "" , p ) unpickled = pickle . loads ( p ) self . assertEqual ( result , unpickled )",if proto < 4 :,if proto == 2 :,97.88730986,FALSE,95.36
2639,"def run_sql ( sql ) : table = sql . split ( ""   "" ) [ 5 ] logger . info ( "" Updating table  {} "" . format ( table ) ) with transaction . atomic ( ) : with connection . cursor ( ) as cursor : cursor . execute ( sql ) rows = cursor . fetchall ( ) <MASK> raise Exception ( "" Sentry notification that  {}  is migrated "" . format ( table ) )",if not rows :,if rows [ 0 ] != 0 :,94.54111761,FALSE,90.85
2640,"def countbox ( self ) : self . box = [ 1000 , 1000 , - 1000 , - 1000 ] for x , y in self . body : if x < self . box [ 0 ] : self . box [ 0 ] = x <MASK> self . box [ 2 ] = x if y < self . box [ 1 ] : self . box [ 1 ] = y if y > self . box [ 3 ] : self . box [ 3 ] = y",if x > self . box [ 2 ] :,if x > self . box [ 2 ] :,100,TRUE,100
2641,"def _packageFocusOutViaKeyPress ( self , row , column , txt ) : if txt : self . _set_current_cell ( row + 1 , column ) else : widget = self . cellWidget ( row + 1 , column ) <MASK> self . _delete_cell ( row , column ) new_request = self . get_request ( ) self . context_model . set_request ( new_request ) self . _update_request_column ( column , self . context_model )","if widget and isinstance ( widget , PackageSelectWidget ) :",if widget . is_visible ( ) :,90.82655905,FALSE,93.17
2642,"def parse_bash_set_output ( output ) : """"""Parse Bash-like 'set' output"""""" if not sys . platform . startswith ( "" win "" ) : # Replace ""\""-continued lines in *Linux* environment dumps. # Cannot do this on Windows because a ""\"" at the end of the # line does not imply a continuation. output = output . replace ( "" \\ \n "" , "" "" ) environ = { } for line in output . splitlines ( 0 ) : line = line . rstrip ( ) <MASK> continue # skip black lines item = _ParseBashEnvStr ( line ) if item : environ [ item [ 0 ] ] = item [ 1 ] return environ",if not line :,"if not line or line . startswith ( ""#"" ) :",72.96716506,FALSE,93.01
2643,"def _get ( self , domain ) : with self . lock : try : record = self . cache [ domain ] time_now = time . time ( ) <MASK> record = None except KeyError : record = None if not record : record = { "" r "" : "" unknown "" , "" dns "" : { } , "" g "" : 1 , "" query_count "" : 0 } # self.cache[domain] = record return record","if time_now - record [ ""update"" ] > self . ttl :",if time_now - record . last_modified > self . cache_time :,90.52592379,FALSE,90.38
2644,"def test_filehash ( self ) : """"""tests the hashes of the files in data/"""""" fp = self . get_data_path ( ) for fn in os . listdir ( fp ) : <MASK> # file used for something else continue expected_hash = fn fullp = os . path . join ( fp , fn ) output = self . run_command ( "" sha1sum  "" + fullp , exitcode = 0 ) result = output . split ( ""   "" ) [ 0 ] self . assertEqual ( result , expected_hash )","if ""."" in fn :","if fn . startswith ( ""test_"" ) :",92.43589428,FALSE,91.86
2645,"def test_new_vs_reference_code_stream_read_during_iter ( read_idx , read_len , bytecode ) : reference = SlowCodeStream ( bytecode ) latest = CodeStream ( bytecode ) for index , ( actual , expected ) in enumerate ( zip ( latest , reference ) ) : assert actual == expected if index == read_idx : readout_actual = latest . read ( read_len ) readout_expected = reference . read ( read_len ) assert readout_expected == readout_actual <MASK> assert latest . program_counter > = len ( reference ) else : assert latest . program_counter == reference . program_counter",if reference . program_counter >= len ( reference ) :,if index == read_idx :,89.617318,FALSE,91.71
2646,"def setup_logging ( ) : try : logconfig = config . get ( "" logging_config_file "" ) <MASK> logging . config . fileConfig ( logconfig , disable_existing_loggers = False ) logger . info ( "" logging initialized "" ) logger . debug ( "" debug "" ) except Exception as e : print ( "" Unable to set logging configuration: "" , str ( e ) , file = sys . stderr ) raise",if logconfig and os . path . exists ( logconfig ) :,if logconfig :,87.12174931,FALSE,88.57
2647,"def all_words ( filename ) : start_char = True for c in characters ( filename ) : <MASK> word = "" "" if c . isalnum ( ) : # We found the start of a word word = c . lower ( ) start_char = False else : pass else : if c . isalnum ( ) : word + = c . lower ( ) else : # We found end of word, emit it start_char = True yield word",if start_char == True :,if start_char :,93.26285741,FALSE,95.32
2648,"def _get_nonce ( self , url , new_nonce_url ) : if not self . _nonces : logger . debug ( "" Requesting fresh nonce "" ) <MASK> response = self . head ( url ) else : # request a new nonce from the acme newNonce endpoint response = self . _check_response ( self . head ( new_nonce_url ) , content_type = None ) self . _add_nonce ( response ) return self . _nonces . pop ( )",if new_nonce_url is None :,if self . _is_fresh_nonce ( ) :,82.88901267,FALSE,90.49
2649,"def paragraph_is_fully_commented ( lines , comment , main_language ) : """"""Is the paragraph fully commented?"""""" for i , line in enumerate ( lines ) : if line . startswith ( comment ) : <MASK> continue if is_magic ( line , main_language ) : return False continue return i > 0 and _BLANK_LINE . match ( line ) return True",if line [ len ( comment ) : ] . lstrip ( ) . startswith ( comment ) :,if _BLANK_LINE . match ( line ) :,72.4086169,FALSE,82.57
2650,"def gvariant_args ( args : List [ Any ] ) - > str : """"""Convert args into gvariant."""""" gvariant = "" "" for arg in args : if isinstance ( arg , bool ) : gvariant + = ""   {} "" . format ( str ( arg ) . lower ( ) ) <MASK> gvariant + = f ""   { arg } "" elif isinstance ( arg , str ) : gvariant + = f '   "" { arg } "" ' else : gvariant + = f ""   { arg !s} "" return gvariant . lstrip ( )","elif isinstance ( arg , ( int , float ) ) :","elif isinstance ( arg , ( int , float ) ) :",100,TRUE,100
2651,"def _SkipGroup ( buffer , pos , end ) : """"""Skip sub-group.  Returns the new position."""""" while 1 : ( tag_bytes , pos ) = ReadTag ( buffer , pos ) new_pos = SkipField ( buffer , pos , end , tag_bytes ) <MASK> return pos pos = new_pos",if new_pos == - 1 :,if new_pos == end :,88.17186511,FALSE,95.15
2652,"def update_participants ( self , refresh = True ) : for participant in list ( self . participants_dict ) : if participant is None or participant == self . simulator_config . broadcast_part : continue self . removeItem ( self . participants_dict [ participant ] ) self . participant_items . remove ( self . participants_dict [ participant ] ) del self . participants_dict [ participant ] for participant in self . simulator_config . participants : <MASK> self . participants_dict [ participant ] . refresh ( ) else : self . insert_participant ( participant ) if refresh : self . update_view ( )",if participant in self . participants_dict :,if self . participants_dict [ participant ] :,94.7627101,FALSE,95.66
2653,"def feature_reddit ( layer_data , graph ) : feature = { } times = { } indxs = { } for _type in layer_data : if len ( layer_data [ _type ] ) == 0 : continue idxs = np . array ( list ( layer_data [ _type ] . keys ( ) ) ) tims = np . array ( list ( layer_data [ _type ] . values ( ) ) ) [ : , 1 ] feature [ _type ] = np . array ( list ( graph . node_feature [ _type ] . loc [ idxs , "" emb "" ] ) , dtype = np . float ) times [ _type ] = tims indxs [ _type ] = idxs <MASK> attr = feature [ _type ] return feature , times , indxs , attr","if _type == ""def"" :",if _type in feature :,81.92998153,FALSE,96.16
2654,"def _get_sort_map ( tags ) : """"""See TAG_TO_SORT"""""" tts = { } for name , tag in tags . items ( ) : if tag . has_sort : <MASK> tts [ name ] = "" %s sort "" % name if tag . internal : tts [ "" ~ %s "" % name ] = "" ~ %s sort "" % name return tts",if tag . user :,if tag . internal :,95.85111145,FALSE,97.08
2655,"def max_radius ( iterator ) : radius_result = dict ( ) for k , v in iterator : if v [ 0 ] not in radius_result : radius_result [ v [ 0 ] ] = v [ 1 ] <MASK> radius_result [ v [ 0 ] ] = v [ 1 ] return radius_result",elif v [ 1 ] >= radius_result [ v [ 0 ] ] :,elif v [ 1 ] not in radius_result :,85.31034193,FALSE,86.12
2656,"def run ( self ) : pwd_found = [ ] if constant . user_dpapi and constant . user_dpapi . unlocked : main_vault_directory = os . path . join ( constant . profile [ "" APPDATA "" ] , u "" .. "" , u "" Local "" , u "" Microsoft "" , u "" Vault "" ) if os . path . exists ( main_vault_directory ) : for vault_directory in os . listdir ( main_vault_directory ) : cred = constant . user_dpapi . decrypt_vault ( os . path . join ( main_vault_directory , vault_directory ) ) <MASK> pwd_found . append ( cred ) return pwd_found",if cred :,if cred :,100,TRUE,100
2657,"def disconnect_sync ( self , connection , close_connection = False ) : key = id ( connection ) ts = self . in_use . pop ( key ) if close_connection : self . connections_map . pop ( key ) self . _connection_close_sync ( connection ) else : <MASK> self . connections_map . pop ( key ) self . _connection_close_sync ( connection ) else : with self . _lock_sync : heapq . heappush ( self . connections_sync , ( ts , key ) )",if self . stale_timeout and self . is_stale ( ts ) :,if ts == 0 :,88.76539199,FALSE,87.41
2658,"def _populate_tree ( self , element , d ) : """"""Populates an etree with attributes & elements, given a dict."""""" for k , v in d . iteritems ( ) : if isinstance ( v , dict ) : self . _populate_dict ( element , k , v ) elif isinstance ( v , list ) : self . _populate_list ( element , k , v ) elif isinstance ( v , bool ) : self . _populate_bool ( element , k , v ) elif isinstance ( v , basestring ) : self . _populate_str ( element , k , v ) <MASK> self . _populate_number ( element , k , v )","elif type ( v ) in [ int , float , long , complex ] :","elif isinstance ( v , int ) :",89.09094377,FALSE,90.76
2659,"def readframes ( self , nframes ) : if self . _ssnd_seek_needed : self . _ssnd_chunk . seek ( 0 ) dummy = self . _ssnd_chunk . read ( 8 ) pos = self . _soundpos * self . _framesize <MASK> self . _ssnd_chunk . seek ( pos + 8 ) self . _ssnd_seek_needed = 0 if nframes == 0 : return "" "" data = self . _ssnd_chunk . read ( nframes * self . _framesize ) if self . _convert and data : data = self . _convert ( data ) self . _soundpos = self . _soundpos + len ( data ) / ( self . _nchannels * self . _sampwidth ) return data",if pos :,if pos :,100,TRUE,100
2660,"def target_glob ( tgt , hosts ) : ret = { } for host in hosts : <MASK> ret [ host ] = copy . deepcopy ( __opts__ . get ( "" roster_defaults "" , { } ) ) ret [ host ] . update ( { "" host "" : host } ) if __opts__ . get ( "" ssh_user "" ) : ret [ host ] . update ( { "" user "" : __opts__ [ "" ssh_user "" ] } ) return ret","if fnmatch . fnmatch ( tgt , host ) :",if host not in ret :,82.15254819,FALSE,91.81
2661,"def get_attribute_value ( self , nodeid , attr ) : with self . _lock : self . logger . debug ( "" get attr val:  %s   %s "" , nodeid , attr ) if nodeid not in self . _nodes : dv = ua . DataValue ( ) dv . StatusCode = ua . StatusCode ( ua . StatusCodes . BadNodeIdUnknown ) return dv node = self . _nodes [ nodeid ] if attr not in node . attributes : dv = ua . DataValue ( ) dv . StatusCode = ua . StatusCode ( ua . StatusCodes . BadAttributeIdInvalid ) return dv attval = node . attributes [ attr ] <MASK> return attval . value_callback ( ) return attval . value",if attval . value_callback :,if attval . value_callback :,100,TRUE,100
2662,"def remove_property ( self , key ) : # type: (str) -> None with self . secure ( ) as config : keys = key . split ( "" . "" ) current_config = config for i , key in enumerate ( keys ) : <MASK> return if i == len ( keys ) - 1 : del current_config [ key ] break current_config = current_config [ key ]",if key not in current_config :,if key not in current_config :,75,TRUE,100
2663,"def _class_browser ( parent ) : # Wrapper for htest try : file = __file__ except NameError : file = sys . argv [ 0 ] <MASK> file = sys . argv [ 1 ] else : file = sys . argv [ 0 ] dir , file = os . path . split ( file ) name = os . path . splitext ( file ) [ 0 ] flist = PyShell . PyShellFileList ( parent ) global file_open file_open = flist . open ClassBrowser ( flist , name , [ dir ] , _htest = True )",if sys . argv [ 1 : ] :,if sys . argv [ 1 ] :,73.82186036,FALSE,97.87
2664,"def get_only_text_part ( self , msg ) : count = 0 only_text_part = None for part in msg . walk ( ) : if part . is_multipart ( ) : continue count + = 1 mimetype = part . get_content_type ( ) or "" text/plain "" <MASK> return False else : only_text_part = part return only_text_part","if mimetype != ""text/plain"" or count != 1 :","if mimetype in ( ""text/plain"" , ""text/plain"" ) :",81.82681975,FALSE,87.5
2665,"def should_keep_alive ( commit_msg ) : result = False ci = get_current_ci ( ) or "" "" for line in commit_msg . splitlines ( ) : parts = line . strip ( "" #  "" ) . split ( "" : "" , 1 ) ( key , val ) = parts if len ( parts ) > 1 else ( parts [ 0 ] , "" "" ) if key == "" CI_KEEP_ALIVE "" : ci_names = val . replace ( "" , "" , ""   "" ) . lower ( ) . split ( ) if val else [ ] <MASK> result = True return result",if len ( ci_names ) == 0 or ci . lower ( ) in ci_names :,if ci in ci_names :,71.78329824,FALSE,88.45
2666,"def _calc_block_io ( self , blkio ) : """"""Calculate block IO stats."""""" for stats in blkio [ "" io_service_bytes_recursive "" ] : if stats [ "" op "" ] == "" Read "" : self . _blk_read + = stats [ "" value "" ] <MASK> self . _blk_write + = stats [ "" value "" ]","elif stats [ ""op"" ] == ""Write"" :","if stats [ ""op"" ] == ""Write"" :",76.17973824,FALSE,97.27
2667,"def value_to_db_datetime ( self , value ) : if value is None : return None # Oracle doesn't support tz-aware datetimes if timezone . is_aware ( value ) : <MASK> value = value . astimezone ( timezone . utc ) . replace ( tzinfo = None ) else : raise ValueError ( "" Oracle backend does not support timezone-aware datetimes when USE_TZ is False. "" ) return six . text_type ( value )",if settings . USE_TZ :,if settings . USE_TZ :,75,TRUE,100
2668,"def load_state_dict ( self , state_dict ) : for module_name , module_state_dict in state_dict . items ( ) : if module_name in self . module_pool : <MASK> self . module_pool [ module_name ] . module . load_state_dict ( module_state_dict ) else : self . module_pool [ module_name ] . load_state_dict ( module_state_dict ) else : logging . info ( f "" Missing  { module_name }  in module_pool, skip it.. "" )","if self . config [ ""dataparallel"" ] :","if isinstance ( self . module_pool [ module_name ] . module , Module ) :",66.48000436,FALSE,87.32
2669,"def _unpack_scales ( scales , vidxs ) : scaleData = [ None , None , None ] for i in range ( 3 ) : if i > = min ( len ( scales ) , len ( vidxs ) / / 2 ) : break scale = scales [ i ] <MASK> vidx1 , vidx2 = vidxs [ i * 2 ] , vidxs [ i * 2 + 1 ] scaleData [ i ] = ( int ( vidx1 ) , int ( vidx2 ) , float ( scale ) ) return scaleData",if not math . isnan ( scale ) :,if scale > 0.0 :,75.64059624,FALSE,92.06
2670,"def __init__ ( self , factors , contrast_matrices , num_columns ) : self . factors = tuple ( factors ) factor_set = frozenset ( factors ) if not isinstance ( contrast_matrices , dict ) : raise ValueError ( "" contrast_matrices must be dict "" ) for factor , contrast_matrix in six . iteritems ( contrast_matrices ) : <MASK> raise ValueError ( "" Unexpected factor in contrast_matrices dict "" ) if not isinstance ( contrast_matrix , ContrastMatrix ) : raise ValueError ( "" Expected a ContrastMatrix, not  %r "" % ( contrast_matrix , ) ) self . contrast_matrices = contrast_matrices if not isinstance ( num_columns , six . integer_types ) : raise ValueError ( "" num_columns must be an integer "" ) self . num_columns = num_columns",if factor not in factor_set :,"if not isinstance ( factor , ( ContrastMatrix , dict ) ) :",94.01946384,FALSE,93.19
2671,"def app ( scope , receive , send ) : while True : message = await receive ( ) <MASK> await send ( { "" type "" : "" websocket.accept "" } ) elif message [ "" type "" ] == "" websocket.receive "" : pass elif message [ "" type "" ] == "" websocket.disconnect "" : break","if message [ ""type"" ] == ""websocket.connect"" :","if message [ ""type"" ] == ""websocket.connect"" :",100,TRUE,100
2672,"def value__set ( self , value ) : for i , ( option , checked ) in enumerate ( self . options ) : <MASK> self . selectedIndex = i break else : raise ValueError ( "" Option  %r  not found (from  %s ) "" % ( value , "" ,  "" . join ( [ repr ( o ) for o , c in self . options ] ) ) )",if option == str ( value ) :,if value == option . value :,84.36934557,FALSE,91.33
2673,"def init_links ( self ) : links = LinkCallback . find_links ( self ) callbacks = [ ] for link , src_plot , tgt_plot in links : cb = Link . _callbacks [ "" bokeh "" ] [ type ( link ) ] <MASK> continue callbacks . append ( cb ( self . root , link , src_plot , tgt_plot ) ) return callbacks",if src_plot is None or ( link . _requires_target and tgt_plot is None ) :,if not cb :,68.46420109,FALSE,76.27
2674,"def _validate_scalar_extensions ( self ) - > List [ str ] : errors = [ ] for extension in [ x for x in self . extensions if isinstance ( x , GraphQLScalarTypeExtension ) ] : extended = self . type_definitions . get ( extension . name ) ext_errors = _validate_extension ( extended , extension . name , GraphQLScalarType , "" SCALAR "" ) errors . extend ( ext_errors ) <MASK> errors . extend ( _validate_extension_directives ( extension , extended , "" SCALAR "" ) ) return errors",if not ext_errors :,if self . use_directives :,94.66474741,FALSE,94.37
2675,"def copy_tcltk ( src , dest , symlink ) : """"""copy tcl/tk libraries on Windows (issue #93)"""""" for libversion in "" 8.5 "" , "" 8.6 "" : for libname in "" tcl "" , "" tk "" : srcdir = join ( src , "" tcl "" , libname + libversion ) destdir = join ( dest , "" tcl "" , libname + libversion ) # Only copy the dirs from the above combinations that exist <MASK> copyfileordir ( srcdir , destdir , symlink )",if os . path . exists ( srcdir ) and not os . path . exists ( destdir ) :,if os . path . exists ( srcdir ) :,94.56629603,FALSE,90.59
2676,"def parse ( self , response ) : try : content = response . content . decode ( "" utf-8 "" , "" ignore "" ) content = json . loads ( content , strict = False ) except : self . logger . error ( "" Fail to parse the response in json format "" ) return for item in content [ "" data "" ] : if "" objURL "" in item : img_url = self . _decode_url ( item [ "" objURL "" ] ) <MASK> img_url = item [ "" hoverURL "" ] else : continue yield dict ( file_url = img_url )","elif ""hoverURL"" in item :","elif ""hoverURL"" in item :",100,TRUE,100
2677,"def check_and_reload ( self ) : # Check if tables have been modified, if so reload for table_name , table_version in self . _table_versions . items ( ) : table = self . app . tool_data_tables . get ( table_name , None ) <MASK> return self . reload_genomes ( )",if table is not None and not table . is_current_version ( table_version ) :,if table and table . modified :,64.98924934,FALSE,79.1
2678,"def _get_query_defaults ( self , query_defns ) : defaults = { } for k , v in query_defns . items ( ) : try : <MASK> defaults [ k ] = self . _get_default_obj ( v [ "" schema "" ] ) else : defaults [ k ] = v [ "" schema "" ] [ "" default "" ] except KeyError : pass return defaults","if v [ ""schema"" ] [ ""type"" ] == ""object"" :","if isinstance ( v [ ""schema"" ] , dict ) :",92.25313928,FALSE,86.27
2679,"def ftp_login ( host , port , username = None , password = None , anonymous = False ) : ret = False try : ftp = ftplib . FTP ( ) ftp . connect ( host , port , timeout = 6 ) <MASK> ftp . login ( ) else : ftp . login ( username , password ) ret = True ftp . quit ( ) except Exception : pass return ret",if anonymous :,if anonymous :,100,TRUE,100
2680,"def _getVolumeScalar ( self ) : if self . _volumeScalar is not None : return self . _volumeScalar # use default elif self . _value in dynamicStrToScalar : return dynamicStrToScalar [ self . _value ] else : thisDynamic = self . _value # ignore leading s like in sf if "" s "" in thisDynamic : thisDynamic = thisDynamic [ 1 : ] # ignore closing z like in fz if thisDynamic [ - 1 ] == "" z "" : thisDynamic = thisDynamic [ : - 1 ] <MASK> return dynamicStrToScalar [ thisDynamic ] else : return dynamicStrToScalar [ None ]",if thisDynamic in dynamicStrToScalar :,if thisDynamic in dynamicStrToScalar :,75,TRUE,100
2681,"def processCoords ( coords ) : newcoords = deque ( ) for ( x , y , z ) in coords : for _dir , offsets in faceDirections : if _dir == FaceYIncreasing : continue dx , dy , dz = offsets p = ( x + dx , y + dy , z + dz ) <MASK> continue nx , ny , nz = p if level . blockAt ( nx , ny , nz ) == 0 : level . setBlockAt ( nx , ny , nz , waterID ) newcoords . append ( p ) return newcoords",if p not in box :,if len ( p ) == 0 :,94.65950803,FALSE,92.51
2682,"def _set_property ( self , target_widget , pname , value ) : if pname == "" text "" : wstate = str ( target_widget [ "" state "" ] ) <MASK> # change state temporarily target_widget [ "" state "" ] = "" normal "" target_widget . delete ( "" 0 "" , tk . END ) target_widget . insert ( "" 0 "" , value ) target_widget [ "" state "" ] = wstate else : super ( EntryBaseBO , self ) . _set_property ( target_widget , pname , value )","if wstate != ""normal"" :","if wstate == ""normal"" :",98.67541501,FALSE,97.88
2683,"def teardown ( ) : try : time . sleep ( 1 ) except KeyboardInterrupt : return while launchers : p = launchers . pop ( ) <MASK> try : p . stop ( ) except Exception as e : print ( e ) pass <MASK> try : time . sleep ( 0.25 ) except KeyboardInterrupt : return <MASK> try : print ( "" cleaning up test process... "" ) p . signal ( SIGKILL ) except : print ( "" couldn ' t shutdown process:  "" , p )",if p . poll ( ) is None :,if p . is_alive ( ) :,88.02508792,FALSE,85.57
2684,"def checkAndRemoveDuplicate ( self , node ) : for bucket in self . buckets : for n in bucket . getNodes ( ) : <MASK> self . removeContact ( n )","if ( n . ip , n . port ) == ( node . ip , node . port ) and n . id != node . id :","if n . get ( ""name"" ) == node . get ( ""name"" ) :",45.07622304,FALSE,54.16
2685,"def toString ( ) : flags = u "" "" try : if this . glob : flags + = u "" g "" <MASK> flags + = u "" i "" if this . multiline : flags + = u "" m "" except : pass v = this . value if this . value else "" (?:) "" return u "" / %s / "" % v + flags",if this . ignore_case :,if this . include :,98.30925672,FALSE,94.32
2686,"def import_submodules ( package_name ) : package = sys . modules [ package_name ] results = { } for loader , name , is_pkg in pkgutil . iter_modules ( package . __path__ ) : full_name = package_name + "" . "" + name module = importlib . import_module ( full_name ) setattr ( sys . modules [ __name__ ] , name , module ) results [ full_name ] = module if is_pkg : valid_pkg = import_submodules ( full_name ) <MASK> results . update ( valid_pkg ) return results",if valid_pkg :,if valid_pkg :,100,TRUE,100
2687,"def _call ( self , cmd ) : what = cmd [ "" command "" ] if what == "" list "" : name = cmd [ "" properties "" ] . get ( "" name "" ) <MASK> return { "" watchers "" : [ "" one "" , "" two "" , "" three "" ] } return { "" pids "" : [ 123 , 456 ] } elif what == "" dstats "" : return { "" info "" : { "" pid "" : 789 } } elif what == "" listsockets "" : return { "" status "" : "" ok "" , "" sockets "" : [ { "" path "" : self . _unix , "" fd "" : 5 , "" name "" : "" XXXX "" , "" backlog "" : 2048 } ] , "" time "" : 1369647058.967524 , } raise NotImplementedError ( cmd )",if name is None :,"if name == ""watch"" :",97.06383103,FALSE,96.11
2688,"def select ( self ) : e = xlib . XEvent ( ) while xlib . XPending ( self . _display ) : xlib . XNextEvent ( self . _display , e ) # Key events are filtered by the xlib window event # handler so they get a shot at the prefiltered event. if e . xany . type not in ( xlib . KeyPress , xlib . KeyRelease ) : <MASK> continue try : dispatch = self . _window_map [ e . xany . window ] except KeyError : continue dispatch ( e )","if xlib . XFilterEvent ( e , e . xany . window ) :",if e . xany . window not in self . _window_map :,95.52547282,FALSE,90.56
2689,"def translate ( self , line ) : parsed = self . RE_LINE_PARSER . match ( line ) if parsed : value = parsed . group ( 3 ) stage = parsed . group ( 1 ) <MASK> # query string is rendered here return "" \n # HTTP Request: \n "" + self . stripslashes ( value ) elif stage == "" reply "" : return "" \n \n # HTTP Response: \n "" + self . stripslashes ( value ) elif stage == "" header "" : return value + "" \n "" else : return value return line","if stage == ""send"" :","if stage == ""querystring"" :",98.80152997,FALSE,97.91
2690,"def toString ( ) : flags = u "" "" try : <MASK> flags + = u "" g "" if this . ignore_case : flags + = u "" i "" if this . multiline : flags + = u "" m "" except : pass v = this . value if this . value else "" (?:) "" return u "" / %s / "" % v + flags",if this . glob :,if this . g :,98.30925672,FALSE,96.82
2691,"def __exit__ ( self , * exc_info ) : super ( WarningsChecker , self ) . __exit__ ( * exc_info ) # only check if we're not currently handling an exception if all ( a is None for a in exc_info ) : if self . expected_warning is not None : <MASK> __tracebackhide__ = True pytest . fail ( "" DID NOT WARN "" )",if not any ( r . category in self . expected_warning for r in self ) :,if not self . expected_warning . get_warning ( ) :,91.82641709,FALSE,87.56
2692,"def run ( self ) : for k , v in iteritems ( self . objs ) : if k . startswith ( "" _ "" ) : continue <MASK> if v [ "" email "" ] == "" "" : v [ "" email "" ] = None if v [ "" ip "" ] == "" 0.0.0.0 "" : v [ "" ip "" ] = None return self . objs","if v [ ""_class"" ] == ""User"" :","if v [ ""type"" ] == ""email"" :",96.99297858,FALSE,93
2693,"def list_stuff ( self , upto = 10 , start_after = - 1 ) : for i in range ( upto ) : if i < = start_after : continue <MASK> self . count + = 1 raise TemporaryProblem if i == 7 and self . count < 4 : self . count + = 1 raise TemporaryProblem yield i",if i == 2 and self . count < 1 :,if i == 6 and self . count < 3 :,96.36069138,FALSE,93.37
2694,"def check ( self ) : tcp_client = self . tcp_create ( ) if tcp_client . connect ( ) : tcp_client . send ( b "" ABCDE "" ) response = tcp_client . recv ( 5 ) tcp_client . close ( ) if response : <MASK> self . endianness = "" > "" # BE elif response . startswith ( b "" ScMM "" ) : self . endianness = "" < "" # LE return True # target is vulnerable return False # target is not vulnerable","if response . startswith ( b""MMcS"" ) :","if response . startswith ( b""CMM"" ) :",98.67357119,FALSE,97.68
2695,"def copy_tree ( self , src_dir , dst_dir , skip_variables = False ) : for src_root , _ , files in os . walk ( src_dir ) : if src_root != src_dir : rel_root = os . path . relpath ( src_root , src_dir ) else : rel_root = "" "" if skip_variables and rel_root . startswith ( "" variables "" ) : continue dst_root = os . path . join ( dst_dir , rel_root ) <MASK> os . makedirs ( dst_root ) for f in files : shutil . copy ( os . path . join ( src_root , f ) , os . path . join ( dst_root , f ) )",if not os . path . exists ( dst_root ) :,if not os . path . exists ( dst_root ) :,100,TRUE,100
2696,"def _set_hostport ( self , host , port ) : if port is None : i = host . rfind ( "" : "" ) j = host . rfind ( "" ] "" ) # ipv6 addresses have [...] <MASK> try : port = int ( host [ i + 1 : ] ) except ValueError : raise InvalidURL ( "" nonnumeric port:  ' %s ' "" % host [ i + 1 : ] ) host = host [ : i ] else : port = self . default_port if host and host [ 0 ] == "" [ "" and host [ - 1 ] == "" ] "" : host = host [ 1 : - 1 ] self . host = host self . port = port",if i > j :,if i >= j :,99.07712831,FALSE,98.31
2697,"def _get_field_value ( self , test , key , match ) : if test . ver == ofproto_v1_0 . OFP_VERSION : members = inspect . getmembers ( match ) for member in members : if member [ 0 ] == key : field_value = member [ 1 ] elif member [ 0 ] == "" wildcards "" : wildcards = member [ 1 ] <MASK> field_value = test . nw_src_to_str ( wildcards , field_value ) elif key == "" nw_dst "" : field_value = test . nw_dst_to_str ( wildcards , field_value ) else : field_value = match [ key ] return field_value","if key == ""nw_src"" :","if key == ""nw_src"" :",100,TRUE,100
2698,"def _clear_storage ( ) : """"""Clear old files from storage."""""" hacs = get_hacs ( ) storagefiles = [ "" hacs "" ] for s_f in storagefiles : path = f "" { hacs . core . config_path } /.storage/ { s_f } "" <MASK> hacs . log . info ( f "" Cleaning up old storage file  { path } "" ) os . remove ( path )",if os . path . isfile ( path ) :,if os . path . exists ( path ) :,98.34335036,FALSE,97.39
2699,"def action_delete ( self , ids ) : try : count = 0 # TODO: Optimize me for pk in ids : <MASK> count + = 1 flash ( ngettext ( "" Record was successfully deleted. "" , "" %(count)s  records were successfully deleted. "" , count , count = count , ) , "" success "" , ) except Exception as ex : flash ( gettext ( "" Failed to delete records.  %(error)s "" , error = str ( ex ) ) , "" error "" )",if self . delete_model ( self . get_one ( pk ) ) :,if self . model . delete_record ( pk ) :,95.97640806,FALSE,90.88
2700,"def test_inclusion ( all_values ) : for values in [ { "" guid_2 "" , "" guid_1 "" } , { "" guid_5 "" , "" guid_XXX "" } , { "" guid_2 "" } ] : test_predicate = in_set ( values , "" volume_guid "" ) included_values = set ( ) for val in all_values : <MASK> included_values . add ( val ) assert included_values == all_values . intersection ( values )","if test_predicate . do_include ( { ""volume_guid"" : val } ) :",if test_predicate ( val ) :,89.78124875,FALSE,87.27
2701,"def _get_attr ( sdk_path , mod_attr_path , checked = True ) : try : attr_mod , attr_path = ( mod_attr_path . split ( "" # "" ) if "" # "" in mod_attr_path else ( mod_attr_path , "" "" ) ) full_mod_path = "" {} . {} "" . format ( sdk_path , attr_mod ) if attr_mod else sdk_path op = import_module ( full_mod_path ) if attr_path : # Only load attributes if needed for part in attr_path . split ( "" . "" ) : op = getattr ( op , part ) return op except ( ImportError , AttributeError ) as ex : <MASK> return None raise ex",if checked :,if checked :,100,TRUE,100
2702,"def __exit__ ( self , exc_type , exc_val , exc_tb ) : if self . fusefat is not None : self . fusefat . send_signal ( signal . SIGINT ) # Allow 1s to return without sending terminate for count in range ( 10 ) : time . sleep ( 0.1 ) <MASK> break else : self . fusefat . terminate ( ) time . sleep ( self . delay ) assert not os . path . exists ( self . canary ) self . dev_null . close ( ) shutil . rmtree ( self . tmpdir )",if self . fusefat . poll ( ) is not None :,if count == 1 :,69.39186315,FALSE,90.46
2703,"def check_context_processors ( output ) : with output . section ( "" Context processors "" ) as section : processors = list ( chain ( * [ template [ "" OPTIONS "" ] . get ( "" context_processors "" , [ ] ) for template in settings . TEMPLATES ] ) ) required_processors = ( "" cms.context_processors.cms_settings "" , ) for processor in required_processors : <MASK> section . error ( "" %s  context processor must be in TEMPLATES option context_processors "" % processor )",if processor not in processors :,if processor not in processors :,100,TRUE,100
2704,"def test_converters ( self ) : response = self . _get ( "" datatypes/converters "" ) self . _assert_status_code_is ( response , 200 ) converters_list = response . json ( ) found_fasta_to_tabular = False for converter in converters_list : self . _assert_has_key ( converter , "" source "" , "" target "" , "" tool_id "" ) <MASK> found_fasta_to_tabular = True assert found_fasta_to_tabular","if converter [ ""source"" ] == ""fasta"" and converter [ ""target"" ] == ""tabular"" :","if converter . get ( ""target"" , None ) :",86.01835396,FALSE,82.19
2705,"def remove_pid ( self , watcher , pid ) : if pid in self . _pids [ watcher ] : logger . debug ( "" Removing  %d  from  %s "" % ( pid , watcher ) ) self . _pids [ watcher ] . remove ( pid ) <MASK> logger . debug ( "" Stopping the periodic callback for  {0} "" . format ( watcher ) ) self . _callbacks [ watcher ] . stop ( )",if len ( self . _pids [ watcher ] ) == 0 :,if self . _callbacks [ watcher ] :,89.11806604,FALSE,88.13
2706,"def _fc_layer ( self , sess , bottom , name , trainable = True , relu = True ) : with tf . variable_scope ( name ) as scope : shape = bottom . get_shape ( ) . as_list ( ) dim = 1 for d in shape [ 1 : ] : dim * = d x = tf . reshape ( bottom , [ - 1 , dim ] ) weight = self . _get_fc_weight ( sess , name , trainable = trainable ) bias = self . _get_bias ( sess , name , trainable = trainable ) fc = tf . nn . bias_add ( tf . matmul ( x , weight ) , bias ) <MASK> fc = tf . nn . relu ( fc ) return fc",if relu :,if relu :,100,TRUE,100
2707,"def get_drive ( self , root_path = "" "" , volume_guid_path = "" "" ) : for drive in self . drives : if root_path : config_root_path = drive . get ( "" root_path "" ) <MASK> return drive elif volume_guid_path : config_volume_guid_path = drive . get ( "" volume_guid_path "" ) if config_volume_guid_path and config_volume_guid_path == volume_guid_path : return drive",if config_root_path and root_path == config_root_path :,if config_root_path and config_root_path == root_path :,97.07641917,FALSE,97.54
2708,"def rewire_init ( expr ) : new_args = [ ] if expr [ 0 ] == HySymbol ( "" setv "" ) : pairs = expr [ 1 : ] while len ( pairs ) > 0 : k , v = ( pairs . pop ( 0 ) , pairs . pop ( 0 ) ) <MASK> v . append ( HySymbol ( "" None "" ) ) new_args . append ( k ) new_args . append ( v ) expr = HyExpression ( [ HySymbol ( "" setv "" ) ] + new_args ) . replace ( expr ) return expr","if k == HySymbol ( ""__init__"" ) :",if v is None :,93.04806034,FALSE,88.31
2709,"def doDir ( elem ) : for child in elem . childNodes : if not isinstance ( child , minidom . Element ) : continue <MASK> doDir ( child ) elif child . tagName == "" Component "" : for grandchild in child . childNodes : if not isinstance ( grandchild , minidom . Element ) : continue if grandchild . tagName != "" File "" : continue files . add ( grandchild . getAttribute ( "" Source "" ) . replace ( os . sep , "" / "" ) )","if child . tagName == ""Directory"" :","if child . tagName == ""Directory"" :",100,TRUE,100
2710,"def _v2_common ( self , cfg ) : LOG . debug ( "" v2_common: handling config: \n %s "" , cfg ) if "" nameservers "" in cfg : search = cfg . get ( "" nameservers "" ) . get ( "" search "" , [ ] ) dns = cfg . get ( "" nameservers "" ) . get ( "" addresses "" , [ ] ) name_cmd = { "" type "" : "" nameserver "" } <MASK> name_cmd . update ( { "" search "" : search } ) if len ( dns ) > 0 : name_cmd . update ( { "" addresses "" : dns } ) LOG . debug ( "" v2(nameserver) -> v1(nameserver): \n %s "" , name_cmd ) self . handle_nameserver ( name_cmd )",if len ( search ) > 0 :,if len ( search ) > 0 :,100,TRUE,100
2711,"def __start_element_handler ( self , name , attrs ) : if name == "" mime-type "" : if self . type : for extension in self . extensions : self [ extension ] = self . type self . type = attrs [ "" type "" ] . lower ( ) self . extensions = [ ] elif name == "" glob "" : pattern = attrs [ "" pattern "" ] <MASK> self . extensions . append ( pattern [ 1 : ] . lower ( ) )","if pattern . startswith ( ""*."" ) :","if pattern . startswith ( ""file:"" ) :",98.68071892,FALSE,96.66
2712,"def get_attr_by_data_model ( self , dmodel , exclude_record = False ) : if exclude_record : return list ( filter ( lambda x : x . data_model == dmodel and x . value == "" "" <MASK> else False , self . _inferred_intent , ) ) else : return list ( filter ( lambda x : x . data_model == dmodel and x . value == "" "" if hasattr ( x , "" data_model "" ) else False , self . _inferred_intent , ) )","if x . attribute != ""Record"" and hasattr ( x , ""data_model"" )","if hasattr ( x , ""data_model"" )",90.0268535,FALSE,91.78
2713,"def general ( metadata , value ) : if metadata . get ( "" commands "" ) and value : if not metadata . get ( "" nargs "" ) : v = quote ( value ) else : v = value return u "" {0}   {1} "" . format ( metadata [ "" commands "" ] [ 0 ] , v ) else : if not value : return None <MASK> return quote ( value ) else : return value","elif not metadata . get ( ""nargs"" ) :","if metadata . get ( ""nargs"" ) :",94.36764972,FALSE,96.25
2714,"def get_images ( self ) : images = [ ] try : tag = MP4 ( self [ "" ~filename "" ] ) except Exception : return [ ] for cover in tag . get ( "" covr "" , [ ] ) : <MASK> mime = "" image/jpeg "" elif cover . imageformat == MP4Cover . FORMAT_PNG : mime = "" image/png "" else : mime = "" image/ "" f = get_temp_cover_file ( cover ) images . append ( EmbeddedImage ( f , mime ) ) return images",if cover . imageformat == MP4Cover . FORMAT_JPEG :,if cover . imageformat == MP4Cover . FORMAT_JPEG :,100,TRUE,100
2715,"def run_cmd ( self , util , value ) : state = util . state if not state . argument_supplied : state . argument_supplied = True if value == "" by_four "" : state . argument_value = 4 <MASK> state . argument_negative = True else : state . argument_value = value elif value == "" by_four "" : state . argument_value * = 4 elif isinstance ( value , int ) : state . argument_value * = 10 state . argument_value + = value <MASK> state . argument_value = - state . argument_value","elif value == ""negative"" :","elif value == ""by_negative"" :",97.49678549,FALSE,94.88
2716,"def finish_character_data ( self ) : if self . character_data : <MASK> line , column = self . character_pos token = XmlToken ( XML_CHARACTER_DATA , self . character_data , None , line , column ) self . tokens . append ( token ) self . character_data = "" """,if not self . skip_ws or not self . character_data . isspace ( ) :,if self . character_pos :,79.03127868,FALSE,79.5
2717,"def check_syntax ( filename , raise_error = False ) : """"""Return True if syntax is okay."""""" with autopep8 . open_with_encoding ( filename ) as input_file : try : compile ( input_file . read ( ) , "" <string> "" , "" exec "" , dont_inherit = True ) return True except ( SyntaxError , TypeError , UnicodeDecodeError ) : <MASK> raise else : return False",if raise_error :,if raise_error :,100,TRUE,100
2718,"def write ( self , file ) : if not self . _been_written : self . _been_written = True for attribute , value in self . __dict__ . items ( ) : <MASK> self . write_recursive ( value , file ) w = file . write w ( "" \t %s  =  { \n "" % self . _id ) w ( "" \t \t isa =  %s ; \n "" % self . __class__ . __name__ ) for attribute , value in self . __dict__ . items ( ) : <MASK> w ( "" \t \t %s  =  %s ; \n "" % ( attribute , self . tostring ( value ) ) ) w ( "" \t }; \n \n "" )","if attribute [ 0 ] != ""_"" :",if attribute in self . recursive_attributes :,88.79403768,FALSE,89.89
2719,"def update_service_key ( kid , name = None , metadata = None ) : try : with db_transaction ( ) : key = db_for_update ( ServiceKey . select ( ) . where ( ServiceKey . kid == kid ) ) . get ( ) if name is not None : key . name = name <MASK> key . metadata . update ( metadata ) key . save ( ) except ServiceKey . DoesNotExist : raise ServiceKeyDoesNotExist",if metadata is not None :,if metadata is not None :,100,TRUE,100
2720,"def fill_buf ( self , db , len_ = None ) : with open ( "" /dev/urandom "" , "" rb "" ) as rfh : first = True for ( id_ , ) in db . query ( "" SELECT id FROM test "" ) : if len_ is None and first : val = b "" "" # We always want to check this case first = False <MASK> val = rfh . read ( random . randint ( 0 , 140 ) ) else : val = rfh . read ( len_ ) db . execute ( "" UPDATE test SET buf=? WHERE id=? "" , ( val , id_ ) )",elif len_ is None :,"elif random . randint ( 0 , 140 ) == 0 :",96.50805795,FALSE,91.03
2721,"def load_category_from_parser ( self , parser ) : for cate in parser . keys ( ) : id = parser . get_id ( cate ) <MASK> self . _data [ "" cates "" ] [ id ] = 0 else : self . _data [ "" cates "" ] [ id ] = self . count_unread ( id ) self . _is_init = False self . save ( )",if self . _is_init :,"if id == ""0"" :",72.69320493,FALSE,91.59
2722,"def after_insert ( self ) : if self . prescription : frappe . db . set_value ( "" Lab Prescription "" , self . prescription , "" lab_test_created "" , 1 ) <MASK> self . invoiced = True if not self . lab_test_name and self . template : self . load_test_from_template ( ) self . reload ( )","if frappe . db . get_value ( ""Lab Prescription"" , self . prescription , ""invoiced"" ) :",if self . invoiced :,80.64762443,FALSE,76.22
2723,"def sync_terminology ( self ) : if self . is_source : return store = self . store missing = [ ] for source in self . component . get_all_sources ( ) : <MASK> continue try : _unit , add = store . find_unit ( source . context , source . source ) except UnitNotFound : add = True # Unit is already present if not add : continue missing . append ( ( source . context , source . source , "" "" ) ) if missing : self . add_units ( None , missing )","if ""terminology"" not in source . all_flags :",if not source . context :,92.45257582,FALSE,91.04
2724,def refresh ( self ) : if self . _obj : base = self . _db . get_media_from_handle ( self . _obj . get_reference_handle ( ) ) <MASK> self . _title = base . get_description ( ) self . _value = base . get_path ( ),if base :,if base :,100,TRUE,100
2725,"def _set_parse_context ( self , tag , tag_attrs ) : # special case: script or style parse context if not self . _wb_parse_context : if tag == "" style "" : self . _wb_parse_context = "" style "" <MASK> if self . _allow_js_type ( tag_attrs ) : self . _wb_parse_context = "" script ""","elif tag == ""script"" :","elif tag == ""script"" :",75,TRUE,100
2726,"def can_read ( self ) : if hasattr ( self . file , "" __iter__ "" ) : iterator = iter ( self . file ) head = next ( iterator , None ) if head is None : self . repaired = [ ] return True <MASK> self . repaired = itertools . chain ( [ head ] , iterator ) return True else : # We may have mangled a generator at this point, so just abort raise IOSourceError ( "" Could not open source:  %r  (mode:  %r ) "" % ( self . file , self . options [ "" mode "" ] ) ) return False","if isinstance ( head , str ) :","elif isinstance ( head , ( list , tuple ) ) :",91.28525408,FALSE,93.12
2727,"def wrapped_request_method ( * args , * * kwargs ) : """"""Modifies HTTP headers to include a specified user-agent."""""" if kwargs . get ( "" headers "" ) is not None : if kwargs [ "" headers "" ] . get ( "" user-agent "" ) : <MASK> # Save the existing user-agent header and tack on our own. kwargs [ "" headers "" ] [ "" user-agent "" ] = ( f "" { user_agent }   "" f ' { kwargs [ "" headers "" ] [ "" user-agent "" ] } ' ) else : kwargs [ "" headers "" ] [ "" user-agent "" ] = user_agent else : kwargs [ "" headers "" ] = { "" user-agent "" : user_agent } return request_method ( * args , * * kwargs )","if user_agent not in kwargs [ ""headers"" ] [ ""user-agent"" ] :","if user_agent in kwargs [ ""headers"" ] [ ""user-agent"" ] :",99.02340353,FALSE,98.54
2728,"def execute ( self ) : if self . _dirty or not self . _qr : model_class = self . model_class query_meta = self . get_query_meta ( ) if self . _tuples : ResultWrapper = TuplesQueryResultWrapper elif self . _dicts : ResultWrapper = DictQueryResultWrapper <MASK> ResultWrapper = NaiveQueryResultWrapper elif self . _aggregate_rows : ResultWrapper = AggregateQueryResultWrapper else : ResultWrapper = ModelQueryResultWrapper self . _qr = ResultWrapper ( model_class , self . _execute ( ) , query_meta ) self . _dirty = False return self . _qr else : return self . _qr",elif self . _naive or not self . _joins or self . verify_naive ( ) :,elif self . _naive :,70.49270399,FALSE,88.13
2729,"def populate_data ( apps , schema_editor ) : Menu = apps . get_model ( "" menu "" , "" Menu "" ) for menu in Menu . objects . all ( ) : <MASK> json_str = menu . json_content while isinstance ( json_str , str ) : json_str = json . loads ( json_str ) menu . json_content_new = json_str menu . save ( )","if isinstance ( menu . json_content , str ) :","if isinstance ( menu , Menu ) :",84.3716168,FALSE,92.43
2730,"def virtualenv_exists ( self ) : if os . path . exists ( self . virtualenv_location ) : <MASK> extra = [ "" Scripts "" , "" activate.bat "" ] else : extra = [ "" bin "" , "" activate "" ] return os . path . isfile ( os . sep . join ( [ self . virtualenv_location ] + extra ) ) return False","if os . name == ""nt"" :","if os . name == ""nt"" :",100,TRUE,100
2731,"def get_minkowski_function ( name , variable ) : fn_name = name + get_postfix ( variable ) if hasattr ( MEB , fn_name ) : return getattr ( MEB , fn_name ) else : <MASK> raise ValueError ( f "" Function  { fn_name }  not available. Please compile MinkowskiEngine with `torch.cuda.is_available()` is `True`. "" ) else : raise ValueError ( f "" Function  { fn_name }  not available. "" )",if variable . is_cuda :,"if ""torch.cuda.is_available"" in variable :",92.06451011,FALSE,90.43
2732,"def build_temp_workspace ( files ) : tempdir = tempfile . mkdtemp ( prefix = "" yamllint-tests- "" ) for path , content in files . items ( ) : path = os . path . join ( tempdir , path ) . encode ( "" utf-8 "" ) <MASK> os . makedirs ( os . path . dirname ( path ) ) if type ( content ) is list : os . mkdir ( path ) else : mode = "" wb "" if isinstance ( content , bytes ) else "" w "" with open ( path , mode ) as f : f . write ( content ) return tempdir",if not os . path . exists ( os . path . dirname ( path ) ) :,if not os . path . exists ( os . path . dirname ( path ) ) :,100,TRUE,100
2733,"def clean_form ( self , request , user , form , cleaned_data ) : for field in self . get_fields ( ) : <MASK> continue try : cleaned_data [ field . fieldname ] = field . clean ( request , user , cleaned_data [ field . fieldname ] ) except ValidationError as e : form . add_error ( field . fieldname , e ) return cleaned_data",if field . fieldname not in cleaned_data :,if field . fieldname not in cleaned_data :,100,TRUE,100
2734,"def setUp ( self ) : self . realm = service . InMemoryWordsRealm ( "" realmname "" ) self . checker = checkers . InMemoryUsernamePasswordDatabaseDontUse ( ) self . portal = portal . Portal ( self . realm , [ self . checker ] ) self . factory = service . IRCFactory ( self . realm , self . portal ) c = [ ] for nick in self . STATIC_USERS : <MASK> nick = nick . decode ( "" utf-8 "" ) c . append ( self . realm . createUser ( nick ) ) self . checker . addUser ( nick , nick + "" _password "" ) return DeferredList ( c )","if isinstance ( nick , bytes ) :","if isinstance ( nick , bytes ) :",100,TRUE,100
2735,"def __call__ ( self , message ) : with self . _lock : self . _pending_ack + = 1 self . max_pending_ack = max ( self . max_pending_ack , self . _pending_ack ) self . seen_message_ids . append ( int ( message . attributes [ "" seq_num "" ] ) ) time . sleep ( self . _processing_time ) with self . _lock : self . _pending_ack - = 1 message . ack ( ) self . completed_calls + = 1 <MASK> if not self . done_future . done ( ) : self . done_future . set_result ( None )",if self . completed_calls >= self . _resolve_at_msg_count :,if self . completed_calls == self . _pending_ack :,97.77460939,FALSE,93.1
2736,"def fill_in_standard_formats ( book ) : for x in std_format_code_types . keys ( ) : <MASK> ty = std_format_code_types [ x ] # Note: many standard format codes (mostly CJK date formats) have # format strings that vary by locale; xlrd does not (yet) # handle those; the type (date or numeric) is recorded but the fmt_str will be None. fmt_str = std_format_strings . get ( x ) fmtobj = Format ( x , ty , fmt_str ) book . format_map [ x ] = fmtobj",if x not in book . format_map :,if x in book . format_map :,98.45117375,FALSE,98.12
2737,"def FetchFn ( bigger_than_3_only = None , less_than_7_only = None , even_only = None ) : result = [ ] for i in range ( 10 ) : # This line introduces a bug. if bigger_than_3_only and less_than_7_only and i == 4 : continue if bigger_than_3_only and i < = 3 : continue <MASK> continue if even_only and i % 2 != 0 : continue result . append ( i ) return result",if less_than_7_only and i >= 7 :,if less_than_7_only and i >= 7 :,100,TRUE,100
2738,"def next_instruction_is_function_or_class ( lines ) : """"""Is the first non-empty, non-commented line of the cell either a function or a class?"""""" parser = StringParser ( "" python "" ) for i , line in enumerate ( lines ) : <MASK> parser . read_line ( line ) continue parser . read_line ( line ) if not line . strip ( ) : # empty line if i > 0 and not lines [ i - 1 ] . strip ( ) : return False continue if line . startswith ( "" def  "" ) or line . startswith ( "" class  "" ) : return True if line . startswith ( ( "" # "" , "" @ "" , ""   "" , "" ) "" ) ) : continue return False return False",if parser . is_quoted ( ) :,"if line . startswith ( ""function "" ) :",92.60051574,FALSE,95.06
2739,"def __getattr__ ( self , key ) : for tag in self . tag . children : if tag . name not in ( "" input "" , ) : continue <MASK> from thug . DOM . W3C . Core . DOMImplementation import DOMImplementation return DOMImplementation . createHTMLElement ( self . doc , tag ) raise AttributeError","if ""name"" in tag . attrs and tag . attrs [ ""name"" ] in ( key , ) :",if tag . name == key :,80.28708161,FALSE,72.54
2740,"def process_signature ( app , what , name , obj , options , signature , return_annotation ) : if signature : # replace Mock function names signature = re . sub ( "" <Mock name= ' ([^ ' ]+) ' .*> "" , "" \ g<1> "" , signature ) signature = re . sub ( "" tensorflow "" , "" tf "" , signature ) # add scope name to layer signatures: <MASK> if obj . use_scope : signature = signature [ 0 ] + "" variable_scope_name,  "" + signature [ 1 : ] elif obj . use_scope is None : signature = signature [ 0 ] + "" [variable_scope_name,]  "" + signature [ 1 : ] # signature: arg list return signature , return_annotation","if hasattr ( obj , ""use_scope"" ) :","if name == ""Mock"" :",97.19619812,FALSE,93.53
2741,"def countbox ( self ) : self . box = [ 1000 , 1000 , - 1000 , - 1000 ] for x , y in self . body : if x < self . box [ 0 ] : self . box [ 0 ] = x if x > self . box [ 2 ] : self . box [ 2 ] = x if y < self . box [ 1 ] : self . box [ 1 ] = y <MASK> self . box [ 3 ] = y",if y > self . box [ 3 ] :,if y > self . box [ 3 ] :,100,TRUE,100
2742,"def find_shell ( ) : global DEFAULT_SHELL if not DEFAULT_SHELL : for shell in propose_shell ( ) : <MASK> DEFAULT_SHELL = shell break if not DEFAULT_SHELL : DEFAULT_SHELL = "" /bin/sh "" return DEFAULT_SHELL","if os . path . isfile ( shell ) and os . access ( shell , os . X_OK ) :",if shell . is_file ( ) :,62.78069297,FALSE,70.65
2743,"def addAggregators ( sheet , cols , aggrnames ) : "" Add each aggregator in list of *aggrnames* to each of *cols*. "" for aggrname in aggrnames : aggrs = vd . aggregators . get ( aggrname ) aggrs = aggrs if isinstance ( aggrs , list ) else [ aggrs ] for aggr in aggrs : for c in cols : if not hasattr ( c , "" aggregators "" ) : c . aggregators = [ ] <MASK> c . aggregators + = [ aggr ]",if aggr and aggr not in c . aggregators :,"if not isinstance ( c . aggregators [ - 1 ] , ( list , tuple ) )",88.53443014,FALSE,84.5
2744,"def run ( self , paths = [ ] ) : items = [ ] for item in SideBarSelection ( paths ) . getSelectedItems ( ) : items . append ( item . pathAbsoluteFromProjectEncoded ( ) ) if len ( items ) > 0 : sublime . set_clipboard ( "" \n "" . join ( items ) ) <MASK> sublime . status_message ( "" Items copied "" ) else : sublime . status_message ( "" Item copied "" )",if len ( items ) > 1 :,if len ( items ) > 1 :,100,TRUE,100
2745,"def social_user ( backend , uid , user = None , * args , * * kwargs ) : provider = backend . name social = backend . strategy . storage . user . get_social_auth ( provider , uid ) if social : <MASK> msg = "" This account is already in use. "" raise AuthAlreadyAssociated ( backend , msg ) elif not user : user = social . user return { "" social "" : social , "" user "" : user , "" is_new "" : user is None , "" new_association "" : social is None , }",if user and social . user != user :,"if provider in ( ""account"" , ""active"" ) :",89.03873223,FALSE,89.78
2746,"def _text ( bitlist ) : out = "" "" for typ , text in bitlist : if not typ : out + = text elif typ == "" em "" : out + = "" \\ fI %s \\ fR "" % text <MASK> out + = "" \\ fB %s \\ fR "" % text else : raise ValueError ( "" unexpected tag  %r  inside text "" % ( typ , ) ) out = out . strip ( ) out = re . sub ( re . compile ( r "" ^ \ s+ "" , re . M ) , "" "" , out ) return out","elif typ in [ ""strong"" , ""code"" ] :","elif typ == ""b"" :",77.99861978,FALSE,91.58
2747,"def OnRadioSelect ( self , event ) : fitID = self . mainFrame . getActiveFit ( ) if fitID is not None : self . mainFrame . command . Submit ( cmd . GuiChangeImplantLocationCommand ( fitID = fitID , source = ImplantLocation . FIT <MASK> else ImplantLocation . CHARACTER , ) )",if self . rbFit . GetValue ( ),if fitID == 0,57.9380874,FALSE,85.15
2748,"def hexdump ( data ) : """"""yield lines with hexdump of data"""""" values = [ ] ascii = [ ] offset = 0 for h , a in sixteen ( data ) : <MASK> yield ( offset , ""   "" . join ( [ "" "" . join ( values ) , "" "" . join ( ascii ) ] ) ) del values [ : ] del ascii [ : ] offset + = 0x10 else : values . append ( h ) ascii . append ( a )",if h is None :,if h == 0x20 :,95.16370019,FALSE,95.46
2749,"def submit ( self ) : bot_token = self . config [ "" bot_token "" ] chat_ids = self . config [ "" chat_id "" ] chat_ids = [ chat_ids ] if isinstance ( chat_ids , str ) else chat_ids text = "" \n "" . join ( super ( ) . submit ( ) ) if not text : logger . debug ( "" Not calling telegram API (no changes) "" ) return result = None for chunk in chunkstring ( text , self . MAX_LENGTH , numbering = True ) : for chat_id in chat_ids : res = self . submitToTelegram ( bot_token , chat_id , chunk ) <MASK> result = res return result",if res . status_code != requests . codes . ok or res is None :,if res :,91.04243053,FALSE,89.36
2750,"def onMessage ( self , payload , isBinary ) : if not isBinary : self . result = "" Expected binary message with payload, but got binary. "" else : <MASK> self . result = ( "" Expected binary message with payload of length  %d , but got  %d . "" % ( self . DATALEN , len ( payload ) ) ) else : ## FIXME : check actual content ## self . behavior = Case . OK self . result = "" Received binary message of length  %d . "" % len ( payload ) self . p . createWirelog = True self . p . sendClose ( self . p . CLOSE_STATUS_CODE_NORMAL )",if len ( payload ) != self . DATALEN :,if len ( payload ) != self . DATALEN :,100,TRUE,100
2751,"def verify_output ( actual , expected ) : actual = _read_file ( actual , "" Actual "" ) expected = _read_file ( join ( CURDIR , expected ) , "" Expected "" ) if len ( expected ) != len ( actual ) : raise AssertionError ( "" Lengths differ. Expected  %d  lines but got  %d "" % ( len ( expected ) , len ( actual ) ) ) for exp , act in zip ( expected , actual ) : tester = fnmatchcase if "" * "" in exp else eq <MASK> raise AssertionError ( "" Lines differ. \n Expected:  %s \n Actual:    %s "" % ( exp , act ) )","if not tester ( act . rstrip ( ) , exp . rstrip ( ) ) :",if tester != act :,85.69108532,FALSE,88.94
2752,"def _in_out_vector_helper ( self , name1 , name2 , ceil ) : vector = [ ] stats = self . record if ceil is None : ceil = self . _get_max_rate ( name1 , name2 ) maxlen = self . config . get_stats_history_length ( ) for n in [ name1 , name2 ] : for i in range ( maxlen + 1 ) : <MASK> vector . append ( float ( stats [ i ] [ n ] ) / ceil ) else : vector . append ( 0.0 ) return vector",if i < len ( stats ) :,if stats [ i ] [ n ] is not None :,92.85700943,FALSE,90.77
2753,"def _init_param ( param , mode ) : if isinstance ( param , str ) : param = _resolve ( param ) elif isinstance ( param , ( list , tuple ) ) : param = [ _init_param ( p , mode ) for p in param ] elif isinstance ( param , dict ) : <MASK> param = from_params ( param , mode = mode ) else : param = { k : _init_param ( v , mode ) for k , v in param . items ( ) } return param","if { ""ref"" , ""class_name"" , ""config_path"" } . intersection ( param . keys ( ) ) :","if isinstance ( param , ( list , tuple ) ) :",85.16556321,FALSE,80.97
2754,"def link_pantsrefs ( soups , precomputed ) : """"""Transorm soups: <a pantsref=""foo""> becomes <a href=""../foo_page.html#foo"">"""""" for ( page , soup ) in soups . items ( ) : for a in soup . find_all ( "" a "" ) : if not a . has_attr ( "" pantsref "" ) : continue pantsref = a [ "" pantsref "" ] <MASK> raise TaskError ( f ' Page  { page }  has pantsref  "" { pantsref } ""  and I cannot find pantsmark for it ' ) a [ "" href "" ] = rel_href ( page , precomputed . pantsref [ pantsref ] )",if pantsref not in precomputed . pantsref :,if pantsref not in precomputed . pantsref :,100,TRUE,100
2755,"def _gridconvvalue ( self , value ) : if isinstance ( value , ( str , _tkinter . Tcl_Obj ) ) : try : svalue = str ( value ) if not svalue : return None <MASK> return getdouble ( svalue ) else : return getint ( svalue ) except ValueError : pass return value","elif ""."" in svalue :",elif svalue . isdigit ( ) :,65.95449343,FALSE,90.45
2756,"def default ( self , o ) : try : <MASK> return str ( o ) else : # remove unwanted attributes from the provider object during conversion to json if hasattr ( o , "" profile "" ) : del o . profile if hasattr ( o , "" credentials "" ) : del o . credentials if hasattr ( o , "" metadata_path "" ) : del o . metadata_path if hasattr ( o , "" services_config "" ) : del o . services_config return vars ( o ) except Exception as e : return str ( o )",if type ( o ) == datetime . datetime :,"if isinstance ( o , dict ) :",62.83549712,FALSE,91.87
2757,"def transform_kwarg ( self , name , value , split_single_char_options ) : if len ( name ) == 1 : <MASK> return [ "" - %s "" % name ] elif value not in ( False , None ) : if split_single_char_options : return [ "" - %s "" % name , "" %s "" % value ] else : return [ "" - %s %s "" % ( name , value ) ] else : <MASK> return [ "" -- %s "" % dashify ( name ) ] elif value is not False and value is not None : return [ "" -- %s = %s "" % ( dashify ( name ) , value ) ] return [ ]",if value is True :,"if value in ( True , None ) :",67.36058533,FALSE,90.82
2758,"def handle ( self , context , sign , * args ) : if context . rounding in ( ROUND_HALF_UP , ROUND_HALF_EVEN , ROUND_HALF_DOWN , ROUND_UP ) : return Infsign [ sign ] if sign == 0 : <MASK> return Infsign [ sign ] return Decimal ( ( sign , ( 9 , ) * context . prec , context . Emax - context . prec + 1 ) ) if sign == 1 : if context . rounding == ROUND_FLOOR : return Infsign [ sign ] return Decimal ( ( sign , ( 9 , ) * context . prec , context . Emax - context . prec + 1 ) )",if context . rounding == ROUND_CEILING :,if context . rounding == ROUND_DOWN :,98.94401678,FALSE,98.2
2759,"def OnLeftUp ( self , event ) : # Stop Drawing if self . Drawing : self . Drawing = False <MASK> world_rect = ( self . Canvas . PixelToWorld ( self . RBRect [ 0 ] ) , self . Canvas . ScalePixelToWorld ( self . RBRect [ 1 ] ) , ) wx . CallAfter ( self . CallBack , world_rect ) self . RBRect = None",if self . RBRect :,if self . RBRect :,75,TRUE,100
2760,"def _map_answers ( answers ) : result = [ ] for a in answers . split ( "" | "" ) : user_answers = [ ] result . append ( dict ( sourcerAnswers = user_answers ) ) for r in a . split ( "" , "" ) : <MASK> user_answers . append ( dict ( noAnswer = True ) ) else : start_ , end_ = map ( int , r . split ( "" : "" ) ) user_answers . append ( dict ( s = start_ , e = end_ ) ) return result","if r == ""None"" :","if "":"" not in r :",81.35063718,FALSE,94.43
2761,"def parse_edges ( self , pcb ) : edges = [ ] drawings = list ( pcb . GetDrawings ( ) ) bbox = None for m in pcb . GetModules ( ) : for g in m . GraphicalItems ( ) : drawings . append ( g ) for d in drawings : <MASK> parsed_drawing = self . parse_drawing ( d ) if parsed_drawing : edges . append ( parsed_drawing ) if bbox is None : bbox = d . GetBoundingBox ( ) else : bbox . Merge ( d . GetBoundingBox ( ) ) if bbox : bbox . Normalize ( ) return edges , bbox",if d . GetLayer ( ) == pcbnew . Edge_Cuts :,"if d . GetType ( ) == ""Shape"" :",96.00806609,FALSE,93.02
2762,"def get_size ( self ) : size = self . start_size for operation in self . ran_operations : <MASK> size = operation [ 1 ] [ 0 ] elif operation [ 0 ] == "" crop "" : crop = operation [ 1 ] [ 0 ] size = crop [ 2 ] - crop [ 0 ] , crop [ 3 ] - crop [ 1 ] return size","if operation [ 0 ] == ""resize"" :","if operation [ 0 ] == ""size"" :",98.34584377,FALSE,97.05
2763,"def migrate_account_metadata ( account_id ) : from inbox . models . session import session_scope from inbox . models import Account with session_scope ( versioned = False ) as db_session : account = db_session . query ( Account ) . get ( account_id ) <MASK> create_categories_for_easfoldersyncstatuses ( account , db_session ) else : create_categories_for_folders ( account , db_session ) if account . discriminator == "" gmailaccount "" : set_labels_for_imapuids ( account , db_session ) db_session . commit ( )","if account . discriminator == ""easaccount"" :","if account . discriminator == ""easfoldersyncstatus"" :",98.49540604,FALSE,98.04
2764,"def OnEndDrag ( self , event ) : self . StopDragging ( ) dropTarget = event . GetItem ( ) if not dropTarget : dropTarget = self . GetRootItem ( ) if self . IsValidDropTarget ( dropTarget ) : self . UnselectAll ( ) <MASK> self . SelectItem ( dropTarget ) self . OnDrop ( dropTarget , self . _dragItem )",if dropTarget != self . GetRootItem ( ) :,if self . IsValidDropTarget ( dropTarget ) :,92.77112909,FALSE,89.44
2765,"def validate ( self , frame , value ) : if self . sep and isinstance ( value , string_types ) : value = value . split ( self . sep ) if isinstance ( value , list ) : <MASK> return [ self . specs [ 0 ] . validate ( frame , v ) for v in value ] else : return [ [ s . validate ( frame , v ) for ( v , s ) in izip ( val , self . specs ) ] for val in value ] raise ValueError ( "" Invalid MultiSpec data:  %r "" % value )",if len ( self . specs ) == 1 :,if len ( self . specs ) == 1 :,100,TRUE,100
2766,"def __init__ ( self , action_space = None , network = None , network_kwargs = None , hparams = None ) : QNetBase . __init__ ( self , hparams = hparams ) with tf . variable_scope ( self . variable_scope ) : <MASK> action_space = Space ( low = 0 , high = self . _hparams . action_space , dtype = np . int32 ) self . _action_space = action_space self . _append_output_layer ( )",if action_space is None :,if action_space is None :,100,TRUE,100
2767,"def n_weights ( self ) : """"""Return the number of weights (parameters) in this network."""""" n_weights = 0 for i , w in enumerate ( self . all_weights ) : n = 1 # for s in p.eval().shape: for s in w . get_shape ( ) : try : s = int ( s ) except : s = 1 <MASK> n = n * s n_weights = n_weights + n # print(""num of weights (parameters) %d"" % n_weights) return n_weights",if s :,if s > 1 :,98.47747779,FALSE,97.15
2768,"def _arg_desc ( name , ctx ) : for param in ctx . command . params : if param . name == name : desc = param . opts [ - 1 ] <MASK> desc = param . human_readable_name return desc raise AssertionError ( name )","if desc [ 0 ] != ""-"" :",if desc is None :,66.06271809,FALSE,84.53
2769,"def walk ( directory , path_so_far ) : for name in sorted ( os . listdir ( directory ) ) : if any ( fnmatch ( name , pattern ) for pattern in basename_ignore ) : continue path = path_so_far + "" / "" + name if path_so_far else name if any ( fnmatch ( path , pattern ) for pattern in path_ignore ) : continue full_name = os . path . join ( directory , name ) if os . path . isdir ( full_name ) : for file_path in walk ( full_name , path ) : yield file_path <MASK> yield path",elif os . path . isfile ( full_name ) :,elif os . path . isfile ( path ) :,98.20734013,FALSE,96.75
2770,"def cache_dst ( self ) : final_dst = None final_linenb = None for linenb , assignblk in enumerate ( self ) : for dst , src in viewitems ( assignblk ) : if dst . is_id ( "" IRDst "" ) : <MASK> raise ValueError ( "" Multiple destinations! "" ) final_dst = src final_linenb = linenb self . _dst = final_dst self . _dst_linenb = final_linenb return final_dst",if final_dst is not None :,if len ( src ) > 1 :,90.08273158,FALSE,92.29
2771,"def run ( self , args , * * kwargs ) : if args . resource_ref or args . policy_type : filters = { } <MASK> filters [ "" resource_ref "" ] = args . resource_ref if args . policy_type : filters [ "" policy_type "" ] = args . policy_type filters . update ( * * kwargs ) return self . manager . query ( * * filters ) else : return self . manager . get_all ( * * kwargs )",if args . resource_ref :,if args . resource_ref :,100,TRUE,100
2772,"def __init__ ( self , folders ) : self . folders = folders self . duplicates = { } for folder , path in folders . items ( ) : duplicates = [ ] for other_folder , other_path in folders . items ( ) : if other_folder == folder : continue if other_path == path : duplicates . append ( other_folder ) <MASK> self . duplicates [ folder ] = duplicates",if len ( duplicates ) :,if duplicates :,94.30395969,FALSE,94.16
2773,"def limit_clause ( self , select , * * kw ) : text = "" "" if select . _limit_clause is not None : text + = "" \n  LIMIT  "" + self . process ( select . _limit_clause , * * kw ) if select . _offset_clause is not None : <MASK> text + = "" \n  LIMIT  "" + self . process ( sql . literal ( - 1 ) ) text + = ""  OFFSET  "" + self . process ( select . _offset_clause , * * kw ) else : text + = ""  OFFSET  "" + self . process ( sql . literal ( 0 ) , * * kw ) return text",if select . _limit_clause is None :,if select . _limit_clause == - 1 :,96.91155204,FALSE,96.16
2774,"def _get_activation ( self , act ) : """"""Get activation block based on the name."""""" if isinstance ( act , str ) : if act . lower ( ) == "" gelu "" : return GELU ( ) <MASK> return GELU ( approximate = True ) else : return gluon . nn . Activation ( act ) assert isinstance ( act , gluon . Block ) return act","elif act . lower ( ) == ""approx_gelu"" :","elif act . lower ( ) == ""gelu"" :",98.33373676,FALSE,96.12
2775,"def __eq__ ( self , other ) : try : if self . type != other . type : return False if self . type == "" ASK "" : return self . askAnswer == other . askAnswer <MASK> return self . vars == other . vars and self . bindings == other . bindings else : return self . graph == other . graph except : return False","elif self . type == ""SELECT"" :","elif self . type == ""VARIABLE"" :",73.31710501,FALSE,97.08
2776,"def _get_text_nodes ( nodes , html_body ) : text = [ ] open_tags = 0 for node in nodes : if isinstance ( node , HtmlTag ) : <MASK> open_tags + = 1 elif node . tag_type == CLOSE_TAG : open_tags - = 1 elif ( isinstance ( node , HtmlDataFragment ) and node . is_text_content and open_tags == 0 ) : text . append ( html_body [ node . start : node . end ] ) return text",if node . tag_type == OPEN_TAG :,if node . tag_type == OPEN_TAG :,100,TRUE,100
2777,"def test_do_change ( self ) : """"""Test if VTK object changes when trait is changed."""""" p = Prop ( ) p . edge_visibility = not p . edge_visibility p . representation = "" p "" p . opacity = 0.5 p . color = ( 0 , 1 , 0 ) p . diffuse_color = ( 1 , 1 , 1 ) p . specular_color = ( 1 , 1 , 0 ) for t , g in p . _updateable_traits_ : val = getattr ( p . _vtk_obj , g ) ( ) <MASK> self . assertEqual ( val , getattr ( p , t + "" _ "" ) ) else : self . assertEqual ( val , getattr ( p , t ) )","if t == ""representation"" :","if t in ( ""p"" , ""p_edge"" ) :",76.53194197,FALSE,92.13
2778,"def update_item ( source_doc , target_doc , source_parent ) : target_doc . t_warehouse = "" "" if source_doc . material_request_item and source_doc . material_request : add_to_transit = frappe . db . get_value ( "" Stock Entry "" , source_name , "" add_to_transit "" ) <MASK> warehouse = frappe . get_value ( "" Material Request Item "" , source_doc . material_request_item , "" warehouse "" ) target_doc . t_warehouse = warehouse target_doc . s_warehouse = source_doc . t_warehouse target_doc . qty = source_doc . qty - source_doc . transferred_qty",if add_to_transit :,if add_to_transit :,100,TRUE,100
2779,"def get_drive ( self , root_path = "" "" , volume_guid_path = "" "" ) : for drive in self . drives : <MASK> config_root_path = drive . get ( "" root_path "" ) if config_root_path and root_path == config_root_path : return drive elif volume_guid_path : config_volume_guid_path = drive . get ( "" volume_guid_path "" ) if config_volume_guid_path and config_volume_guid_path == volume_guid_path : return drive",if root_path :,if root_path :,100,TRUE,100
2780,"def f_freeze ( _ ) : repos = utils . get_repos ( ) for name , path in repos . items ( ) : url = "" "" cp = subprocess . run ( [ "" git "" , "" remote "" , "" -v "" ] , cwd = path , capture_output = True ) <MASK> url = cp . stdout . decode ( "" utf-8 "" ) . split ( "" \n "" ) [ 0 ] . split ( ) [ 1 ] print ( f "" { url } , { name } , { path } "" )",if cp . returncode == 0 :,if cp . returncode :,86.29987764,FALSE,96.09
2781,"def conj ( self ) : dtype = self . dtype if issubclass ( self . dtype . type , np . complexfloating ) : <MASK> raise RuntimeError ( "" only contiguous arrays may  "" "" be used as arguments to this operation "" ) if self . flags . f_contiguous : order = "" F "" else : order = "" C "" result = self . _new_like_me ( order = order ) func = elementwise . get_conj_kernel ( dtype ) func . prepared_async_call ( self . _grid , self . _block , None , self . gpudata , result . gpudata , self . mem_size ) return result else : return self",if not self . flags . forc :,if self . flags . f_contiguous :,93.14821705,FALSE,95.67
2782,"def detect_reentrancy ( self , contract ) : for function in contract . functions_and_modifiers_declared : if function . is_implemented : <MASK> continue self . _explore ( function . entry_point , [ ] ) function . context [ self . KEY ] = True",if self . KEY in function . context :,"if self . _explore ( function . entry_point , [ ] ) :",88.62104049,FALSE,81.55
2783,"def test_default_configuration_no_encoding ( self ) : transformations = [ ] for i in range ( 2 ) : transformation , original = _test_preprocessing ( NoEncoding ) self . assertEqual ( transformation . shape , original . shape ) self . assertTrue ( ( transformation == original ) . all ( ) ) transformations . append ( transformation ) <MASK> self . assertTrue ( ( transformations [ - 1 ] == transformations [ - 2 ] ) . all ( ) )",if len ( transformations ) > 1 :,if len ( transformations ) > 1 :,100,TRUE,100
2784,"def main ( ) : """"""main function"""""" # todo: lookuo real description parser = argparse . ArgumentParser ( description = "" Let a cow speak for you "" ) parser . add_argument ( "" text "" , nargs = "" * "" , default = None , help = "" text to say "" ) ns = parser . parse_args ( ) if ( ns . text is None ) or ( len ( ns . text ) == 0 ) : text = "" "" while True : inp = sys . stdin . read ( 4096 ) if inp . endswith ( "" \n "" ) : inp = inp [ : - 1 ] <MASK> break text + = inp else : text = ""   "" . join ( ns . text ) cow = get_cow ( text ) print ( cow )",if not inp :,"if inp == """" :",73.61773727,FALSE,96.21
2785,"def prehook ( self , emu , op , eip ) : if op in self . badops : emu . stopEmu ( ) raise v_exc . BadOpBytes ( op . va ) if op . mnem in STOS : if self . arch == "" i386 "" : reg = emu . getRegister ( envi . archs . i386 . REG_EDI ) elif self . arch == "" amd64 "" : reg = emu . getRegister ( envi . archs . amd64 . REG_RDI ) <MASK> self . vw . makePointer ( reg , follow = True )",if self . vw . isValidPointer ( reg ) and self . vw . getLocation ( reg ) is None :,if reg :,81.52822223,FALSE,82.98
2786,"def get_boarding_status ( project ) : status = "" Pending "" if project : doc = frappe . get_doc ( "" Project "" , project ) if flt ( doc . percent_complete ) > 0.0 and flt ( doc . percent_complete ) < 100.0 : status = "" In Process "" <MASK> status = "" Completed "" return status",elif flt ( doc . percent_complete ) == 100.0 :,elif doc . percent_complete == 0.0 and doc . percent_complete == 0,69.44296464,FALSE,83
2787,"def set_weights ( self , new_weights ) : weights = self . get_weights ( ) if len ( weights ) != len ( new_weights ) : raise ValueError ( "" len of lists mismatch "" ) tuples = [ ] for w , new_w in zip ( weights , new_weights ) : <MASK> new_w = new_w . reshape ( w . shape ) tuples . append ( ( w , new_w ) ) nn . batch_set_value ( tuples )",if len ( w . shape ) != new_w . shape :,"if isinstance ( new_w , nn . Conv2d ) :",87.93279625,FALSE,89.22
2788,"def reload_json_api_settings ( * args , * * kwargs ) : django_setting = kwargs [ "" setting "" ] setting = django_setting . replace ( JSON_API_SETTINGS_PREFIX , "" "" ) value = kwargs [ "" value "" ] if setting in DEFAULTS . keys ( ) : <MASK> setattr ( json_api_settings , setting , value ) elif hasattr ( json_api_settings , setting ) : delattr ( json_api_settings , setting )",if value is not None :,if value :,93.57961783,FALSE,95.55
2789,"def knamn ( self , sup , cdict ) : cname = cdict [ sup ] . class_name if not cname : ( namesp , tag ) = cdict [ sup ] . name . split ( "" . "" ) <MASK> ctag = self . root . modul [ namesp ] . factory ( tag ) . __class__ . __name__ cname = "" %s . %s "" % ( namesp , ctag ) else : cname = tag + "" _ "" return cname",if namesp :,if namesp in self . root . modul :,81.41756227,FALSE,92.73
2790,"def setdefault ( self , key , default = None ) : try : o = self . data [ key ] ( ) except KeyError : o = None if o is None : <MASK> self . _commit_removals ( ) self . data [ key ] = KeyedRef ( default , self . _remove , key ) return default else : return o",if self . _pending_removals :,if self . _remove :,98.13475461,FALSE,93.86
2791,"def __on_item_activated ( self , event ) : if self . __module_view : module = self . get_event_module ( event ) self . __module_view . set_selection ( module . module_num ) <MASK> self . input_list_ctrl . deactivate_active_item ( ) else : self . list_ctrl . deactivate_active_item ( ) for index in range ( self . list_ctrl . GetItemCount ( ) ) : if self . list_ctrl . IsSelected ( index ) : self . list_ctrl . Select ( index , False ) self . __controller . enable_module_controls_panel_buttons ( )",if event . EventObject is self . list_ctrl :,if self . __input_list_ctrl . GetItemCount ( ) == 0 :,86.27125369,FALSE,90.37
2792,"def _create_valid_graph ( graph ) : nodes = graph . nodes ( ) for i in range ( len ( nodes ) ) : for j in range ( len ( nodes ) ) : if i == j : continue edge = ( nodes [ i ] , nodes [ j ] ) <MASK> graph . del_edge ( edge ) graph . add_edge ( edge , 1 )",if graph . has_edge ( edge ) :,if edge in graph :,93.76128018,FALSE,89.48
2793,"def _parse_param_value ( name , datatype , default ) : if datatype == "" bool "" : <MASK> return True elif default . lower ( ) == "" false "" : return False else : _s = "" {} : Invalid default value  ' {} '  for bool parameter  {} "" raise SyntaxError ( _s . format ( self . name , default , p ) ) elif datatype == "" int "" : if type ( default ) == int : return default else : return int ( default , 0 ) elif datatype == "" real "" : if type ( default ) == float : return default else : return float ( default ) else : return str ( default )","if default . lower ( ) == ""true"" :","if default . lower ( ) == ""true"" :",100,TRUE,100
2794,"def get_size ( self , shape_info ) : # The size is the data, that have constant size. state = np . random . RandomState ( ) . get_state ( ) size = 0 for elem in state : <MASK> size + = len ( elem ) elif isinstance ( elem , np . ndarray ) : size + = elem . size * elem . itemsize elif isinstance ( elem , int ) : size + = np . dtype ( "" int "" ) . itemsize elif isinstance ( elem , float ) : size + = np . dtype ( "" float "" ) . itemsize else : raise NotImplementedError ( ) return size","if isinstance ( elem , str ) :","if isinstance ( elem , ( list , tuple ) ) :",73.21181085,FALSE,95.16
2795,"def _merge_substs ( self , subst , new_substs ) : subst = subst . copy ( ) for new_subst in new_substs : for name , var in new_subst . items ( ) : if name not in subst : subst [ name ] = var <MASK> subst [ name ] . PasteVariable ( var ) return subst",elif subst [ name ] is not var :,"if isinstance ( subst [ name ] , Subst ) :",83.48673996,FALSE,88.14
2796,"def _load_weights_if_possible ( self , model , init_weight_path = None ) : """"""Loads model weights when it is provided."""""" if init_weight_path : logging . info ( "" Load weights:  {} "" . format ( init_weight_path ) ) <MASK> checkpoint = tf . train . Checkpoint ( model = model , optimizer = self . _create_optimizer ( ) ) checkpoint . restore ( init_weight_path ) else : model . load_weights ( init_weight_path ) else : logging . info ( "" Weights not loaded from path: {} "" . format ( init_weight_path ) )",if self . use_tpu :,if self . _save_weights :,98.68371758,FALSE,96.35
2797,"def _cleanup_inactive_receivexlogs ( self , site ) : if site in self . receivexlogs : if not self . receivexlogs [ site ] . running : <MASK> self . receivexlogs [ site ] . join ( ) del self . receivexlogs [ site ]",if self . receivexlogs [ site ] . is_alive ( ) :,if self . receivexlogs [ site ] . is_active :,92.87588603,FALSE,92.53
2798,"def get_asset ( self , path ) : """"""Loads an asset by path."""""" clean_path = cleanup_path ( path ) . strip ( "" / "" ) nodes = [ self . asset_root ] + self . theme_asset_roots for node in nodes : for piece in clean_path . split ( "" / "" ) : node = node . get_child ( piece ) <MASK> break if node is not None : return node return None",if node is None :,if node is None :,100,TRUE,100
2799,"def palindromic_substrings ( s ) : if not s : return [ [ ] ] results = [ ] for i in range ( len ( s ) , 0 , - 1 ) : sub = s [ : i ] <MASK> for rest in palindromic_substrings ( s [ i : ] ) : results . append ( [ sub ] + rest ) return results",if sub == sub [ : : - 1 ] :,"if sub . startswith ( ""sub"" ) :",88.68135483,FALSE,87.23
2800,"def debug_tree ( tree ) : l = [ ] for elt in tree : <MASK> l . append ( _names . get ( elt , elt ) ) elif isinstance ( elt , str ) : l . append ( elt ) else : l . append ( debug_tree ( elt ) ) return l","if isinstance ( elt , ( int , long ) ) :","if isinstance ( elt , ( int , float ) ) :",98.09508053,FALSE,96.41
2801,"def shared_username ( account ) : username = os . environ . get ( "" SHARED_USERNAME "" , "" PKKid "" ) for user in account . users ( ) : <MASK> return username elif ( user . username and user . email and user . id and username . lower ( ) in ( user . username . lower ( ) , user . email . lower ( ) , str ( user . id ) ) ) : return username pytest . skip ( "" Shared user  %s  wasn`t found in your MyPlex account "" % username )",if user . title . lower ( ) == username . lower ( ) :,if user . username and username . lower ( ) in ( user . email . lower ( ),92.24941115,FALSE,90.62
2802,"def process_schema_element ( self , e ) : if e . name is None : return self . debug1 ( "" adding element:  %s "" , e . name ) t = self . get_type ( e . type ) if t : <MASK> del self . pending_elements [ e . name ] self . retval [ self . tns ] . elements [ e . name ] = e else : self . pending_elements [ e . name ] = e",if e . name in self . pending_elements :,if e . name in self . retval :,88.59598188,FALSE,95.6
2803,"def __setitem__ ( self , key , value ) : with self . _lock : try : link = self . _get_link_and_move_to_front_of_ll ( key ) except KeyError : <MASK> self . _set_key_and_add_to_front_of_ll ( key , value ) else : evicted = self . _set_key_and_evict_last_in_ll ( key , value ) super ( LRI , self ) . __delitem__ ( evicted ) super ( LRI , self ) . __setitem__ ( key , value ) else : link [ VALUE ] = value",if len ( self ) < self . max_size :,if self . _is_linked :,92.32989013,FALSE,92.8
2804,"def __delattr__ ( self , name ) : if name == "" __dict__ "" : raise AttributeError ( "" %r  object attribute  ' __dict__ '  is read-only "" % self . __class__ . __name__ ) if name in self . _local_type_vars : <MASK> # A data descriptor, like a property or a slot. type_attr = getattr ( self . _local_type , name , _marker ) type ( type_attr ) . __delete__ ( type_attr , self ) return # Otherwise it goes directly in the dict # Begin inlined function _get_dict() dct = _local_get_dict ( self ) try : del dct [ name ] except KeyError : raise AttributeError ( name )",if name in self . _local_type_del_descriptors :,"if hasattr ( self . _local_type , name ) :",94.3938459,FALSE,95
2805,"def update_participants ( self , refresh = True ) : for participant in list ( self . participants_dict ) : <MASK> continue self . removeItem ( self . participants_dict [ participant ] ) self . participant_items . remove ( self . participants_dict [ participant ] ) del self . participants_dict [ participant ] for participant in self . simulator_config . participants : if participant in self . participants_dict : self . participants_dict [ participant ] . refresh ( ) else : self . insert_participant ( participant ) if refresh : self . update_view ( )",if participant is None or participant == self . simulator_config . broadcast_part :,if participant not in self . participant_items :,92.00220662,FALSE,88.34
2806,"def insert_bigger_b_add ( node ) : if node . op == theano . tensor . add : inputs = list ( node . inputs ) <MASK> inputs [ - 1 ] = theano . tensor . concatenate ( ( inputs [ - 1 ] , inputs [ - 1 ] ) ) return [ node . op ( * inputs ) ] return False",if inputs [ - 1 ] . owner is None :,if len ( inputs ) > 1 :,73.18942852,FALSE,87.35
2807,"def _activate_cancel_status ( self , cancel_status ) : if self . _cancel_status is not None : self . _cancel_status . _tasks . remove ( self ) self . _cancel_status = cancel_status if self . _cancel_status is not None : self . _cancel_status . _tasks . add ( self ) <MASK> self . _attempt_delivery_of_any_pending_cancel ( )",if self . _cancel_status . effectively_cancelled :,if self . _cancel_status . _tasks :,97.91012319,FALSE,95.79
2808,"def writeLibraryGeometry ( fp , meshes , config , shapes = None ) : progress = Progress ( len ( meshes ) , None ) fp . write ( "" \n   <library_geometries> \n "" ) for mIdx , mesh in enumerate ( meshes ) : <MASK> shape = None else : shape = shapes [ mIdx ] writeGeometry ( fp , mesh , config , shape ) progress . step ( ) fp . write ( ""   </library_geometries> \n "" )",if shapes is None :,if mIdx == 0 :,91.52932655,FALSE,94.28
2809,"def init_module_config ( module_json , config , config_path = default_config_path ) : if "" config "" in module_json [ "" meta "" ] : if module_json [ "" meta "" ] [ "" config "" ] : if module_json [ "" name "" ] not in config : config . add_section ( module_json [ "" name "" ] ) for config_var in module_json [ "" meta "" ] [ "" config "" ] : <MASK> config . set ( module_json [ "" name "" ] , config_var , "" "" ) return config","if config_var not in config [ module_json [ ""name"" ] ] :",if config_var not in config :,93.03807469,FALSE,91.5
2810,"def get_const_defines ( flags , prefix = "" "" ) : defs = [ ] for k , v in globals ( ) . items ( ) : if isinstance ( v , int ) : if v & flags : <MASK> if k . startswith ( prefix ) : defs . append ( k ) else : defs . append ( k ) return defs",if prefix :,if prefix :,100,TRUE,100
2811,"def __init__ ( self , source , encoding = DEFAULT_ENCODING ) : self . data = { } with open ( source , encoding = encoding ) as file_ : for line in file_ : line = line . strip ( ) <MASK> continue k , v = line . split ( "" = "" , 1 ) k = k . strip ( ) v = v . strip ( ) if len ( v ) > = 2 and ( ( v [ 0 ] == "" ' "" and v [ - 1 ] == "" ' "" ) or ( v [ 0 ] == ' "" ' and v [ - 1 ] == ' "" ' ) ) : v = v . strip ( "" ' \"" "" ) self . data [ k ] = v","if not line or line . startswith ( ""#"" ) or ""="" not in line :",if not line :,72.01515711,FALSE,90.05
2812,"def __detect_console_logger ( self ) : logger = self . log while logger : for handler in logger . handlers [ : ] : <MASK> if handler . stream in ( sys . stdout , sys . stderr ) : self . logger_handlers . append ( handler ) if logger . root == logger : break else : logger = logger . root","if isinstance ( handler , StreamHandler ) :","if isinstance ( handler , logging . FileHandler ) :",94.24151167,FALSE,94.31
2813,"def check_heuristic_in_sql ( ) : heurs = set ( ) excluded = [ "" Equal assembly or pseudo-code "" , "" All or most attributes "" ] for heur in HEURISTICS : name = heur [ "" name "" ] <MASK> continue sql = heur [ "" sql "" ] if sql . lower ( ) . find ( name . lower ( ) ) == - 1 : print ( ( "" SQL command not correctly associated to  %s "" % repr ( name ) ) ) print ( sql ) assert sql . find ( name ) != - 1 heurs . add ( name ) print ( "" Heuristics: "" ) import pprint pprint . pprint ( heurs )",if name in excluded :,if name in excluded :,100,TRUE,100
2814,"def read ( self , size = - 1 ) : buf = bytearray ( ) while size != 0 and self . cursor < self . maxpos : <MASK> self . seek_to_block ( self . cursor ) part = self . current_stream . read ( size ) if size > 0 : if len ( part ) == 0 : raise EOFError ( ) size - = len ( part ) self . cursor + = len ( part ) buf + = part return bytes ( buf )",if not self . in_current_block ( self . cursor ) :,if self . current_stream is None :,89.80521413,FALSE,88.1
2815,"def get_project_dir ( env ) : project_file = workon_home / env / "" .project "" if project_file . exists ( ) : with project_file . open ( ) as f : project_dir = f . readline ( ) . strip ( ) <MASK> return project_dir else : err ( "" Corrupted or outdated: "" , project_file , "" \n Directory "" , project_dir , "" doesn ' t exist. "" , )",if os . path . exists ( project_dir ) :,if os . path . exists ( project_dir ) :,100,TRUE,100
2816,"def _cache_mem ( curr_out , prev_mem , mem_len , reuse_len = None ) : """"""cache hidden states into memory."""""" if mem_len is None or mem_len == 0 : return None else : <MASK> curr_out = curr_out [ : reuse_len ] if prev_mem is None : new_mem = curr_out [ - mem_len : ] else : new_mem = tf . concat ( [ prev_mem , curr_out ] , 0 ) [ - mem_len : ] return tf . keras . backend . stop_gradient ( new_mem )",if reuse_len is not None and reuse_len > 0 :,if reuse_len is not None :,93.68893564,FALSE,94.79
2817,"def cleanup_channel ( self , to_cleanup ) : public_key , id_ = to_cleanup # TODO: Maybe run it threaded? try : with db_session : channel = self . session . mds . ChannelMetadata . get_for_update ( public_key = public_key , id_ = id_ ) <MASK> return channel . local_version = 0 channel . contents . delete ( bulk = True ) except Exception as e : self . _logger . warning ( "" Exception while cleaning unsubscribed channel:  % "" , str ( e ) )",if not channel :,if not channel . contents :,98.58122244,FALSE,96.98
2818,"def best_image ( width , height ) : # A heuristic for finding closest sized image to required size. image = images [ 0 ] for img in images : if img . width == width and img . height == height : # Exact match always used return img <MASK> # At least wide enough, and largest area image = img return image",elif img . width >= width and img . width * img . height > image . width * image . height :,"if img . size ( ) < ( width * 2 , height * 2 ) :",62.16821175,FALSE,75.12
2819,"def add_peer_to_blob ( self , contact : "" KademliaPeer "" , key : bytes ) - > None : now = self . loop . time ( ) if key in self . _data_store : current = list ( filter ( lambda x : x [ 0 ] == contact , self . _data_store [ key ] ) ) <MASK> self . _data_store [ key ] [ self . _data_store [ key ] . index ( current [ 0 ] ) ] = ( contact , now , ) else : self . _data_store [ key ] . append ( ( contact , now ) ) else : self . _data_store [ key ] = [ ( contact , now ) ]",if len ( current ) > 0 :,if len ( current ) == 1 :,77.58778207,FALSE,97.06
2820,"def dump ( self ) : self . ql . log . info ( "" [*] Dumping object:  %s "" % ( self . sf_name ) ) for field in self . _fields_ : if isinstance ( getattr ( self , field [ 0 ] ) , POINTER64 ) : self . ql . log . info ( "" %s : 0x %x "" % ( field [ 0 ] , getattr ( self , field [ 0 ] ) . value ) ) elif isinstance ( getattr ( self , field [ 0 ] ) , int ) : self . ql . log . info ( "" %s :  %d "" % ( field [ 0 ] , getattr ( self , field [ 0 ] ) ) ) <MASK> self . ql . log . info ( "" %s :  %s "" % ( field [ 0 ] , getattr ( self , field [ 0 ] ) . decode ( ) ) )","elif isinstance ( getattr ( self , field [ 0 ] ) , bytes ) :","elif isinstance ( getattr ( self , field [ 0 ] ) , bytes ) :",100,TRUE,100
2821,"def GeneratePageMetatadata ( self , task ) : address_space = self . session . GetParameter ( "" default_address_space "" ) for vma in task . mm . mmap . walk_list ( "" vm_next "" ) : start = vma . vm_start end = vma . vm_end # Skip the entire region. <MASK> continue # Done. if start > self . plugin_args . end : break for vaddr in utils . xrange ( start , end , 0x1000 ) : if self . plugin_args . start < = vaddr < = self . plugin_args . end : yield vaddr , self . _CreateMetadata ( address_space . describe_vtop ( vaddr ) )",if end < self . plugin_args . start :,if start < self . plugin_args . start :,98.93406054,FALSE,98.27
2822,"def _available_symbols ( self , scoperef , expr ) : cplns = [ ] found_names = set ( ) while scoperef : elem = self . _elem_from_scoperef ( scoperef ) for child in elem : name = child . get ( "" name "" , "" "" ) if name . startswith ( expr ) : if name not in found_names : found_names . add ( name ) ilk = child . get ( "" ilk "" ) or child . tag cplns . append ( ( ilk , name ) ) scoperef = self . parent_scoperef_from_scoperef ( scoperef ) <MASK> break return sorted ( cplns , key = operator . itemgetter ( 1 ) )",if not scoperef :,if scoperef is None :,96.71050899,FALSE,96.92
2823,"def get_xenapi_host ( self ) : """"""Return the xenapi host on which nova-compute runs on."""""" with self . _get_session ( ) as session : <MASK> return session . xenapi . host . get_by_uuid ( self . host_uuid ) else : return session . xenapi . session . get_this_host ( session . handle )",if self . host_uuid :,if self . host_uuid :,100,TRUE,100
2824,"def stream_docker_log ( log_stream ) : async for line in log_stream : <MASK> logger . debug ( line [ "" stream "" ] . strip ( ) ) elif "" status "" in line : logger . debug ( line [ "" status "" ] . strip ( ) ) elif "" error "" in line : logger . error ( line [ "" error "" ] . strip ( ) ) raise DockerBuildError","if ""stream"" in line and line [ ""stream"" ] . strip ( ) :","if ""stream"" in line :",89.19170211,FALSE,86.7
2825,"def test_wildcard_import ( ) : bonobo = __import__ ( "" bonobo "" ) assert bonobo . __version__ for name in dir ( bonobo ) : # ignore attributes starting by underscores <MASK> continue attr = getattr ( bonobo , name ) if inspect . ismodule ( attr ) : continue assert name in bonobo . __all__","if name . startswith ( ""_"" ) :","if name . startswith ( ""_"" ) :",100,TRUE,100
2826,"def _coerce_to_bool ( self , node , var , true_val = True ) : """"""Coerce the values in a variable to bools."""""" bool_var = self . program . NewVariable ( ) for b in var . bindings : v = b . data if isinstance ( v , mixin . PythonConstant ) and isinstance ( v . pyval , bool ) : const = v . pyval is true_val <MASK> const = not true_val elif not compare . compatible_with ( v , False ) : const = true_val else : const = None bool_var . AddBinding ( self . convert . bool_values [ const ] , { b } , node ) return bool_var","elif not compare . compatible_with ( v , True ) :","elif compare . compatible_with ( v , True ) :",94.45462971,FALSE,98.38
2827,"def _parse_policies ( self , policies_yaml ) : for item in policies_yaml : id_ = required_key ( item , "" id "" ) controls_ids = required_key ( item , "" controls "" ) if not isinstance ( controls_ids , list ) : <MASK> msg = "" Policy  {id_}  contains invalid controls list  {controls} . "" . format ( id_ = id_ , controls = str ( controls_ids ) ) raise ValueError ( msg ) self . policies [ id_ ] = controls_ids","if controls_ids != ""all"" :",if not controls_ids :,94.87124007,FALSE,93.14
2828,"def pong ( self , payload : Union [ str , bytes ] = "" "" ) - > None : if self . trace_enabled and self . ping_pong_trace_enabled : <MASK> payload = payload . decode ( "" utf-8 "" ) self . logger . debug ( "" Sending a pong data frame  "" f "" (session id:  { self . session_id } , payload:  { payload } ) "" ) data = _build_data_frame_for_sending ( payload , FrameHeader . OPCODE_PONG ) with self . sock_send_lock : self . sock . send ( data )","if isinstance ( payload , bytes ) :","if isinstance ( payload , bytes ) :",100,TRUE,100
2829,"def _extract_curve_feature_log ( arg ) : """"""extract sampled curve feature for log items"""""" try : inp , res = arg config = inp . config with inp . target : sch , args = inp . task . instantiate ( config ) fea = feature . get_buffer_curve_sample_flatten ( sch , args , sample_n = 20 ) x = np . concatenate ( ( fea , list ( config . get_other_option ( ) . values ( ) ) ) ) <MASK> y = inp . task . flop / np . mean ( res . costs ) else : y = 0.0 return x , y except Exception : # pylint: disable=broad-except return None",if res . error_no == 0 :,if res . costs :,79.44221305,FALSE,94.99
2830,"def messageSourceStamps ( self , source_stamps ) : text = "" "" for ss in source_stamps : source = "" "" if ss [ "" branch "" ] : source + = "" [branch  %s ]  "" % ss [ "" branch "" ] <MASK> source + = str ( ss [ "" revision "" ] ) else : source + = "" HEAD "" if ss [ "" patch "" ] is not None : source + = ""  (plus patch) "" discriminator = "" "" if ss [ "" codebase "" ] : discriminator = ""   ' %s ' "" % ss [ "" codebase "" ] text + = "" Build Source Stamp %s :  %s \n "" % ( discriminator , source ) return text","if ss [ ""revision"" ] :","if ss [ ""revision"" ] is not None :",98.43487386,FALSE,96.98
2831,"def find_repository ( ) : orig_path = path = os . path . realpath ( "" . "" ) drive , path = os . path . splitdrive ( path ) while path : current_path = os . path . join ( drive , path ) current_repo = LocalRepository ( current_path ) if current_repo . isValid ( ) : return current_repo path , path_tail = os . path . split ( current_path ) <MASK> raise CannotFindRepository ( "" Cannot find repository for  %s "" % ( orig_path , ) )",if not path_tail :,if path_tail == orig_path :,95.87945017,FALSE,93.21
2832,"def compute_indices ( text : str , tokens ) : indices = [ ] for i , token in enumerate ( tokens ) : <MASK> current_index = indices [ - 1 ] + len ( tokens [ i - 1 ] [ 0 ] ) indices . append ( current_index + text [ current_index : ] . find ( token [ 0 ] ) ) else : indices . append ( text . find ( token [ 0 ] ) ) return indices",if 1 <= i :,if i > 0 :,88.07108826,FALSE,94.42
2833,"def _add_defaults_data_files ( self ) : # getting distribution.data_files if self . distribution . has_data_files ( ) : for item in self . distribution . data_files : if isinstance ( item , str ) : # plain file item = convert_path ( item ) <MASK> self . filelist . append ( item ) else : # a (dirname, filenames) tuple dirname , filenames = item for f in filenames : f = convert_path ( f ) if os . path . isfile ( f ) : self . filelist . append ( f )",if os . path . isfile ( item ) :,if os . path . isfile ( item ) :,75,TRUE,100
2834,"def libcxx_define ( settings ) : compiler = _base_compiler ( settings ) libcxx = settings . get_safe ( "" compiler.libcxx "" ) if not compiler or not libcxx : return "" "" if str ( compiler ) in GCC_LIKE : <MASK> return "" _GLIBCXX_USE_CXX11_ABI=0 "" elif str ( libcxx ) == "" libstdc++11 "" : return "" _GLIBCXX_USE_CXX11_ABI=1 "" return "" ""","if str ( libcxx ) == ""libstdc++"" :","if str ( libcxx ) == ""libstdc++0"" :",98.16417085,FALSE,97.64
2835,"def _populate_tree ( self , element , d ) : """"""Populates an etree with attributes & elements, given a dict."""""" for k , v in d . iteritems ( ) : if isinstance ( v , dict ) : self . _populate_dict ( element , k , v ) elif isinstance ( v , list ) : self . _populate_list ( element , k , v ) elif isinstance ( v , bool ) : self . _populate_bool ( element , k , v ) <MASK> self . _populate_str ( element , k , v ) elif type ( v ) in [ int , float , long , complex ] : self . _populate_number ( element , k , v )","elif isinstance ( v , basestring ) :","elif type ( v ) in [ str , unicode ] :",93.63582712,FALSE,93.46
2836,"def test_seek ( self ) : <MASK> print ( "" create large file via seek (may be sparse file) ... "" ) with self . open ( TESTFN , "" wb "" ) as f : f . write ( b "" z "" ) f . seek ( 0 ) f . seek ( size ) f . write ( b "" a "" ) f . flush ( ) <MASK> print ( "" check file size with os.fstat "" ) self . assertEqual ( os . fstat ( f . fileno ( ) ) [ stat . ST_SIZE ] , size + 1 )",if verbose :,if DEBUG :,97.69306414,FALSE,95.74
2837,"def serialize_review_url_field ( self , obj , * * kwargs ) : if obj . review_ui : review_request = obj . get_review_request ( ) <MASK> local_site_name = review_request . local_site . name else : local_site_name = None return local_site_reverse ( "" file-attachment "" , local_site_name = local_site_name , kwargs = { "" review_request_id "" : review_request . display_id , "" file_attachment_id "" : obj . pk , } , ) return "" """,if review_request . local_site_id :,if review_request . local_site :,84.06243223,FALSE,97.33
2838,"def on_item_down_clicked ( self , button ) : model = self . treeview . get_model ( ) for s in self . _get_selected ( ) : <MASK> # XXX need model.swap old = model . get_iter ( s [ 0 ] ) iter = model . insert ( s [ 0 ] + 2 ) for i in range ( 3 ) : model . set_value ( iter , i , model . get_value ( old , i ) ) model . remove ( old ) self . treeview . get_selection ( ) . select_iter ( iter ) self . _update_filter_string ( )",if s [ 0 ] < len ( model ) - 1 :,if s [ 0 ] != 0 :,86.52472054,FALSE,93.97
2839,"def writer ( self ) : """"""loop forever and copy socket->serial"""""" while self . alive : try : data = self . socket . recv ( 1024 ) <MASK> break self . serial . write ( b "" "" . join ( self . rfc2217 . filter ( data ) ) ) except socket . error as msg : self . log . error ( "" {} "" . format ( msg ) ) # probably got disconnected break self . stop ( )",if not data :,if not data :,100,TRUE,100
2840,"def __getitem__ ( self , key ) : if key == 1 : return self . get_value ( ) elif key == 0 : return self . cell [ 0 ] elif isinstance ( key , slice ) : s = list ( self . cell . __getitem__ ( key ) ) <MASK> s [ s . index ( self . cell [ 1 ] ) ] = self . get_value ( ) return s else : raise IndexError ( key )",if self . cell [ 1 ] in s :,if len ( s ) > 1 :,77.98952928,FALSE,91.27
2841,"def test_error_stream ( environ , start_response ) : writer = start_response ( "" 200 OK "" , [ ] ) wsgi_errors = environ [ "" wsgi.errors "" ] error_msg = None for method in [ "" flush "" , "" write "" , "" writelines "" , ] : <MASK> error_msg = "" wsgi.errors has no  ' %s '  attr "" % method if not error_msg and not callable ( getattr ( wsgi_errors , method ) ) : error_msg = "" wsgi.errors. %s  attr is not callable "" % method if error_msg : break return_msg = error_msg or "" success "" writer ( return_msg ) return [ ]","if not hasattr ( wsgi_errors , method ) :",if not error_msg :,93.87683969,FALSE,93.94
2842,"def job_rule_modules ( app ) : rules_module_list = [ ] for rules_module_name in __job_rule_module_names ( app ) : rules_module = sys . modules . get ( rules_module_name , None ) <MASK> # if using a non-default module, it's not imported until a JobRunnerMapper is instantiated when the first # JobWrapper is created rules_module = importlib . import_module ( rules_module_name ) rules_module_list . append ( rules_module ) return rules_module_list",if not rules_module :,if rules_module is None :,93.54306131,FALSE,95.71
2843,"def discover_hdfstore ( f ) : d = dict ( ) for key in f . keys ( ) : d2 = d key2 = key . lstrip ( "" / "" ) while "" / "" in key2 : group , key2 = key2 . split ( "" / "" , 1 ) <MASK> d2 [ group ] = dict ( ) d2 = d2 [ group ] d2 [ key2 ] = f . get_storer ( key ) return discover ( d )",if group not in d2 :,if group not in d2 :,100,TRUE,100
2844,"def test_update_zone ( self ) : zone = self . driver . list_zones ( ) [ 0 ] updated_zone = self . driver . update_zone ( zone = zone , domain = "" "" , extra = { "" paused "" : True } ) self . assertEqual ( zone . id , updated_zone . id ) self . assertEqual ( zone . domain , updated_zone . domain ) self . assertEqual ( zone . type , updated_zone . type ) self . assertEqual ( zone . ttl , updated_zone . ttl ) for key in set ( zone . extra ) | set ( updated_zone . extra ) : <MASK> self . assertNotEqual ( zone . extra [ key ] , updated_zone . extra [ key ] ) else : self . assertEqual ( zone . extra [ key ] , updated_zone . extra [ key ] )","if key in ( ""paused"" , ""modified_on"" ) :","if key in ( ""paused"" , ""paused"" ) :",99.19812201,FALSE,97.57
2845,"def ESP ( phrase ) : for num , name in enumerate ( devname ) : <MASK> dev = devid [ num ] if custom_action_keyword [ "" Dict "" ] [ "" On "" ] in phrase : ctrl = "" =ON "" say ( "" Turning On  "" + name ) elif custom_action_keyword [ "" Dict "" ] [ "" Off "" ] in phrase : ctrl = "" =OFF "" say ( "" Turning Off  "" + name ) rq = requests . head ( "" https:// "" + ip + dev + ctrl , verify = False )",if name . lower ( ) in phrase :,if num in devid :,90.45762461,FALSE,93.12
2846,"def filter_ports ( self , dpid , in_port , nw_id , allow_nw_id_external = None ) : assert nw_id != self . nw_id_unknown ret = [ ] for port in self . get_ports ( dpid ) : nw_id_ = port . network_id if port . port_no == in_port : continue if nw_id_ == nw_id : ret . append ( port . port_no ) <MASK> ret . append ( port . port_no ) return ret",elif allow_nw_id_external is not None and nw_id_ == allow_nw_id_external :,if allow_nw_id_external and port . port_no in allow_n,87.67404517,FALSE,84.74
2847,"def tail ( filename ) : if os . path . isfile ( filename ) : file = open ( filename , "" r "" ) st_results = os . stat ( filename ) st_size = st_results [ 6 ] file . seek ( st_size ) while 1 : where = file . tell ( ) line = file . readline ( ) <MASK> time . sleep ( 1 ) file . seek ( where ) else : print ( line , ) # already has newline else : print_error ( "" File not found, cannot tail. "" )",if not line :,"if line == ""\n"" :",94.33455822,FALSE,92.96
2848,"def proc_day_of_week ( d ) : if expanded [ 4 ] [ 0 ] != "" * "" : diff_day_of_week = nearest_diff_method ( d . isoweekday ( ) % 7 , expanded [ 4 ] , 7 ) if diff_day_of_week is not None and diff_day_of_week != 0 : <MASK> d + = relativedelta ( days = diff_day_of_week , hour = 23 , minute = 59 , second = 59 ) else : d + = relativedelta ( days = diff_day_of_week , hour = 0 , minute = 0 , second = 0 ) return True , d return False , d",if is_prev :,"if expanded [ 4 ] [ 0 ] == ""*"" :",93.1087774,FALSE,91.25
2849,"def __call__ ( self ) : """"""Run all check_* methods."""""" if self . on : oldformatwarning = warnings . formatwarning warnings . formatwarning = self . formatwarning try : for name in dir ( self ) : if name . startswith ( "" check_ "" ) : method = getattr ( self , name ) <MASK> method ( ) finally : warnings . formatwarning = oldformatwarning",if method and callable ( method ) :,if callable ( method ) :,93.42736898,FALSE,95.89
2850,"def get ( self , request , * args , * * kwargs ) : if self . revision : <MASK> try : return send_file ( request , self . revision . file . path , self . revision . created , self . attachment . original_filename , ) except OSError : pass else : return HttpResponseRedirect ( self . revision . file . url ) raise Http404",if settings . USE_LOCAL_PATH :,if self . revision . file :,95.20516692,FALSE,89.45
2851,"def _close ( self ) : super ( Recording , self ) . _close ( ) if self . _log_n is not None : for i in range ( self . n ) : <MASK> self . _log_n [ i ] . close ( ) self . _log_n [ i ] = None",if self . _log_n [ i ] is not None :,if self . _log_n [ i ] is not None :,100,TRUE,100
2852,"def addTags ( self , rpcObjects = None ) : hosts = self . _getOnlyHostObjects ( rpcObjects ) if hosts : title = "" Add Tags "" body = "" What tags should be added? \n \n Use a comma or space between each "" ( tags , choice ) = self . getText ( title , body , "" "" ) <MASK> tags = str ( tags ) . replace ( ""   "" , "" , "" ) . split ( "" , "" ) for host in hosts : self . cuebotCall ( host . addTags , "" Add Tags to  %s  Failed "" % host . data . name , tags ) self . _update ( )",if choice :,if choice :,100,TRUE,100
2853,"def available_datasets ( self ) : """"""Automatically determine datasets provided by this file"""""" res = self . resolution coordinates = [ "" pixel_longitude "" , "" pixel_latitude "" ] for var_name , val in self . file_content . items ( ) : <MASK> ds_info = { "" file_type "" : self . filetype_info [ "" file_type "" ] , "" resolution "" : res , } if not self . is_geo : ds_info [ "" coordinates "" ] = coordinates yield DatasetID ( name = var_name , resolution = res ) , ds_info","if isinstance ( val , netCDF4 . Variable ) :",if val is not None and val not in coordinates :,89.87722217,FALSE,92.31
2854,"def extract_from_file ( fname : PathIsh ) - > Iterator [ Extraction ] : path = Path ( fname ) fallback_dt = file_mtime ( path ) p = Parser ( path ) for r in p . walk ( ) : <MASK> yield r else : yield Visit ( url = r . url , dt = fallback_dt , locator = Loc . file ( fname ) , # TODO line number context = r . context , )","if isinstance ( r , Exception ) :","if r . name == ""file"" :",91.23246446,FALSE,90.38
2855,"def init_module_config ( module_json , config , config_path = default_config_path ) : if "" config "" in module_json [ "" meta "" ] : if module_json [ "" meta "" ] [ "" config "" ] : <MASK> config . add_section ( module_json [ "" name "" ] ) for config_var in module_json [ "" meta "" ] [ "" config "" ] : if config_var not in config [ module_json [ "" name "" ] ] : config . set ( module_json [ "" name "" ] , config_var , "" "" ) return config","if module_json [ ""name"" ] not in config :","if module_json [ ""name"" ] not in config :",100,TRUE,100
2856,"def _create_entities ( parsed_entities , sidx , eidx ) : entities = [ ] for k , vs in parsed_entities . items ( ) : <MASK> vs = [ vs ] for value in vs : entities . append ( { "" entity "" : k , "" start "" : sidx , "" end "" : eidx , # can't be more specific "" value "" : value , } ) return entities","if not isinstance ( vs , list ) :","if not isinstance ( vs , list ) :",100,TRUE,100
2857,"def _telegram_upload_stream ( self , stream , * * kwargs ) : """"""Perform upload defined in a stream."""""" msg = None try : stream . accept ( ) msg = self . _telegram_special_message ( chat_id = stream . identifier . id , content = stream . raw , msg_type = stream . stream_type , * * kwargs , ) except Exception : log . exception ( f "" Upload of  { stream . name }  to  { stream . identifier }  failed. "" ) else : <MASK> stream . error ( ) else : stream . success ( )",if msg is None :,if msg is None :,100,TRUE,100
2858,"def readlines ( self , size = - 1 ) : if self . _nbr == self . _size : return [ ] # leave all additional logic to our readline method, we just check the size out = [ ] nbr = 0 while True : line = self . readline ( ) <MASK> break out . append ( line ) if size > - 1 : nbr + = len ( line ) if nbr > size : break # END handle size constraint # END readline loop return out",if not line :,if not line :,100,TRUE,100
2859,"def clean_permissions ( cls , requestor : "" User "" , group : auth_models . Group , errors : Dict [ Optional [ str ] , List [ ValidationError ] ] , cleaned_input : dict , ) : field = "" add_permissions "" permission_items = cleaned_input . get ( field ) if permission_items : cleaned_input [ field ] = get_permissions ( permission_items ) <MASK> cls . ensure_can_manage_permissions ( requestor , errors , field , permission_items )",if not requestor . is_superuser :,if cleaned_input [ field ] :,75.67514423,FALSE,93.35
2860,"def _bwd ( subj = None , obj = None , seen = None ) : seen . add ( obj ) for s , o in evalPath ( graph , ( None , self . path , obj ) ) : <MASK> yield s , o if self . more : if s in seen : continue for s2 , o2 in _bwd ( None , s , seen ) : yield s2 , o",if not subj or subj == s :,if subj :,90.23272824,FALSE,90.07
2861,"def generate_data ( self , request ) : """"""Generate data for the widget."""""" uptime = { } cache_stats = get_cache_stats ( ) if cache_stats : for hosts , stats in cache_stats : <MASK> uptime [ "" value "" ] = stats [ "" uptime "" ] / 60 / 60 / 24 uptime [ "" unit "" ] = _ ( "" days "" ) elif stats [ "" uptime "" ] > 3600 : uptime [ "" value "" ] = stats [ "" uptime "" ] / 60 / 60 uptime [ "" unit "" ] = _ ( "" hours "" ) else : uptime [ "" value "" ] = stats [ "" uptime "" ] / 60 uptime [ "" unit "" ] = _ ( "" minutes "" ) return { "" cache_stats "" : cache_stats , "" uptime "" : uptime }","if stats [ ""uptime"" ] > 86400 :","if stats [ ""uptime"" ] > 3600 :",98.45904304,FALSE,98.6
2862,def refresh ( self ) : if self . _handle : source = self . _db . get_repository_from_handle ( self . _handle ) <MASK> self . _title = str ( source . get_type ( ) ) self . _value = source . get_name ( ),if source :,if source :,100,TRUE,100
2863,"def _gridconvvalue ( self , value ) : if isinstance ( value , ( str , _tkinter . Tcl_Obj ) ) : try : svalue = str ( value ) <MASK> return None elif "" . "" in svalue : return getdouble ( svalue ) else : return getint ( svalue ) except ValueError : pass return value",if not svalue :,"if svalue == """" :",68.04026125,FALSE,90.53
2864,"def parseGrants ( self , tree ) : for grant in tree . findall ( "" .//Grant "" ) : grantee = Grantee ( ) g = grant . find ( "" .//Grantee "" ) grantee . xsi_type = g . attrib [ "" { http://www.w3.org/2001/XMLSchema-instance}type "" ] grantee . permission = grant . find ( "" Permission "" ) . text for el in g : <MASK> grantee . display_name = el . text else : grantee . tag = el . tag grantee . name = el . text self . grantees . append ( grantee )","if el . tag == ""DisplayName"" :","if el . tag == ""display"" :",98.79549044,FALSE,98.07
2865,"def __init__ ( self , name : Optional [ str ] = None , order : int = 0 ) : if name is None : if order == 0 : name = "" std_dev "" <MASK> name = "" sample_std_dev "" else : name = f "" std_dev { order } ) "" super ( ) . __init__ ( name = name , order = order ) self . order = order",elif order == 1 :,elif order == 1 :,100,TRUE,100
2866,"def _shouldRollover ( self ) : if self . maxBytes > 0 : # are we rolling over? try : self . stream . seek ( 0 , 2 ) # due to non-posix-compliant Windows feature except IOError : return True <MASK> return True else : self . _degrade ( False , "" Rotation done or not needed at this time "" ) return False",if self . stream . tell ( ) >= self . maxBytes :,if self . stream . tell ( ) == self . maxBytes - 1 :,96.72751255,FALSE,92.56
2867,"def userfullname ( ) : """"""Get the user's full name."""""" global _userfullname if not _userfullname : uid = os . getuid ( ) entry = pwd_from_uid ( uid ) <MASK> _userfullname = entry [ 4 ] . split ( "" , "" ) [ 0 ] or entry [ 0 ] if not _userfullname : _userfullname = "" user %d "" % uid return _userfullname",if entry :,if entry :,100,TRUE,100
2868,"def drop ( self ) : # mssql sql = "" if object_id( ' %s ' ) is not null drop table  %s "" % ( self . tname , self . tname ) try : self . execute ( sql ) except Exception as e : self . conn . rollback ( ) <MASK> raise # sqlite sql = "" drop table if exists  %s "" % self . tname self . execute ( sql )","if ""syntax error"" not in str ( e ) :",if e . errno != errno . EEXIST :,69.32019311,FALSE,87.81
2869,"def _find_delimiter ( f , block_size = 2 * * 16 ) : delimiter = b "" \n "" if f . tell ( ) == 0 : return 0 while True : b = f . read ( block_size ) <MASK> return f . tell ( ) elif delimiter in b : return f . tell ( ) - len ( b ) + b . index ( delimiter ) + 1",if not b :,if len ( b ) == 0 :,93.62266787,FALSE,90.64
2870,"def _convert ( container ) : if _value_marker in container : force_list = False values = container . pop ( _value_marker ) <MASK> force_list = True values . extend ( _convert ( x [ 1 ] ) for x in sorted ( container . items ( ) ) ) if not force_list and len ( values ) == 1 : values = values [ 0 ] if not container : return values return _convert ( container ) el<MASK> return [ _convert ( x [ 1 ] ) for x in sorted ( container . items ( ) ) ] return dict_cls ( ( k , _convert ( v ) ) for k , v in iteritems ( container ) )","if container . pop ( _list_marker , False ) :",if len ( container ) == 1 :,85.78957788,FALSE,85.4
2871,"def fitting ( self , value ) : self . _fitting = value if self . _fitting is not None : <MASK> try : os . makedirs ( dirname ( self . checkpoint_path ( ) ) ) except FileExistsError as ex : pass # race to create if not os . path . exists ( dirname ( self . tensorboard_path ( ) ) ) : try : os . makedirs ( dirname ( self . tensorboard_path ( ) ) ) except FileExistsError as ex : pass # race to create",if not os . path . exists ( dirname ( self . checkpoint_path ( ) ) ) :,if not os . path . exists ( dirname ( self . checkpoint_path ( ) ) ),94.36666886,FALSE,97.83
2872,"def _make_headers ( self ) : libraries = self . _df . columns . to_list ( ) columns = [ ] for library in libraries : version = self . _package_versions [ library ] library_description = self . _libraries_description . get ( library ) <MASK> library + = ""   {} "" . format ( library_description ) columns . append ( "" {library} <br><small> {version} </small> "" . format ( library = library , version = version ) ) return [ "" "" ] + columns",if library_description :,if library_description :,100,TRUE,100
2873,"def plugin_on_song_ended ( self , song , stopped ) : if song is not None : poll = self . rating_box . poll_vote ( ) <MASK> ups = int ( song . get ( "" ~#wins "" ) or 0 ) downs = int ( song . get ( "" ~#losses "" ) or 0 ) ups + = poll [ 0 ] downs + = poll [ 1 ] song [ "" ~#wins "" ] = ups song [ "" ~#losses "" ] = downs song [ "" ~#rating "" ] = ups / max ( ( ups + downs ) , 2 ) # note: ^^^ Look into implementing w/ confidence intervals! song [ "" ~#score "" ] = ups - downs",if poll [ 0 ] >= 1 or poll [ 1 ] >= 1 :,if stopped :,72.07855674,FALSE,89.74
2874,"def submit ( self , pig_script , params ) : workflow = None try : workflow = self . _create_workflow ( pig_script , params ) mapping = dict ( [ ( param [ "" name "" ] , param [ "" value "" ] ) for param in workflow . get_parameters ( ) ] ) oozie_wf = _submit_workflow ( self . user , self . fs , self . jt , workflow , mapping ) finally : <MASK> workflow . delete ( skip_trash = True ) return oozie_wf",if workflow :,if workflow :,100,TRUE,100
2875,"def test_parse ( self ) : correct = 0 for example in EXAMPLES : try : schema . parse ( example . schema_string ) <MASK> correct + = 1 else : self . fail ( "" Invalid schema was parsed:  "" + example . schema_string ) except : if not example . valid : correct + = 1 else : self . fail ( "" Valid schema failed to parse:  "" + example . schema_string ) fail_msg = "" Parse behavior correct on  %d  out of  %d  schemas. "" % ( correct , len ( EXAMPLES ) , ) self . assertEqual ( correct , len ( EXAMPLES ) , fail_msg )",if example . valid :,if example . valid :,100,TRUE,100
2876,"def handle_sent ( self , elt ) : sent = [ ] for child in elt : if child . tag in ( "" wf "" , "" punc "" ) : itm = self . handle_word ( child ) <MASK> sent . extend ( itm ) else : sent . append ( itm ) else : raise ValueError ( "" Unexpected element  %s "" % child . tag ) return SemcorSentence ( elt . attrib [ "" snum "" ] , sent )","if self . _unit == ""word"" :","if isinstance ( itm , list ) :",85.6198519,FALSE,89.21
2877,"def _set_property ( self , target_widget , pname , value ) : if pname == "" text "" : state = target_widget . cget ( "" state "" ) <MASK> target_widget . configure ( state = tk . NORMAL ) target_widget . insert ( "" 0.0 "" , value ) target_widget . configure ( state = tk . DISABLED ) else : target_widget . insert ( "" 0.0 "" , value ) else : super ( TKText , self ) . _set_property ( target_widget , pname , value )",if state == tk . DISABLED :,if state == tk . NORMAL :,95.09169908,FALSE,97.82
2878,"def get_vrf_tables ( self , vrf_rf = None ) : vrf_tables = { } for ( scope_id , table_id ) , table in self . _tables . items ( ) : if scope_id is None : continue <MASK> continue vrf_tables [ ( scope_id , table_id ) ] = table return vrf_tables",if vrf_rf is not None and table_id != vrf_rf :,"if ( scope_id , table_id ) not in vrf_rf :",86.16292118,FALSE,86.9
2879,"def new_f ( self , * args , * * kwargs ) : for obj in f ( self , * args , * * kwargs ) : if self . protected == False : if "" user "" in obj and obj [ "" user "" ] [ "" protected "" ] : continue <MASK> continue yield obj","elif ""protected"" in obj and obj [ ""protected"" ] :","if ""user"" in obj and obj [ ""user"" ] [ ""protected"" ] :",67.58196809,FALSE,87.83
2880,"def draw ( self , context ) : col = self . layout . column ( ) col . operator ( "" node.sv_show_latest_commits "" ) if context . scene . sv_new_version : col_alert = self . layout . column ( ) col_alert . alert = True col_alert . operator ( "" node.sverchok_update_addon "" , text = "" Upgrade Sverchok addon "" ) else : col . operator ( "" node.sverchok_check_for_upgrades_wsha "" , text = "" Check for updates "" ) with sv_preferences ( ) as prefs : <MASK> col . operator ( "" node.sv_run_pydoc "" )",if prefs . developer_mode :,"if prefs . has_key ( ""version"" ) :",96.03155065,FALSE,93.72
2881,"def generate_tag_1_data ( ids ) : if len ( ids ) != SAMPLE_NUM : raise ValueError ( "" len ids should equal to sample number "" ) counter = 0 for sample_i in range ( SAMPLE_NUM ) : one_data = [ ids [ sample_i ] ] valid_set = [ x for x in range ( TAG_INTERVAL [ 0 ] , TAG_INTERVAL [ 1 ] ) ] features = np . random . choice ( valid_set , FEATURE_NUM , replace = False ) one_data + = [ "" : "" . join ( [ x , "" 1.0 "" ] ) for x in features ] counter + = 1 <MASK> print ( "" generate data  {} "" . format ( counter ) ) yield one_data",if counter % 10000 == 0 :,if counter % 10000 == 0 :,100,TRUE,100
2882,"def handle_api_languages ( self , http_context ) : mgr = PluginManager . get ( aj . context ) languages = set ( ) for id in mgr : locale_dir = mgr . get_content_path ( id , "" locale "" ) <MASK> for lang in os . listdir ( locale_dir ) : if lang != "" app.pot "" : languages . add ( lang ) return sorted ( list ( languages ) )",if os . path . isdir ( locale_dir ) :,if os . path . isdir ( locale_dir ) :,100,TRUE,100
2883,"def update ( self , t ) : # direction right - up for i in range ( self . grid . x ) : for j in range ( self . grid . y ) : distance = self . test_func ( i , j , t ) <MASK> self . turn_off_tile ( i , j ) elif distance < 1 : self . transform_tile ( i , j , distance ) else : self . turn_on_tile ( i , j )",if distance == 0 :,if distance > 0 :,73.54348298,FALSE,96.44
2884,"def _handle_autocomplete_request_for_text ( text ) : if not hasattr ( text , "" autocompleter "" ) : if isinstance ( text , ( CodeViewText , ShellText ) ) and text . is_python_text ( ) : if isinstance ( text , CodeViewText ) : text . autocompleter = Completer ( text ) <MASK> text . autocompleter = ShellCompleter ( text ) text . bind ( "" <1> "" , text . autocompleter . on_text_click ) else : return text . autocompleter . handle_autocomplete_request ( )","elif isinstance ( text , ShellText ) :","elif isinstance ( text , ShellText ) :",100,TRUE,100
2885,"def test_create_repository ( repo_name , expected_status , client ) : with client_with_identity ( "" devtable "" , client ) as cl : body = { "" namespace "" : "" devtable "" , "" repository "" : repo_name , "" visibility "" : "" public "" , "" description "" : "" foo "" , } result = conduct_api_call ( client , RepositoryList , "" post "" , None , body , expected_code = expected_status ) . json <MASK> assert result [ "" name "" ] == repo_name assert ( model . repository . get_repository ( "" devtable "" , repo_name ) . name == repo_name )",if expected_status == 201 :,"if ""name"" in result :",93.9780404,FALSE,94.8
2886,"def _apply_filter ( filter_item , filter_list ) : for filter_method in filter_list : try : <MASK> return False except Exception as e : raise MessageException ( "" Toolbox filter exception from  ' {} ' :  {} . "" . format ( filter_method . __name__ , unicodify ( e ) ) ) return True","if not filter_method ( context , filter_item ) :",if filter_item . __name__ == filter_method . __name__ :,87.68865262,FALSE,79.64
2887,"def printsumfp ( fp , filename , out = sys . stdout ) : m = md5 ( ) try : while 1 : data = fp . read ( bufsize ) if not data : break <MASK> data = data . encode ( fp . encoding ) m . update ( data ) except IOError as msg : sys . stderr . write ( "" %s : I/O error:  %s \n "" % ( filename , msg ) ) return 1 out . write ( "" %s   %s \n "" % ( m . hexdigest ( ) , filename ) ) return 0","if isinstance ( data , str ) :",if fp . encoding :,93.06785076,FALSE,93.66
2888,"def get_block_loc_keys ( block ) : """"""Extract loc_keys used by @block"""""" symbols = set ( ) for instr in block . lines : <MASK> if isinstance ( instr . raw , list ) : for expr in instr . raw : symbols . update ( get_expr_locs ( expr ) ) else : for arg in instr . args : symbols . update ( get_expr_locs ( arg ) ) return symbols","if isinstance ( instr , AsmRaw ) :","if instr . name == ""@loc_keys"" :",91.92681406,FALSE,88.13
2889,"def get_operations ( cls , info , operations : List [ ProductAttributeAssignInput ] ) : """"""Resolve all passed global ids into integer PKs of the Attribute type."""""" product_attrs_pks = [ ] variant_attrs_pks = [ ] for operation in operations : pk = from_global_id_strict_type ( operation . id , only_type = Attribute , field = "" operations "" ) <MASK> product_attrs_pks . append ( pk ) else : variant_attrs_pks . append ( pk ) return product_attrs_pks , variant_attrs_pks",if operation . type == ProductAttributeType . PRODUCT :,"if info . get ( ""product_id"" ) :",84.25462475,FALSE,91.29
2890,"def _collect_manual_intervention_nodes ( pipeline_tree ) : for act in pipeline_tree [ "" activities "" ] . values ( ) : if act [ "" type "" ] == "" SubProcess "" : _collect_manual_intervention_nodes ( act [ "" pipeline "" ] ) <MASK> manual_intervention_nodes . add ( act [ "" id "" ] )","elif act [ ""component"" ] [ ""code"" ] in MANUAL_INTERVENTION_COMP_CODES :","elif act [ ""type"" ] == ""Process"" :",64.28484454,FALSE,82.51
2891,"def prompt_authorization ( self , stacks : List [ Stack ] ) : auth_required_per_resource = auth_per_resource ( stacks ) for resource , authorization_required in auth_required_per_resource : <MASK> auth_confirm = confirm ( f "" \t { self . start_bold } { resource }  may not have authorization defined, Is this okay? { self . end_bold } "" , default = False , ) if not auth_confirm : raise GuidedDeployFailedError ( msg = "" Security Constraints Not Satisfied! "" )",if not authorization_required :,if not authorization_required :,100,TRUE,100
2892,"def get_cloud_credential ( self ) : """"""Return the credential which is directly tied to the inventory source type."""""" credential = None for cred in self . credentials . all ( ) : if self . source in CLOUD_PROVIDERS : <MASK> credential = cred break else : # these need to be returned in the API credential field if cred . credential_type . kind != "" vault "" : credential = cred break return credential","if cred . kind == self . source . replace ( ""ec2"" , ""aws"" ) :","if cred . credential_type . kind == ""inventory"" :",86.84436102,FALSE,85.53
2893,"def validate_party_details ( self ) : if self . party : if not frappe . db . exists ( self . party_type , self . party ) : frappe . throw ( _ ( "" Invalid  {0} :  {1} "" ) . format ( self . party_type , self . party ) ) <MASK> self . validate_account_type ( self . party_account , [ erpnext . get_party_account_type ( self . party_type ) ] )","if self . party_account and self . party_type in ( ""Customer"" , ""Supplier"" ) :",if self . party_account :,87.14045818,FALSE,84.61
2894,"def __iter__ ( self ) : it = DiskHashMerger . __iter__ ( self ) direct_upstreams = self . direct_upstreams for k , groups in it : t = list ( [ [ ] for _ in range ( self . size ) ] ) for i , g in enumerate ( groups ) : <MASK> if i in direct_upstreams : t [ i ] = g else : g . sort ( key = itemgetter ( 0 ) ) g1 = [ ] for _ , vs in g : g1 . extend ( vs ) t [ i ] = g1 yield k , tuple ( t )",if g :,"if isinstance ( g , list ) :",96.32934852,FALSE,94.34
2895,"def _unpack_scales ( scales , vidxs ) : scaleData = [ None , None , None ] for i in range ( 3 ) : <MASK> break scale = scales [ i ] if not math . isnan ( scale ) : vidx1 , vidx2 = vidxs [ i * 2 ] , vidxs [ i * 2 + 1 ] scaleData [ i ] = ( int ( vidx1 ) , int ( vidx2 ) , float ( scale ) ) return scaleData","if i >= min ( len ( scales ) , len ( vidxs ) // 2 ) :",if i >= len ( scales ) :,76.6993907,FALSE,86.88
2896,"def _make_ext_obj ( self , obj ) : ext = self . _get_ext_class ( obj . objname ) ( ) for name , val in obj . body : <MASK> raise Exception ( "" Error val should be a list, this is a python-opcua bug "" , name , type ( val ) , val , ) else : for attname , v in val : self . _set_attr ( ext , attname , v ) return ext","if not isinstance ( val , list ) :","if not isinstance ( val , ( list , tuple ) ) :",93.20153379,FALSE,93.87
2897,"def insertLine ( self , refnum , linenum , line ) : i = - 1 for i , row in enumerate ( self . rows ) : if row [ 0 ] == linenum : if row [ refnum + 1 ] is None : row [ refnum + 1 ] = line return # else keep looking <MASK> break self . rows . insert ( i , self . newRow ( linenum , refnum , line ) )",elif row [ 0 ] > linenum :,if row [ i ] == line :,95.48454808,FALSE,90.21
2898,"def valid_localparts ( strip_delimiters = False ) : for line in ABRIDGED_LOCALPART_VALID_TESTS . split ( "" \n "" ) : # strip line, skip over empty lines line = line . strip ( ) if line == "" "" : continue # skip over comments or empty lines match = COMMENT . match ( line ) if match : continue # skip over localparts with delimiters if strip_delimiters : <MASK> continue yield line","if "","" in line or "";"" in line :","if not line . strip ( ) . startswith ( ""delimiters"" ) :",93.42540425,FALSE,86.26
2899,"def encodingChanged ( self , idx ) : encoding = str ( self . mode_combo . currentText ( ) ) validator = None if encoding == "" hex "" : # only clear the box if there are non-hex chars # before setting the validator. txt = str ( self . data_edit . text ( ) ) <MASK> self . data_edit . setText ( "" "" ) regex = QtCore . QRegExp ( "" ^[0-9A-Fa-f]+$ "" ) validator = QtGui . QRegExpValidator ( regex ) self . data_edit . setValidator ( validator ) self . renderMemory ( )",if not all ( c in string . hexdigits for c in txt ) :,"if txt . startswith ( ""0123456789"" ) :",93.62808413,FALSE,89.94
2900,"def _compare_single_run ( self , compares_done ) : try : compare_id , redo = self . in_queue . get ( timeout = float ( self . config [ "" ExpertSettings "" ] [ "" block_delay "" ] ) ) except Empty : pass else : if self . _decide_whether_to_process ( compare_id , redo , compares_done ) : <MASK> self . db_interface . delete_old_compare_result ( compare_id ) compares_done . add ( compare_id ) self . _process_compare ( compare_id ) if self . callback : self . callback ( )",if redo :,if compare_id not in compares_done :,96.54645945,FALSE,93.13
2901,"def _transform_bin ( self , X : DataFrame ) : if self . _bin_map : <MASK> X = X . copy ( deep = True ) with pd . option_context ( "" mode.chained_assignment "" , None ) : # Pandas complains about SettingWithCopyWarning, but this should be valid. for column in self . _bin_map : X [ column ] = binning . bin_column ( series = X [ column ] , mapping = self . _bin_map [ column ] , dtype = self . _astype_map [ column ] , ) return X",if not self . inplace :,if self . _astype_map :,91.35082987,FALSE,94.46
2902,"def escape ( text , newline = False ) : """"""Escape special html characters."""""" if isinstance ( text , str ) : if "" & "" in text : text = text . replace ( "" & "" , "" &amp; "" ) if "" > "" in text : text = text . replace ( "" > "" , "" &gt; "" ) if "" < "" in text : text = text . replace ( "" < "" , "" &lt; "" ) if ' "" ' in text : text = text . replace ( ' "" ' , "" &quot; "" ) <MASK> text = text . replace ( "" ' "" , "" &quot; "" ) if newline : if "" \n "" in text : text = text . replace ( "" \n "" , "" <br> "" ) return text","if ""'"" in text :","if '""' in text :",99.17227596,FALSE,97.83
2903,"def read ( self ) : """"""Reads the robots.txt URL and feeds it to the parser."""""" try : f = urllib . request . urlopen ( self . url ) except urllib . error . HTTPError as err : <MASK> self . disallow_all = True elif err . code > = 400 and err . code < 500 : self . allow_all = True else : raw = f . read ( ) self . parse ( raw . decode ( "" utf-8 "" ) . splitlines ( ) )","if err . code in ( 401 , 403 ) :",if err . code == 302 and err . code == 302 :,92.08876776,FALSE,90.03
2904,"def post_create ( self , user , billing = None ) : from weblate . trans . models import Change if billing : billing . projects . add ( self ) <MASK> self . access_control = Project . ACCESS_PRIVATE else : self . access_control = Project . ACCESS_PUBLIC self . save ( ) if not user . is_superuser : self . add_user ( user , "" @Administration "" ) Change . objects . create ( action = Change . ACTION_CREATE_PROJECT , project = self , user = user , author = user )",if billing . plan . change_access_control :,if self . is_private :,94.18052829,FALSE,91.66
2905,"def visitConst ( self , node ) : if self . documentable : <MASK> self . documentable . append ( make_docstring ( node . value , node . lineno ) ) else : self . documentable = None","if type ( node . value ) in ( StringType , UnicodeType ) :","if isinstance ( node . value , ast . Constant ) :",83.84763454,FALSE,80.8
2906,"def requires ( self ) : requires = copy . deepcopy ( self . _requires ) # Auto add dependencies when parameters reference the Ouptuts of # another stack. parameters = self . parameters for value in parameters . values ( ) : if isinstance ( value , basestring ) and "" :: "" in value : stack_name , _ = value . split ( "" :: "" ) else : continue <MASK> requires . add ( stack_name ) return requires",if stack_name not in requires :,if stack_name not in requires :,100,TRUE,100
2907,"def __load_protos ( ) : g = globals ( ) for k , v in g . items ( ) : <MASK> name = k [ 4 : ] modname = name . lower ( ) try : mod = __import__ ( modname , g , level = 1 ) PPP . set_p ( v , getattr ( mod , name ) ) except ( ImportError , AttributeError ) : continue","if k . startswith ( ""PPP_"" ) :","if k . startswith ( ""prot"" ) :",98.36979107,FALSE,96.08
2908,"def init_weights ( self ) : """"""Initialize model weights."""""" for m in self . predict_layers . modules ( ) : if isinstance ( m , nn . Conv2d ) : kaiming_init ( m ) elif isinstance ( m , nn . BatchNorm2d ) : constant_init ( m , 1 ) <MASK> normal_init ( m , std = 0.01 )","elif isinstance ( m , nn . Linear ) :","elif isinstance ( m , nn . Linear ) :",100,TRUE,100
2909,"def get_data ( self ) : """"""get all data from sockets"""""" si = self . inputs parameters = [ ] for socket in si : <MASK> parameters . append ( socket . sv_get ( ) ) else : parameters . append ( socket . sv_get ( default = [ [ ] ] ) ) return match_long_repeat ( parameters )",if len ( socket . prop_name ) > 0 :,"if isinstance ( socket , SSLSocket ) :",89.34476342,FALSE,87.47
2910,"def test_parse_query_params_comparable_field ( self ) : query_params = { "" filter[int_field][gt] "" : 42 , "" filter[int_field][lte] "" : 9000 } fields = self . view . parse_query_params ( query_params ) for key , field_name in fields . items ( ) : if field_name [ "" int_field "" ] [ "" op "" ] == "" gt "" : assert_equal ( field_name [ "" int_field "" ] [ "" value "" ] , 42 ) <MASK> assert_equal ( field_name [ "" int_field "" ] [ "" value "" ] , 9000 ) else : self . fail ( )","elif field_name [ ""int_field"" ] [ ""op"" ] == ""lte"" :","elif field_name [ ""int_field"" ] [ ""op"" ] == """,94.5398554,FALSE,97.37
2911,"def _create_examples ( self , lines , set_type ) : """"""Creates examples for the training and dev sets."""""" examples = [ ] for ( i , line ) in enumerate ( lines ) : <MASK> continue guid = "" %s - %s "" % ( set_type , i ) text = line [ 0 ] bbox = line [ 1 ] label = line [ 2 ] examples . append ( DocExample ( guid = guid , text_a = text , text_b = None , bbox = bbox , label = label ) ) return examples",if i == 0 :,if i == 0 :,100,TRUE,100
2912,"def _get_attr ( sdk_path , mod_attr_path , checked = True ) : try : attr_mod , attr_path = ( mod_attr_path . split ( "" # "" ) if "" # "" in mod_attr_path else ( mod_attr_path , "" "" ) ) full_mod_path = "" {} . {} "" . format ( sdk_path , attr_mod ) if attr_mod else sdk_path op = import_module ( full_mod_path ) <MASK> # Only load attributes if needed for part in attr_path . split ( "" . "" ) : op = getattr ( op , part ) return op except ( ImportError , AttributeError ) as ex : if checked : return None raise ex",if attr_path :,if attr_path :,100,TRUE,100
2913,"def _load_ui_modules ( self , modules : Any ) - > None : if isinstance ( modules , types . ModuleType ) : self . _load_ui_modules ( dict ( ( n , getattr ( modules , n ) ) for n in dir ( modules ) ) ) elif isinstance ( modules , list ) : for m in modules : self . _load_ui_modules ( m ) else : assert isinstance ( modules , dict ) for name , cls in modules . items ( ) : try : <MASK> self . ui_modules [ name ] = cls except TypeError : pass","if issubclass ( cls , UIModule ) :","if issubclass ( cls , UIModule ) :",100,TRUE,100
2914,"def _remove_obsolete_leafs ( input_dict ) : if not isinstance ( input_dict , dict ) : return if input_dict [ LEAF_MARKER ] : bottom_leafs = input_dict [ LEAF_MARKER ] for leaf in bottom_leafs : <MASK> input_dict [ LEAF_MARKER ] . remove ( leaf ) for subtree in input_dict . keys ( ) : _remove_obsolete_leafs ( input_dict [ subtree ] )",if leaf in input_dict :,if leaf in input_dict [ LEAF_MARKER ] :,93.24546553,FALSE,93.64
2915,"def decode ( self , value , force = False ) : "" Return a unicode string from the bytes-like representation "" if self . decode_responses or force : <MASK> value = value . tobytes ( ) if isinstance ( value , bytes ) : value = value . decode ( self . encoding , self . encoding_errors ) return value","if isinstance ( value , memoryview ) :","if isinstance ( value , six . text_type ) :",94.01493534,FALSE,91.24
2916,"def audit ( self , directive ) : value = _get_value ( directive ) if not value : return server_side = directive . name . startswith ( "" proxy_ "" ) for var in compile_script ( value ) : char = "" "" <MASK> char = "" \\ n "" elif not server_side and var . can_contain ( "" \r "" ) : char = "" \\ r "" else : continue reason = ' At least variable  "" $ {var} ""  can contain  "" {char} "" ' . format ( var = var . name , char = char ) self . add_issue ( directive = [ directive ] + var . providers , reason = reason )","if var . can_contain ( ""\n"" ) :","if server_side and var . can_contain ( ""\n"" ) :",96.34136653,FALSE,96.43
2917,"def checkFilename ( filename ) : # useful in case of drag and drop while True : if filename [ 0 ] == "" ' "" : filename = filename [ 1 : ] <MASK> filename = filename [ : - 1 ] if os . path . exists ( filename ) : return filename filename = input ( "" [!] Cannot find  ' %s ' . \n [*] Enter a valid name of the file containing the paths to test ->  "" % filename )","if filename [ len ( filename ) - 1 ] == ""'"" :","if filename [ - 1 ] == '""' :",70.90245353,FALSE,91.53
2918,"def findfiles ( self , dir , base , rec ) : try : names = os . listdir ( dir or os . curdir ) except os . error as msg : print ( msg ) return [ ] list = [ ] subdirs = [ ] for name in names : fn = os . path . join ( dir , name ) <MASK> subdirs . append ( fn ) else : if fnmatch . fnmatch ( name , base ) : list . append ( fn ) if rec : for subdir in subdirs : list . extend ( self . findfiles ( subdir , base , rec ) ) return list",if os . path . isdir ( fn ) :,if os . path . isdir ( fn ) :,100,TRUE,100
2919,"def loop ( handler , obj ) : handler . response . write ( "" <table> "" ) for k , v in obj . __dict__ . items ( ) : <MASK> style = "" color: red "" if not v else "" "" handler . response . write ( ' <tr style= "" {} "" ><td> {} :</td><td> {} </td></tr> ' . format ( style , k , v ) ) handler . response . write ( "" </table> "" )","if not k in ( ""data"" , ""gae_user"" , ""credentials"" , ""content"" , ""config"" ) :","if k . startswith ( ""color:"" ) :",85.99570446,FALSE,81.82
2920,"def anypython ( request ) : name = request . param executable = getexecutable ( name ) if executable is None : if sys . platform == "" win32 "" : executable = winpymap . get ( name , None ) if executable : executable = py . path . local ( executable ) <MASK> return executable pytest . skip ( "" no suitable  %s  found "" % ( name , ) ) return executable",if executable . check ( ) :,if not os . path . exists ( executable ) :,92.49149421,FALSE,89.69
2921,"def __init__ ( self , socketpath = None ) : if socketpath is None : <MASK> socketpath = "" /var/run/usbmuxd "" else : socketpath = "" /var/run/usbmuxd "" self . socketpath = socketpath self . listener = MuxConnection ( socketpath , BinaryProtocol ) try : self . listener . listen ( ) self . version = 0 self . protoclass = BinaryProtocol except MuxVersionError : self . listener = MuxConnection ( socketpath , PlistProtocol ) self . listener . listen ( ) self . protoclass = PlistProtocol self . version = 1 self . devices = self . listener . devices","if sys . platform == ""darwin"" :","if sys . platform == ""win32"" :",98.82087677,FALSE,97.95
2922,"def _validate_distinct_on_different_types_and_field_orders ( self , collection , query , expected_results , get_mock_result ) : self . count = 0 self . get_mock_result = get_mock_result query_iterable = collection . query_items ( query , enable_cross_partition_query = True ) results = list ( query_iterable ) for i in range ( len ( expected_results ) ) : <MASK> self . assertDictEqual ( results [ i ] , expected_results [ i ] ) elif isinstance ( results [ i ] , list ) : self . assertListEqual ( results [ i ] , expected_results [ i ] ) else : self . assertEqual ( results [ i ] , expected_results [ i ] ) self . count = 0","if isinstance ( results [ i ] , dict ) :","if isinstance ( results [ i ] , dict ) :",100,TRUE,100
2923,"def getRootId ( self , id ) : with self . connect ( ) as cu : while True : stmt = "" select parent_path_id from hierarchy where path_id = ? "" cu . execute ( stmt , ( id , ) ) parent_id = cu . fetchone ( ) [ 0 ] <MASK> return id id = parent_id",if parent_id is None or parent_id == id :,if parent_id == id :,89.29250132,FALSE,92.41
2924,"def add ( self , path ) : with self . get_lock ( path ) : <MASK> self . entries [ path ] = { } self . entries [ path ] [ "" lock "" ] = self . new_locks [ path ] del self . new_locks [ path ] self . lru . append ( path )",if not path in self . entries :,if path not in self . entries :,91.5512628,FALSE,95.67
2925,"def _get_coordinates_for_dataset_key ( self , dsid ) : """"""Get the coordinate dataset keys for *dsid*."""""" ds_info = self . ids [ dsid ] cids = [ ] for cinfo in ds_info . get ( "" coordinates "" , [ ] ) : if not isinstance ( cinfo , dict ) : cinfo = { "" name "" : cinfo } cinfo [ "" resolution "" ] = ds_info [ "" resolution "" ] <MASK> cinfo [ "" polarization "" ] = ds_info [ "" polarization "" ] cid = DatasetID ( * * cinfo ) cids . append ( self . get_dataset_key ( cid ) ) return cids","if ""polarization"" in ds_info :","if ""polymorphicization"" in ds_info :",98.8741757,FALSE,98.26
2926,"def build_from_gdobj ( cls , gdobj , steal = False ) : # Avoid calling cls.__init__ by first instanciating a placeholder, then # overloading it __class__ to turn it into an instance of the right class ret = BuiltinInitPlaceholder ( ) if steal : assert ffi . typeof ( gdobj ) . kind == "" pointer "" ret . _gd_ptr = gdobj else : <MASK> ret . _gd_ptr = cls . _copy_gdobj ( gdobj ) else : ret . _gd_ptr = cls . _copy_gdobj ( ffi . addressof ( gdobj ) ) ret . __class__ = cls return ret","if ffi . typeof ( gdobj ) . kind == ""pointer"" :","if isinstance ( gdobj , ( GdObject , GdObject ) ) :",95.23080471,FALSE,90.76
2927,"def _listen_output ( self ) : "" NB! works in background thread "" try : while True : chars = self . _proc . read ( 1 ) <MASK> as_bytes = chars . encode ( self . encoding ) self . _make_output_available ( as_bytes ) else : self . _error = "" EOF "" break except Exception as e : self . _error = str ( e )",if len ( chars ) > 0 :,if chars :,78.79578935,FALSE,91.8
2928,"def result ( metrics : Dict [ metric_types . MetricKey , Any ] ) - > Dict [ metric_types . AttributionsKey , Dict [ Text , Union [ float , np . ndarray ] ] ] : """"""Returns mean attributions."""""" total_attributions = metrics [ total_attributions_key ] weighted_count = metrics [ weighted_example_count_key ] attributions = { } for k , v in total_attributions . items ( ) : <MASK> attributions [ k ] = float ( "" nan "" ) else : attributions [ k ] = v / weighted_count return { key : attributions }","if np . isclose ( weighted_count , 0.0 ) :",if v is None :,84.87843544,FALSE,91.3
2929,"def write_if_changed ( path , data ) : if isinstance ( data , str ) : data = data . encode ( ) changed = False with open ( os . open ( path , os . O_CREAT | os . O_RDWR ) , "" wb+ "" ) as f : f . seek ( 0 ) current = f . read ( ) <MASK> changed = True f . seek ( 0 ) f . write ( data ) f . truncate ( ) os . fsync ( f ) return changed",if current != data :,if current != data :,100,TRUE,100
2930,"def detect_ssl_option ( self ) : for option in self . ssl_options ( ) : if scan_argv ( self . argv , option ) is not None : for other_option in self . ssl_options ( ) : <MASK> if scan_argv ( self . argv , other_option ) is not None : raise ConfigurationError ( "" Cannot give both  %s  and  %s "" % ( option , other_option ) ) return option",if option != other_option :,"if scan_argv ( self . argv , other_option ) is not None :",67.34670376,FALSE,85.73
2931,"def _infer_return_type ( * args ) : """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args : <MASK> continue if isinstance ( arg , bytes ) : if return_type is str : raise TypeError ( "" Can ' t mix bytes and non-bytes in  "" "" path components. "" ) return_type = bytes else : if return_type is bytes : raise TypeError ( "" Can ' t mix bytes and non-bytes in  "" "" path components. "" ) return_type = str if return_type is None : return str # tempfile APIs return a str by default. return return_type",if arg is None :,if arg is None :,100,TRUE,100
2932,"def _get_app ( self , body = None ) : app = self . _app if app is None : try : tasks = self . tasks . tasks # is a group except AttributeError : tasks = self . tasks <MASK> app = tasks [ 0 ] . _app if app is None and body is not None : app = body . _app return app if app is not None else current_app",if len ( tasks ) :,"if tasks is not None and isinstance ( tasks [ 0 ] , Group ) :",94.86826173,FALSE,85.55
2933,"def add_field ( self , field ) : self . remove_field ( field . name ) self . fields [ field . name ] = field self . columns [ field . db_column ] = field self . _sorted_field_list . insert ( field ) self . _update_field_lists ( ) if field . default is not None : self . defaults [ field ] = field . default <MASK> self . _default_callables [ field ] = field . default self . _default_callable_list . append ( ( field . name , field . default ) ) else : self . _default_dict [ field ] = field . default self . _default_by_name [ field . name ] = field . default",if callable ( field . default ) :,if callable ( field . default ) :,100,TRUE,100
2934,"def _get_families ( self ) : families = [ ] for name , ext in self . _get_family_dirs ( ) : <MASK> # is a directory family = self . get_resource ( FileSystemPackageFamilyResource . key , location = self . location , name = name ) else : family = self . get_resource ( FileSystemCombinedPackageFamilyResource . key , location = self . location , name = name , ext = ext , ) families . append ( family ) return families",if ext is None :,if not os . path . isdir ( name ) :,89.80128493,FALSE,89.63
2935,"def test ( model , data_loader , device = None ) : device = device or torch . device ( "" cpu "" ) model . eval ( ) correct = 0 total = 0 with torch . no_grad ( ) : for batch_idx , ( data , target ) in enumerate ( data_loader ) : <MASK> break data , target = data . to ( device ) , target . to ( device ) outputs = model ( data ) _ , predicted = torch . max ( outputs . data , 1 ) total + = target . size ( 0 ) correct + = ( predicted == target ) . sum ( ) . item ( ) return correct / total",if batch_idx * len ( data ) > TEST_SIZE :,if batch_idx == 0 :,95.15732703,FALSE,92.68
2936,"def __animate_progress ( self ) : """"""Change the status message, mostly used to animate progress."""""" while True : sleep_time = ThreadPool . PROGRESS_IDLE_DELAY with self . __progress_lock : <MASK> sleep_time = ThreadPool . PROGRESS_IDLE_DELAY elif self . __show_animation : self . __progress_status . update_progress ( self . __current_operation_name ) sleep_time = ThreadPool . PROGRESS_UPDATE_DELAY else : self . __progress_status . show_as_ready ( ) sleep_time = ThreadPool . PROGRESS_IDLE_DELAY # Allow some time for progress status to be updated. time . sleep ( sleep_time )",if not self . __progress_status :,if self . __progress_status . show_as_ready ( ) :,91.99024288,FALSE,93.33
2937,"def _parse_subtitles ( self , video_data , url_key ) : subtitles = { } for translation in video_data . get ( "" translations "" , [ ] ) : vtt_path = translation . get ( url_key ) <MASK> continue lang = translation . get ( "" language_w3c "" ) or ISO639Utils . long2short ( translation [ "" language_medium "" ] ) subtitles . setdefault ( lang , [ ] ) . append ( { "" ext "" : "" vtt "" , "" url "" : vtt_path , } ) return subtitles",if not vtt_path :,if not vtt_path :,100,TRUE,100
2938,"def postprocess_message ( self , msg ) : if msg [ "" type "" ] == "" sample "" and msg [ "" value "" ] is not None : fn , value = msg [ "" fn "" ] , msg [ "" value "" ] value_batch_ndims = jnp . ndim ( value ) - fn . event_dim fn_batch_ndim = len ( fn . batch_shape ) <MASK> prepend_shapes = ( 1 , ) * ( value_batch_ndims - fn_batch_ndim ) msg [ "" fn "" ] = tree_map ( lambda x : jnp . reshape ( x , prepend_shapes + jnp . shape ( x ) ) , fn )",if fn_batch_ndim < value_batch_ndims :,if fn_batch_ndim > 0 :,87.08877489,FALSE,94.97
2939,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) <MASK> self . set_filename ( d . getPrefixedString ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 10 :,if tt == 10 :,100,TRUE,100
2940,"def createError ( self , line , pos , description ) : global ENABLE_PYIMPORT msg = "" Line  "" + unicode ( line ) + "" :  "" + unicode ( description ) if ENABLE_JS2PY_ERRORS : <MASK> import js2py . base return js2py . base . MakeError ( "" SyntaxError "" , msg ) else : return ENABLE_JS2PY_ERRORS ( msg ) else : return JsSyntaxError ( msg )","if isinstance ( ENABLE_JS2PY_ERRORS , bool ) :",if ENABLE_PYIMPORT :,63.44467996,FALSE,87.48
2941,"def extract ( self , page , start_index = 0 , end_index = None ) : items = [ ] for extractor in self . extractors : extracted = extractor . extract ( page , start_index , end_index , self . template . ignored_regions ) for item in arg_to_iter ( extracted ) : <MASK> if isinstance ( item , ( ItemProcessor , dict ) ) : item [ u "" _template "" ] = self . template . id items . append ( item ) return items",if item :,if item is not None :,96.04308319,FALSE,95.72
2942,"def create_volume ( self , volume ) : """"""Create a volume."""""" try : cmd = [ "" volume "" , "" create "" , volume [ "" name "" ] , "" %s G "" % ( volume [ "" size "" ] ) ] <MASK> cmd . append ( "" pool "" ) cmd . append ( self . configuration . eqlx_pool ) if self . configuration . san_thin_provision : cmd . append ( "" thin-provision "" ) out = self . _eql_execute ( * cmd ) self . add_multihost_access ( volume ) return self . _get_volume_data ( out ) except Exception : with excutils . save_and_reraise_exception ( ) : LOG . error ( ' Failed to create volume  "" %s "" . ' , volume [ "" name "" ] )","if self . configuration . eqlx_pool != ""default"" :",if self . configuration . eqlx_pool :,97.00191554,FALSE,96.36
2943,"def clean ( self ) : # TODO: check for clashes if the random code is already taken if not self . code : self . code = u "" static- %s "" % uuid . uuid4 ( ) if not self . site : placeholders = StaticPlaceholder . objects . filter ( code = self . code , site__isnull = True ) <MASK> placeholders = placeholders . exclude ( pk = self . pk ) if placeholders . exists ( ) : raise ValidationError ( _ ( "" A static placeholder with the same site and code already exists "" ) )",if self . pk :,if self . pk :,75,TRUE,100
2944,"def spawnMenu ( self , event ) : clickedPos = self . getRowByAbs ( event . Position ) self . ensureSelection ( clickedPos ) selection = self . getSelectedBoosters ( ) mainBooster = None if clickedPos != - 1 : try : booster = self . boosters [ clickedPos ] except IndexError : pass else : <MASK> mainBooster = booster itemContext = None if mainBooster is None else _t ( "" Booster "" ) menu = ContextMenu . getMenu ( self , mainBooster , selection , ( "" boosterItem "" , itemContext ) , ( "" boosterItemMisc "" , itemContext ) , ) if menu : self . PopupMenu ( menu )",if booster in self . original :,if booster :,92.87096957,FALSE,95.33
2945,"def init_errorhandler ( ) : # http error handling for ex in default_exceptions : <MASK> app . register_error_handler ( ex , error_http ) elif ex == 500 : app . register_error_handler ( ex , internal_error ) if services . ldap : # Only way of catching the LDAPException upon logging in with LDAP server down @app . errorhandler ( services . ldap . LDAPException ) def handle_exception ( e ) : log . debug ( "" LDAP server not accessible while trying to login to opds feed "" ) return error_http ( FailedDependency ( ) )",if ex < 500 :,"if isinstance ( ex , LDAPError ) :",72.16035517,FALSE,94.11
2946,"def reloadCols ( self ) : self . columns = [ ] for i , ( name , fmt , * shape ) in enumerate ( self . npy . dtype . descr ) : <MASK> t = anytype elif "" M "" in fmt : self . addColumn ( Column ( name , type = date , getter = lambda c , r , i = i : str ( r [ i ] ) ) ) continue elif "" i "" in fmt : t = int elif "" f "" in fmt : t = float else : t = anytype self . addColumn ( ColumnItem ( name , i , type = t ) )",if shape :,"if ""D"" in fmt :",93.1239585,FALSE,94.72
2947,"def Proc2 ( IntParIO ) : IntLoc = IntParIO + 10 while True : if Char1Glob == "" A "" : IntLoc = IntLoc - 1 IntParIO = IntLoc - IntGlob EnumLoc = Ident1 <MASK> break return IntParIO",if EnumLoc == Ident1 :,if EnumLoc == Ident1 :,100,TRUE,100
2948,"def opengroup ( self , name = None ) : gid = self . groups self . groupwidths . append ( None ) if self . groups > MAXGROUPS : raise error ( "" too many groups "" ) if name is not None : ogid = self . groupdict . get ( name , None ) <MASK> raise error ( "" redefinition of group name  %r  as group  %d ;  "" "" was group  %d "" % ( name , gid , ogid ) ) self . groupdict [ name ] = gid return gid",if ogid is not None :,if ogid is not None :,100,TRUE,100
2949,"def __setattr__ ( self , name : str , val : Any ) : if name . startswith ( "" COMPUTED_ "" ) : if name in self : old_val = self [ name ] <MASK> return raise KeyError ( "" Computed attributed  ' {} '  already exists  "" "" with a different value! old= {} , new= {} . "" . format ( name , old_val , val ) ) self [ name ] = val else : super ( ) . __setattr__ ( name , val )",if old_val == val :,if old_val == val :,100,TRUE,100
2950,"def get_all_function_symbols ( self , module = "" kernel "" ) : """"""Gets all the function tuples for the given module"""""" ret = [ ] symtable = self . type_map if module in symtable : mod = symtable [ module ] for ( addr , ( name , _sym_types ) ) in mod . items ( ) : <MASK> addr = addr + self . shift_address ret . append ( [ name , addr ] ) else : debug . info ( "" All symbols requested for non-existent module  %s "" % module ) return ret",if self . shift_address and addr :,if name in self . _sym_types :,91.90855163,FALSE,93.08
2951,"def __call__ ( self , frame : FrameType , event : str , arg : Any ) - > "" CallTracer "" : code = frame . f_code if ( event not in SUPPORTED_EVENTS or code . co_name == "" trace_types "" or self . should_trace and not self . should_trace ( code ) ) : return self try : <MASK> self . handle_call ( frame ) elif event == EVENT_RETURN : self . handle_return ( frame , arg ) else : logger . error ( "" Cannot handle event  %s "" , event ) except Exception : logger . exception ( "" Failed collecting trace "" ) return self",if event == EVENT_CALL :,if event == EVENT_CALL :,100,TRUE,100
2952,"def test_update_topic ( self ) : async with self . chat_client : await self . _create_thread ( ) topic = "" update topic "" async with self . chat_thread_client : await self . chat_thread_client . update_topic ( topic = topic ) # delete chat threads <MASK> await self . chat_client . delete_chat_thread ( self . thread_id )",if not self . is_playback ( ) :,if self . thread_id :,94.45598076,FALSE,90.64
2953,"def render_observation ( self ) : x = self . read_head_position label = "" Observation Grid    :  "" x_str = "" "" for j in range ( - 1 , self . rows + 1 ) : if j != - 1 : x_str + = ""   "" * len ( label ) for i in range ( - 2 , self . input_width + 2 ) : <MASK> x_str + = colorize ( self . _get_str_obs ( ( i , j ) ) , "" green "" , highlight = True ) else : x_str + = self . _get_str_obs ( ( i , j ) ) x_str + = "" \n "" x_str = label + x_str return x_str",if i == x [ 0 ] and j == x [ 1 ] :,if i == j :,89.63543136,FALSE,92.12
2954,"def build ( opt ) : dpath = os . path . join ( opt [ "" datapath "" ] , "" QA-ZRE "" ) version = None if not build_data . built ( dpath , version_string = version ) : print ( "" [building data:  "" + dpath + "" ] "" ) <MASK> # An older version exists, so remove these outdated files. build_data . remove_dir ( dpath ) build_data . make_dir ( dpath ) # Download the data. for downloadable_file in RESOURCES : downloadable_file . download_file ( dpath ) # Mark the data as built. build_data . mark_done ( dpath , version_string = version )",if build_data . built ( dpath ) :,if build_data . built ( dpath ) :,100,TRUE,100
2955,"def git_pull ( args ) : if len ( args ) < = 1 : repo = _get_repo ( ) _confirm_dangerous ( ) url = args [ 0 ] if len ( args ) == 1 else repo . remotes . get ( "" origin "" , "" "" ) <MASK> origin = url url = repo . remotes . get ( origin ) if url : repo . pull ( origin_uri = url ) else : print ( "" No pull URL. "" ) else : print ( command_help [ "" git pull "" ] )",if url in repo . remotes :,if not origin :,75.59404135,FALSE,94.36
2956,"def FindAndDelete ( script , sig ) : """"""Consensus critical, see FindAndDelete() in Satoshi codebase"""""" r = b "" "" last_sop_idx = sop_idx = 0 skip = True for ( opcode , data , sop_idx ) in script . raw_iter ( ) : <MASK> r + = script [ last_sop_idx : sop_idx ] last_sop_idx = sop_idx if script [ sop_idx : sop_idx + len ( sig ) ] == sig : skip = True else : skip = False <MASK> r + = script [ last_sop_idx : ] return CScript ( r )",if not skip :,if not skip :,100,TRUE,100
2957,"def get_ip_info ( ipaddress ) : """"""Returns device information by IP address"""""" result = { } try : ip = IPAddress . objects . select_related ( ) . get ( address = ipaddress ) except IPAddress . DoesNotExist : pass else : if ip . venture is not None : result [ "" venture_id "" ] = ip . venture . id if ip . device is not None : result [ "" device_id "" ] = ip . device . id <MASK> result [ "" venture_id "" ] = ip . device . venture . id return result",if ip . device . venture is not None :,if ip . venture is not None :,85.38549014,FALSE,97.79
2958,"def restore ( self , state ) : """"""Restore the state of a mesh previously saved using save()"""""" import pickle state = pickle . loads ( state ) for k in state : if isinstance ( state [ k ] , list ) : <MASK> state [ k ] = [ [ v . x ( ) , v . y ( ) , v . z ( ) ] for v in state [ k ] ] state [ k ] = np . array ( state [ k ] ) setattr ( self , k , state [ k ] )","if isinstance ( state [ k ] [ 0 ] , QtGui . QVector3D ) :",if len ( state [ k ] ) != len ( self . mesh . mesh_data ),73.96391787,FALSE,87.11
2959,"def get_extra_lines ( tup ) : ext_name , pyopencl_ver = tup if ext_name is not None : <MASK> # capital letters -> CL version, not extension yield "" "" yield ""     Available with OpenCL  %s . "" % ( ext_name [ 3 : ] ) yield "" "" else : yield "" "" yield ""     Available with the `` %s `` extension. "" % ext_name yield "" "" if pyopencl_ver is not None : yield "" "" yield ""     .. versionadded::  %s "" % pyopencl_ver yield "" ""","if ext_name . startswith ( ""CL_"" ) :","if ext_name [ : 3 ] == ""CL"" :",85.0595568,FALSE,92.57
2960,"def _gen_remote_uri ( fileobj : IO [ bytes ] , remote_uri : Optional [ ParseResult ] , remote_path_prefix : Optional [ str ] , remote_path_suffix : Optional [ str ] , sha256sum : Optional [ str ] , ) - > ParseResult : if remote_uri is None : assert remote_path_prefix is not None and remote_path_suffix is not None <MASK> sha256sum = _hash_fileobj ( fileobj ) return urlparse ( os . path . join ( remote_path_prefix , f "" { sha256sum } { remote_path_suffix } "" ) ) else : return remote_uri",if sha256sum is None :,if sha256sum is None :,100,TRUE,100
2961,"def queries ( self ) : if DEV : cmd = ShellCommand ( "" docker "" , "" ps "" , "" -qf "" , "" name= %s "" % self . path . k8s ) if not cmd . check ( f "" docker check for  { self . path . k8s } "" ) : <MASK> log_cmd = ShellCommand ( "" docker "" , "" logs "" , self . path . k8s , stderr = subprocess . STDOUT ) if log_cmd . check ( f "" docker logs for  { self . path . k8s } "" ) : print ( cmd . stdout ) pytest . exit ( f "" container failed to start for  { self . path . k8s } "" ) return ( )",if not cmd . stdout . strip ( ) :,if self . path . k8s :,90.90231996,FALSE,93.75
2962,"def get_range ( self ) : present = self . xml . find ( "" { %s }range "" % self . namespace ) if present is not None : attributes = present . attrib return_value = dict ( ) <MASK> return_value [ "" minimum "" ] = attributes [ "" min "" ] if "" max "" in attributes : return_value [ "" maximum "" ] = attributes [ "" max "" ] return return_value return False","if ""min"" in attributes :","if ""min"" in attributes :",100,TRUE,100
2963,"def _configuredOn ( self , workerid , builderid = None , masterid = None ) : cfg = [ ] for cs in itervalues ( self . configured ) : <MASK> continue bid , mid = self . db . builders . builder_masters [ cs [ "" buildermasterid "" ] ] if builderid is not None and bid != builderid : continue if masterid is not None and mid != masterid : continue cfg . append ( { "" builderid "" : bid , "" masterid "" : mid } ) return cfg","if cs [ ""workerid"" ] != workerid :","if cs [ ""workerid"" ] != workerid :",100,TRUE,100
2964,"def __exit__ ( self , type , value , traceback ) : try : if type is not None : return self . exception_handler ( type , value , traceback ) finally : final_contexts = _state . contexts _state . contexts = self . old_contexts <MASK> raise StackContextInconsistentError ( "" stack_context inconsistency (may be caused by yield  "" ' within a  "" with StackContext ""  block) ' ) # Break up a reference to itself to allow for faster GC on CPython. self . new_contexts = None",if final_contexts is not self . new_contexts :,if final_contexts != _state . contexts :,93.62323675,FALSE,93.92
2965,"def del_ ( self , key ) : initial_hash = hash_ = self . hash ( key ) while True : <MASK> # That key was never assigned return None elif self . _keys [ hash_ ] == key : # key found, assign with deleted sentinel self . _keys [ hash_ ] = self . _deleted self . _values [ hash_ ] = self . _deleted self . _len - = 1 return hash_ = self . _rehash ( hash_ ) if initial_hash == hash_ : # table is full and wrapped around return None",if self . _keys [ hash_ ] is self . _empty :,if hash_ == 0 :,83.41226958,FALSE,89.33
2966,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) <MASK> self . set_logout_url ( d . getPrefixedString ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 10 :,if tt == 10 :,100,TRUE,100
2967,"def data_generator ( ) : i = 0 max_batch_index = len ( X_train ) / / batch_size tot = 0 while 1 : <MASK> yield ( np . ones ( [ batch_size , input_dim ] ) * np . nan , np . ones ( [ batch_size , num_classes ] ) * np . nan , ) else : yield ( X_train [ i * batch_size : ( i + 1 ) * batch_size ] , y_train [ i * batch_size : ( i + 1 ) * batch_size ] , ) i + = 1 tot + = 1 i = i % max_batch_index",if tot > 3 * len ( X_train ) :,if tot == max_batch_index :,93.41277667,FALSE,93.09
2968,"def title ( self ) : ret = theme [ "" title "" ] if isinstance ( self . name , six . string_types ) : width = self . statwidth ( ) return ( ret + self . name [ 0 : width ] . center ( width ) . replace ( ""   "" , "" - "" ) + theme [ "" default "" ] ) for i , name in enumerate ( self . name ) : width = self . colwidth ( ) ret = ret + name [ 0 : width ] . center ( width ) . replace ( ""   "" , "" - "" ) <MASK> if op . color : ret = ret + theme [ "" frame "" ] + char [ "" dash "" ] + theme [ "" title "" ] else : ret = ret + char [ "" space "" ] return ret",if i + 1 != len ( self . vars ) :,if i == 0 :,94.66654827,FALSE,93.27
2969,"def get_container_from_dport ( dport , docker_client ) : for container in docker_client . containers ( ) : try : ports = container [ "" Ports "" ] for port in ports : <MASK> if port [ "" PublicPort "" ] == int ( dport ) : return container except KeyError : print ( ports ) pass","if ""PublicPort"" in port :","if port [ ""Port"" ] == int ( dport ) :",89.76579946,FALSE,83.66
2970,"def _get_parents_data ( self , data ) : parents = 0 if data [ COLUMN_PARENT ] : family = self . db . get_family_from_handle ( data [ COLUMN_PARENT ] [ 0 ] ) if family . get_father_handle ( ) : parents + = 1 <MASK> parents + = 1 return parents",if family . get_mother_handle ( ) :,elif family . get_object_handle ( ) :,89.62201827,FALSE,93.54
2971,"def wrapper ( filename ) : mtime = getmtime ( filename ) with lock : if filename in cache : old_mtime , result = cache . pop ( filename ) if old_mtime == mtime : # Move to the end cache [ filename ] = old_mtime , result return result result = function ( filename ) with lock : cache [ filename ] = mtime , result # at the end <MASK> cache . popitem ( last = False ) return result",if len ( cache ) > max_size :,"if cache . get ( filename , False ) :",95.65446237,FALSE,90.87
2972,"def execute ( cls , ctx , op : "" DataFrameGroupByAgg "" ) : try : pd . set_option ( "" mode.use_inf_as_na "" , op . use_inf_as_na ) if op . stage == OperandStage . map : cls . _execute_map ( ctx , op ) elif op . stage == OperandStage . combine : cls . _execute_combine ( ctx , op ) <MASK> cls . _execute_agg ( ctx , op ) else : # pragma: no cover raise ValueError ( "" Aggregation operand not executable "" ) finally : pd . reset_option ( "" mode.use_inf_as_na "" )",elif op . stage == OperandStage . agg :,elif op . stage == OperandStage . agg :,100,TRUE,100
2973,"def FindAndDelete ( script , sig ) : """"""Consensus critical, see FindAndDelete() in Satoshi codebase"""""" r = b "" "" last_sop_idx = sop_idx = 0 skip = True for ( opcode , data , sop_idx ) in script . raw_iter ( ) : if not skip : r + = script [ last_sop_idx : sop_idx ] last_sop_idx = sop_idx <MASK> skip = True else : skip = False if not skip : r + = script [ last_sop_idx : ] return CScript ( r )",if script [ sop_idx : sop_idx + len ( sig ) ] == sig :,if opcode == sig :,86.26537301,FALSE,87.58
2974,"def extractall ( zip : typing . Any , path : str ) - > NoneType : for name in zip . namelist ( ) : member = zip . getinfo ( name ) extracted_path = zip . _extract_member ( member , path , None ) attr = member . external_attr >> 16 <MASK> os . chmod ( extracted_path , attr )",if attr != 0 :,if not os . path . exists ( extracted_path ) :,78.86658276,FALSE,84.81
2975,"def find_all_gyptest_files ( directory ) : result = [ ] for root , dirs , files in os . walk ( directory ) : <MASK> dirs . remove ( "" .svn "" ) result . extend ( [ os . path . join ( root , f ) for f in files if is_test_name ( f ) ] ) result . sort ( ) return result","if "".svn"" in dirs :","if "".svn"" in dirs :",100,TRUE,100
2976,"def load ( cls , storefile , template_store ) : # Did we get file or filename? if not hasattr ( storefile , "" read "" ) : storefile = open ( storefile , "" rb "" ) # Adjust store to have translations store = cls . convertfile ( storefile , template_store ) for unit in store . units : if unit . isheader ( ) : continue # HTML does this properly on loading, others need it <MASK> unit . target = unit . source unit . rich_target = unit . rich_source return store",if cls . needs_target_sync :,if unit . is_tag :,72.86256787,FALSE,92.86
2977,"def postOptions ( self ) : _BasicOptions . postOptions ( self ) if self [ "" jobs "" ] : conflicts = [ "" debug "" , "" profile "" , "" debug-stacktraces "" , "" exitfirst "" ] for option in conflicts : if self [ option ] : raise usage . UsageError ( "" You can ' t specify -- %s  when using --jobs "" % option ) if self [ "" nopm "" ] : <MASK> raise usage . UsageError ( "" You must specify --debug when using  "" "" --nopm  "" ) failure . DO_POST_MORTEM = False","if not self [ ""debug"" ] :",if self [ option ] :,95.42923064,FALSE,94.02
2978,"def filterTokenLocation ( ) : i = None entry = None token = None tokens = [ ] i = 0 while 1 : if not ( i < len ( extra . tokens ) ) : break entry = extra . tokens [ i ] token = jsdict ( { "" type "" : entry . type , "" value "" : entry . value , } ) if extra . range : token . range = entry . range <MASK> token . loc = entry . loc tokens . append ( token ) i + = 1 extra . tokens = tokens",if extra . loc :,if extra . loc :,100,TRUE,100
2979,"def on_rebalance_end ( self ) - > None : """"""Call when rebalancing is done."""""" self . rebalancing = False if self . _rebalancing_span : self . _rebalancing_span . finish ( ) self . _rebalancing_span = None sensor_state = self . _rebalancing_sensor_state try : <MASK> self . log . warning ( "" Missing sensor state for rebalance # %s "" , self . rebalancing_count ) else : self . sensors . on_rebalance_end ( self , sensor_state ) finally : self . _rebalancing_sensor_state = None",if not sensor_state :,if sensor_state is None :,92.9373525,FALSE,96.08
2980,"def decorator ( request , * args , * * kwargs ) : if CALENDAR_VIEW_PERM : user = request . user if not user : return HttpResponseRedirect ( settings . LOGIN_URL ) occurrence , event , calendar = get_objects ( request , * * kwargs ) if calendar : allowed = CHECK_CALENDAR_PERM_FUNC ( calendar , user ) <MASK> return HttpResponseRedirect ( settings . LOGIN_URL ) # all checks passed return function ( request , * args , * * kwargs ) return HttpResponseNotFound ( "" <h1>Page not found</h1> "" ) return function ( request , * args , * * kwargs )",if not allowed :,if allowed :,95.86907378,FALSE,98.02
2981,"def reduce_arguments ( self , args ) : assert isinstance ( args , nodes . Arguments ) if args . incorrect_order ( ) : raise InvalidArguments ( "" All keyword arguments must be after positional arguments. "" ) reduced_pos = [ self . reduce_single ( arg ) for arg in args . arguments ] reduced_kw = { } for key in args . kwargs . keys ( ) : <MASK> raise InvalidArguments ( "" Keyword argument name is not a string. "" ) a = args . kwargs [ key ] reduced_kw [ key ] = self . reduce_single ( a ) return ( reduced_pos , reduced_kw )","if not isinstance ( key , str ) :","if not isinstance ( key , six . string_types ) :",95.59140029,FALSE,95.3
2982,"def _encode ( n , nbytes , little_endian = False ) : retval = [ ] n = long ( n ) for i in range ( nbytes ) : <MASK> retval . append ( chr ( n & 0xFF ) ) else : retval . insert ( 0 , chr ( n & 0xFF ) ) n >> = 8 return "" "" . join ( retval )",if little_endian :,if little_endian :,100,TRUE,100
2983,"def copy_shell ( self ) : cls = self . __class__ old_id = cls . id new_i = cls ( ) # create a new group new_i . id = self . id # with the same id cls . id = old_id # Reset the Class counter # Copy all properties for prop in cls . properties : if prop is not "" members "" : <MASK> val = getattr ( self , prop ) setattr ( new_i , prop , val ) # but no members new_i . members = [ ] return new_i",if self . has ( prop ) :,"if hasattr ( self , prop ) :",97.54808367,FALSE,95.86
2984,"def dataspec ( config ) : master = yield fakemaster . make_master ( ) data = connector . DataConnector ( ) data . setServiceParent ( master ) if config [ "" out "" ] != "" -- "" : dirs = os . path . dirname ( config [ "" out "" ] ) <MASK> os . makedirs ( dirs ) f = open ( config [ "" out "" ] , "" w "" ) else : f = sys . stdout if config [ "" global "" ] is not None : f . write ( "" window. "" + config [ "" global "" ] + "" = "" ) f . write ( json . dumps ( data . allEndpoints ( ) , indent = 2 ) ) f . close ( ) defer . returnValue ( 0 )",if dirs and not os . path . exists ( dirs ) :,if dirs is not None :,87.81206224,FALSE,92.79
2985,"def _parseSCDOCDC ( self , src ) : """"""[S|CDO|CDC]*"""""" while 1 : src = src . lstrip ( ) <MASK> src = src [ 4 : ] elif src . startswith ( "" --> "" ) : src = src [ 3 : ] else : break return src","if src . startswith ( ""<!--"" ) :","if src . startswith ( ""<S"" ) :",97.70833537,FALSE,95.22
2986,"def command ( filenames , dirnames , fix ) : for filename in gather_files ( dirnames , filenames ) : visitor = process_file ( filename ) if visitor . needs_fix ( ) : print ( "" %s :  %s "" % ( filename , visitor . get_stats ( ) ) ) <MASK> print ( "" Fixing:  %s "" % filename ) fix_file ( filename )",if fix :,if fix :,100,TRUE,100
2987,"def shutdown ( self ) : """"""Shutdown host system."""""" self . _check_dbus ( MANAGER ) use_logind = self . sys_dbus . logind . is_connected _LOGGER . info ( "" Initialize host power off  %s "" , "" logind "" if use_logind else "" systemd "" ) try : await self . sys_core . shutdown ( ) finally : <MASK> await self . sys_dbus . logind . power_off ( ) else : await self . sys_dbus . systemd . power_off ( )",if use_logind :,if use_logind :,100,TRUE,100
2988,"def _run_split_on_punc ( self , text , never_split = None ) : """"""Splits punctuation on a piece of text."""""" if never_split is not None and text in never_split : return [ text ] chars = list ( text ) i = 0 start_new_word = True output = [ ] while i < len ( chars ) : char = chars [ i ] if _is_punctuation ( char ) : output . append ( [ char ] ) start_new_word = True else : <MASK> output . append ( [ ] ) start_new_word = False output [ - 1 ] . append ( char ) i + = 1 return [ "" "" . join ( x ) for x in output ]",if start_new_word :,if start_new_word :,100,TRUE,100
2989,"def _terminal_messenger ( tp = "" write "" , msg = "" "" , out = sys . stdout ) : try : if tp == "" write "" : out . write ( msg ) <MASK> out . flush ( ) elif tp == "" write_flush "" : out . write ( msg ) out . flush ( ) elif tp == "" print "" : print ( msg , file = out ) else : raise ValueError ( "" Unsupported type:  "" + tp ) except IOError as e : logger . critical ( "" {} :  {} "" . format ( type ( e ) . __name__ , ucd ( e ) ) ) pass","elif tp == ""flush"" :","elif tp == ""write_flush"" :",98.96936433,FALSE,97.5
2990,"def checkClassDeclation ( file ) : localResult = [ ] with open ( file , "" rb "" ) as f : lineNumber = 0 for line in f : m = re . search ( "" class \ s+[^ \ (]*: "" , line ) <MASK> localResult . append ( "" Old class definition found on  {0} "" . format ( m . group ( ) ) ) return localResult",if m :,if m :,100,TRUE,100
2991,"def _evaluate_local_single ( self , iterator ) : for batch in iterator : in_arrays = convert . _call_converter ( self . converter , batch , self . device ) with function . no_backprop_mode ( ) : if isinstance ( in_arrays , tuple ) : results = self . calc_local ( * in_arrays ) <MASK> results = self . calc_local ( * * in_arrays ) else : results = self . calc_local ( in_arrays ) if self . _progress_hook : self . _progress_hook ( batch ) yield results","elif isinstance ( in_arrays , dict ) :","elif isinstance ( in_arrays , list ) :",98.65526439,FALSE,98.04
2992,"def check_billing_view ( user , permission , obj ) : if hasattr ( obj , "" all_projects "" ) : <MASK> return True # This is a billing object return any ( check_permission ( user , permission , prj ) for prj in obj . all_projects ) return check_permission ( user , permission , obj )",if user . is_superuser or obj . owners . filter ( pk = user . pk ) . exists ( ) :,if obj . billing_id :,50.28943357,FALSE,73.36
2993,"def ensure_output_spaces_contain_the_same_data ( self , y , y_ensured ) : stride = y . shape [ 1 ] self . assertEqual ( y . shape [ 0 ] * y . shape [ 1 ] , y_ensured . shape [ 0 ] ) self . assertEqual ( len ( y_ensured . shape ) , 1 ) for row in range ( y . shape [ 0 ] ) : for column in range ( y . shape [ 1 ] ) : <MASK> self . assertEqual ( y [ row , column ] , y_ensured [ row * stride + column ] ) else : self . assertEqual ( y [ row ] [ column ] , y_ensured [ row * stride + column ] )",if sp . issparse ( y ) :,if stride == 1 :,95.10732552,FALSE,95.11
2994,"def train ( self , training_data : TrainingData , config : Optional [ RasaNLUModelConfig ] = None , * * kwargs : Any , ) - > None : """"""Tokenize all training data."""""" for example in training_data . training_examples : for attribute in MESSAGE_ATTRIBUTES : if example . get ( attribute ) is not None and not example . get ( attribute ) == "" "" : <MASK> tokens = self . _split_name ( example , attribute ) else : tokens = self . tokenize ( example , attribute ) example . set ( TOKENS_NAMES [ attribute ] , tokens )","if attribute in [ INTENT , ACTION_NAME , INTENT_RESPONSE_KEY ] :",if attribute in TOKENS_NAMES :,90.4795027,FALSE,89.36
2995,"def refresh_token ( self , strategy , * args , * * kwargs ) : token = self . extra_data . get ( "" refresh_token "" ) or self . extra_data . get ( "" access_token "" ) backend = self . get_backend ( strategy ) if token and backend and hasattr ( backend , "" refresh_token "" ) : backend = backend ( strategy = strategy ) response = backend . refresh_token ( token , * args , * * kwargs ) extra_data = backend . extra_data ( self , self . uid , response , self . extra_data ) <MASK> self . save ( )",if self . set_extra_data ( extra_data ) :,if extra_data and self . save_token :,94.41454107,FALSE,92.12
2996,"def _verify_environ ( _collected_environ ) : try : yield finally : new_environ = dict ( os . environ ) current_test = new_environ . pop ( "" PYTEST_CURRENT_TEST "" , None ) old_environ = dict ( _collected_environ ) old_environ . pop ( "" PYTEST_CURRENT_TEST "" , None ) <MASK> raise DirtyTest ( "" Left over environment variables "" , current_test , _compare_eq_dict ( new_environ , old_environ , verbose = 2 ) , )",if new_environ != old_environ :,if current_test is not None and old_environ is not None :,91.8362024,FALSE,89.54
2997,"def clean_len ( self , line ) : """"""Calculate wisible length of string"""""" if isinstance ( line , basestring ) : return len ( self . screen . markup . clean_markup ( line ) ) elif isinstance ( line , tuple ) or isinstance ( line , list ) : markups = self . screen . markup . get_markup_vars ( ) length = 0 for i in line : <MASK> length + = len ( i ) return length",if i not in markups :,"if markups . has_property ( i , ""length"" ) :",93.17737156,FALSE,87.52
2998,"def _build_merged_dataset_args ( datasets ) : merged_dataset_args = [ ] for dataset in datasets : dataset_code_column = _parse_dataset_code ( dataset ) arg = dataset_code_column [ "" code "" ] column_index = dataset_code_column [ "" column_index "" ] <MASK> arg = ( dataset_code_column [ "" code "" ] , { "" column_index "" : [ column_index ] } ) merged_dataset_args . append ( arg ) return merged_dataset_args",if column_index is not None :,if not arg :,91.48630966,FALSE,93.9
2999,"def update_watch_data_table_paths ( self ) : if hasattr ( self . tool_data_watcher , "" monitored_dirs "" ) : for tool_data_table_path in self . tool_data_paths : <MASK> self . tool_data_watcher . watch_directory ( tool_data_table_path )",if tool_data_table_path not in self . tool_data_watcher . monitored_dirs :,"if hasattr ( self . tool_data_watcher , ""watch_directory"" ) :",60.61493792,FALSE,82.29
3000,"def getsource ( obj ) : """"""Wrapper around inspect.getsource"""""" try : try : src = encoding . to_unicode ( inspect . getsource ( obj ) ) except TypeError : <MASK> src = encoding . to_unicode ( inspect . getsource ( obj . __class__ ) ) else : # Bindings like VTK or ITK require this case src = getdoc ( obj ) return src except ( TypeError , IOError ) : return","if hasattr ( obj , ""__class__"" ) :","if hasattr ( obj , ""__class__"" ) :",100,TRUE,100
3001,"def __iter__ ( self ) : for model in self . app_config . get_models ( ) : admin_model = AdminModel ( model , * * self . options ) for model_re in self . model_res : if model_re . search ( admin_model . name ) : break else : <MASK> continue yield admin_model",if self . model_res :,if not self . _is_admin_model ( admin_model ) :,92.31394439,FALSE,83.42
3002,"def run ( self ) : while True : try : with DelayedKeyboardInterrupt ( ) : raw_inputs = self . _parent_task_queue . get ( ) if self . _has_stop_signal ( raw_inputs ) : self . _rq . put ( raw_inputs , block = True ) break if self . _flow_type == BATCH : self . _rq . put ( raw_inputs , block = True ) <MASK> try : self . _rq . put ( raw_inputs , block = False ) except : pass except KeyboardInterrupt : continue",elif self . _flow_type == REALTIME :,if self . _flow_type == REVERT :,94.01503087,FALSE,95.88
3003,"def dump ( self ) : self . ql . log . info ( "" [*] Dumping object:  %s "" % ( self . sf_name ) ) for field in self . _fields_ : <MASK> self . ql . log . info ( "" %s : 0x %x "" % ( field [ 0 ] , getattr ( self , field [ 0 ] ) . value ) ) elif isinstance ( getattr ( self , field [ 0 ] ) , int ) : self . ql . log . info ( "" %s :  %d "" % ( field [ 0 ] , getattr ( self , field [ 0 ] ) ) ) elif isinstance ( getattr ( self , field [ 0 ] ) , bytes ) : self . ql . log . info ( "" %s :  %s "" % ( field [ 0 ] , getattr ( self , field [ 0 ] ) . decode ( ) ) )","if isinstance ( getattr ( self , field [ 0 ] ) , POINTER64 ) :","if isinstance ( getattr ( self , field [ 0 ] ) , ( int , float ) )",96.26673401,FALSE,96.32
3004,"def validate_configuration ( self , configuration : Optional [ ExpectationConfiguration ] ) : """"""Validating that user has inputted a value set and that configuration has been initialized"""""" super ( ) . validate_configuration ( configuration ) try : assert "" value_set "" in configuration . kwargs , "" value_set is required "" assert isinstance ( configuration . kwargs [ "" value_set "" ] , ( list , set , dict ) ) , "" value_set must be a list or a set "" <MASK> assert ( "" $PARAMETER "" in configuration . kwargs [ "" value_set "" ] ) , ' Evaluation Parameter dict for value_set kwarg must have  "" $PARAMETER ""  key ' except AssertionError as e : raise InvalidExpectationConfigurationError ( str ( e ) ) return True","if isinstance ( configuration . kwargs [ ""value_set"" ] , dict ) :","if isinstance ( configuration . kwargs [ ""value_set"" ] , dict ) :",100,TRUE,100
3005,def test_one_dead_branch ( ) : with deterministic_PRNG ( ) : seen = set ( ) @run_to_buffer def x ( data ) : i = data . draw_bytes ( 1 ) [ 0 ] if i > 0 : data . mark_invalid ( ) i = data . draw_bytes ( 1 ) [ 0 ] if len ( seen ) < 255 : seen . add ( i ) <MASK> data . mark_interesting ( ),elif i not in seen :,if seen [ i ] == 1 :,92.52854707,FALSE,91.09
3006,"def __on_item_activated ( self , event ) : if self . __module_view : module = self . get_event_module ( event ) self . __module_view . set_selection ( module . module_num ) if event . EventObject is self . list_ctrl : self . input_list_ctrl . deactivate_active_item ( ) else : self . list_ctrl . deactivate_active_item ( ) for index in range ( self . list_ctrl . GetItemCount ( ) ) : <MASK> self . list_ctrl . Select ( index , False ) self . __controller . enable_module_controls_panel_buttons ( )",if self . list_ctrl . IsSelected ( index ) :,if self . list_ctrl . GetItem ( index ) is not None :,94.74514662,FALSE,95.39
3007,"def prime ( self , callback ) : <MASK> # import pdb # pdb.set_trace() self . cbhdl = simulator . register_rwsynch_callback ( callback , self ) <MASK> raise_error ( self , "" Unable set up  %s  Trigger "" % ( str ( self ) ) ) Trigger . prime ( self )",if self . cbhdl is None :,if self . cbhdl is None :,100,TRUE,100
3008,"def fstab_configuration ( middleware ) : for command in ( [ [ "" systemctl "" , "" daemon-reload "" ] , [ "" systemctl "" , "" restart "" , "" local-fs.target "" ] , ] if osc . IS_LINUX else [ [ "" mount "" , "" -uw "" , "" / "" ] ] ) : ret = subprocess . run ( command , capture_output = True ) <MASK> middleware . logger . debug ( f ' Failed to execute  "" { ""   "" . join ( command ) } "" :  { ret . stderr . decode ( ) } ' )",if ret . returncode :,if ret . stderr :,98.8403155,FALSE,97.82
3009,"def _generate_table ( self , fromdesc , todesc , diffs ) : if fromdesc or todesc : yield ( simple_colorize ( fromdesc , "" description "" ) , simple_colorize ( todesc , "" description "" ) , ) for i , line in enumerate ( diffs ) : <MASK> # mdiff yields None on separator lines; skip the bogus ones # generated for the first line if i > 0 : yield ( simple_colorize ( "" --- "" , "" separator "" ) , simple_colorize ( "" --- "" , "" separator "" ) , ) else : yield line",if line is None :,if line is None :,100,TRUE,100
3010,"def update_completion ( self ) : """"""Update completion model with exist tags"""""" orig_text = self . widget . text ( ) text = "" ,  "" . join ( orig_text . replace ( "" ,  "" , "" , "" ) . split ( "" , "" ) [ : - 1 ] ) tags = [ ] for tag in self . tags_list : if "" , "" in orig_text : <MASK> tags . append ( "" %s , %s "" % ( text , tag ) ) tags . append ( "" %s ,  %s "" % ( text , tag ) ) else : tags . append ( tag ) if tags != self . completer_model . stringList ( ) : self . completer_model . setStringList ( tags )","if orig_text [ - 1 ] not in ( "","" , "" "" ) :",if tag not in tags :,92.4892598,FALSE,89.54
3011,"def cart_number_checksum_validation ( cls , number ) : digits = [ ] even = False if not number . isdigit ( ) : return False for digit in reversed ( number ) : digit = ord ( digit ) - ord ( "" 0 "" ) <MASK> digit * = 2 if digit > = 10 : digit = digit % 10 + digit / / 10 digits . append ( digit ) even = not even return sum ( digits ) % 10 == 0 if digits else False",if even :,if even :,100,TRUE,100
3012,"def __get_param_string__ ( params ) : params_string = [ ] for key in sorted ( params . keys ( ) ) : <MASK> return value = params [ key ] params_string . append ( "" "" if value == "" null "" else str ( value ) ) return "" | "" . join ( params_string )","if ""REFUND"" in params [ key ] or ""|"" in params [ key ] :","if key == ""null"" :",76.37472798,FALSE,79.35
3013,"def _map_handlers ( self , session , event_class , mapfn ) : for event in DOC_EVENTS : event_handler_name = event . replace ( "" - "" , "" _ "" ) <MASK> event_handler = getattr ( self , event_handler_name ) format_string = DOC_EVENTS [ event ] num_args = len ( format_string . split ( "" . "" ) ) - 2 format_args = ( event_class , ) + ( "" * "" , ) * num_args event_string = event + format_string % format_args unique_id = event_class + event_handler_name mapfn ( event_string , event_handler , unique_id )","if hasattr ( self , event_handler_name ) :","if hasattr ( self , event_handler_name ) :",100,TRUE,100
3014,"def _create_param_lr ( self , param_and_grad ) : # create learning rate variable for every parameter param = param_and_grad [ 0 ] param_lr = param . optimize_attr [ "" learning_rate "" ] if type ( param_lr ) == Variable : return param_lr else : <MASK> return self . _global_learning_rate ( ) else : with default_main_program ( ) . _lr_schedule_guard ( is_with_opt = True ) , framework . name_scope ( "" scale_with_param_lr "" ) : return self . _global_learning_rate ( ) * param_lr",if param_lr == 1.0 :,if param_lr is None :,72.96303858,FALSE,96.88
3015,"def __getitem__ ( self , key ) : try : return self . _clsmap [ key ] except KeyError as e : <MASK> self . _mutex . acquire ( ) try : <MASK> self . _init ( ) self . initialized = True return self . _clsmap [ key ] finally : self . _mutex . release ( ) raise e",if not self . initialized :,if self . initialized :,90.09125743,FALSE,93.56
3016,"def save ( self , force = False ) : if not force : <MASK> return if time . time ( ) - self . last_save_time < 10 : return with self . lock : with open ( self . file_path , "" w "" ) as fd : for ip in self . cache : record = self . cache [ ip ] rule = record [ "" r "" ] connect_time = record [ "" c "" ] update_time = record [ "" update "" ] fd . write ( "" %s   %s   %d   %d \n "" % ( ip , rule , connect_time , update_time ) ) self . last_save_time = time . time ( ) self . need_save = False",if not self . need_save :,if self . need_save :,71.2621781,FALSE,98.38
3017,"def pick ( items , sel ) : for x , s in zip ( items , sel ) : <MASK> yield x elif not x . is_atom ( ) and not s . is_atom ( ) : yield x . restructure ( x . head , pick ( x . leaves , s . leaves ) , evaluation )",if match ( s ) :,if x . is_atom ( ) and s . is_atom ( ) :,90.02401181,FALSE,80.81
3018,"def isValidFloat ( config_param_name , value , constraints ) : if isinstance ( value , float ) : constraints . setdefault ( "" min "" , MIN_VALID_FLOAT_VALUE ) constraints . setdefault ( "" max "" , MAX_VALID_FLOAT_VALUE ) minv = float ( constraints . get ( "" min "" ) ) maxv = float ( constraints . get ( "" max "" ) ) <MASK> if value < = maxv : return value raise FloatValueError ( config_param_name , value , constraints )",if value >= minv :,if minv >= maxv :,96.37071868,FALSE,95.69
3019,"def get_files ( d ) : f = [ ] for root , dirs , files in os . walk ( d ) : for name in files : if "" meta-environment "" in root or "" cross-canadian "" in root : continue <MASK> continue if "" do_build "" not in name and "" do_populate_sdk "" not in name : f . append ( os . path . join ( root , name ) ) return f","if ""qemux86copy-"" in root or ""qemux86-"" in root :","if ""build"" in root and ""do_populate_sdk"" in root :",96.49459931,FALSE,89.12
3020,"def __get_photo ( self , person_or_marriage ) : """"""returns the first photo in the media list or None"""""" media_list = person_or_marriage . get_media_list ( ) for media_ref in media_list : media_handle = media_ref . get_reference_handle ( ) media = self . database . get_media_from_handle ( media_handle ) mime_type = media . get_mime_type ( ) <MASK> return media return None","if mime_type and mime_type . startswith ( ""image"" ) :","if mime_type == ""photo"" :",88.52901741,FALSE,90.12
3021,"def filter ( this , args ) : array = to_object ( this , args . space ) callbackfn = get_arg ( args , 0 ) arr_len = js_arr_length ( array ) if not is_callable ( callbackfn ) : raise MakeError ( "" TypeError "" , "" callbackfn must be a function "" ) _this = get_arg ( args , 1 ) k = 0 res = [ ] while k < arr_len : if array . has_property ( unicode ( k ) ) : kValue = array . get ( unicode ( k ) ) <MASK> res . append ( kValue ) k + = 1 return args . space . ConstructArray ( res )","if to_boolean ( callbackfn . call ( _this , ( kValue , float ( k ) , array ) ) ) :","if callbackfn ( _this , kValue ) :",88.70535766,FALSE,86.54
3022,"def optimize ( self , graph : Graph ) : for v in graph . inputs : if not v . has_attribute ( SplitTarget ) : continue <MASK> DumpGraph ( ) . optimize ( graph ) raise NotImplementedError ( f "" Input Variable  { v }  is too large to handle in WebGL backend "" ) return graph , False",if flags . DEBUG :,if v . get_attribute ( WebGLBackend ) :,92.57523557,FALSE,86.62
3023,"def detach_volume ( self , volume ) : # We need to find the node using this volume for node in self . list_nodes ( ) : if type ( node . image ) is not list : # This node has only one associated image. It is not the one we # are after. continue for disk in node . image : <MASK> # Node found. We can now detach the volume disk_id = disk . extra [ "" disk_id "" ] return self . _do_detach_volume ( node . id , disk_id ) return False",if disk . id == volume . id :,if disk . name == volume . name :,72.57815305,FALSE,95.85
3024,"def Yield ( value , level = 1 ) : g = greenlet . getcurrent ( ) while level != 0 : if not isinstance ( g , genlet ) : raise RuntimeError ( "" yield outside a genlet "" ) <MASK> g . parent . set_child ( g ) g = g . parent level - = 1 g . switch ( value )",if level > 1 :,if g . parent :,93.1250064,FALSE,93.58
3025,"def get_all_pipeline_nodes ( pipeline : pipeline_pb2 . Pipeline , ) - > List [ pipeline_pb2 . PipelineNode ] : """"""Returns all pipeline nodes in the given pipeline."""""" result = [ ] for pipeline_or_node in pipeline . nodes : which = pipeline_or_node . WhichOneof ( "" node "" ) # TODO(goutham): Handle sub-pipelines. # TODO(goutham): Handle system nodes. <MASK> result . append ( pipeline_or_node . pipeline_node ) else : raise NotImplementedError ( "" Only pipeline nodes supported. "" ) return result","if which == ""pipeline_node"" :","if which == ""sub"" :",98.49046422,FALSE,96.52
3026,"def __init__ ( self , * * settings ) : default_settings = self . get_default_settings ( ) for name , value in default_settings . items ( ) : if not hasattr ( self , name ) : setattr ( self , name , value ) for name , value in settings . items ( ) : <MASK> raise ImproperlyConfigured ( "" Invalid setting  ' {} '  for  {} "" . format ( name , self . __class__ . __name__ , ) ) setattr ( self , name , value )",if name not in default_settings :,"if not hasattr ( self , name ) :",93.19996479,FALSE,93.22
3027,"def _check_choice ( self ) : if self . type == "" choice "" : <MASK> raise OptionError ( "" must supply a list of choices for type  ' choice ' "" , self ) elif type ( self . choices ) not in ( types . TupleType , types . ListType ) : raise OptionError ( "" choices must be a list of strings ( ' %s '  supplied) "" % str ( type ( self . choices ) ) . split ( "" ' "" ) [ 1 ] , self , ) elif self . choices is not None : raise OptionError ( "" must not supply choices for type  %r "" % self . type , self )",if self . choices is None :,"if not isinstance ( self . choices , ( list , tuple ) ) :",64.36709888,FALSE,90.78
3028,"def prepare ( self , size = None ) : if _is_seekable ( self . file ) : start_pos = self . file . tell ( ) self . file . seek ( 0 , 2 ) end_pos = self . file . tell ( ) self . file . seek ( start_pos ) fsize = end_pos - start_pos <MASK> self . remain = fsize else : self . remain = min ( fsize , size ) return self . remain",if size is None :,if size is None :,100,TRUE,100
3029,"def _setSitemapTargets ( ) : if not conf . sitemapUrl : return infoMsg = "" parsing sitemap  ' %s ' "" % conf . sitemapUrl logger . info ( infoMsg ) found = False for item in parseSitemap ( conf . sitemapUrl ) : <MASK> found = True kb . targets . add ( ( item . strip ( ) , None , None , None , None ) ) if not found and not conf . forms and not conf . crawlDepth : warnMsg = "" no usable links found (with GET parameters) "" logger . warn ( warnMsg )","if re . match ( r""[^ ]+\?(.+)"" , item , re . I ) :",if item . strip ( ) not in kb . targets :,75.77447933,FALSE,80.64
3030,"def test_CY_decomposition ( self , tol ) : """"""Tests that the decomposition of the CY gate is correct"""""" op = qml . CY ( wires = [ 0 , 1 ] ) res = op . decomposition ( op . wires ) mats = [ ] for i in reversed ( res ) : <MASK> mats . append ( np . kron ( i . matrix , np . eye ( 2 ) ) ) else : mats . append ( i . matrix ) decomposed_matrix = np . linalg . multi_dot ( mats ) assert np . allclose ( decomposed_matrix , op . matrix , atol = tol , rtol = 0 )",if len ( i . wires ) == 1 :,if i . ndim == 3 :,93.86215309,FALSE,93.46
3031,"def _line_ranges ( statements , lines ) : """"""Produce a list of ranges for `format_lines`."""""" statements = sorted ( statements ) lines = sorted ( lines ) pairs = [ ] start = None lidx = 0 for stmt in statements : if lidx > = len ( lines ) : break <MASK> lidx + = 1 if not start : start = stmt end = stmt elif start : pairs . append ( ( start , end ) ) start = None if start : pairs . append ( ( start , end ) ) return pairs",if stmt == lines [ lidx ] :,"if _is_range ( stmt , lines ) :",93.60640808,FALSE,91.67
3032,"def init_params ( net ) : """"""Init layer parameters."""""" for module in net . modules ( ) : if isinstance ( module , nn . Conv2d ) : init . kaiming_normal ( module . weight , mode = "" fan_out "" ) <MASK> init . constant ( module . bias , 0 ) elif isinstance ( module , nn . BatchNorm2d ) : init . constant ( module . weight , 1 ) init . constant ( module . bias , 0 ) elif isinstance ( module , nn . Linear ) : init . normal ( module . weight , std = 1e-3 ) <MASK> init . constant ( module . bias , 0 )",if module . bias :,if module . bias is not None :,95.76805303,FALSE,93.61
3033,"def _get_directory_size_in_bytes ( directory ) : total = 0 try : for entry in os . scandir ( directory ) : <MASK> # if it's a file, use stat() function total + = entry . stat ( ) . st_size elif entry . is_dir ( ) : # if it's a directory, recursively call this function total + = _get_directory_size_in_bytes ( entry . path ) except NotADirectoryError : # if `directory` isn't a directory, get the file size then return os . path . getsize ( directory ) except PermissionError : # if for whatever reason we can't open the folder, return 0 return 0 return total",if entry . is_file ( ) :,if entry . is_file ( ) :,100,TRUE,100
3034,"def run_cmd ( self , util , to , always_push_mark = False ) : if to == "" bof "" : util . push_mark_and_goto_position ( 0 ) elif to == "" eof "" : util . push_mark_and_goto_position ( self . view . size ( ) ) elif to in ( "" eow "" , "" bow "" ) : visible = self . view . visible_region ( ) pos = visible . a if to == "" bow "" else visible . b <MASK> util . push_mark_and_goto_position ( pos ) else : util . set_cursors ( [ sublime . Region ( pos ) ] )",if always_push_mark :,if always_push_mark :,100,TRUE,100
3035,"def parse_results ( cwd ) : optimal_dd = None optimal_measure = numpy . inf for tup in tools . find_conf_files ( cwd ) : dd = tup [ 1 ] if "" results.train_y_misclass "" in dd : if dd [ "" results.train_y_misclass "" ] < optimal_measure : optimal_measure = dd [ "" results.train_y_misclass "" ] optimal_dd = dd print ( "" Optimal results.train_y_misclass: "" , str ( optimal_measure ) ) for key , value in optimal_dd . items ( ) : <MASK> print ( key + "" :  "" + str ( value ) )","if ""hyper_parameters"" in key :","if key not in ( ""results.train_y_misclass"" , ""results.",90.39549896,FALSE,88.87
3036,"def clean_vc_position ( self ) : vc_position = self . cleaned_data [ "" vc_position "" ] if self . validate_vc_position : conflicting_members = Device . objects . filter ( virtual_chassis = self . instance . virtual_chassis , vc_position = vc_position ) <MASK> raise forms . ValidationError ( "" A virtual chassis member already exists in position  {} . "" . format ( vc_position ) ) return vc_position",if conflicting_members . exists ( ) :,if conflicting_members . exists ( ) :,100,TRUE,100
3037,"def cal_pads ( auto_pad , pad_shape ) : spatial_size = len ( pad_shape ) pads = [ 0 ] * spatial_size * 2 for i in range ( spatial_size ) : if auto_pad == "" SAME_LOWER "" : pads [ i + spatial_size ] = pad_shape [ i ] / / 2 pads [ i ] = pad_shape [ i ] - pads [ i + spatial_size ] <MASK> pads [ i ] = pad_shape [ i ] / / 2 pads [ i + spatial_size ] = pad_shape [ i ] - pads [ i ] return pads","elif auto_pad == ""SAME_UPPER"" :","elif auto_pad == ""AME_UPPER"" :",98.77027205,FALSE,98.22
3038,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) <MASK> length = d . getVarInt32 ( ) tmp = ProtocolBuffer . Decoder ( d . buffer ( ) , d . pos ( ) , d . pos ( ) + length ) d . skip ( length ) self . add_presence_response ( ) . TryMerge ( tmp ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 10 :,if tt == 10 :,100,TRUE,100
3039,"def test_cwl_rnaseq ( self , install_test_files ) : with install_cwl_test_files ( ) as work_dir : with utils . chdir ( os . path . join ( work_dir , "" rnaseq "" ) ) : <MASK> shutil . rmtree ( "" cromwell_work "" ) subprocess . check_call ( [ "" bcbio_vm.py "" , "" cwlrun "" , "" cromwell "" , "" rnaseq-workflow "" ] )","if os . path . exists ( ""cromwell_work"" ) :","if os . path . exists ( ""cromwell_work"" ) :",75,TRUE,100
3040,"def files_per_version ( self ) : xpath = "" ./files/file "" files = self . root . findall ( xpath ) versions = { } for file in files : vfile = file . findall ( "" version "" ) for version in vfile : nb = version . attrib [ "" nb "" ] <MASK> versions [ nb ] = [ ] versions [ nb ] . append ( file . attrib [ "" url "" ] ) return versions",if not nb in versions :,if nb not in versions :,95.02301078,FALSE,96.71
3041,"def value_to_db_datetime ( self , value ) : if value is None : return None # SQLite doesn't support tz-aware datetimes if timezone . is_aware ( value ) : <MASK> value = value . astimezone ( timezone . utc ) . replace ( tzinfo = None ) else : raise ValueError ( "" SQLite backend does not support timezone-aware datetimes when USE_TZ is False. "" ) return six . text_type ( value )",if settings . USE_TZ :,if settings . USE_TZ :,75,TRUE,100
3042,"def _toplevelTryFunc ( func , * args , status = status , * * kwargs ) : with ThreadProfiler ( threading . current_thread ( ) ) as prof : t = threading . current_thread ( ) t . name = func . __name__ try : t . status = func ( * args , * * kwargs ) except EscapeException as e : # user aborted t . status = "" aborted by user "" <MASK> status ( "" %s  aborted "" % t . name , priority = 2 ) except Exception as e : t . exception = e t . status = "" exception "" vd . exceptionCaught ( e ) if t . sheet : t . sheet . currentThreads . remove ( t )",if status :,if status :,100,TRUE,100
3043,"def ESP ( phrase ) : for num , name in enumerate ( devname ) : if name . lower ( ) in phrase : dev = devid [ num ] <MASK> ctrl = "" =ON "" say ( "" Turning On  "" + name ) elif custom_action_keyword [ "" Dict "" ] [ "" Off "" ] in phrase : ctrl = "" =OFF "" say ( "" Turning Off  "" + name ) rq = requests . head ( "" https:// "" + ip + dev + ctrl , verify = False )","if custom_action_keyword [ ""Dict"" ] [ ""On"" ] in phrase :","if custom_action_keyword [ ""Dict"" ] [ ""On"" ] in phrase :",100,TRUE,100
3044,"def _table_schema ( self , table ) : rows = self . db . execute_sql ( "" PRAGMA table_info( ' %s ' ) "" % table ) . fetchall ( ) # Build list of fields from table information result = { } for _ , name , data_type , not_null , _ , primary_key in rows : parts = [ data_type ] <MASK> parts . append ( "" PRIMARY KEY "" ) if not_null : parts . append ( "" NOT NULL "" ) result [ name ] = ""   "" . join ( parts ) return result",if primary_key :,if primary_key :,100,TRUE,100
3045,"def _validate_forward_input ( x , n_in ) : if n_in != 1 : if not isinstance ( x , ( tuple , list ) ) : raise TypeError ( f "" Expected input to be a tuple or list; instead got  { type ( x ) } . "" ) <MASK> raise ValueError ( f "" Input tuple length ( { len ( x ) } ) does not equal required  "" f "" number of inputs ( { n_in } ). "" )",if len ( x ) != n_in :,if len ( x ) != n_in :,100,TRUE,100
3046,"def _table_reprfunc ( self , row , col , val ) : if self . _table . column_names [ col ] . endswith ( "" Size "" ) : if isinstance ( val , compat . string_types ) : return ""    %s "" % val <MASK> return ""    %.1f  KB "" % ( val / 1024.0 * * 1 ) elif val < 1024 * * 3 : return ""    %.1f  MB "" % ( val / 1024.0 * * 2 ) else : return ""    %.1f  GB "" % ( val / 1024.0 * * 3 ) if col in ( 0 , "" "" ) : return str ( val ) else : return ""    %s "" % val",elif val < 1024 ** 2 :,elif val < 1024 * 2 :,74.01341085,FALSE,98.77
3047,"def get_path_name ( self ) : if self . is_root ( ) : return "" @ "" + self . name else : parent_name = self . parent . get_path_name ( ) <MASK> return "" / "" . join ( [ parent_name , "" @ "" + self . name ] ) else : return "" @ "" + self . name",if parent_name :,if parent_name :,75,TRUE,100
3048,"def parse ( cls , api , json ) : lst = List ( api ) setattr ( lst , "" _json "" , json ) for k , v in json . items ( ) : <MASK> setattr ( lst , k , User . parse ( api , v ) ) elif k == "" created_at "" : setattr ( lst , k , parse_datetime ( v ) ) else : setattr ( lst , k , v ) return lst","if k == ""user"" :","if k == ""user"" :",100,TRUE,100
3049,"def _bytecode_filenames ( self , py_filenames ) : bytecode_files = [ ] for py_file in py_filenames : if not py_file . endswith ( "" .py "" ) : continue <MASK> bytecode_files . append ( py_file + "" c "" ) if self . optimize > 0 : bytecode_files . append ( py_file + "" o "" ) return bytecode_files",if self . compile :,if self . optimize < 0 :,94.94805526,FALSE,94.83
3050,"def to_json_dict ( self ) : d = super ( ) . to_json_dict ( ) d [ "" bullet_list "" ] = RenderedContent . rendered_content_list_to_json ( self . bullet_list ) if self . header is not None : <MASK> d [ "" header "" ] = self . header . to_json_dict ( ) else : d [ "" header "" ] = self . header if self . subheader is not None : if isinstance ( self . subheader , RenderedContent ) : d [ "" subheader "" ] = self . subheader . to_json_dict ( ) else : d [ "" subheader "" ] = self . subheader return d","if isinstance ( self . header , RenderedContent ) :","if isinstance ( self . header , RenderedContent ) :",100,TRUE,100
3051,"def makeSomeFiles ( pathobj , dirdict ) : pathdict = { } for ( key , value ) in dirdict . items ( ) : child = pathobj . child ( key ) if isinstance ( value , bytes ) : pathdict [ key ] = child child . setContent ( value ) <MASK> child . createDirectory ( ) pathdict [ key ] = makeSomeFiles ( child , value ) else : raise ValueError ( "" only strings and dicts allowed as values "" ) return pathdict","elif isinstance ( value , dict ) :","elif isinstance ( value , dict ) :",100,TRUE,100
3052,"def Restore ( self ) : picker , obj = self . _window , self . _pObject value = obj . RestoreValue ( PERSIST_FILEDIRPICKER_PATH ) if value is not None : <MASK> if type ( value ) == list : value = value [ - 1 ] picker . SetPath ( value ) return True return False","if issubclass ( picker . __class__ , wx . FileDialog ) :",if value is not None :,86.83292422,FALSE,79.42
3053,"def recv ( self , buffer_size ) : try : return super ( SSLConnection , self ) . recv ( buffer_size ) except ssl . SSLError as err : <MASK> return b "" "" if err . args [ 0 ] in ( ssl . SSL_ERROR_EOF , ssl . SSL_ERROR_ZERO_RETURN ) : self . handle_close ( ) return b "" "" raise","if err . args [ 0 ] in ( ssl . SSL_ERROR_WANT_READ , ssl . SSL_ERROR_WANT_WRITE ) :",if err . args [ 0 ] == ssl . SSL_ERROR_NO_DATA :,88.85806086,FALSE,81.79
3054,"def IncrementErrorCount ( self , category ) : """"""Bumps the module's error statistic."""""" self . error_count + = 1 if self . counting in ( "" toplevel "" , "" detailed "" ) : <MASK> category = category . split ( "" / "" ) [ 0 ] if category not in self . errors_by_category : self . errors_by_category [ category ] = 0 self . errors_by_category [ category ] + = 1","if self . counting != ""detailed"" :","if ""/"" in category :",87.21251596,FALSE,91.09
3055,"def _get_y ( self , data_inst ) : if self . stratified : y = [ v for i , v in data_inst . mapValues ( lambda v : v . label ) . collect ( ) ] <MASK> y = self . transform_regression_label ( data_inst ) else : # make dummy y y = [ 0 ] * ( data_inst . count ( ) ) return y",if self . need_transform :,if self . regression :,98.23299474,FALSE,94.84
3056,"def test_all_project_files ( self ) : if sys . platform . startswith ( "" win "" ) : # XXX something with newlines goes wrong on Windows. return for filepath in support . all_project_files ( ) : with open ( filepath , "" rb "" ) as fp : encoding = tokenize . detect_encoding ( fp . readline ) [ 0 ] self . assertIsNotNone ( encoding , "" can ' t detect encoding for  %s "" % filepath ) with open ( filepath , "" r "" ) as fp : source = fp . read ( ) source = source . decode ( encoding ) tree = driver . parse_string ( source ) new = unicode ( tree ) <MASK> self . fail ( "" Idempotency failed:  %s "" % filepath )","if diff ( filepath , new , encoding ) :",if new != encoding :,71.86783434,FALSE,94.33
3057,"def test_resource_arn_override_generator ( self ) : overrides = set ( ) for k , v in manager . resources . items ( ) : arn_gen = bool ( v . __dict__ . get ( "" get_arns "" ) or v . __dict__ . get ( "" generate_arn "" ) ) <MASK> overrides . add ( k ) overrides = overrides . difference ( { "" account "" , "" s3 "" , "" hostedzone "" , "" log-group "" , "" rest-api "" , "" redshift-snapshot "" , "" rest-stage "" , } ) if overrides : raise ValueError ( "" unknown arn overrides in  %s "" % ( "" ,  "" . join ( overrides ) ) )",if arn_gen :,if arn_gen :,100,TRUE,100
3058,"def _check_dsl_runner ( self ) - > None : """"""Checks if runner in dsl is Kubeflow V2 runner."""""" with open ( self . flags_dict [ labels . PIPELINE_DSL_PATH ] , "" r "" ) as f : dsl_contents = f . read ( ) <MASK> raise RuntimeError ( "" KubeflowV2DagRunner not found in dsl. "" )","if ""KubeflowV2DagRunner"" not in dsl_contents :","if "" KubeflowV2DagRunner"" not in dsl_contents :",97.52072351,FALSE,100
3059,"def create_warehouse ( warehouse_name , properties = None , company = None ) : if not company : company = "" _Test Company "" warehouse_id = erpnext . encode_company_abbr ( warehouse_name , company ) if not frappe . db . exists ( "" Warehouse "" , warehouse_id ) : warehouse = frappe . new_doc ( "" Warehouse "" ) warehouse . warehouse_name = warehouse_name warehouse . parent_warehouse = "" All Warehouses - _TCUV "" warehouse . company = company warehouse . account = get_warehouse_account ( warehouse_name , company ) <MASK> warehouse . update ( properties ) warehouse . save ( ) return warehouse . name else : return warehouse_id",if properties :,if properties :,100,TRUE,100
3060,"def _parse ( self , contents ) : entries = [ ] hostnames_found = set ( ) for line in contents . splitlines ( ) : if not len ( line . strip ( ) ) : entries . append ( ( "" blank "" , [ line ] ) ) continue ( head , tail ) = chop_comment ( line . strip ( ) , "" # "" ) <MASK> entries . append ( ( "" all_comment "" , [ line ] ) ) continue entries . append ( ( "" hostname "" , [ head , tail ] ) ) hostnames_found . add ( head ) if len ( hostnames_found ) > 1 : raise IOError ( "" Multiple hostnames ( %s ) found! "" % ( hostnames_found ) ) return entries",if not len ( head ) :,if head in hostnames_found :,95.43617092,FALSE,95.92
3061,"def _get_omega ( self ) : if self . _omega is None : n = self . get_drift_dim ( ) / / 2 omg = sympl . calc_omega ( n ) if self . oper_dtype == Qobj : self . _omega = Qobj ( omg , dims = self . dyn_dims ) self . _omega_qobj = self . _omega <MASK> self . _omega = sp . csr_matrix ( omg ) else : self . _omega = omg return self . _omega",elif self . oper_dtype == sp . csr_matrix :,elif self . oper_dtype == csr_matrix :,73.28829428,FALSE,97.01
3062,"def get_in_inputs ( key , data ) : if isinstance ( data , dict ) : for k , v in data . items ( ) : if k == key : return v <MASK> out = get_in_inputs ( key , v ) if out : return out elif isinstance ( data , ( list , tuple ) ) : out = [ get_in_inputs ( key , x ) for x in data ] out = [ x for x in out if x ] if out : return out [ 0 ]","elif isinstance ( v , ( list , tuple , dict ) ) :","elif isinstance ( v , ( dict , list ) ) :",94.14623745,FALSE,95.08
3063,def visit_binary ( binary ) : if binary . operator == operators . eq : cols = util . column_set ( chain ( * [ c . proxy_set for c in columns . difference ( omit ) ] ) ) <MASK> for c in reversed ( columns ) : if c . shares_lineage ( binary . right ) and ( not only_synonyms or c . name == binary . left . name ) : omit . add ( c ) break,if binary . left in cols and binary . right in cols :,if not only_synonyms and cols :,85.39857691,FALSE,89.06
3064,"def wait_tasks_or_abort ( futures , timeout = 60 , kill_switch_ev = None ) : try : LazySingletonTasksCoordinator . wait_tasks ( futures , return_when = FIRST_EXCEPTION , raise_exceptions = True ) except Exception as e : <MASK> # Used when we want to keep both raise the exception and wait for all tasks to finish kill_switch_ev . set ( ) LazySingletonTasksCoordinator . wait_tasks ( futures , return_when = ALL_COMPLETED , raise_exceptions = False , timeout = timeout , ) raise e",if kill_switch_ev is not None :,if kill_switch_ev :,91.50462047,FALSE,96.16
3065,"def is_valid ( sample ) : if sample is None : return False if isinstance ( sample , tuple ) : for s in sample : <MASK> return False elif isinstance ( s , np . ndarray ) and s . size == 0 : return False elif isinstance ( s , collections . abc . Sequence ) and len ( s ) == 0 : return False return True",if s is None :,"if isinstance ( s , ( list , tuple ) ) and s . size != 0 :",82.89863027,FALSE,79.79
3066,"def setVaName ( self , va , parent = None ) : if parent is None : parent = self curname = self . vw . getName ( va ) if curname is None : curname = "" "" name , ok = QInputDialog . getText ( parent , "" Enter... "" , "" Name "" , text = curname ) if ok : name = str ( name ) <MASK> raise Exception ( "" Duplicate Name:  %s "" % name ) self . vw . makeName ( va , name )",if self . vw . vaByName ( name ) :,if name in self . vw . names :,95.54335258,FALSE,93.49
3067,"def generic_tag_compiler ( params , defaults , name , node_class , parser , token ) : "" Returns a template.Node subclass. "" bits = token . split_contents ( ) [ 1 : ] bmax = len ( params ) def_len = defaults and len ( defaults ) or 0 bmin = bmax - def_len if len ( bits ) < bmin or len ( bits ) > bmax : <MASK> message = "" %s  takes  %s  arguments "" % ( name , bmin ) else : message = "" %s  takes between  %s  and  %s  arguments "" % ( name , bmin , bmax ) raise TemplateSyntaxError ( message ) return node_class ( bits )",if bmin == bmax :,if bmin == bmax :,100,TRUE,100
3068,"def extract_segmentation_mask ( annotation ) : poly_specs = annotation [ DensePoseDataRelative . S_KEY ] if isinstance ( poly_specs , torch . Tensor ) : # data is already given as mask tensors, no need to decode return poly_specs import pycocotools . mask as mask_utils segm = torch . zeros ( ( DensePoseDataRelative . MASK_SIZE , ) * 2 , dtype = torch . float32 ) for i in range ( DensePoseDataRelative . N_BODY_PARTS ) : poly_i = poly_specs [ i ] <MASK> mask_i = mask_utils . decode ( poly_i ) segm [ mask_i > 0 ] = i + 1 return segm",if poly_i :,if pycocotools . is_mask_supported ( poly_i ) :,97.75205612,FALSE,91.78
3069,"def module_list ( target , fast ) : """"""Find the list of modules to be compiled"""""" modules = [ ] native = native_modules ( target ) basedir = os . path . join ( ouroboros_repo_folder ( ) , "" ouroboros "" ) for name in os . listdir ( basedir ) : module_name , ext = os . path . splitext ( name ) <MASK> if module_name not in IGNORE_MODULES and module_name not in native : if not ( fast and module_name in KNOWN_PROBLEM_MODULES ) : modules . append ( module_name ) return set ( modules )","if ext == "".py"" or ext == """" and os . path . isdir ( os . path . join ( basedir , name ) ) :","if ext == "".py"" :",80.81206152,FALSE,82.76
3070,"def filelist_from_patterns ( pats , rootdir = None ) : if rootdir is None : rootdir = "" . "" # filelist = [] fileset = set ( [ ] ) lines = [ line . strip ( ) for line in pats ] for line in lines : pat = line [ 2 : ] newfiles = glob ( osp . join ( rootdir , pat ) ) if line . startswith ( "" + "" ) : fileset . update ( newfiles ) <MASK> fileset . difference_update ( newfiles ) else : raise ValueError ( "" line must start with + or - "" ) filelist = list ( fileset ) return filelist","elif line . startswith ( ""-"" ) :","elif line . startswith ( ""-"" ) :",100,TRUE,100
3071,"def get_upstream_statuses_events ( self , upstream : Set ) - > Dict [ str , V1Statuses ] : statuses_by_refs = { u : [ ] for u in upstream } events = self . events or [ ] # type: List[V1EventTrigger] for e in events : entity_ref = contexts_refs . get_entity_ref ( e . ref ) if not entity_ref : continue if entity_ref not in statuses_by_refs : continue for kind in e . kinds : status = V1EventKind . events_statuses_mapping . get ( kind ) <MASK> statuses_by_refs [ entity_ref ] . append ( status ) return statuses_by_refs",if status :,if status :,100,TRUE,100
3072,"def __setitem__ ( self , key , value ) : if isinstance ( value , ( tuple , list ) ) : info , reference = value <MASK> self . _reverse_infos [ info ] = len ( self . _infos ) self . _infos . append ( info ) if reference not in self . _reverse_references : self . _reverse_references [ reference ] = len ( self . _references ) self . _references . append ( reference ) self . _trails [ key ] = "" %d , %d "" % ( self . _reverse_infos [ info ] , self . _reverse_references [ reference ] , ) else : raise Exception ( "" unsupported type  ' %s ' "" % type ( value ) )",if info not in self . _reverse_infos :,if info not in self . _reverse_infos :,100,TRUE,100
3073,"def ChangeStyle ( self , combos ) : style = 0 for combo in combos : <MASK> if combo . GetLabel ( ) == "" TR_VIRTUAL "" : style = style | HTL . TR_VIRTUAL else : try : style = style | eval ( "" wx. "" + combo . GetLabel ( ) ) except : style = style | eval ( "" HTL. "" + combo . GetLabel ( ) ) if self . GetAGWWindowStyleFlag ( ) != style : self . SetAGWWindowStyleFlag ( style )",if combo . GetValue ( ) == 1 :,"if isinstance ( combo , wx . Control ) :",93.19081341,FALSE,91.72
3074,"def _parse_csrf ( self , response ) : for d in response : if d . startswith ( "" Set-Cookie: "" ) : for c in d . split ( "" : "" , 1 ) [ 1 ] . split ( "" ; "" ) : <MASK> self . _CSRFtoken = c . strip ( ""   \r \n "" ) log . verbose ( "" Got new cookie:  %s "" , self . _CSRFtoken ) break if self . _CSRFtoken != None : break","if c . strip ( ) . startswith ( ""CSRF-Token-"" ) :","if c . startswith ( ""CSRF-TOKEN="" ) :",86.64975774,FALSE,93.28
3075,"def test_page_size_matching_max_returned_rows ( app_client_returned_rows_matches_page_size , ) : fetched = [ ] path = "" /fixtures/no_primary_key.json "" while path : response = app_client_returned_rows_matches_page_size . get ( path ) fetched . extend ( response . json [ "" rows "" ] ) assert len ( response . json [ "" rows "" ] ) in ( 1 , 50 ) path = response . json [ "" next_url "" ] <MASK> path = path . replace ( "" http://localhost "" , "" "" ) assert 201 == len ( fetched )",if path :,if path :,100,TRUE,100
3076,"def get_mapping_exception_message ( mappings : List [ Tuple [ Text , Text ] ] ) : """"""Return a message given a list of duplicates."""""" message = "" "" for name , action_name in mappings : <MASK> message + = "" \n "" message + = ( "" Intent  ' {} '  is set to trigger action  ' {} ' , which is  "" "" not defined in the domain. "" . format ( name , action_name ) ) return message",if message :,if message :,100,TRUE,100
3077,def cut ( sentence ) : sentence = strdecode ( sentence ) blocks = re_han . split ( sentence ) for blk in blocks : if re_han . match ( blk ) : for word in __cut ( blk ) : if word not in Force_Split_Words : yield word else : for c in word : yield c else : tmp = re_skip . split ( blk ) for x in tmp : <MASK> yield x,if x :,if x not in Force_Split_Words :,96.2126004,FALSE,91.08
3078,"def chop ( expr , delta = 10.0 * * ( - 10.0 ) ) : if isinstance ( expr , Real ) : if - delta < expr . get_float_value ( ) < delta : return Integer ( 0 ) elif isinstance ( expr , Complex ) and expr . is_inexact ( ) : real , imag = expr . real , expr . imag if - delta < real . get_float_value ( ) < delta : real = Integer ( 0 ) <MASK> imag = Integer ( 0 ) return Complex ( real , imag ) elif isinstance ( expr , Expression ) : return Expression ( chop ( expr . head ) , * [ chop ( leaf ) for leaf in expr . leaves ] ) return expr",if - delta < imag . get_float_value ( ) < delta :,if - delta < imag . get_float_value ( ) < delta :,100,TRUE,100
3079,"def make_row ( self ) : res = [ ] for i in range ( self . num_cols ) : t = sqlite3_column_type ( self . stmnt , i ) # print(""type"", t) if t == SQLITE_INTEGER : res . append ( sqlite3_column_int ( self . stmnt , i ) ) elif t == SQLITE_FLOAT : res . append ( sqlite3_column_double ( self . stmnt , i ) ) <MASK> res . append ( sqlite3_column_text ( self . stmnt , i ) ) else : raise NotImplementedError return tuple ( res )",elif t == SQLITE_TEXT :,elif t == SQLITE_TEXT :,100,TRUE,100
3080,"def try_convert ( self , string ) : string = string . strip ( ) try : return int ( string ) except : try : return float ( string ) except : if string == "" True "" : return True <MASK> return False return string","if string == ""False"" :","if string == ""False"" :",100,TRUE,100
3081,"def configure_create_table_epilogue ( store ) : for val in [ "" "" , ""  ENGINE=InnoDB "" ] : store . config [ "" create_table_epilogue "" ] = val store . _set_sql_flavour ( ) <MASK> store . log . info ( "" create_table_epilogue= ' %s ' "" , val ) return raise Exception ( "" Can not create a transactional table. "" )",if store . _test_transaction ( ) :,if store . _transactional_create ( ) :,98.12383872,FALSE,95.42
3082,"def _check_rule ( self , match , target_dict , cred_dict ) : """"""Recursively checks credentials based on the brains rules."""""" try : new_match_list = self . rules [ match ] except KeyError : <MASK> new_match_list = ( "" rule: %s "" % self . default_rule , ) else : return False return self . check ( new_match_list , target_dict , cred_dict )",if self . default_rule and match != self . default_rule :,if self . default_rule is not None :,61.96322143,FALSE,90.32
3083,"def get_civil_names ( self ) : congresspeople_ids = self . get_all_congresspeople_ids ( ) for i , congress_id in enumerate ( congresspeople_ids ) : if not np . math . isnan ( float ( congress_id ) ) : percentage = i / self . total * 100 msg = "" Processed  {}  out of  {}  ( {:.2f} % ) "" print ( msg . format ( i , self . total , percentage ) , end = "" \r "" ) data = self . fetch_data_repository ( congress_id ) <MASK> yield dict ( data )",if data is not None :,if data :,95.33790681,FALSE,96.49
3084,"def parse_network_whitelist ( self , network_whitelist_location ) : networks = [ ] with open ( network_whitelist_location , "" r "" ) as text_file : for line in text_file : line = line . strip ( ) . strip ( "" ' "" ) . strip ( ' "" ' ) <MASK> networks . append ( line ) return networks",if isIPv4 ( line ) or isIPv6 ( line ) :,if line :,86.73838609,FALSE,87.4
3085,"def _pick ( self , cum ) : if self . _isleaf ( ) : return self . bd [ 0 ] , self . s else : <MASK> return self . left . _pick ( cum ) else : return self . right . _pick ( cum - self . left . s )",if cum < self . left . s :,if cum < self . left . s :,75,TRUE,100
3086,"def serialize_content_range ( value ) : if isinstance ( value , ( tuple , list ) ) : if len ( value ) not in ( 2 , 3 ) : raise ValueError ( "" When setting content_range to a list/tuple, it must  "" "" be length 2 or 3 (not  %r ) "" % value ) <MASK> begin , end = value length = None else : begin , end , length = value value = ContentRange ( begin , end , length ) value = str ( value ) . strip ( ) if not value : return None return value",if len ( value ) == 2 :,if len ( value ) == 2 :,100,TRUE,100
3087,"def make_index_fields ( rec ) : fields = { } for k , v in rec . iteritems ( ) : <MASK> fields [ k ] = v continue if k == "" full_title "" : fields [ "" title "" ] = [ read_short_title ( v ) ] return fields","if k in ( ""lccn"" , ""oclc"" , ""isbn"" ) :","if k == ""id"" :",83.72579892,FALSE,79.89
3088,"def _sample_translation ( reference , max_len ) : translation = reference [ : ] while np . random . uniform ( ) < 0.8 and 1 < len ( translation ) < max_len : trans_len = len ( translation ) ind = np . random . randint ( trans_len ) action = np . random . choice ( actions ) if action == "" deletion "" : del translation [ ind ] <MASK> ind_rep = np . random . randint ( trans_len ) translation [ ind ] = translation [ ind_rep ] else : ind_insert = np . random . randint ( trans_len ) translation . insert ( ind , translation [ ind_insert ] ) return translation","elif action == ""replacement"" :","elif action == ""insert"" :",98.95783192,FALSE,98.28
3089,"def __call__ ( self , text : str ) - > str : for t in self . cleaner_types : if t == "" tacotron "" : text = tacotron_cleaner . cleaners . custom_english_cleaners ( text ) elif t == "" jaconv "" : text = jaconv . normalize ( text ) <MASK> if vietnamese_cleaners is None : raise RuntimeError ( "" Please install underthesea "" ) text = vietnamese_cleaners . vietnamese_cleaner ( text ) else : raise RuntimeError ( f "" Not supported: type= { t } "" ) return text","elif t == ""vietnamese"" :","elif t == ""vietnamese"" :",100,TRUE,100
3090,"def hook_GetVariable ( ql , address , params ) : if params [ "" VariableName "" ] in ql . env : var = ql . env [ params [ "" VariableName "" ] ] read_len = read_int64 ( ql , params [ "" DataSize "" ] ) <MASK> write_int64 ( ql , params [ "" Attributes "" ] , 0 ) write_int64 ( ql , params [ "" DataSize "" ] , len ( var ) ) if read_len < len ( var ) : return EFI_BUFFER_TOO_SMALL if params [ "" Data "" ] != 0 : ql . mem . write ( params [ "" Data "" ] , var ) return EFI_SUCCESS return EFI_NOT_FOUND","if params [ ""Attributes"" ] != 0 :",if read_len == 0 :,95.09156124,FALSE,94.34
3091,"def test_setupapp ( self , overrideRootMenu ) : "" Call setupApp with each possible graphics type. "" root = self . root flist = FileList ( root ) for tktype in alltypes : with self . subTest ( tktype = tktype ) : macosx . _tk_type = tktype macosx . setupApp ( root , flist ) <MASK> self . assertTrue ( overrideRootMenu . called ) overrideRootMenu . reset_mock ( )","if tktype in ( ""carbon"" , ""cocoa"" ) :",if overrideRootMenu :,90.00687101,FALSE,85.41
3092,"def names ( self , persistent = None ) : u = set ( ) result = [ ] for s in [ self . __storage ( None ) , self . __storage ( self . __category ) , ] : for b in s : if persistent is not None and b . persistent != persistent : continue <MASK> continue if b . name not in u : result . append ( b . name ) u . add ( b . name ) return result","if b . name . startswith ( ""__"" ) :",if b . category is not None and b . category != self . __category :,90.50543908,FALSE,85.92
3093,"def _check_extra_specs ( key , value = None ) : extra_specs = diff . get ( "" extra_specs "" ) specific_type = extra_specs . get ( key ) if extra_specs else None old_type = None new_type = None if specific_type : old_type , new_type = specific_type <MASK> old_type = True if old_type and old_type . upper ( ) == value else False new_type = True if new_type and new_type . upper ( ) == value else False return old_type , new_type",if value :,if value :,100,TRUE,100
3094,"def _write_lock_file ( self , repo , force = True ) : # type: (Repository, bool) -> None if force or ( self . _update and self . _write_lock ) : updated_lock = self . _locker . set_lock_data ( self . _package , repo . packages ) <MASK> self . _io . write_line ( "" "" ) self . _io . write_line ( "" <info>Writing lock file</> "" )",if updated_lock :,if updated_lock :,100,TRUE,100
3095,"def process_message ( self , msg ) : if msg [ "" type "" ] == "" sample "" : batch_shape = msg [ "" fn "" ] . batch_shape <MASK> batch_shape = [ 1 ] * ( - self . dim - len ( batch_shape ) ) + list ( batch_shape ) batch_shape [ self . dim ] = self . size msg [ "" fn "" ] = msg [ "" fn "" ] . expand ( torch . Size ( batch_shape ) )",if len ( batch_shape ) < - self . dim or batch_shape [ self . dim ] != self . size :,if self . dim :,68.98119093,FALSE,79.06
3096,"def _test_reducibility ( self ) : # make a copy of the graph graph = networkx . DiGraph ( self . _graph ) # preprocess: make it a super graph self . _make_supergraph ( graph ) while True : changed = False # find a node with a back-edge, remove the edge (deleting the loop), and replace it with a MultiNode changed | = self . _remove_self_loop ( graph ) # find a node that has only one predecessor, and merge it with its predecessor (replace them with a # MultiNode) changed | = self . _merge_single_entry_node ( graph ) <MASK> # a fixed-point is reached break",if not changed :,if changed :,73.7111418,FALSE,98.15
3097,"def __init__ ( self , roberta , num_classes = 2 , dropout = 0.0 , prefix = None , params = None ) : super ( RoBERTaClassifier , self ) . __init__ ( prefix = prefix , params = params ) self . roberta = roberta self . _units = roberta . _units with self . name_scope ( ) : self . classifier = nn . HybridSequential ( prefix = prefix ) <MASK> self . classifier . add ( nn . Dropout ( rate = dropout ) ) self . classifier . add ( nn . Dense ( units = self . _units , activation = "" tanh "" ) ) <MASK> self . classifier . add ( nn . Dropout ( rate = dropout ) ) self . classifier . add ( nn . Dense ( units = num_classes ) )",if dropout :,if num_classes > 1 :,95.44821083,FALSE,92.31
3098,"def get_object_from_name ( self , name , check_symlinks = True ) : if not name : return None name = name . rstrip ( "" \\ "" ) for a , o in self . objects . items ( ) : if not o . name : continue <MASK> return o if check_symlinks : m = [ sl [ 1 ] for sl in self . symlinks if name . lower ( ) == sl [ 0 ] . lower ( ) ] if m : name = m [ 0 ] return self . get_object_from_name ( name , False )",if o . name . lower ( ) == name . lower ( ) :,if name == o . name :,89.84933394,FALSE,90.02
3099,"def __call__ ( self ) : """"""Run all check_* methods."""""" if self . on : oldformatwarning = warnings . formatwarning warnings . formatwarning = self . formatwarning try : for name in dir ( self ) : <MASK> method = getattr ( self , name ) if method and callable ( method ) : method ( ) finally : warnings . formatwarning = oldformatwarning","if name . startswith ( ""check_"" ) :","if name . startswith ( ""check_"" ) :",100,TRUE,100
3100,"def __print__ ( self , defaults = False ) : if defaults : print_func = str else : print_func = repr pieces = [ ] default_values = self . __defaults__ for k in self . __fields__ : value = getattr ( self , k ) if not defaults and value == default_values [ k ] : continue <MASK> print_func = repr # keep quotes around strings pieces . append ( "" %s = %s "" % ( k , print_func ( value ) ) ) if pieces or self . __base__ : return "" %s ( %s ) "" % ( self . __class__ . __name__ , "" ,  "" . join ( pieces ) ) else : return "" ""","if isinstance ( value , basestring ) :",if value is not None :,75.65850643,FALSE,95.66
3101,"def apply ( self , * * kwargs : Any ) - > None : for node in self . document . traverse ( nodes . target ) : <MASK> continue if ( "" ismod "" in node and node . parent . __class__ is nodes . section and # index 0 is the section title node node . parent . index ( node ) == 1 ) : node . parent [ "" ids "" ] [ 0 : 0 ] = node [ "" ids "" ] node . parent . remove ( node )","if not node [ ""ids"" ] :",if node . parent is None :,90.16179648,FALSE,92.38
3102,"def add_special_token_2d ( values : List [ List [ int ] ] , special_token : int = 0 , use_first_value : bool = False ) - > List [ List [ int ] ] : results = torch . jit . annotate ( List [ List [ int ] ] , [ ] ) for value in values : result = torch . jit . annotate ( List [ int ] , [ ] ) <MASK> special_token = value [ 0 ] result . append ( special_token ) result . extend ( value ) result . append ( special_token ) results . append ( result ) return results",if use_first_value and len ( value ) > 0 :,if use_first_value and special_token == 0 :,83.30618329,FALSE,95.26
3103,"def test_import ( self ) : TIMEOUT = 5 # Test for a deadlock when importing a module that runs the # ThreadedResolver at import-time. See resolve_test.py for # full explanation. command = [ sys . executable , "" -c "" , "" import tornado.test.resolve_test_helper "" ] start = time . time ( ) popen = Popen ( command , preexec_fn = lambda : signal . alarm ( TIMEOUT ) ) while time . time ( ) - start < TIMEOUT : return_code = popen . poll ( ) <MASK> self . assertEqual ( 0 , return_code ) return # Success. time . sleep ( 0.05 ) self . fail ( "" import timed out "" )",if return_code is not None :,if return_code :,97.72348082,FALSE,96.9
3104,"def find_item_for_key ( self , e ) : for item in self . _items : if item . keycode == e . key and item . shift == e . shift and item . alt == e . alt : focus = get_focus ( ) <MASK> return self . _items . index ( item ) else : return - 1 return - 1","if self . command_is_enabled ( item , focus ) :",if focus . keycode == e . key and focus . alt == e . alt :,61.55732143,FALSE,80.48
3105,"def check_app_config_brackets ( self ) : for sn , app in cherrypy . tree . apps . items ( ) : if not isinstance ( app , cherrypy . Application ) : continue <MASK> continue for key in app . config . keys ( ) : if key . startswith ( "" [ "" ) or key . endswith ( "" ] "" ) : warnings . warn ( "" The application mounted at  %r  has config  "" "" section names with extraneous brackets:  %r .  "" "" Config *files* need brackets; config *dicts*  "" "" (e.g. passed to tree.mount) do not. "" % ( sn , key ) )",if not app . config :,"if not isinstance ( app . config , dict ) :",91.83804665,FALSE,94.28
3106,"def got_arbiter_module_type_defined ( self , mod_type ) : for a in self . arbiters : # Do like the linkify will do after.... for m in getattr ( a , "" modules "" , [ ] ) : # So look at what the arbiter try to call as module m = m . strip ( ) # Ok, now look in modules... for mod in self . modules : # try to see if this module is the good type <MASK> # if so, the good name? if getattr ( mod , "" module_name "" , "" "" ) . strip ( ) == m : return True return False","if getattr ( mod , ""module_type"" , """" ) . strip ( ) == mod_type . strip ( ) :",if mod_type == mod . __name__ :,92.92566797,FALSE,84.61
3107,"def write_config_to_file ( self , folder , filename , config ) : do_not_write = [ "" hyperparameter_search_space_updates "" ] with open ( os . path . join ( folder , filename ) , "" w "" ) as f : f . write ( "" \n "" . join ( [ ( key + "" = "" + str ( value ) ) for ( key , value ) in sorted ( config . items ( ) , key = lambda x : x [ 0 ] ) <MASK> ] ) )",if not key in do_not_write,if key not in do_not_write,92.89100644,FALSE,97.37
3108,"def parsing ( self , parsing ) : # type: (bool) -> None self . _parsed = parsing for k , v in self . _body : <MASK> v . value . parsing ( parsing ) elif isinstance ( v , AoT ) : for t in v . body : t . value . parsing ( parsing )","if isinstance ( v , Table ) :","if isinstance ( v , Braced ) :",98.06806098,FALSE,96.41
3109,"def test_crashers_crash ( self ) : for fname in glob . glob ( CRASHER_FILES ) : <MASK> continue # Some ""crashers"" only trigger an exception rather than a # segfault. Consider that an acceptable outcome. if test . support . verbose : print ( "" Checking crasher: "" , fname ) assert_python_failure ( fname )",if os . path . basename ( fname ) in infinite_loops :,if not os . path . isfile ( fname ) :,56.39226582,FALSE,88.18
3110,"def __getitem__ ( self , k ) - > "" SimMemView "" : if isinstance ( k , slice ) : if k . step is not None : raise ValueError ( "" Slices with strides are not supported "" ) elif k . start is None : raise ValueError ( "" Must specify start index "" ) <MASK> raise ValueError ( "" Slices with stop index are not supported "" ) else : addr = k . start elif self . _type is not None and self . _type . _can_refine_int : return self . _type . _refine ( self , k ) else : addr = k return self . _deeper ( addr = addr )",elif k . stop is not None :,elif k . stop is not None :,100,TRUE,100
3111,"def get_lowest_wall_time ( jsons ) : lowest_wall = None for j in jsons : <MASK> lowest_wall = j [ "" wall_time "" ] if lowest_wall > j [ "" wall_time "" ] : lowest_wall = j [ "" wall_time "" ] return lowest_wall",if lowest_wall is None :,if lowest_wall is None :,100,TRUE,100
3112,"def extract_wav_headers ( data ) : # def search_subchunk(data, subchunk_id): pos = 12 # The size of the RIFF chunk descriptor subchunks = [ ] while pos + 8 < = len ( data ) and len ( subchunks ) < 10 : subchunk_id = data [ pos : pos + 4 ] subchunk_size = struct . unpack_from ( "" <I "" , data [ pos + 4 : pos + 8 ] ) [ 0 ] subchunks . append ( WavSubChunk ( subchunk_id , pos , subchunk_size ) ) <MASK> # 'data' is the last subchunk break pos + = subchunk_size + 8 return subchunks","if subchunk_id == b""data"" :",if subchunk_size == 0 :,73.05431324,FALSE,94.46
3113,"def _any_targets_have_native_sources ( self , targets ) : # TODO(#5949): convert this to checking if the closure of python requirements has any # platform-specific packages (maybe find the platforms there too?). for tgt in targets : for type_constraint , target_predicate in self . _native_target_matchers . items ( ) : <MASK> return True return False",if type_constraint . satisfied_by ( tgt ) and target_predicate ( tgt ) :,"if tgt . _target_predicate ( type_constraint , target_predicate ) :",66.92444906,FALSE,86.75
3114,"def validate_memory ( self , value ) : for k , v in value . viewitems ( ) : if v is None : # use NoneType to unset a value continue <MASK> raise serializers . ValidationError ( "" Process types can only contain [a-z] "" ) if not re . match ( MEMLIMIT_MATCH , str ( v ) ) : raise serializers . ValidationError ( "" Limit format: <number><unit>, where unit = B, K, M or G "" ) return value","if not re . match ( PROCTYPE_MATCH , k ) :","if k . startswith ( ""a-"" ) :",70.10306574,FALSE,89.95
3115,"def cart_number_checksum_validation ( cls , number ) : digits = [ ] even = False if not number . isdigit ( ) : return False for digit in reversed ( number ) : digit = ord ( digit ) - ord ( "" 0 "" ) if even : digit * = 2 <MASK> digit = digit % 10 + digit / / 10 digits . append ( digit ) even = not even return sum ( digits ) % 10 == 0 if digits else False",if digit >= 10 :,if digit % 10 == 0 :,95.16659972,FALSE,93.93
3116,"def transform ( a , cmds ) : buf = a . split ( "" \n "" ) for cmd in cmds : ctype , line , col , char = cmd if ctype == "" D "" : <MASK> buf [ line ] = buf [ line ] [ : col ] + buf [ line ] [ col + len ( char ) : ] else : buf [ line ] = buf [ line ] + buf [ line + 1 ] del buf [ line + 1 ] elif ctype == "" I "" : buf [ line ] = buf [ line ] [ : col ] + char + buf [ line ] [ col : ] buf = "" \n "" . join ( buf ) . split ( "" \n "" ) return "" \n "" . join ( buf )","if char != ""\n"" :",if char :,96.68321085,FALSE,95.31
3117,"def get_partners ( self ) - > Dict [ AbstractNode , Set [ int ] ] : partners = { } # type: Dict[AbstractNode, Set[int]] for edge in self . edges : if edge . is_dangling ( ) : raise ValueError ( "" Cannot contract copy tensor with dangling edges "" ) if self . _is_my_trace ( edge ) : continue partner_node , shared_axis = self . _get_partner ( edge ) <MASK> partners [ partner_node ] = set ( ) partners [ partner_node ] . add ( shared_axis ) return partners",if partner_node not in partners :,if partner_node not in partners :,75,TRUE,100
3118,"def _bind_interactive_rez ( self ) : if config . set_prompt and self . settings . prompt : stored_prompt = os . getenv ( "" REZ_STORED_PROMPT_CMD "" ) curr_prompt = stored_prompt or os . getenv ( "" PROMPT "" , "" "" ) <MASK> self . setenv ( "" REZ_STORED_PROMPT_CMD "" , curr_prompt ) new_prompt = "" %% REZ_ENV_PROMPT %% "" new_prompt = ( ( new_prompt + ""   %s "" ) if config . prefix_prompt else ( "" %s   "" + new_prompt ) ) new_prompt = new_prompt % curr_prompt self . _addline ( "" set PROMPT= %s "" % new_prompt )",if not stored_prompt :,if curr_prompt :,89.07229691,FALSE,97.8
3119,"def __listingColumns ( self ) : columns = [ ] for name in self . __getColumns ( ) : definition = column ( name ) if not definition : IECore . msg ( IECore . Msg . Level . Error , "" GafferImageUI.CatalogueUI "" , "" No column registered with name  ' %s ' "" % name , ) continue <MASK> c = GafferUI . PathListingWidget . IconColumn ( definition . title ( ) , "" "" , name ) else : c = GafferUI . PathListingWidget . StandardColumn ( definition . title ( ) , name ) columns . append ( c ) return columns","if isinstance ( definition , IconColumn ) :",if definition . isIcon ( ) :,91.35684578,FALSE,94.92
3120,"def _check_invalid_keys ( self , section_name , section ) : for key in section : key_name = str ( key ) valid_key_names = [ s [ 0 ] for s in self . keys ] is_valid_key = key_name in valid_key_names <MASK> err_msg = ( "" ' {0} '  is not a valid key name for  ' {1} ' . Must  "" "" be one of these:  {2} "" ) . format ( key_name , section_name , "" ,  "" . join ( valid_key_names ) ) raise InvalidConfig ( err_msg )",if not is_valid_key :,if not is_valid_key :,100,TRUE,100
3121,"def _get_startup_packages ( lib_path : Path , packages ) - > Set [ str ] : names = set ( ) for path in lib_path . iterdir ( ) : name = path . name if name == "" __pycache__ "" : continue if name . endswith ( "" .py "" ) : names . add ( name . split ( "" . "" ) [ 0 ] ) <MASK> names . add ( name ) if packages : packages = { package . lower ( ) . replace ( "" - "" , "" _ "" ) for package in packages } if len ( names & packages ) == len ( packages ) : return packages return names","elif path . is_dir ( ) and ""."" not in name :","elif name . startswith ( ""py"" ) :",93.24372692,FALSE,90.55
3122,"def sortkeypicker ( keynames ) : negate = set ( ) for i , k in enumerate ( keynames ) : <MASK> keynames [ i ] = k [ 1 : ] negate . add ( k [ 1 : ] ) def getit ( adict ) : composite = [ adict [ k ] for k in keynames ] for i , ( k , v ) in enumerate ( zip ( keynames , composite ) ) : if k in negate : composite [ i ] = - v return composite return getit","if k [ : 1 ] == ""-"" :","if k [ 0 ] == ""-"" :",86.58361482,FALSE,96.64
3123,"def iter_symbols ( code ) : """"""Yield names and strings used by `code` and its nested code objects"""""" for name in code . co_names : yield name for const in code . co_consts : if isinstance ( const , six . string_types ) : yield const <MASK> for name in iter_symbols ( const ) : yield name","elif isinstance ( const , CodeType ) :","elif isinstance ( const , six . symbol_types ) :",91.71904474,FALSE,92.28
3124,"def set_study_directions ( self , study_id : int , directions : Sequence [ StudyDirection ] ) - > None : with self . _lock : <MASK> current_directions = self . _studies [ study_id ] . directions if directions == current_directions : return elif ( len ( current_directions ) == 1 and current_directions [ 0 ] == StudyDirection . NOT_SET ) : self . _studies [ study_id ] . directions = list ( directions ) self . _backend . set_study_directions ( study_id , directions ) return self . _backend . set_study_directions ( study_id , directions )",if study_id in self . _studies :,if study_id in self . _studies :,100,TRUE,100
3125,"def PreprocessConditionalStatement ( self , IfList , ReplacedLine ) : while self : <MASK> x = 1 elif not IfList : if self < = 2 : continue RegionSizeGuid = 3 if not RegionSizeGuid : RegionLayoutLine = 5 continue RegionLayoutLine = self . CurrentLineNumber return 1",if self . __Token :,if ReplacedLine :,89.06361762,FALSE,86.97
3126,"def _check_blocking ( self , current_time ) : if self . _switch_flag is False : active_greenlet = self . _active_greenlet <MASK> self . _notify_greenlet_blocked ( active_greenlet , current_time ) self . _switch_flag = False",if active_greenlet is not None and active_greenlet != self . _hub :,if active_greenlet is not None :,80.78049762,FALSE,83.9
3127,"def detect ( get_page ) : retval = False for vector in WAF_ATTACK_VECTORS : page , headers , code = get_page ( get = vector ) retval = ( re . search ( r "" BlockDos \ .net "" , headers . get ( HTTP_HEADER . SERVER , "" "" ) , re . I ) is not None ) <MASK> break return retval",if retval :,if retval :,100,TRUE,100
3128,"def _fastqc_data_section ( self , section_name ) : out = [ ] in_section = False data_file = os . path . join ( self . _dir , "" fastqc_data.txt "" ) if os . path . exists ( data_file ) : with open ( data_file ) as in_handle : for line in in_handle : <MASK> in_section = True elif in_section : if line . startswith ( "" >>END "" ) : break out . append ( line . rstrip ( "" \r \n "" ) ) return out","if line . startswith ( "">>%s"" % section_name ) :",if line . startswith ( section_name ) :,95.76446175,FALSE,93.59
3129,"def shortcut ( self , input , ch_out , stride , is_first , name ) : ch_in = input . shape [ 1 ] if ch_in != ch_out or stride != 1 : <MASK> return self . conv_bn_layer ( input , ch_out , 1 , stride , name = name ) else : return self . conv_bn_layer_new ( input , ch_out , 1 , stride , name = name ) elif is_first : return self . conv_bn_layer ( input , ch_out , 1 , stride , name = name ) else : return input",if is_first or stride == 1 :,if is_first :,63.77250879,FALSE,95.17
3130,"def get_value_from_string ( self , string_value ) : """"""Return internal representation starting from CFN/user-input value."""""" param_value = self . get_default_value ( ) try : if string_value is not None : string_value = str ( string_value ) . strip ( ) <MASK> param_value = int ( string_value ) except ValueError : self . pcluster_config . warn ( "" Unable to convert the value  ' {0} '  to an Integer.  "" "" Using default value for parameter  ' {1} ' "" . format ( string_value , self . key ) ) return param_value","if string_value != ""NONE"" :",if string_value . isdigit ( ) :,94.71267456,FALSE,95.43
3131,"def get_running ( workers ) : running = [ ] for worker in workers : current_test_name = worker . current_test_name <MASK> continue dt = time . monotonic ( ) - worker . start_time if dt > = PROGRESS_MIN_TIME : text = "" %s  ( %s ) "" % ( current_test_name , format_duration ( dt ) ) running . append ( text ) return running",if not current_test_name :,"if current_test_name in [ ""test"" , ""test_start"" ] :",92.55326125,FALSE,85.59
3132,"def generate_data ( self , request ) : """"""Generate data for the widget."""""" uptime = { } cache_stats = get_cache_stats ( ) if cache_stats : for hosts , stats in cache_stats : if stats [ "" uptime "" ] > 86400 : uptime [ "" value "" ] = stats [ "" uptime "" ] / 60 / 60 / 24 uptime [ "" unit "" ] = _ ( "" days "" ) <MASK> uptime [ "" value "" ] = stats [ "" uptime "" ] / 60 / 60 uptime [ "" unit "" ] = _ ( "" hours "" ) else : uptime [ "" value "" ] = stats [ "" uptime "" ] / 60 uptime [ "" unit "" ] = _ ( "" minutes "" ) return { "" cache_stats "" : cache_stats , "" uptime "" : uptime }","elif stats [ ""uptime"" ] > 3600 :","elif stats [ ""uptime"" ] > 3600 :",100,TRUE,100
3133,"def add_actors ( self ) : """"""Adds `self.actors` to the scene."""""" if not self . _actors_added : self . reader . render_window = self . scene . render_window self . _update_reader ( ) self . _actors_added = True <MASK> self . _visible_changed ( self . visible ) self . scene . render ( )",if not self . visible :,if self . visible :,95.11604405,FALSE,97.1
3134,"def _add_uniqu_suffix ( self , titles ) : counters = dict ( ) titles_with_suffix = [ ] for title in titles : counters [ title ] = counters [ title ] + 1 if title in counters else 1 <MASK> title = f "" { title }  ( { counters [ title ] } ) "" titles_with_suffix . append ( title ) return titles_with_suffix",if counters [ title ] > 1 :,if counters [ title ] > 1 :,100,TRUE,100
3135,"def _verify_udf_resources ( self , job , config ) : udf_resources = config . get ( "" userDefinedFunctionResources "" , ( ) ) self . assertEqual ( len ( job . udf_resources ) , len ( udf_resources ) ) for found , expected in zip ( job . udf_resources , udf_resources ) : <MASK> self . assertEqual ( found . udf_type , "" resourceUri "" ) self . assertEqual ( found . value , expected [ "" resourceUri "" ] ) else : self . assertEqual ( found . udf_type , "" inlineCode "" ) self . assertEqual ( found . value , expected [ "" inlineCode "" ] )","if ""resourceUri"" in expected :","if isinstance ( found . udf_type , str ) :",94.0846807,FALSE,91.69
3136,"def __init__ ( self , layout , value = None , string = None , * , dtype : np . dtype = np . float64 ) - > None : """"""Constructor."""""" self . layout = layout if value is None : <MASK> self . value = np . zeros ( ( self . layout . gaDims , ) , dtype = dtype ) else : self . value = layout . parse_multivector ( string ) . value else : self . value = np . array ( value ) if self . value . shape != ( self . layout . gaDims , ) : raise ValueError ( "" value must be a sequence of length  %s "" % self . layout . gaDims )",if string is None :,if string is None :,100,TRUE,100
3137,"def read_file ( filename , print_error = True ) : """"""Returns the contents of a file."""""" try : for encoding in [ "" utf-8 "" , "" latin1 "" ] : try : with io . open ( filename , encoding = encoding ) as fp : return fp . read ( ) except UnicodeDecodeError : pass except IOError as exception : <MASK> print ( exception , file = sys . stderr ) return None",if print_error :,if print_error :,100,TRUE,100
3138,"def get_albums_for_iter ( self , iter_ ) : obj = self . get_value ( iter_ ) if isinstance ( obj , AlbumNode ) : return { obj . album } albums = set ( ) for child_iter , value in self . iterrows ( iter_ ) : <MASK> albums . add ( value . album ) else : albums . update ( self . get_albums_for_iter ( child_iter ) ) return albums","if isinstance ( value , AlbumNode ) :","if isinstance ( value , AlbumNode ) :",100,TRUE,100
3139,"def wait_til_ready ( cls , connector = None ) : if connector is None : connector = cls . connector while True : now = time . time ( ) next_iteration = now / / 1.0 + 1 <MASK> break else : await cls . _clock . run_til ( next_iteration ) await asyncio . sleep ( 1.0 )",if connector . ready :,if connector . is_ready ( ) :,94.27540723,FALSE,91.67
3140,"def remove_property ( self , key ) : # type: (str) -> None with self . secure ( ) as config : keys = key . split ( "" . "" ) current_config = config for i , key in enumerate ( keys ) : if key not in current_config : return <MASK> del current_config [ key ] break current_config = current_config [ key ]",if i == len ( keys ) - 1 :,if current_config [ key ] == self . default_property :,68.82084927,FALSE,85.24
3141,"def get ( self , hash160 , default = None ) : v = self . p2s_for_hash ( hash160 ) <MASK> return v if hash160 not in self . _secret_exponent_cache : v = self . path_for_hash160 ( hash160 ) <MASK> fingerprint , path = v for key in self . _secrets . get ( fingerprint , [ ] ) : subkey = key . subkey_for_path ( path ) self . _add_key_to_cache ( subkey ) return self . _secret_exponent_cache . get ( hash160 , default )",if v :,if v is not None :,92.29140769,FALSE,92.68
3142,"def fetch_all ( self , api_client , fetchstatuslogger , q , targets ) : self . fetchstatuslogger = fetchstatuslogger if targets != None : # Ensure targets is a tuple <MASK> targets = tuple ( targets , ) elif type ( targets ) != tuple : targets = tuple ( targets ) for target in targets : self . _fetch_targets ( api_client , q , target )",if type ( targets ) != list and type ( targets ) != tuple :,if type ( targets ) == tuple :,93.93097654,FALSE,88.66
3143,"def dgl_mp_batchify_fn ( data ) : if isinstance ( data [ 0 ] , tuple ) : data = zip ( * data ) return [ dgl_mp_batchify_fn ( i ) for i in data ] for dt in data : <MASK> if isinstance ( dt , dgl . DGLGraph ) : return [ d for d in data if isinstance ( d , dgl . DGLGraph ) ] elif isinstance ( dt , nd . NDArray ) : pad = Pad ( axis = ( 1 , 2 ) , num_shards = 1 , ret_length = False ) data_list = [ dt for dt in data if dt is not None ] return pad ( data_list )",if dt is not None :,"if isinstance ( dt , ( list , tuple ) ) :",93.90987882,FALSE,92.31
3144,"def capture_server ( evt , buf , serv ) : try : serv . listen ( 5 ) conn , addr = serv . accept ( ) except socket . timeout : pass else : n = 200 while n > 0 : r , w , e = select . select ( [ conn ] , [ ] , [ ] ) <MASK> data = conn . recv ( 10 ) # keep everything except for the newline terminator buf . write ( data . replace ( "" \n "" , "" "" ) ) if "" \n "" in data : break n - = 1 time . sleep ( 0.01 ) conn . close ( ) finally : serv . close ( ) evt . set ( )",if r :,if r :,100,TRUE,100
3145,"def elem ( ) : if ints_only : return random . randint ( 0 , 10000000000 ) else : t = random . randint ( 0 , 2 ) if t == 0 : return random . randint ( 0 , 10000000000 ) elif t == 1 : return float ( random . randint ( 0 , 10000000000 ) ) <MASK> return strings [ random . randint ( 0 , len ( strings ) - 1 ) ] return random_string ( random . randint ( 100 , 1000 ) )",elif strings is not None :,elif t == 2 :,92.75940907,FALSE,94.45
3146,"def has_changed ( self , initial , data ) : if self . disabled : return False if initial is None : initial = [ "" "" for x in range ( 0 , len ( data ) ) ] else : <MASK> initial = self . widget . decompress ( initial ) for field , initial , data in zip ( self . fields , initial , data ) : try : initial = field . to_python ( initial ) except ValidationError : return True if field . has_changed ( initial , data ) : return True return False","if not isinstance ( initial , list ) :","if isinstance ( initial , list ) :",79.23617208,FALSE,97.77
3147,"def _load_testfile ( filename , package , module_relative ) : if module_relative : package = _normalize_module ( package , 3 ) filename = _module_relative_path ( package , filename ) <MASK> if hasattr ( package . __loader__ , "" get_data "" ) : file_contents = package . __loader__ . get_data ( filename ) # get_data() opens files as 'rb', so one must do the equivalent # conversion as universal newlines would do. return file_contents . replace ( os . linesep , "" \n "" ) , filename return open ( filename ) . read ( ) , filename","if hasattr ( package , ""__loader__"" ) :","if hasattr ( package , ""__loader__"" ) :",100,TRUE,100
3148,"def release ( self ) : tid = _thread . get_ident ( ) with self . lock : if self . owner != tid : raise RuntimeError ( "" cannot release un-acquired lock "" ) assert self . count > 0 self . count - = 1 if self . count == 0 : self . owner = None <MASK> self . waiters - = 1 self . wakeup . release ( )",if self . waiters :,if self . waiters > 0 :,96.72691748,FALSE,95.73
3149,"def stage ( self , x , num_modules , num_blocks , channels , multi_scale_output = True , name = None ) : out = x for i in range ( num_modules ) : <MASK> out = self . high_resolution_module ( out , num_blocks , channels , multi_scale_output = False , name = name + "" _ "" + str ( i + 1 ) , ) else : out = self . high_resolution_module ( out , num_blocks , channels , name = name + "" _ "" + str ( i + 1 ) ) return out",if i == num_modules - 1 and multi_scale_output == False :,if i == num_modules - 1 :,91.78249144,FALSE,92.32
3150,"def changeFrontAlteration ( intV , alter ) : # fati = front alteration transpose interval fati = self . frontAlterationTransposeInterval if fati : newFati = interval . add ( [ fati , intV ] ) self . frontAlterationTransposeInterval = newFati self . frontAlterationAccidental . alter = ( self . frontAlterationAccidental . alter + alter ) <MASK> self . frontAlterationTransposeInterval = None self . frontAlterationAccidental = None else : self . frontAlterationTransposeInterval = intV self . frontAlterationAccidental = pitch . Accidental ( alter )",if self . frontAlterationAccidental . alter == 0 :,if intV == 0 :,71.41975632,FALSE,92.77
3151,"def set_to_train ( self ) : for T in self . trainable_attributes ( ) : for k , v in T . items ( ) : <MASK> c_f . set_requires_grad ( v , requires_grad = False ) v . eval ( ) else : v . train ( ) self . maybe_freeze_trunk_batchnorm ( )",if k in self . freeze_these :,"if k == ""train"" :",67.4551046,FALSE,90.66
3152,"def _migrate ( self , sig = None , compact = True ) : with self . lock : sig = sig or self . sig <MASK> return if sig in self . WORDS and len ( self . WORDS [ sig ] ) > 0 : PostingList . Append ( self . session , sig , self . WORDS [ sig ] , sig = sig , compact = compact ) del self . WORDS [ sig ]",if sig in GPL_NEVER_MIGRATE :,if not self . _is_valid_sig ( sig ) :,93.7858274,FALSE,86
3153,"def on_prediction_step ( self , args , state , control , eval_dataloader = None , * * kwargs ) : if self . prediction_bar is None : <MASK> self . prediction_bar = self . training_tracker . add_child ( len ( eval_dataloader ) ) else : self . prediction_bar = NotebookProgressBar ( len ( eval_dataloader ) ) self . prediction_bar . update ( 1 ) else : self . prediction_bar . update ( self . prediction_bar . value + 1 )",if self . training_tracker is not None :,if self . training_tracker :,94.04405494,FALSE,96.02
3154,"def show ( self , indent = 0 ) : """"""Pretty print this structure."""""" if indent == 0 : print ( "" struct  {} "" . format ( self . name ) ) for field in self . fields : if field . offset is None : offset = "" 0x?? "" else : offset = "" 0x {:02x} "" . format ( field . offset ) print ( "" {} + {}   {}   {} "" . format ( ""   "" * indent , offset , field . name , field . type ) ) <MASK> field . type . show ( indent + 1 )","if isinstance ( field . type , Structure ) :","if hasattr ( field . type , ""show"" ) :",95.14240997,FALSE,94.73
3155,"def __exit__ ( self , exc , value , tb ) : for key in self . overrides . keys ( ) : old_value = self . old [ key ] <MASK> delattr ( self . instance , key ) else : setattr ( self . instance , key , old_value ) self . instance . save ( )",if old_value is NULL :,if old_value is None :,90.70948003,FALSE,96.46
3156,"def complete ( self , block ) : with self . _condition : <MASK> return False if self . _complete ( ) : self . _calculate_state_root_if_not_already_done ( ) return True if block : self . _condition . wait_for ( self . _complete ) self . _calculate_state_root_if_not_already_done ( ) return True return False",if not self . _final :,if self . _condition . wait_for ( self . _complete ) :,83.15782749,FALSE,85.58
3157,"def parseArguments ( self ) : args = [ ] self . expect ( "" ( "" ) if not self . match ( "" ) "" ) : while self . startIndex < self . length : args . append ( self . isolateCoverGrammar ( self . parseAssignmentExpression ) ) <MASK> break self . expectCommaSeparator ( ) self . expect ( "" ) "" ) return args","if self . match ( "")"" ) :","if self . match ( "","" ) :",98.3030794,FALSE,96.65
3158,"def isValidDateString ( config_param_name , value , valid_value ) : try : if value == "" DD-MM-YYYY "" : return value day , month , year = value . split ( "" - "" ) <MASK> raise DateStringValueError ( config_param_name , value ) if int ( month ) < 1 or int ( month ) > 12 : raise DateStringValueError ( config_param_name , value ) if int ( year ) < 1900 or int ( year ) > 2013 : raise DateStringValueError ( config_param_name , value ) return value except Exception : raise DateStringValueError ( config_param_name , value )",if int ( day ) < 1 or int ( day ) > 31 :,if int ( day ) < 10 or int ( day ) > 20 :,97.77533016,FALSE,96.25
3159,"def build_tree ( path ) : tree = Tree ( ) for basename , entry in trees [ path ] . items ( ) : <MASK> mode = stat . S_IFDIR sha = build_tree ( pathjoin ( path , basename ) ) else : ( mode , sha ) = entry tree . add ( basename , mode , sha ) object_store . add_object ( tree ) return tree . id","if isinstance ( entry , dict ) :","if basename . endswith ( "".py"" ) :",92.09449236,FALSE,89.63
3160,"def get_quarantine_count ( self ) : """"""get obj/container/account quarantine counts"""""" qcounts = { "" objects "" : 0 , "" containers "" : 0 , "" accounts "" : 0 } qdir = "" quarantined "" for device in os . listdir ( self . devices ) : for qtype in qcounts : qtgt = os . path . join ( self . devices , device , qdir , qtype ) <MASK> linkcount = os . lstat ( qtgt ) . st_nlink if linkcount > 2 : qcounts [ qtype ] + = linkcount - 2 return qcounts",if os . path . exists ( qtgt ) :,if os . path . isdir ( qtgt ) :,98.81963958,FALSE,97.93
3161,"def _is_static_shape ( self , shape ) : if shape is None or not isinstance ( shape , list ) : return False for dim_value in shape : <MASK> return False if dim_value < 0 : raise Exception ( "" Negative dimension is illegal:  %d "" % dim_value ) return True","if not isinstance ( dim_value , int ) :",if dim_value > 1 :,80.04075615,FALSE,87.44
3162,"def BraceDetectAll ( words ) : # type: (List[compound_word]) -> List[word_t] """"""Return a new list of words, possibly with BracedTree instances."""""" out = [ ] # type: List[word_t] for w in words : # The shortest possible brace expansion is {,}.  This heuristic prevents # a lot of garbage from being created, since otherwise nearly every word # would be checked.  We could be even more precise but this is cheap. if len ( w . parts ) > = 3 : brace_tree = _BraceDetect ( w ) <MASK> out . append ( brace_tree ) continue out . append ( w ) return out",if brace_tree :,if brace_tree :,75,TRUE,100
3163,"def __init__ ( original , self , * args , * * kwargs ) : data = args [ 0 ] if len ( args ) > 0 else kwargs . get ( "" data "" ) if data is not None : try : <MASK> raise Exception ( "" cannot gather example input when dataset is loaded from a file. "" ) input_example_info = _InputExampleInfo ( input_example = deepcopy ( data [ : INPUT_EXAMPLE_SAMPLE_ROWS ] ) ) except Exception as e : input_example_info = _InputExampleInfo ( error_msg = str ( e ) ) setattr ( self , "" input_example_info "" , input_example_info ) original ( self , * args , * * kwargs )","if isinstance ( data , str ) :",if not data . read ( INPUT_EXAMPLE_SAMPLE_ROWS ) :,94.44117126,FALSE,91.88
3164,"def setRow ( self , row , vals ) : if row > self . rowCount ( ) - 1 : self . setRowCount ( row + 1 ) for col in range ( len ( vals ) ) : val = vals [ col ] item = self . itemClass ( val , row ) item . setEditable ( self . editable ) sortMode = self . sortModes . get ( col , None ) <MASK> item . setSortMode ( sortMode ) format = self . _formats . get ( col , self . _formats [ None ] ) item . setFormat ( format ) self . items . append ( item ) self . setItem ( row , col , item ) item . setValue ( val ) # Required--the text-change callback is invoked",if sortMode is not None :,if sortMode is not None :,100,TRUE,100
3165,"def wakeUp ( self ) : """"""Write one byte to the pipe, and flush it."""""" # We don't use fdesc.writeToFD since we need to distinguish # between EINTR (try again) and EAGAIN (do nothing). if self . o is not None : try : util . untilConcludes ( os . write , self . o , b "" x "" ) except OSError as e : # XXX There is no unit test for raising the exception # for other errnos. See #4285. <MASK> raise",if e . errno != errno . EAGAIN :,if e . errno != errno . EINTR :,73.78882641,FALSE,97.95
3166,"def _setup ( self , field_name , owner_model ) : # Resolve possible name-based model references. resolved_classes = [ ] for m in self . model_classes : <MASK> if m == owner_model . __name__ : resolved_classes . append ( owner_model ) else : raise Exception ( "" PolyModelType: Unable to resolve model  ' {} ' . "" . format ( m ) ) else : resolved_classes . append ( m ) self . model_classes = tuple ( resolved_classes ) super ( PolyModelType , self ) . _setup ( field_name , owner_model )","if isinstance ( m , string_type ) :","if isinstance ( m , types . ClassType ) :",73.23322709,FALSE,96.62
3167,"def _wrap_forwarded ( self , key , value ) : if isinstance ( value , SourceCode ) and value . late_binding : # get cached return value if present value_ = self . _late_binding_returnvalues . get ( key , KeyError ) if value_ is KeyError : # evaluate the late-bound function value_ = self . _eval_late_binding ( value ) schema = self . late_bind_schemas . get ( key ) <MASK> value_ = schema . validate ( value_ ) # cache result of late bound func self . _late_binding_returnvalues [ key ] = value_ return value_ else : return value",if schema is not None :,if schema :,97.38571753,FALSE,96.62
3168,"def convert ( self , ctx , argument ) : arg = argument . replace ( "" 0x "" , "" "" ) . lower ( ) if arg [ 0 ] == "" # "" : arg = arg [ 1 : ] try : value = int ( arg , base = 16 ) <MASK> raise BadColourArgument ( arg ) return discord . Colour ( value = value ) except ValueError : arg = arg . replace ( ""   "" , "" _ "" ) method = getattr ( discord . Colour , arg , None ) if arg . startswith ( "" from_ "" ) or method is None or not inspect . ismethod ( method ) : raise BadColourArgument ( arg ) return method ( )",if not ( 0 <= value <= 0xFFFFFF ) :,if value < 0 :,94.56011813,FALSE,92.48
3169,"def get_versions ( * , all = False , quiet = None ) : import bonobo from bonobo . util . pkgs import bonobo_packages yield _format_version ( bonobo , quiet = quiet ) if all : for name in sorted ( bonobo_packages ) : <MASK> try : mod = __import__ ( name . replace ( "" - "" , "" _ "" ) ) try : yield _format_version ( mod , name = name , quiet = quiet ) except Exception as exc : yield "" {}  ( {} ) "" . format ( name , exc ) except ImportError as exc : yield "" {}  is not importable ( {} ). "" . format ( name , exc )","if name != ""bonobo"" :","if name . startswith ( ""bonobo-"" ) :",95.64462353,FALSE,94.59
3170,"def assertOperationsInjected ( self , plan , * * kwargs ) : for migration , _backward in plan : operations = iter ( migration . operations ) for operation in operations : <MASK> next_operation = next ( operations ) self . assertIsInstance ( next_operation , contenttypes_management . RenameContentType ) self . assertEqual ( next_operation . app_label , migration . app_label ) self . assertEqual ( next_operation . old_model , operation . old_name_lower ) self . assertEqual ( next_operation . new_model , operation . new_name_lower )","if isinstance ( operation , migrations . RenameModel ) :",if operation . app_label == migration . app_label :,89.97641872,FALSE,89.61
3171,"def valid_localparts ( strip_delimiters = False ) : for line in ABRIDGED_LOCALPART_VALID_TESTS . split ( "" \n "" ) : # strip line, skip over empty lines line = line . strip ( ) if line == "" "" : continue # skip over comments or empty lines match = COMMENT . match ( line ) if match : continue # skip over localparts with delimiters <MASK> if "" , "" in line or "" ; "" in line : continue yield line",if strip_delimiters :,if strip_delimiters :,100,TRUE,100
3172,"def read_lccn ( line , is_marc8 = False ) : found = [ ] for k , v in get_raw_subfields ( line , [ "" a "" ] ) : lccn = v . strip ( ) if re_question . match ( lccn ) : continue m = re_lccn . search ( lccn ) if not m : continue # remove letters and bad chars lccn = re_letters_and_bad . sub ( "" "" , m . group ( 1 ) ) . strip ( ) <MASK> found . append ( lccn ) return found",if lccn :,if is_marc8 and lccn not in found :,97.56770428,FALSE,92.24
3173,"def test_named_parameters_and_constraints ( self ) : likelihood = gpytorch . likelihoods . GaussianLikelihood ( ) model = ExactGPModel ( None , None , likelihood ) for name , _param , constraint in model . named_parameters_and_constraints ( ) : if name == "" likelihood.noise_covar.raw_noise "" : self . assertIsInstance ( constraint , gpytorch . constraints . GreaterThan ) elif name == "" mean_module.constant "" : self . assertIsNone ( constraint ) elif name == "" covar_module.raw_outputscale "" : self . assertIsInstance ( constraint , gpytorch . constraints . Positive ) <MASK> self . assertIsInstance ( constraint , gpytorch . constraints . Positive )","elif name == ""covar_module.base_kernel.raw_lengthscale"" :","elif name == ""covar_module.raw_outputscale"" :",98.80960374,FALSE,95.25
3174,"def _cleanupSocket ( self ) : """"""Close the Connection's socket."""""" try : self . _sock . shutdown ( socket . SHUT_WR ) except : return try : while True : r , w , e = select . select ( [ self . _sock ] , [ ] , [ ] ) <MASK> break except : pass self . _sock . close ( )",if not r or not self . _sock . recv ( 1024 ) :,if r :,61.17226547,FALSE,83.37
3175,"def fadeIn ( self , acts = None , t = None , duration = None ) : """"""Gradually switch on the input list of meshes by increasing opacity."""""" if self . bookingMode : acts , t , duration , rng = self . _parse ( acts , t , duration ) for tt in rng : alpha = linInterpolate ( tt , [ t , t + duration ] , [ 0 , 1 ] ) self . events . append ( ( tt , self . fadeIn , acts , alpha ) ) else : for a in self . _performers : <MASK> continue a . alpha ( self . _inputvalues ) return self",if a . alpha ( ) >= self . _inputvalues :,if a . _inputvalues is None :,94.08070196,FALSE,93.17
3176,"def get_config_updates_recursive ( self ) : config_updates = self . config_updates . copy ( ) for sr_path , subrunner in self . subrunners . items ( ) : <MASK> continue update = subrunner . get_config_updates_recursive ( ) if update : config_updates [ rel_path ( self . path , sr_path ) ] = update return config_updates","if not is_prefix ( self . path , sr_path ) :",if not subrunner . is_subrun :,85.20161642,FALSE,86.41
3177,"def setArgs ( self , * * kwargs ) : """"""See GridSearchCostGamma"""""" for key , value in list ( kwargs . items ( ) ) : if key in ( "" folds "" , "" nfolds "" ) : self . _n_folds = int ( value ) <MASK> self . _validator_kwargs [ "" max_epochs "" ] = value else : GridSearchDOE . setArgs ( self , * * { key : value } )","elif key in ( ""max_epochs"" ) :","elif key == ""max_epochs"" :",92.07120215,FALSE,93.73
3178,"def _parse_composite_axis ( composite_axis_name : str ) : axes_names = [ axis for axis in composite_axis_name . split ( ""   "" ) if len ( axis ) > 0 ] for axis in axes_names : <MASK> continue assert "" a "" < = axis [ 0 ] < = "" z "" for letter in axis : assert str . isdigit ( letter ) or "" a "" < = letter < = "" z "" return axes_names","if axis == ""_"" :","if not isinstance ( axis , ( list , tuple ) ) :",92.07577648,FALSE,88.58
3179,"def visit_For ( self , node , for_branch = "" body "" , * * kwargs ) : if for_branch == "" body "" : self . sym_visitor . visit ( node . target , store_as_param = True ) branch = node . body elif for_branch == "" else "" : branch = node . else_ elif for_branch == "" test "" : self . sym_visitor . visit ( node . target , store_as_param = True ) <MASK> self . sym_visitor . visit ( node . test ) return else : raise RuntimeError ( "" Unknown for branch "" ) for item in branch or ( ) : self . sym_visitor . visit ( item )",if node . test is not None :,if node . test :,74.76381508,FALSE,97.02
3180,def contains_only_whitespace ( node ) : if is_tag ( node ) : <MASK> if not any ( [ unicode ( s ) . strip ( ) for s in node . contents ] ) : return True return False,if not any ( [ not is_text ( s ) for s in node . contents ] ) :,if node . contents :,50.33606062,FALSE,67.41
3181,"def dir_tag_click ( event ) : mouse_index = self . path_bar . index ( "" @ %d , %d "" % ( event . x , event . y ) ) lineno = int ( float ( mouse_index ) ) if lineno == 1 : self . request_focus_into ( "" "" ) else : assert lineno == 2 dir_range = get_dir_range ( event ) if dir_range : _ , end_index = dir_range path = self . path_bar . get ( "" 2.0 "" , end_index ) <MASK> path + = "" \\ "" self . request_focus_into ( path )","if path . endswith ( "":"" ) :","if path [ - 1 ] != ""\\"" :",95.39580396,FALSE,92.74
3182,"def validate_employee_id ( self ) : if self . employee : sales_person = frappe . db . get_value ( "" Sales Person "" , { "" employee "" : self . employee } ) <MASK> frappe . throw ( _ ( "" Another Sales Person  {0}  exists with the same Employee id "" ) . format ( sales_person ) )",if sales_person and sales_person != self . name :,if sales_person . employee :,62.89982,FALSE,87.49
3183,"def pytest_collection_modifyitems ( items ) : for item in items : if item . nodeid . startswith ( "" tests/infer "" ) : if "" stage "" not in item . keywords : item . add_marker ( pytest . mark . stage ( "" unit "" ) ) <MASK> item . add_marker ( pytest . mark . init ( rng_seed = 123 ) )","if ""init"" not in item . keywords :","if ""init"" not in item . keywords :",100,TRUE,100
3184,"def poll ( self , timeout ) : if timeout < 0 : timeout = None # kqueue behaviour events = self . _kqueue . control ( None , KqueueLoop . MAX_EVENTS , timeout ) results = defaultdict ( lambda : POLL_NULL ) for e in events : fd = e . ident if e . filter == select . KQ_FILTER_READ : results [ fd ] | = POLL_IN <MASK> results [ fd ] | = POLL_OUT return results . items ( )",elif e . filter == select . KQ_FILTER_WRITE :,elif e . filter == select . KQ_FILTER_WRITE :,100,TRUE,100
3185,"def _read_dimensions ( self , * dimnames , * * kwargs ) : path = kwargs . get ( "" path "" , "" / "" ) try : <MASK> return [ self . rootgrp . dimensions [ dname ] for dname in dimnames ] group = self . path2group [ path ] return [ group . dimensions [ dname ] for dname in dimnames ] except KeyError : raise self . Error ( "" In file  %s : \n Error while reading dimensions: ` %s ` with kwargs: ` %s ` "" % ( self . path , dimnames , kwargs ) )","if path == ""/"" :",if path not in self . path2group :,90.90706461,FALSE,94.5
3186,"def spam_to_me ( address ) : sock = eventlet . connect ( address ) while True : try : sock . sendall ( b "" hello world "" ) # Arbitrary delay to not use all available CPU, keeps the test # running quickly and reliably under a second time . sleep ( 0.001 ) except socket . error as e : <MASK> return raise",if get_errno ( e ) == errno . EPIPE :,if e . errno == errno . EINTR :,94.71004912,FALSE,88.07
3187,"def has_hash_of ( self , destpath , code , package_level ) : """"""Determine if a file has the hash of the code."""""" if destpath is not None and os . path . isfile ( destpath ) : with univ_open ( destpath , "" r "" ) as opened : compiled = readfile ( opened ) hashash = gethash ( compiled ) <MASK> return True return False","if hashash is not None and hashash == self . comp . genhash ( code , package_level ) :",if hashash == code :,73.08434606,FALSE,80.02
3188,"def insert ( self , index , item ) : if len ( self . lists ) == 1 : self . lists [ 0 ] . insert ( index , item ) self . _balance_list ( 0 ) else : list_idx , rel_idx = self . _translate_index ( index ) <MASK> raise IndexError ( ) self . lists [ list_idx ] . insert ( rel_idx , item ) self . _balance_list ( list_idx ) return",if list_idx is None :,if list_idx >= len ( self . lists ) :,93.53961304,FALSE,91.06
3189,"def _parse_class_simplified ( symbol ) : results = { } name = symbol . name + "" ( "" name + = "" ,  "" . join ( [ analyzer . expand_attribute ( base ) for base in symbol . bases ] ) name + = "" ) "" for sym in symbol . body : if isinstance ( sym , ast . FunctionDef ) : result = _parse_function_simplified ( sym , symbol . name ) results . update ( result ) <MASK> result = _parse_class_simplified ( sym ) results . update ( result ) lineno = symbol . lineno for decorator in symbol . decorator_list : lineno + = 1 results [ lineno ] = ( name , "" c "" ) return results","elif isinstance ( sym , ast . ClassDef ) :","elif isinstance ( sym , ast . ClassDef ) :",100,TRUE,100
3190,"def append_vars ( pairs , result ) : for name , value in sorted ( pairs . items ( ) ) : if isinstance ( value , list ) : value = "" [ %s ] "" % "" , "" . join ( value ) <MASK> result . append ( "" %s : %s = %s "" % ( package , name , value ) ) else : result . append ( "" %s = %s "" % ( name , value ) )",if package :,if package :,100,TRUE,100
3191,"def nextEditable ( self ) : """"""Moves focus of the cursor to the next editable window"""""" if self . currentEditable is None : <MASK> self . _currentEditableRef = self . _editableChildren [ 0 ] else : for ref in weakref . getweakrefs ( self . currentEditable ) : if ref in self . _editableChildren : cei = self . _editableChildren . index ( ref ) nei = cei + 1 if nei > = len ( self . _editableChildren ) : nei = 0 self . _currentEditableRef = self . _editableChildren [ nei ] return self . currentEditable",if len ( self . _editableChildren ) :,if self . _editableChildren :,88.01290373,FALSE,94.86
3192,"def everythingIsUnicode ( d ) : """"""Takes a dictionary, recursively verifies that every value is unicode"""""" for k , v in d . iteritems ( ) : if isinstance ( v , dict ) and k != "" headers "" : if not everythingIsUnicode ( v ) : return False elif isinstance ( v , list ) : for i in v : if isinstance ( i , dict ) and not everythingIsUnicode ( i ) : return False elif isinstance ( i , _bytes ) : return False <MASK> return False return True","elif isinstance ( v , _bytes ) :",elif not everythingIsUnicode ( i ) :,95.08146349,FALSE,93.36
3193,"def is_valid ( sample ) : if sample is None : return False if isinstance ( sample , tuple ) : for s in sample : if s is None : return False <MASK> return False elif isinstance ( s , collections . abc . Sequence ) and len ( s ) == 0 : return False return True","elif isinstance ( s , np . ndarray ) and s . size == 0 :","elif isinstance ( s , ( list , tuple ) ) and s [ 0 ] != 0",77.41604752,FALSE,82.96
3194,"def scan_resource_conf ( self , conf ) : if "" properties "" in conf : if "" attributes "" in conf [ "" properties "" ] : if "" exp "" in conf [ "" properties "" ] [ "" attributes "" ] : <MASK> return CheckResult . PASSED return CheckResult . FAILED","if conf [ ""properties"" ] [ ""attributes"" ] [ ""exp"" ] :","if conf [ ""properties"" ] [ ""attributes"" ] [ ""exp"" ] :",75,TRUE,100
3195,"def encode ( self ) : if self . expr in gpregs . expr : self . value = gpregs . expr . index ( self . expr ) self . parent . rot2 . value = 0 elif isinstance ( self . expr , ExprOp ) and self . expr . op == allshifts [ 3 ] : reg , value = self . expr . args <MASK> return False self . value = gpregs . expr . index ( reg ) if not isinstance ( value , ExprInt ) : return False value = int ( value ) if not value in [ 8 , 16 , 24 ] : return False self . parent . rot2 . value = value / / 8 return True",if reg not in gpregs . expr :,if reg not in gpregs . expr :,100,TRUE,100
3196,"def validate_transaction_reference ( self ) : bank_account = self . paid_to if self . payment_type == "" Receive "" else self . paid_from bank_account_type = frappe . db . get_value ( "" Account "" , bank_account , "" account_type "" ) if bank_account_type == "" Bank "" : <MASK> frappe . throw ( _ ( "" Reference No and Reference Date is mandatory for Bank transaction "" ) )",if not self . reference_no or not self . reference_date :,"if not frappe . db . get_value ( ""Date"" , """" ) :",86.19301553,FALSE,85.95
3197,"def monad ( self ) : if not self . cls_bl_idname : return None for monad in bpy . data . node_groups : if hasattr ( monad , "" cls_bl_idname "" ) : <MASK> return monad return None",if monad . cls_bl_idname == self . cls_bl_idname :,if monad . cls_bl_idname == self . cls_bl_idname,93.05109606,FALSE,96.04
3198,"def _create_mask ( self , plen ) : mask = [ ] for i in range ( 16 ) : if plen > = 8 : mask . append ( 0xFF ) <MASK> mask . append ( 0xFF >> ( 8 - plen ) << ( 8 - plen ) ) else : mask . append ( 0x00 ) plen - = 8 return mask",elif plen > 0 :,elif i % 2 == 0 :,93.10801643,FALSE,91.35
3199,"def dataset_to_stream ( dataset , input_name ) : """"""Takes a tf.Dataset and creates a numpy stream of ready batches."""""" # All input-pipeline processing should be on CPU. for example in fastmath . dataset_as_numpy ( dataset ) : features = example [ 0 ] inp , out = features [ input_name ] , example [ 1 ] mask = features [ "" mask "" ] if "" mask "" in features else None # Some accelerators don't handle uint8 well, cast to int. <MASK> inp = inp . astype ( np . int32 ) if isinstance ( out , np . uint8 ) : out = out . astype ( np . int32 ) yield ( inp , out ) if mask is None else ( inp , out , mask )","if isinstance ( inp , np . uint8 ) :","if isinstance ( inp , np . uint8 ) :",75,TRUE,100
3200,"def _idle_redraw_cb ( self ) : assert self . _idle_redraw_src_id is not None queue = self . _idle_redraw_queue if len ( queue ) > 0 : bbox = queue . pop ( 0 ) <MASK> super ( CanvasRenderer , self ) . queue_draw ( ) else : super ( CanvasRenderer , self ) . queue_draw_area ( * bbox ) if len ( queue ) == 0 : self . _idle_redraw_src_id = None return False return True",if bbox is None :,if bbox is None :,100,TRUE,100
3201,"def mutated ( self , indiv ) : """"""mutate some genes of the given individual"""""" res = indiv . copy ( ) # to avoid having a child identical to one of the currentpopulation''' for i in range ( self . numParameters ) : <MASK> if self . xBound is None : res [ i ] = indiv [ i ] + gauss ( 0 , self . mutationStdDev ) else : res [ i ] = max ( min ( indiv [ i ] + gauss ( 0 , self . mutationStdDev ) , self . maxs [ i ] ) , self . mins [ i ] , ) return res",if random ( ) < self . mutationProb :,if self . mins [ i ] is not None :,96.41501141,FALSE,92.33
3202,"def _justifyDrawParaLine ( tx , offset , extraspace , words , last = 0 ) : setXPos ( tx , offset ) text = b ""   "" . join ( words ) if last : # last one, left align tx . _textOut ( text , 1 ) else : nSpaces = len ( words ) - 1 <MASK> tx . setWordSpace ( extraspace / float ( nSpaces ) ) tx . _textOut ( text , 1 ) tx . setWordSpace ( 0 ) else : tx . _textOut ( text , 1 ) setXPos ( tx , - offset ) return offset",if nSpaces :,if nSpaces > extraspace :,98.79936021,FALSE,96.82
3203,"def _read_0 ( self , stream ) : r = b "" "" while True : c = stream . read ( 2 ) <MASK> raise EOFError ( ) if c == b "" \x00 \x00 "" : break r + = c return r . decode ( self . encoding )",if len ( c ) != 2 :,if not c :,88.97026217,FALSE,87.02
3204,"def run ( self , app , editor , args ) : line_nums = [ ] for cursor in editor . cursors : <MASK> line_nums . append ( cursor . y ) data = editor . lines [ cursor . y ] . get_data ( ) . upper ( ) editor . lines [ cursor . y ] . set_data ( data )",if cursor . y not in line_nums :,if cursor . y not in line_nums :,100,TRUE,100
3205,"def create_default_energy_point_rules ( ) : for rule in get_default_energy_point_rules ( ) : # check if any rule for ref. doctype exists rule_exists = frappe . db . exists ( "" Energy Point Rule "" , { "" reference_doctype "" : rule . get ( "" reference_doctype "" ) } ) <MASK> continue doc = frappe . get_doc ( rule ) doc . insert ( ignore_permissions = True )",if rule_exists :,if not rule_exists :,73.53259058,FALSE,97.44
3206,"def __new__ ( cls , * nodes ) : if not nodes : raise TypeError ( "" DisjunctionNode() requires at least one node "" ) elif len ( nodes ) == 1 : return nodes [ 0 ] self = super ( DisjunctionNode , cls ) . __new__ ( cls ) self . __nodes = [ ] # TODO: Remove duplicates? for node in nodes : if not isinstance ( node , Node ) : raise TypeError ( "" DisjunctionNode() expects Node instances as arguments; "" ""  received a non-Node instance  %r "" % node ) <MASK> self . __nodes . extend ( node . __nodes ) else : self . __nodes . append ( node ) return self","if isinstance ( node , DisjunctionNode ) :","if hasattr ( node , ""__nodes"" ) :",98.00613036,FALSE,94.11
3207,def dfs ( v : str ) - > Iterator [ Set [ str ] ] : index [ v ] = len ( stack ) stack . append ( v ) boundaries . append ( index [ v ] ) for w in edges [ v ] : if w not in index : yield from dfs ( w ) <MASK> while index [ w ] < boundaries [ - 1 ] : boundaries . pop ( ) if boundaries [ - 1 ] == index [ v ] : boundaries . pop ( ) scc = set ( stack [ index [ v ] : ] ) del stack [ index [ v ] : ] identified . update ( scc ) yield scc,elif w not in identified :,if boundaries [ - 1 ] :,94.08363437,FALSE,94.28
3208,"def unpack_item_obj ( map_uuid_global_id , misp_obj ) : obj_meta = get_object_metadata ( misp_obj ) obj_id = None io_content = None for attribute in misp_obj . attributes : <MASK> obj_id = attribute . value # # TODO: sanitize io_content = attribute . data # # TODO: check if type == io if obj_id and io_content : res = Item . create_item ( obj_id , obj_meta , io_content ) map_uuid_global_id [ misp_obj . uuid ] = get_global_id ( "" item "" , obj_id )","if attribute . object_relation == ""raw-data"" :","if attribute . name == ""id"" :",97.56393663,FALSE,95.33
3209,"def parse ( self , response ) : soup = BeautifulSoup ( response . content . decode ( "" utf-8 "" , "" ignore "" ) , "" lxml "" ) image_divs = soup . find_all ( "" div "" , class_ = "" imgpt "" ) pattern = re . compile ( r "" murl \"" : \"" (.*?) \ .jpg "" ) for div in image_divs : href_str = html_parser . HTMLParser ( ) . unescape ( div . a [ "" m "" ] ) match = pattern . search ( href_str ) <MASK> name = match . group ( 1 ) if six . PY3 else match . group ( 1 ) . encode ( "" utf-8 "" ) img_url = "" {} .jpg "" . format ( name ) yield dict ( file_url = img_url )",if match :,if match :,100,TRUE,100
3210,"def filter_errors ( self , errors : List [ str ] ) - > List [ str ] : real_errors : List [ str ] = list ( ) current_file = __file__ current_path = os . path . split ( current_file ) for line in errors : line = line . strip ( ) if not line : continue fn , lno , lvl , msg = self . parse_trace_line ( line ) <MASK> _path = os . path . split ( fn ) if _path [ - 1 ] != current_path [ - 1 ] : continue real_errors . append ( line ) return real_errors",if fn is not None :,if fn :,96.16257958,FALSE,96.69
3211,"def decompileFormat1 ( self , reader , otFont ) : self . classDefs = classDefs = [ ] startGlyphID = reader . readUShort ( ) glyphCount = reader . readUShort ( ) for i in range ( glyphCount ) : glyphName = otFont . getglyphName ( startGlyphID + i ) classValue = reader . readUShort ( ) <MASK> classDefs . append ( ( glyphName , classValue ) )",if classValue :,if classValue :,100,TRUE,100
3212,"def compress ( self , data_list ) : if len ( data_list ) == 2 : value , lookup_expr = data_list <MASK> if lookup_expr not in EMPTY_VALUES : return Lookup ( value = value , lookup_expr = lookup_expr ) else : raise forms . ValidationError ( self . error_messages [ "" lookup_required "" ] , code = "" lookup_required "" ) return None",if value not in EMPTY_VALUES :,if value is not None :,90.08681295,FALSE,93.15
3213,"def open_compat ( path , mode = "" r "" ) : if mode in [ "" r "" , "" rb "" ] and not os . path . exists ( path ) : raise FileNotFoundError ( u ' The file  "" %s ""  could not be found ' % path ) if sys . version_info > = ( 3 , ) : encoding = "" utf-8 "" errors = "" replace "" <MASK> encoding = None errors = None return open ( path , mode , encoding = encoding , errors = errors ) else : return open ( path , mode )","if mode in [ ""rb"" , ""wb"" , ""ab"" ] :","if mode in [ ""r"" , ""rb"" ] :",93.98831421,FALSE,93.51
3214,"def filter_errors ( self , errors : List [ str ] ) - > List [ str ] : real_errors : List [ str ] = list ( ) current_file = __file__ current_path = os . path . split ( current_file ) for line in errors : line = line . strip ( ) if not line : continue fn , lno , lvl , msg = self . parse_trace_line ( line ) if fn is not None : _path = os . path . split ( fn ) <MASK> continue real_errors . append ( line ) return real_errors",if _path [ - 1 ] != current_path [ - 1 ] :,if _path != current_path :,91.98857657,FALSE,91.81
3215,"def filter_by_level ( record , level_per_module ) : name = record [ "" name "" ] level = 0 if name in level_per_module : level = level_per_module [ name ] elif name is not None : lookup = "" "" if "" "" in level_per_module : level = level_per_module [ "" "" ] for n in name . split ( "" . "" ) : lookup + = n <MASK> level = level_per_module [ lookup ] lookup + = "" . "" if level is False : return False return record [ "" level "" ] . no > = level",if lookup in level_per_module :,if lookup in level_per_module :,100,TRUE,100
3216,"def CountButtons ( self ) : """"""Returns the number of visible buttons in the docked pane."""""" n = 0 if self . HasCaption ( ) or self . HasCaptionLeft ( ) : if isinstance ( wx . GetTopLevelParent ( self . window ) , AuiFloatingFrame ) : return 1 if self . HasCloseButton ( ) : n + = 1 <MASK> n + = 1 if self . HasMinimizeButton ( ) : n + = 1 if self . HasPinButton ( ) : n + = 1 return n",if self . HasMaximizeButton ( ) :,if self . HasMinimizeButton ( ) :,98.67233756,FALSE,97.49
3217,"def search ( a , b , desired ) : if a == b : return a if abs ( b - a ) < 0.005 : ca = count ( a ) cb = count ( b ) dista = abs ( desired - ca ) distb = abs ( desired - cb ) <MASK> return a else : return b m = ( a + b ) / 2.0 cm = count ( m ) if desired < cm : return search ( m , b , desired ) else : return search ( a , m , desired )",if dista < distb :,if dista < distb :,100,TRUE,100
3218,"def force_ipv4 ( self , * args ) : """"""only ipv4 localhost in /etc/hosts"""""" logg . debug ( "" checking /etc/hosts for  ' ::1 localhost ' "" ) lines = [ ] for line in open ( self . etc_hosts ( ) ) : <MASK> newline = re . sub ( "" \\ slocalhost \\ s "" , ""   "" , line ) if line != newline : logg . info ( "" /etc/hosts:  ' %s '  =>  ' %s ' "" , line . rstrip ( ) , newline . rstrip ( ) ) line = newline lines . append ( line ) f = open ( self . etc_hosts ( ) , "" w "" ) for line in lines : f . write ( line ) f . close ( )","if ""::1"" in line :","if line . startswith ( ""ipv4"" ) :",96.20854477,FALSE,94.88
3219,"def aiter_cogs ( cls ) - > AsyncIterator [ Tuple [ str , str ] ] : yield "" Core "" , "" 0 "" for _dir in data_manager . cog_data_path ( ) . iterdir ( ) : fpath = _dir / "" settings.json "" <MASK> continue with fpath . open ( ) as f : try : data = json . load ( f ) except json . JSONDecodeError : continue if not isinstance ( data , dict ) : continue cog_name = _dir . stem for cog_id , inner in data . items ( ) : if not isinstance ( inner , dict ) : continue yield cog_name , cog_id",if not fpath . exists ( ) :,if not fpath . exists ( ) :,100,TRUE,100
3220,"def _get_dbutils ( ) : try : import IPython ip_shell = IPython . get_ipython ( ) <MASK> raise _NoDbutilsError return ip_shell . ns_table [ "" user_global "" ] [ "" dbutils "" ] except ImportError : raise _NoDbutilsError except KeyError : raise _NoDbutilsError",if ip_shell is None :,"if ""dbutils"" not in ip_shell . ns_table [ ""user_global",76.43068612,FALSE,77.06
3221,"def _bytecode_filenames ( self , py_filenames ) : bytecode_files = [ ] for py_file in py_filenames : # Since build_py handles package data installation, the # list of outputs can contain more than just .py files. # Make sure we only report bytecode for the .py files. ext = os . path . splitext ( os . path . normcase ( py_file ) ) [ 1 ] <MASK> continue if self . compile : bytecode_files . append ( py_file + "" c "" ) if self . optimize > 0 : bytecode_files . append ( py_file + "" o "" ) return bytecode_files",if ext != PYTHON_SOURCE_EXTENSION :,"if ext . startswith ( "".py"" ) :",97.69298487,FALSE,93.28
3222,"def compute_distances_mu ( line , pts , result , gates , tolerance ) : """"""calculate all distances with mathuutils"""""" line_origin = V ( line [ 0 ] ) line_end = V ( line [ - 1 ] ) local_result = [ [ ] , [ ] , [ ] , [ ] , [ ] ] for point in pts : data = compute_distance ( V ( point ) , line_origin , line_end , tolerance ) for i , res in enumerate ( local_result ) : res . append ( data [ i ] ) for i , res in enumerate ( result ) : <MASK> res . append ( local_result [ i ] )",if gates [ i ] :,if i < gates :,95.75851805,FALSE,96.53
3223,"def _get_next_segment ( self , segment_path , page_size , segment_cursor = None ) : if segment_path : <MASK> return None return Segment ( self . client , segment_path , page_size , segment_cursor ) return None",if self . end_time and self . _is_later_than_end_time ( segment_path ) :,if not self . client . get_segment ( segment_path ) :,79.35361543,FALSE,75.47
3224,"def _check_number_of_sessions ( ) : nb_desktop_sessions = sessions . get_number_of_desktop_sessions ( ignore_gdm = True ) if nb_desktop_sessions > 1 : print ( "" WARNING : There are  %d  other desktop sessions open. The GPU switch will not become effective until you have manually "" ""  logged out from ALL desktop sessions. \n "" "" Continue ? (y/N) "" % ( nb_desktop_sessions - 1 ) ) confirmation = ask_confirmation ( ) <MASK> sys . exit ( 0 )",if not confirmation :,if not confirmation :,100,TRUE,100
3225,"def delete_compute_environment ( self , compute_environment_name ) : if compute_environment_name is None : raise InvalidParameterValueException ( "" Missing computeEnvironment parameter "" ) compute_env = self . get_compute_environment ( compute_environment_name ) if compute_env is not None : # Pop ComputeEnvironment self . _compute_environments . pop ( compute_env . arn ) # Delete ECS cluster self . ecs_backend . delete_cluster ( compute_env . ecs_name ) <MASK> # Delete compute environment instance_ids = [ instance . id for instance in compute_env . instances ] self . ec2_backend . terminate_instances ( instance_ids )","if compute_env . env_type == ""MANAGED"" :",if compute_env . instances is not None :,97.39500229,FALSE,93.54
3226,"def run ( self ) : results = { } for func_name in [ # Execute every function starting with check_* fn for fn in self . check_functions # if the user does not specify any name if not self . args . get ( "" check "" ) # of if specify the current function name or self . args . get ( "" check "" ) == fn ] : function = getattr ( self , func_name ) log . warn ( function . __doc__ ) result = function ( ) <MASK> log . info ( "" \n "" . join ( result ) ) results . update ( { func_name : result } ) return results",if result :,if result :,75,TRUE,100
3227,"def invalidate ( self , layers = None ) : if layers is None : layers = Layer . AllLayers if layers : layers = set ( layers ) self . invalidLayers . update ( layers ) blockRenderers = [ br for br in self . blockRenderers <MASK> ] if len ( blockRenderers ) < len ( self . blockRenderers ) : self . forgetDisplayLists ( ) self . blockRenderers = blockRenderers if self . renderer . showRedraw and Layer . Blocks in layers : self . needsRedisplay = True",if br . layer is Layer . Blocks or br . layer not in layers,if br . layers and br . layers . contains ( layers ),87.69595893,FALSE,87.99
3228,"def get_library_dirs ( platform , arch = None ) : if platform == "" win32 "" : jre_home = get_jre_home ( platform ) jdk_home = JAVA_HOME <MASK> jre_home = jre_home . decode ( "" utf-8 "" ) return [ join ( jdk_home , "" lib "" ) , join ( jdk_home , "" bin "" , "" server "" ) ] elif platform == "" android "" : return [ "" libs/ {} "" . format ( arch ) ] return [ ]","if isinstance ( jre_home , bytes ) :","if isinstance ( jre_home , bytes ) :",100,TRUE,100
3229,"def save_plugin_options ( self ) : for name , option_widgets in self . _plugin_option_widgets . items ( ) : <MASK> self . config [ "" plugins "" ] [ name ] = { } plugin_config = self . config [ "" plugins "" ] [ name ] # use or instead of get incase the value is actually None for option_name , option_widget in option_widgets . items ( ) : plugin_config [ option_name ] = option_widget . option . get_widget_value ( option_widget . widget )","if name not in self . config [ ""plugins"" ] :","if name not in self . config [ ""plugins"" ] :",100,TRUE,100
3230,"def _select_block ( str_in , start_tag , end_tag ) : """"""Select first block delimited by start_tag and end_tag"""""" start_pos = str_in . find ( start_tag ) if start_pos < 0 : raise ValueError ( "" start_tag not found "" ) depth = 0 for pos in range ( start_pos , len ( str_in ) ) : if str_in [ pos ] == start_tag : depth + = 1 elif str_in [ pos ] == end_tag : depth - = 1 <MASK> break sel = str_in [ start_pos + 1 : pos ] return sel",if depth == 0 :,if depth == 0 :,100,TRUE,100
3231,"def _coerce_to_bool ( self , node , var , true_val = True ) : """"""Coerce the values in a variable to bools."""""" bool_var = self . program . NewVariable ( ) for b in var . bindings : v = b . data if isinstance ( v , mixin . PythonConstant ) and isinstance ( v . pyval , bool ) : const = v . pyval is true_val elif not compare . compatible_with ( v , True ) : const = not true_val <MASK> const = true_val else : const = None bool_var . AddBinding ( self . convert . bool_values [ const ] , { b } , node ) return bool_var","elif not compare . compatible_with ( v , False ) :","elif compare . compatible_with ( v , False ) :",98.86639442,FALSE,98.38
3232,def multiline_indentation ( self ) : if self . _multiline_indentation is None : offset = 0 <MASK> offset = 2 indentation = make_indentation ( 3 * self . indent_size + offset ) self . _multiline_indentation = indentation if self . current_rule : indent_extra = make_indentation ( self . indent_size ) return self . _multiline_indentation + indent_extra return self . _multiline_indentation,if self . show_aligned_keywords :,if self . current_rule . multiline_indent :,75.50872202,FALSE,91.67
3233,"def __call__ ( self , event , data = None ) : datatype , delta = event self . midi_ctrl . delta + = delta if TIMING_CLOCK in datatype and not self . played : self . midi_ctrl . pulse + = 1 <MASK> t_master = 60.0 self . midi_ctrl . bpm = round ( 60.0 / self . midi_ctrl . delta , 0 ) self . midi_ctrl . pulse = 0 self . midi_ctrl . delta = 0.0",if self . midi_ctrl . pulse == self . midi_ctrl . ppqn :,if self . midi_ctrl . pulse == 0 :,92.08835342,FALSE,92.49
3234,"def handle_sent ( self , elt ) : sent = [ ] for child in elt : <MASK> itm = self . handle_word ( child ) if self . _unit == "" word "" : sent . extend ( itm ) else : sent . append ( itm ) else : raise ValueError ( "" Unexpected element  %s "" % child . tag ) return SemcorSentence ( elt . attrib [ "" snum "" ] , sent )","if child . tag in ( ""wf"" , ""punc"" ) :","if child . tag == ""sent"" :",90.92469301,FALSE,88.62
3235,"def _handle_def_errors ( testdef ) : # If the test generation had an error, raise if testdef . error : <MASK> if isinstance ( testdef . exception , Exception ) : raise testdef . exception else : raise Exception ( testdef . exception ) else : raise Exception ( "" Test parse failure "" )",if testdef . exception :,if testdef . exception :,75,TRUE,100
3236,"def _authorized_sid ( self , jid , sid , ifrom , iq ) : with self . _preauthed_sids_lock : <MASK> del self . _preauthed_sids [ ( jid , sid , ifrom ) ] return True return False","if ( jid , sid , ifrom ) in self . _preauthed_sids :","if ( jid , sid , ifrom ) in self . _preauthed_sids",97.38578954,FALSE,95.91
3237,"def wait ( self , timeout = None ) : if self . returncode is None : <MASK> msecs = _subprocess . INFINITE else : msecs = max ( 0 , int ( timeout * 1000 + 0.5 ) ) res = _subprocess . WaitForSingleObject ( int ( self . _handle ) , msecs ) if res == _subprocess . WAIT_OBJECT_0 : code = _subprocess . GetExitCodeProcess ( self . _handle ) if code == TERMINATE : code = - signal . SIGTERM self . returncode = code return self . returncode",if timeout is None :,if timeout is None :,100,TRUE,100
3238,"def _gen_legal_y_s_t ( self ) : while True : y = self . _gen_random_scalar ( ) s = self . tec_arithmetic . mul ( scalar = y , a = self . tec_arithmetic . get_generator ( ) ) # S = yG t = self . _hash_tec_element ( s ) <MASK> # Both S and T are legal LOGGER . info ( "" randomly generated y, S, T "" ) return y , s , t",if self . tec_arithmetic . is_in_group ( s ) and type ( t ) != int :,if t != self . _hash_tec_element ( s ) :,92.20761795,FALSE,84.45
3239,"def write_out ( ) : while True : <MASK> time . sleep ( 0.1 ) continue data_str = self . instrument_queue . get ( ) data_str = data_str . splitlines ( ) tb . write ( "" "" ) # position cursor to end for line in data_str : tb . write ( line ) tb . write ( "" \n "" )",if self . instrument_queue . empty ( ) :,if self . instrument_queue . empty ( ) :,100,TRUE,100
3240,"def _parse_preamble ( self ) : """"""Parse metadata about query (PRIVATE)."""""" meta = { } while self . line : regx = re . search ( _RE_QUERY , self . line ) if regx : self . query_id = regx . group ( 1 ) <MASK> self . seq_len = int ( self . line . strip ( ) . split ( ) [ 1 ] ) self . line = self . handle . readline ( ) . strip ( ) return meta","if self . line . startswith ( ""Match_columns"" ) :","if self . query_id == ""seq_len"" :",83.71909925,FALSE,90.57
3241,"def init_sequence ( self , coll_name , seq_config ) : if not isinstance ( seq_config , list ) : raise Exception ( ' "" sequence ""  config must be a list ' ) handlers = [ ] for entry in seq_config : <MASK> raise Exception ( ' "" sequence ""  entry must be a dict ' ) name = entry . get ( "" name "" , "" "" ) handler = self . load_coll ( name , entry ) handlers . append ( handler ) return HandlerSeq ( handlers )","if not isinstance ( entry , dict ) :","if not isinstance ( entry , dict ) :",100,TRUE,100
3242,"def change_args_to_dict ( string ) : if string is None : return None ans = [ ] strings = string . split ( "" \n "" ) ind = 1 start = 0 while ind < = len ( strings ) : if ind < len ( strings ) and strings [ ind ] . startswith ( ""   "" ) : ind + = 1 else : <MASK> ans . append ( "" \n "" . join ( strings [ start : ind ] ) ) start = ind ind + = 1 d = { } for line in ans : if "" : "" in line and len ( line ) > 0 : lines = line . split ( "" : "" ) d [ lines [ 0 ] ] = lines [ 1 ] . strip ( ) return d",if start < ind :,if start :,97.06771198,FALSE,97.73
3243,"def wait ( self ) : while True : return_code = self . _process . poll ( ) if return_code is not None : line = self . _process . stdout . readline ( ) . decode ( "" utf-8 "" ) <MASK> break log . debug ( line . strip ( "" \n "" ) ) return True","if line == """" :",if not line :,93.55917161,FALSE,91.07
3244,"def __getattr__ ( self , key ) : for tag in self . tag . children : <MASK> continue if "" name "" in tag . attrs and tag . attrs [ "" name "" ] in ( key , ) : from thug . DOM . W3C . Core . DOMImplementation import DOMImplementation return DOMImplementation . createHTMLElement ( self . doc , tag ) raise AttributeError","if tag . name not in ( ""input"" , ) :","if tag . tag_name != ""input"" :",90.88072376,FALSE,89.17
3245,"def compare_hash ( hash_of_gold , path_to_file ) : with open ( path_to_file , "" rb "" ) as f : hash_of_file = hashlib . sha256 ( f . read ( ) ) . hexdigest ( ) <MASK> print ( "" ########## Hash sum of "" , path_to_file , "" differs from the target, the topology will be deleted !!! ########## "" , ) shutil . rmtree ( os . path . dirname ( path_to_file ) )",if hash_of_file != hash_of_gold :,if hash_of_file != hash_of_gold :,100,TRUE,100
3246,def on_completed2 ( ) : doner [ 0 ] = True if not qr : <MASK> observer . on_next ( False ) observer . on_completed ( ) elif donel [ 0 ] : observer . on_next ( True ) observer . on_completed ( ),if len ( ql ) > 0 :,if doner [ 1 ] :,80.15152952,FALSE,87.36
3247,"def get_other ( self , data , items ) : is_tuple = False if type ( data ) == tuple : data = list ( data ) is_tuple = True if type ( data ) == list : m_items = items . copy ( ) for idx , item in enumerate ( items ) : <MASK> m_items [ idx ] = len ( data ) - abs ( item ) for i in sorted ( set ( m_items ) , reverse = True ) : if i < len ( data ) and i > - 1 : del data [ i ] if is_tuple : return tuple ( data ) else : return data else : return None",if item < 0 :,if abs ( item ) < 0 :,96.33931481,FALSE,96.25
3248,"def _open_url ( cls , url ) : if config . browser : cmd = [ config . browser , url ] <MASK> print ( "" running command:  %s "" % ""   "" . join ( cmd ) ) p = Popen ( cmd ) p . communicate ( ) else : <MASK> print ( "" opening URL in browser:  %s "" % url ) webbrowser . open_new ( url )",if not config . quiet :,if verbose :,85.94324013,FALSE,87.39
3249,"def setLabel ( self , s , protect = False ) : """"""Set the label of the minibuffer."""""" c , k , w = self . c , self , self . w if w : # Support for the curses gui. if hasattr ( g . app . gui , "" set_minibuffer_label "" ) : g . app . gui . set_minibuffer_label ( c , s ) w . setAllText ( s ) n = len ( s ) w . setSelectionRange ( n , n , insert = n ) <MASK> k . mb_prefix = s",if protect :,if protect :,100,TRUE,100
3250,"def __init__ ( self , path ) : self . symcaches = [ ] for path in path . split ( "" ; "" ) : if os . path . isdir ( path ) : self . symcaches . append ( SymbolCache ( dirname = path ) ) continue <MASK> import cobra self . symcaches . append ( cobra . CobraProxy ( path ) ) continue","if path . startswith ( ""cobra://"" ) or path . startswith ( ""cobrassl://"" ) :","if os . name == ""nt"" :",89.08313242,FALSE,73.59
3251,"def init_params ( net ) : """"""Init layer parameters."""""" for module in net . modules ( ) : if isinstance ( module , nn . Conv2d ) : init . kaiming_normal ( module . weight , mode = "" fan_out "" ) if module . bias : init . constant ( module . bias , 0 ) <MASK> init . constant ( module . weight , 1 ) init . constant ( module . bias , 0 ) elif isinstance ( module , nn . Linear ) : init . normal ( module . weight , std = 1e-3 ) if module . bias : init . constant ( module . bias , 0 )","elif isinstance ( module , nn . BatchNorm2d ) :","elif isinstance ( module , nn . BatchNorm2d ) :",100,TRUE,100
3252,"def _diff_dict ( self , old , new ) : diff = { } removed = [ ] added = [ ] for key , value in old . items ( ) : <MASK> removed . append ( key ) elif old [ key ] != new [ key ] : # modified is indicated by a remove and add removed . append ( key ) added . append ( key ) for key , value in new . items ( ) : if key not in old : added . append ( key ) if removed : diff [ "" removed "" ] = sorted ( removed ) if added : diff [ "" added "" ] = sorted ( added ) return diff",if key not in new :,if key not in new :,100,TRUE,100
3253,"def __init__ ( self , * args , * * kwargs ) : _kwargs = { "" max_length "" : 20 , "" widget "" : forms . TextInput ( attrs = { "" autocomplete "" : "" off "" } ) , "" label "" : _ ( "" Card number "" ) , } if "" types "" in kwargs : self . accepted_cards = set ( kwargs . pop ( "" types "" ) ) difference = self . accepted_cards - VALID_CARDS <MASK> raise ImproperlyConfigured ( "" The following accepted_cards are  "" "" unknown:  %s "" % difference ) _kwargs . update ( kwargs ) super ( ) . __init__ ( * args , * * _kwargs )",if difference :,if difference not in VALID_CARDS :,97.60941361,FALSE,95.64
3254,"def dumps ( self ) : sections = [ ] for name , env_info in self . _dependencies_ . items ( ) : sections . append ( "" [ENV_ %s ] "" % name ) for var , values in sorted ( env_info . vars . items ( ) ) : tmp = "" %s = "" % var <MASK> tmp + = "" [ %s ] "" % "" , "" . join ( [ ' "" %s "" ' % val for val in values ] ) else : tmp + = "" %s "" % values sections . append ( tmp ) return "" \n "" . join ( sections )","if isinstance ( values , list ) :","if isinstance ( values , ( list , tuple ) ) :",88.86203274,FALSE,95.44
3255,"def air_quality ( self ) : aqi_data = self . _get_aqi_data ( ) if aqi_data : if aqi_data . get ( "" status "" ) == "" ok "" : aqi_data = self . _organize ( aqi_data ) aqi_data = self . _manipulate ( aqi_data ) <MASK> self . py3 . error ( aqi_data . get ( "" data "" ) ) return { "" cached_until "" : self . py3 . time_in ( self . cache_timeout ) , "" full_text "" : self . py3 . safe_format ( self . format , aqi_data ) , }","elif aqi_data . get ( ""status"" ) == ""error"" :","if not aqi_data . get ( ""data"" ) :",94.60657326,FALSE,92.43
3256,"def _blend ( x , y ) : # pylint: disable=invalid-name """"""Implements the ""blend"" strategy for `deep_merge`."""""" if isinstance ( x , ( dict , OrderedDict ) ) : if not isinstance ( y , ( dict , OrderedDict ) ) : return y return _merge ( x , y , recursion_func = _blend ) if isinstance ( x , ( list , tuple ) ) : <MASK> return y result = [ _blend ( * i ) for i in zip ( x , y ) ] if len ( x ) > len ( y ) : result + = x [ len ( y ) : ] elif len ( x ) < len ( y ) : result + = y [ len ( x ) : ] return result return y","if not isinstance ( y , ( list , tuple ) ) :","if not isinstance ( y , ( list , tuple ) ) :",75,TRUE,100
3257,"def _rate ( cls , sample1 , sample2 ) : "" Simple rate "" try : interval = sample2 [ 0 ] - sample1 [ 0 ] <MASK> raise Infinity ( ) delta = sample2 [ 1 ] - sample1 [ 1 ] if delta < 0 : raise UnknownValue ( ) return ( sample2 [ 0 ] , delta / interval , sample2 [ 2 ] , sample2 [ 3 ] ) except Infinity : raise except UnknownValue : raise except Exception as e : raise NaN ( e )",if interval == 0 :,if interval > 1 :,98.16474962,FALSE,95.28
3258,"def wrapped_request_method ( * args , * * kwargs ) : """"""Modifies HTTP headers to include a specified user-agent."""""" if kwargs . get ( "" headers "" ) is not None : <MASK> if user_agent not in kwargs [ "" headers "" ] [ "" user-agent "" ] : # Save the existing user-agent header and tack on our own. kwargs [ "" headers "" ] [ "" user-agent "" ] = ( f "" { user_agent }   "" f ' { kwargs [ "" headers "" ] [ "" user-agent "" ] } ' ) else : kwargs [ "" headers "" ] [ "" user-agent "" ] = user_agent else : kwargs [ "" headers "" ] = { "" user-agent "" : user_agent } return request_method ( * args , * * kwargs )","if kwargs [ ""headers"" ] . get ( ""user-agent"" ) :","if ""user-agent"" in kwargs [ ""headers"" ] :",85.18481573,FALSE,95.01
3259,"def remove_addons ( auth , resource_object_list ) : for config in AbstractNode . ADDONS_AVAILABLE : try : settings_model = config . node_settings except LookupError : settings_model = None <MASK> addon_list = settings_model . objects . filter ( owner__in = resource_object_list , is_deleted = False ) for addon in addon_list : addon . after_delete ( auth . user )",if settings_model :,if settings_model :,100,TRUE,100
3260,"def Decorator ( * args , * * kwargs ) : delay = 0.2 num_attempts = 15 cur_attempt = 0 while True : try : return f ( * args , * * kwargs ) except exceptions . WebDriverException as e : logging . warning ( "" Selenium raised  %s "" , utils . SmartUnicode ( e ) ) cur_attempt + = 1 <MASK> raise time . sleep ( delay )",if cur_attempt == num_attempts :,if cur_attempt >= num_attempts :,98.30734653,FALSE,97.05
3261,"def _cleanup_parts_dir ( parts_dir , local_plugins_dir , parts ) : if os . path . exists ( parts_dir ) : logger . info ( "" Cleaning up parts directory "" ) for subdirectory in os . listdir ( parts_dir ) : path = os . path . join ( parts_dir , subdirectory ) <MASK> try : shutil . rmtree ( path ) except NotADirectoryError : os . remove ( path ) for part in parts : part . mark_cleaned ( steps . BUILD ) part . mark_cleaned ( steps . PULL )",if path != local_plugins_dir :,if os . path . isdir ( path ) :,89.69516033,FALSE,92.3
3262,"def traverse_trees ( node_pos , sample , trees : List [ HeteroDecisionTreeGuest ] ) : if node_pos [ "" reach_leaf_node "" ] . all ( ) : return node_pos for t_idx , tree in enumerate ( trees ) : cur_node_idx = node_pos [ "" node_pos "" ] [ t_idx ] # reach leaf <MASK> continue rs , reach_leaf = HeteroSecureBoostingTreeGuest . traverse_a_tree ( tree , sample , cur_node_idx ) if reach_leaf : node_pos [ "" reach_leaf_node "" ] [ t_idx ] = True node_pos [ "" node_pos "" ] [ t_idx ] = rs return node_pos",if cur_node_idx == - 1 :,if cur_node_idx == 0 :,98.26810795,FALSE,97.76
3263,"def get_measurements ( self , pipeline , object_name , category ) : if self . get_categories ( pipeline , object_name ) == [ category ] : results = [ ] if self . do_corr_and_slope : if object_name == "" Image "" : results + = [ "" Correlation "" , "" Slope "" ] else : results + = [ "" Correlation "" ] if self . do_overlap : results + = [ "" Overlap "" , "" K "" ] <MASK> results + = [ "" Manders "" ] if self . do_rwc : results + = [ "" RWC "" ] if self . do_costes : results + = [ "" Costes "" ] return results return [ ]",if self . do_manders :,if self . do_manders :,100,TRUE,100
3264,"def create_connection ( self , infos , f2 , laddr_infos , protocol ) : for family in infos : try : <MASK> for laddr in laddr_infos : try : break except OSError : protocol = "" foo "" else : continue except OSError : protocol = "" bar "" else : break else : raise return protocol",if f2 :,if family == f2 :,92.20262734,FALSE,93.19
3265,"def app_middleware ( next , root , info , * * kwargs ) : app_auth_header = "" HTTP_AUTHORIZATION "" prefix = "" bearer "" request = info . context if request . path == API_PATH : if not hasattr ( request , "" app "" ) : request . app = None auth = request . META . get ( app_auth_header , "" "" ) . split ( ) if len ( auth ) == 2 : auth_prefix , auth_token = auth <MASK> request . app = SimpleLazyObject ( lambda : get_app ( auth_token ) ) return next ( root , info , * * kwargs )",if auth_prefix . lower ( ) == prefix :,if auth_prefix == prefix :,95.34372932,FALSE,96.07
3266,"def when ( self , matches , context ) : ret = [ ] for episode in matches . named ( "" episode "" , lambda match : len ( match . initiator ) == 1 ) : group = matches . markers . at_match ( episode , lambda marker : marker . name == "" group "" , index = 0 ) <MASK> if not matches . range ( * group . span , predicate = lambda match : match . name == "" title "" ) : ret . append ( episode ) return ret",if group :,if group :,100,TRUE,100
3267,def locate_via_pep514 ( spec ) : with _PY_LOCK : if not _PY_AVAILABLE : from . import pep514 _PY_AVAILABLE . extend ( pep514 . discover_pythons ( ) ) _PY_AVAILABLE . append ( CURRENT ) for cur_spec in _PY_AVAILABLE : <MASK> return cur_spec . path,if cur_spec . satisfies ( spec ) :,if cur_spec . name == spec . name :,90.05998171,FALSE,90.59
3268,"def setCorkImageDefault ( self ) : if settings . corkBackground [ "" image "" ] != "" "" : i = self . cmbCorkImage . findData ( settings . corkBackground [ "" image "" ] ) <MASK> self . cmbCorkImage . setCurrentIndex ( i )",if i != - 1 :,if i != - 1 :,100,TRUE,100
3269,"def _split_key ( key ) : if isinstance ( key , util . string_types ) : # coerce fooload('*') into ""default loader strategy"" if key == _WILDCARD_TOKEN : return ( _DEFAULT_TOKEN , ) # coerce fooload("".*"") into ""wildcard on default entity"" <MASK> key = key [ 1 : ] return key . split ( "" . "" ) else : return ( key , )","elif key . startswith ( ""."" + _WILDCARD_TOKEN ) :","elif key . startswith ( ""*"" ) :",96.95590118,FALSE,92.03
3270,"def detach_volume ( self , volume ) : # We need to find the node using this volume for node in self . list_nodes ( ) : <MASK> # This node has only one associated image. It is not the one we # are after. continue for disk in node . image : if disk . id == volume . id : # Node found. We can now detach the volume disk_id = disk . extra [ "" disk_id "" ] return self . _do_detach_volume ( node . id , disk_id ) return False",if type ( node . image ) is not list :,"if node . id == ""node"" :",70.07040171,FALSE,91.95
3271,"def create ( self , private = False ) : try : <MASK> log . info ( "" Creating private channel  %s . "" , self ) self . _bot . api_call ( "" conversations.create "" , data = { "" name "" : self . name , "" is_private "" : True } ) else : log . info ( "" Creating channel  %s . "" , self ) self . _bot . api_call ( "" conversations.create "" , data = { "" name "" : self . name } ) except SlackAPIResponseError as e : if e . error == "" user_is_bot "" : raise RoomError ( f "" Unable to create channel.  { USER_IS_BOT_HELPTEXT } "" ) else : raise RoomError ( e )",if private :,if private :,100,TRUE,100
3272,"def test_dataset_has_valid_etag ( self , dataset_name ) : py_script_path = list ( filter ( lambda x : x , dataset_name . split ( "" / "" ) ) ) [ - 1 ] + "" .py "" dataset_url = hf_bucket_url ( dataset_name , filename = py_script_path , dataset = True ) etag = None try : response = requests . head ( dataset_url , allow_redirects = True , proxies = None , timeout = 10 ) <MASK> etag = response . headers . get ( "" Etag "" ) except ( EnvironmentError , requests . exceptions . Timeout ) : pass self . assertIsNotNone ( etag )",if response . status_code == 200 :,if response . status_code == 200 :,100,TRUE,100
3273,"def set_dir_modes ( self , dirname , mode ) : if not self . is_chmod_supported ( ) : return for dirpath , dirnames , fnames in os . walk ( dirname ) : if os . path . islink ( dirpath ) : continue log . info ( "" changing mode of  %s  to  %o "" , dirpath , mode ) <MASK> os . chmod ( dirpath , mode )",if not self . dry_run :,"if not os . access ( dirpath , mode ) :",86.53062717,FALSE,89.51
3274,"def _clean ( self ) : logger . info ( "" Cleaning up... "" ) if self . _process is not None : <MASK> for _ in range ( 3 ) : self . _process . terminate ( ) time . sleep ( 0.5 ) if self . _process . poll ( ) is not None : break else : self . _process . kill ( ) self . _process . wait ( ) logger . error ( "" KILLED "" ) if os . path . exists ( self . _tmp_dir ) : shutil . rmtree ( self . _tmp_dir ) self . _process = None self . _ws = None logger . info ( "" Cleanup complete "" )",if self . _process . poll ( ) is None :,if self . _process . is_alive ( ) :,94.50055296,FALSE,96.25
3275,"def iter_chars_to_words ( self , chars ) : current_word = [ ] for char in chars : if not self . keep_blank_chars and char [ "" text "" ] . isspace ( ) : <MASK> yield current_word current_word = [ ] elif current_word and self . char_begins_new_word ( current_word , char ) : yield current_word current_word = [ char ] else : current_word . append ( char ) <MASK> yield current_word",if current_word :,if current_word :,100,TRUE,100
3276,"def _lookup ( components , specs , provided , name , i , l ) : if i < l : for spec in specs [ i ] . __sro__ : comps = components . get ( spec ) if comps : r = _lookup ( comps , specs , provided , name , i + 1 , l ) <MASK> return r else : for iface in provided : comps = components . get ( iface ) if comps : r = comps . get ( name ) <MASK> return r return None",if r is not None :,if r :,65.80967648,FALSE,91.58
3277,"def run ( cmd , task = None ) : process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , close_fds = True ) output_lines = [ ] while True : line = process . stdout . readline ( ) <MASK> break line = line . decode ( "" utf-8 "" ) output_lines + = [ line ] logger . info ( line . rstrip ( "" \n "" ) ) process . stdout . close ( ) exit_code = process . wait ( ) if exit_code : output = "" "" . join ( output_lines ) raise subprocess . CalledProcessError ( exit_code , cmd , output = output )",if not line :,if not line :,100,TRUE,100
3278,"def process_response ( self , request , response ) : if ( response . status_code == 404 and request . path_info . endswith ( "" / "" ) and not is_valid_path ( request . path_info ) and is_valid_path ( request . path_info [ : - 1 ] ) ) : # Use request.path because we munged app/locale in path_info. newurl = request . path [ : - 1 ] <MASK> with safe_query_string ( request ) : newurl + = "" ? "" + request . META [ "" QUERY_STRING "" ] return HttpResponsePermanentRedirect ( newurl ) return response",if request . GET :,"if request . META [ ""QUERY_STRING"" ] :",98.05814614,FALSE,93.18
3279,"def dependencies ( self ) : deps = [ ] midx = None if self . ref is not None : query = TypeQuery ( self . ref ) super = query . execute ( self . schema ) if super is None : log . debug ( self . schema ) raise TypeNotFound ( self . ref ) <MASK> deps . append ( super ) midx = 0 return ( midx , deps )",if not super . builtin ( ) :,if super . is_module :,92.58264471,FALSE,91.56
3280,"def _get_vtkjs ( self ) : if self . _vtkjs is None and self . object is not None : if isinstance ( self . object , string_types ) and self . object . endswith ( "" .vtkjs "" ) : <MASK> with open ( self . object , "" rb "" ) as f : vtkjs = f . read ( ) else : data_url = urlopen ( self . object ) vtkjs = data_url . read ( ) elif hasattr ( self . object , "" read "" ) : vtkjs = self . object . read ( ) self . _vtkjs = vtkjs return self . _vtkjs",if isfile ( self . object ) :,"if hasattr ( self . object , ""read"" ) :",94.78633038,FALSE,93.93
3281,"def _save ( self ) : fd , tempname = tempfile . mkstemp ( ) fd = os . fdopen ( fd , "" w "" ) json . dump ( self . _cache , fd , indent = 2 , separators = ( "" , "" , "" :  "" ) ) fd . close ( ) # Silently ignore errors try : <MASK> os . makedirs ( os . path . dirname ( self . filename ) ) shutil . move ( tempname , self . filename ) except ( IOError , OSError ) : os . remove ( tempname )",if not os . path . exists ( os . path . dirname ( self . filename ) ) :,if not os . path . exists ( os . path . dirname ( self . filename ) ),98.96479488,FALSE,97.95
3282,"def refiner_configs ( self ) : rv = { } for refiner in refiner_manager : <MASK> rv [ refiner . name ] = { k : v for k , v in self . config . items ( refiner . name ) } return rv",if self . config . has_section ( refiner . name ) :,if refiner . name in self . config :,86.77452647,FALSE,82.21
3283,"def com_slice ( self , primary , node , assigning ) : # short_slice:  [lower_bound] "":"" [upper_bound] lower = upper = None if len ( node . children ) == 2 : <MASK> upper = self . com_node ( node . children [ 1 ] ) else : lower = self . com_node ( node . children [ 0 ] ) elif len ( node . children ) == 3 : lower = self . com_node ( node . children [ 0 ] ) upper = self . com_node ( node . children [ 2 ] ) return Slice ( primary , assigning , lower , upper , lineno = extractLineNo ( node ) )",if node . children [ 0 ] . type == token . COLON :,if assigning :,69.69224185,FALSE,90.25
3284,"def close ( self , * args , * * kwargs ) : super ( mytqdm , self ) . close ( * args , * * kwargs ) # If it was not run in a notebook, sp is not assigned, check for it if hasattr ( self , "" sp "" ) : # Try to detect if there was an error or KeyboardInterrupt # in manual mode: if n < total, things probably got wrong if self . total and self . n < self . total : self . sp ( bar_style = "" danger "" ) else : <MASK> self . sp ( bar_style = "" success "" ) else : self . sp ( close = True )",if self . leave :,if self . n == 0 :,98.64112139,FALSE,95.99
3285,"def test_alloc ( self ) : b = bytearray ( ) alloc = b . __alloc__ ( ) self . assertTrue ( alloc > = 0 ) seq = [ alloc ] for i in range ( 100 ) : b + = b "" x "" alloc = b . __alloc__ ( ) self . assertTrue ( alloc > = len ( b ) ) <MASK> seq . append ( alloc )",if alloc not in seq :,if alloc not in seq :,100,TRUE,100
3286,"def flush_file ( self , key , f ) : f . flush ( ) <MASK> f . compress = zlib . compressobj ( 9 , zlib . DEFLATED , - zlib . MAX_WBITS , zlib . DEF_MEM_LEVEL , 0 ) if len ( self . files ) > self . MAX_OPEN_FILES : <MASK> open_files = sum ( 1 for f in self . files . values ( ) if f . fileobj is not None ) if open_files > self . MAX_OPEN_FILES : f . fileobj . close ( ) f . fileobj = None else : f . close ( ) self . files . pop ( key )",if self . compress :,if self . compress :,100,TRUE,100
3287,"def _run ( self ) : # Low-level run method to do the actual scheduling loop. self . running = True while self . running : try : self . sched . run ( ) except Exception as x : logging . error ( "" Error during scheduler execution:  %s "" % str ( x ) , exc_info = True ) # queue is empty; sleep a short while before checking again <MASK> time . sleep ( 5 )",if self . running :,if self . queue :,73.47537733,FALSE,97.15
3288,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 10 : self . set_app_id ( d . getPrefixedString ( ) ) continue <MASK> self . set_max_rows ( d . getVarInt32 ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 16 :,if tt == 16 :,100,TRUE,100
3289,"def check ( dbdef ) : "" drop script must clear the database "" for version in dbdef : connector = MemConnector ( ) . bound ( None ) create ( dbdef , version , connector ) drop ( dbdef , version , connector ) remaining = connector . execute ( "" SELECT * FROM sqlite_master WHERE name NOT LIKE  ' sqlite_ % ' "" ) . fetchall ( ) <MASK> yield "" {0} :drop.sql "" . format ( version ) , remaining",if remaining :,if remaining :,100,TRUE,100
3290,"def test_open_overwrite_offset_size ( self , sftp ) : """"""Test writing data at a specific offset"""""" f = None try : self . _create_file ( "" file "" , "" xxxxyyyy "" ) f = yield from sftp . open ( "" file "" , "" r+ "" ) yield from f . write ( "" zz "" , 3 ) yield from f . close ( ) with open ( "" file "" ) as localf : self . assertEqual ( localf . read ( ) , "" xxxzzyyy "" ) finally : <MASK> # pragma: no branch yield from f . close ( ) remove ( "" file "" )",if f :,if f :,100,TRUE,100
3291,"def pump ( ) : import sys as _sys while self . countdown_active ( ) : if not ( self . connected ( "" send "" ) and other . connected ( "" recv "" ) ) : break try : data = other . recv ( timeout = 0.05 ) except EOFError : break <MASK> return if not data : continue try : self . send ( data ) except EOFError : break <MASK> return self . shutdown ( "" send "" ) other . shutdown ( "" recv "" )",if not _sys :,if self . shutdown_active ( ) :,83.93134739,FALSE,84.86
3292,"def parse_results ( cwd ) : optimal_dd = None optimal_measure = numpy . inf for tup in tools . find_conf_files ( cwd ) : dd = tup [ 1 ] if "" results.train_y_misclass "" in dd : <MASK> optimal_measure = dd [ "" results.train_y_misclass "" ] optimal_dd = dd print ( "" Optimal results.train_y_misclass: "" , str ( optimal_measure ) ) for key , value in optimal_dd . items ( ) : if "" hyper_parameters "" in key : print ( key + "" :  "" + str ( value ) )","if dd [ ""results.train_y_misclass"" ] < optimal_measure :","if ""results.train_y_misclass"" in dd :",93.49690915,FALSE,93.29
3293,"def valid ( self ) : valid = True <MASK> return valid else : try : with io . open ( self . pathfile , "" w "" , encoding = "" utf-8 "" ) as f : f . close ( ) # do nothing except OSError : valid = False <MASK> os . remove ( self . pathfile ) return valid",if os . path . exists ( self . pathfile ) :,if os . path . exists ( self . pathfile ) :,100,TRUE,100
3294,"def __getitem__ ( self , key ) : try : value = self . cache [ key ] except KeyError : f = BytesIO ( self . dict [ key . encode ( self . keyencoding ) ] ) value = Unpickler ( f ) . load ( ) <MASK> self . cache [ key ] = value return value",if self . writeback :,if value is not None :,85.1317165,FALSE,91.8
3295,"def hasMenu ( cls , callingWindow , mainItem , selection , * fullContexts ) : for i , fullContext in enumerate ( fullContexts ) : srcContext = fullContext [ 0 ] for menuHandler in cls . menus : m = menuHandler ( ) <MASK> return True return False","if m . _baseDisplay ( callingWindow , srcContext , mainItem , selection ) :","if m . match ( mainItem , srcContext ) :",71.39070766,FALSE,83.13
3296,"def lr_read_tables ( module = tab_module , optimize = 0 ) : global _lr_action , _lr_goto , _lr_productions , _lr_method try : exec ( "" import  %s  as parsetab "" % module ) global parsetab # declare the name of the imported module <MASK> _lr_action = parsetab . _lr_action _lr_goto = parsetab . _lr_goto _lr_productions = parsetab . _lr_productions _lr_method = parsetab . _lr_method return 1 else : return 0 except ( ImportError , AttributeError ) : return 0",if ( optimize ) or ( Signature . digest ( ) == parsetab . _lr_signature ) :,if optimize :,91.22145696,FALSE,84.88
3297,"def _Determine_Do ( self ) : if sys . platform . startswith ( "" win "" ) : self . applicable = 1 for opt , optarg in self . chosenOptions : if opt == "" --moz-tools "" : self . value = os . path . abspath ( os . path . normpath ( optarg ) ) break else : <MASK> self . value = os . environ [ self . name ] else : self . value = None else : self . applicable = 0 self . determined = 1",if os . environ . has_key ( self . name ) :,if self . name in os . environ :,92.6984288,FALSE,90.19
3298,"def parse_chunked ( self , unreader ) : ( size , rest ) = self . parse_chunk_size ( unreader ) while size > 0 : while size > len ( rest ) : size - = len ( rest ) yield rest rest = unreader . read ( ) if not rest : raise NoMoreData ( ) yield rest [ : size ] # Remove \r\n after chunk rest = rest [ size : ] while len ( rest ) < 2 : rest + = unreader . read ( ) <MASK> raise ChunkMissingTerminator ( rest [ : 2 ] ) ( size , rest ) = self . parse_chunk_size ( unreader , data = rest [ 2 : ] )","if rest [ : 2 ] != b""\r\n"" :",if not rest [ : 2 ] :,98.04854715,FALSE,91.75
3299,"def _scroll_down ( self , cli ) : "" Scroll window down. "" info = self . render_info if self . vertical_scroll < info . content_height - info . window_height : <MASK> self . content . move_cursor_down ( cli ) self . vertical_scroll + = 1",if info . cursor_position . y <= info . configured_scroll_offsets . top :,if self . content :,81.71396814,FALSE,75.03
3300,"def _add_defaults_data_files ( self ) : # getting distribution.data_files if self . distribution . has_data_files ( ) : for item in self . distribution . data_files : if isinstance ( item , str ) : # plain file item = convert_path ( item ) if os . path . isfile ( item ) : self . filelist . append ( item ) else : # a (dirname, filenames) tuple dirname , filenames = item for f in filenames : f = convert_path ( f ) <MASK> self . filelist . append ( f )",if os . path . isfile ( f ) :,if os . path . isfile ( f ) :,75,TRUE,100
3301,"def list_stuff ( self , upto = 10 , start_after = - 1 ) : for i in range ( upto ) : <MASK> continue if i == 2 and self . count < 1 : self . count + = 1 raise TemporaryProblem if i == 7 and self . count < 4 : self . count + = 1 raise TemporaryProblem yield i",if i <= start_after :,if i == start_after :,98.16245406,FALSE,96.7
3302,"def is_open ( self ) : if self . signup_code : return True else : <MASK> if self . messages . get ( "" invalid_signup_code "" ) : messages . add_message ( self . request , self . messages [ "" invalid_signup_code "" ] [ "" level "" ] , self . messages [ "" invalid_signup_code "" ] [ "" text "" ] . format ( * * { "" code "" : self . get_code ( ) , } ) , ) return settings . ACCOUNT_OPEN_SIGNUP",if self . signup_code_present :,if settings . ACCOUNT_OPEN_SIGNUP :,91.39231203,FALSE,93.37
3303,"def on_delete_from_disk ( self , widget , data = None ) : model , iter = self . get_selection ( ) . get_selected ( ) if iter : path = model . get_value ( iter , COLUMN_PATH ) <MASK> ErrorDialog ( _ ( "" Can ' t delete system item from disk. "" ) ) . launch ( ) else : os . remove ( path ) self . update_items ( )",if self . is_defaultitem ( path ) :,if not os . path . exists ( path ) :,85.37712022,FALSE,92.53
3304,"def get_detections_for_batch ( self , images ) : images = images [ . . . , : : - 1 ] detected_faces = self . face_detector . detect_from_batch ( images . copy ( ) ) results = [ ] for i , d in enumerate ( detected_faces ) : <MASK> results . append ( None ) continue d = d [ 0 ] d = np . clip ( d , 0 , None ) x1 , y1 , x2 , y2 = map ( int , d [ : - 1 ] ) results . append ( ( x1 , y1 , x2 , y2 ) ) return results",if len ( d ) == 0 :,if i == 0 :,90.04337288,FALSE,95.84
3305,def on_update ( self ) : # # Calculate maximum # of planes per well # self . max_per_well = 0 for pd in list ( self . plate_well_site . values ( ) ) : for wd in list ( pd . values ( ) ) : nplanes = sum ( [ len ( x ) for x in list ( wd . values ( ) ) ] ) <MASK> self . max_per_well = nplanes for registrant in self . registrants : registrant ( ),if nplanes > self . max_per_well :,if nplanes > self . max_per_well :,75,TRUE,100
3306,"def is_writable ( self , path ) : result = False while not result : if os . path . exists ( path ) : result = os . access ( path , os . W_OK ) break parent = os . path . dirname ( path ) <MASK> break path = parent return result",if parent == path :,if parent == path :,100,TRUE,100
3307,"def _check_seed ( self , seed ) : if seed is not None : <MASK> self . _raise_error ( "" The random number generator seed value, seed, should be integer type or None. "" ) if seed < 0 : self . _raise_error ( "" The random number generator seed value, seed, should be non-negative integer or None. "" )",if type ( seed ) != int :,"if not isinstance ( seed , ( int , long ) ) :",82.60492987,FALSE,86.74
3308,"def write ( self , x ) : # try to use backslash and surrogate escape strategies before failing self . _errors = "" backslashescape "" if self . encoding != "" mbcs "" else "" surrogateescape "" try : return io . TextIOWrapper . write ( self , to_text ( x , errors = self . _errors ) ) except UnicodeDecodeError : <MASK> self . _errors = "" surrogateescape "" else : self . _errors = "" replace "" return io . TextIOWrapper . write ( self , to_text ( x , errors = self . _errors ) )","if self . _errors != ""surrogateescape"" :","if self . encoding == ""mbcs"" :",72.88858879,FALSE,94.28
3309,"def post ( self , request , * args , * * kwargs ) : validated_session = [ ] for session_id in request . data : session = get_object_or_none ( Session , id = session_id ) <MASK> validated_session . append ( session_id ) self . model . objects . create ( name = "" kill_session "" , args = session . id , terminal = session . terminal , ) return Response ( { "" ok "" : validated_session } )",if session and not session . is_finished :,if session . is_valid :,92.05084936,FALSE,94.2
3310,"def _has_list_or_dict_var_value_before ( self , arg_index ) : for idx , value in enumerate ( self . args ) : <MASK> return False if variablematcher . is_list_variable ( value ) and not variablematcher . is_list_variable_subitem ( value ) : return True if robotapi . is_dict_var ( value ) and not variablematcher . is_dict_var_access ( value ) : return True return False",if idx > arg_index :,if idx == arg_index :,98.05576899,FALSE,96.53
3311,"def test_return_correct_type ( self ) : for proto in protocols : # Protocol 0 supports only ASCII strings. <MASK> self . _check_return_correct_type ( "" abc "" , 0 ) else : for obj in [ b "" abc \n "" , "" abc \n "" , - 1 , - 1.1 * 0.1 , str ] : self . _check_return_correct_type ( obj , proto )",if proto == 0 :,if proto == 0 :,75,TRUE,100
3312,"def backward_impl ( self , inputs , outputs , prop_down , accum ) : # inputs: [inputs_fwd_graph] + [inputs_bwd_graph] or # [inputs_fwd_graph] + [outputs_fwd_graph] + [inputs_bwd_graph] # Args axis = self . forward_func . info . args [ "" axis "" ] # Compute ## w.r.t. dy if prop_down [ - 1 ] : g_dy = inputs [ - 1 ] . grad g_dy_ = F . stack ( * [ o . grad for o in outputs ] , axis = axis ) <MASK> g_dy + = g_dy_ else : g_dy . copy_from ( g_dy_ )",if accum [ - 1 ] :,if accum :,72.32700021,FALSE,96.62
3313,"def remove ( self , url ) : try : i = self . items . index ( url ) except ( ValueError , IndexError ) : pass else : was_selected = i in self . selectedindices ( ) self . list . delete ( i ) del self . items [ i ] if not self . items : self . mp . hidepanel ( self . name ) elif was_selected : <MASK> i = len ( self . items ) - 1 self . list . select_set ( i )",if i >= len ( self . items ) :,if self . items [ i ] == url :,89.95439001,FALSE,91.95
3314,"def prepend ( self , value ) : """"""prepend value to nodes"""""" root , root_text = self . _get_root ( value ) for i , tag in enumerate ( self ) : if not tag . text : tag . text = "" "" if len ( root ) > 0 : root [ - 1 ] . tail = tag . text tag . text = root_text else : tag . text = root_text + tag . text <MASK> root = deepcopy ( list ( root ) ) tag [ : 0 ] = root root = tag [ : len ( root ) ] return self",if i > 0 :,if i == 0 :,98.8741757,FALSE,97.22
3315,"def _get_tracks_compositors_list ( ) : tracks_list = [ ] tracks = current_sequence ( ) . tracks compositors = current_sequence ( ) . compositors for track_index in range ( 1 , len ( tracks ) - 1 ) : track_compositors = [ ] for j in range ( 0 , len ( compositors ) ) : comp = compositors [ j ] <MASK> track_compositors . append ( comp ) tracks_list . append ( track_compositors ) return tracks_list",if comp . transition . b_track == track_index :,if comp . index == track_index :,95.04308754,FALSE,94.19
3316,"def __getattr__ ( self , name ) : if name in self . _sections : return "" \n "" . join ( self . _sections [ name ] ) else : <MASK> return "" "" else : raise ConanException ( "" ConfigParser: Unrecognized field  ' %s ' "" % name )",if self . _allowed_fields and name in self . _allowed_fields :,if name in self . _sections :,65.11007983,FALSE,81.98
3317,"def get_first_param_index ( self , group_id , param_group , partition_id ) : for index , param in enumerate ( param_group ) : param_id = self . get_param_id ( param ) <MASK> return index return None",if partition_id in self . param_to_partition_ids [ group_id ] [ param_id ] :,if self . get_partition_id ( param_id ) == group_id and self,75.20269377,FALSE,74.28
3318,"def handle_uv_sockets ( self , context ) : u_socket = self . inputs [ "" U "" ] v_socket = self . inputs [ "" V "" ] if self . cast_mode == "" Sphere "" : u_socket . hide_safe = True v_socket . hide_safe = True elif self . cast_mode in [ "" Cylinder "" , "" Prism "" ] : v_socket . hide_safe = True <MASK> u_socket . hide_safe = False else : <MASK> u_socket . hide_safe = False if v_socket . hide_safe : v_socket . hide_safe = False",if u_socket . hide_safe :,if u_socket . hide_safe :,100,TRUE,100
3319,"def _scrub_generated_timestamps ( self , target_workdir ) : """"""Remove the first line of comment from each file if it contains a timestamp."""""" for root , _ , filenames in safe_walk ( target_workdir ) : for filename in filenames : source = os . path . join ( root , filename ) with open ( source , "" r "" ) as f : lines = f . readlines ( ) <MASK> return with open ( source , "" w "" ) as f : if not self . _COMMENT_WITH_TIMESTAMP_RE . match ( lines [ 0 ] ) : f . write ( lines [ 0 ] ) for line in lines [ 1 : ] : f . write ( line )",if len ( lines ) < 1 :,if not lines :,95.47214139,FALSE,95.23
3320,"def inner ( request , * args , * * kwargs ) : page = request . current_page if page : if page . login_required and not request . user . is_authenticated : return redirect_to_login ( urlquote ( request . get_full_path ( ) ) , settings . LOGIN_URL ) site = get_current_site ( ) <MASK> return _handle_no_page ( request ) return func ( request , * args , * * kwargs )","if not user_can_view_page ( request . user , page , site ) :",if site . is_anonymous and not site . is_anonymous :,86.32975816,FALSE,84.7
3321,"def flush ( self , * args , * * kwargs ) : with self . _lock : self . _last_updated = time . time ( ) try : <MASK> self . _locked_flush_without_tempfile ( ) else : mailbox . mbox . flush ( self , * args , * * kwargs ) except OSError : if "" _create_temporary "" in traceback . format_exc ( ) : self . _locked_flush_without_tempfile ( ) else : raise self . _last_updated = time . time ( )","if kwargs . get ( ""in_place"" , False ) :","if ""_create_temporary"" in traceback . format_exc ( ) :",92.98582743,FALSE,89.52
3322,"def sanitize_event_keys ( kwargs , valid_keys ) : # Sanity check: Don't honor keys that we don't recognize. for key in list ( kwargs . keys ( ) ) : if key not in valid_keys : kwargs . pop ( key ) # Truncate certain values over 1k for key in [ "" play "" , "" role "" , "" task "" , "" playbook "" ] : <MASK> if len ( kwargs [ "" event_data "" ] [ key ] ) > 1024 : kwargs [ "" event_data "" ] [ key ] = Truncator ( kwargs [ "" event_data "" ] [ key ] ) . chars ( 1024 )","if isinstance ( kwargs . get ( ""event_data"" , { } ) . get ( key ) , str ) :","if key in kwargs [ ""event_data"" ] :",67.54945629,FALSE,86.16
3323,"def parse_auth ( val ) : if val is not None : authtype , params = val . split ( ""   "" , 1 ) <MASK> if authtype == "" Basic "" and ' "" ' not in params : # this is the ""Authentication: Basic XXXXX=="" case pass else : params = parse_auth_params ( params ) return authtype , params return val",if authtype in known_auth_schemes :,if params :,89.61826779,FALSE,89.41
3324,"def _memoized ( * args ) : now = time . time ( ) try : value , last_update = self . cache [ args ] age = now - last_update if self . _call_count > self . ctl or age > self . ttl : self . _call_count = 0 raise AttributeError if self . ctl : self . _call_count + = 1 return value except ( KeyError , AttributeError ) : value = func ( * args ) <MASK> self . cache [ args ] = ( value , now ) return value except TypeError : return func ( * args )",if value :,if value is not None :,97.0233878,FALSE,96.28
3325,"def _get_md_bg_color_down ( self ) : t = self . theme_cls c = self . md_bg_color # Default to no change on touch # Material design specifies using darker hue when on Dark theme if t . theme_style == "" Dark "" : if self . md_bg_color == t . primary_color : c = t . primary_dark <MASK> c = t . accent_dark return c",elif self . md_bg_color == t . accent_color :,elif self . md_bg_color == t . accent_color :,100,TRUE,100
3326,def _init_table_h ( ) : _table_h = [ ] for i in range ( 256 ) : part_l = i part_h = 0 for j in range ( 8 ) : rflag = part_l & 1 part_l >> = 1 <MASK> part_l | = 1 << 31 part_h >> = 1 if rflag : part_h ^ = 0xD8000000 _table_h . append ( part_h ) return _table_h,if part_h & 1 :,if j :,93.29020234,FALSE,93.68
3327,"def migrate_Stats ( self ) : for old_obj in self . session_old . query ( self . model_from [ "" Stats "" ] ) : if not old_obj . summary : self . entries_count [ "" Stats "" ] - = 1 continue new_obj = self . model_to [ "" Stats "" ] ( ) for key in new_obj . __table__ . columns . _data . keys ( ) : <MASK> continue setattr ( new_obj , key , getattr ( old_obj , key ) ) self . session_new . add ( new_obj )",if key not in old_obj . __table__ . columns :,"if key in self . entries_count [ ""Stats"" ] :",93.01607131,FALSE,89.98
3328,"def get_in_turn_repetition ( pred , is_cn = False ) : """"""Get in-turn repetition."""""" if len ( pred ) == 0 : return 1.0 if isinstance ( pred [ 0 ] , str ) : pred = [ tok . lower ( ) for tok in pred ] if is_cn : pred = "" "" . join ( pred ) tri_grams = set ( ) for i in range ( len ( pred ) - 2 ) : tri_gram = tuple ( pred [ i : i + 3 ] ) <MASK> return 1.0 tri_grams . add ( tri_gram ) return 0.0",if tri_gram in tri_grams :,if tri_gram in tri_grams :,100,TRUE,100
3329,"def translate ( ) : assert Lex . next ( ) is AttributeList reader . read ( ) # Discard attribute list from reader. attrs = { } d = AttributeList . match . groupdict ( ) for k , v in d . items ( ) : if v is not None : <MASK> v = subs_attrs ( v ) if v : parse_attributes ( v , attrs ) else : AttributeList . attrs [ k ] = v AttributeList . subs ( attrs ) AttributeList . attrs . update ( attrs )","if k == ""attrlist"" :","if isinstance ( v , dict ) :",72.00391058,FALSE,92.8
3330,"def _parse ( self , engine ) : """"""Parse the layer."""""" if isinstance ( self . args , dict ) : if "" axis "" in self . args : self . axis = engine . evaluate ( self . args [ "" axis "" ] , recursive = True ) <MASK> raise ParsingError ( ' "" axis ""  must be an integer. ' ) if "" momentum "" in self . args : self . momentum = engine . evaluate ( self . args [ "" momentum "" ] , recursive = True ) if not isinstance ( self . momentum , ( int , float ) ) : raise ParsingError ( ' "" momentum ""  must be numeric. ' )","if not isinstance ( self . axis , int ) :","if not isinstance ( self . axis , ( int , float ) ) :",96.35835321,FALSE,95.66
3331,"def __getattr__ ( self , attrname ) : if attrname in ( "" visamp "" , "" visamperr "" , "" visphi "" , "" visphierr "" ) : return ma . masked_array ( self . __dict__ [ "" _ "" + attrname ] , mask = self . flag ) elif attrname in ( "" cflux "" , "" cfluxerr "" ) : <MASK> return ma . masked_array ( self . __dict__ [ "" _ "" + attrname ] , mask = self . flag ) else : return None else : raise AttributeError ( attrname )","if self . __dict__ [ ""_"" + attrname ] != None :",if self . flag :,66.45588705,FALSE,86.89
3332,"def draw ( self , context ) : layout = self . layout presets . draw_presets_ops ( layout , context = context ) for category in presets . get_category_names ( ) : <MASK> <MASK> class_name = preset_category_menus [ category ] . __name__ layout . menu ( class_name )",if category in preset_category_menus :,if category in preset_category_menus :,100,TRUE,100
3333,"def __setitem__ ( self , key , value ) : if isinstance ( value , ( tuple , list ) ) : info , reference = value if info not in self . _reverse_infos : self . _reverse_infos [ info ] = len ( self . _infos ) self . _infos . append ( info ) <MASK> self . _reverse_references [ reference ] = len ( self . _references ) self . _references . append ( reference ) self . _trails [ key ] = "" %d , %d "" % ( self . _reverse_infos [ info ] , self . _reverse_references [ reference ] , ) else : raise Exception ( "" unsupported type  ' %s ' "" % type ( value ) )",if reference not in self . _reverse_references :,if reference not in self . _reverse_references :,100,TRUE,100
3334,"def format_bpe_text ( symbols , delimiter = b "" @@ "" ) : """"""Convert a sequence of bpe words into sentence."""""" words = [ ] word = b "" "" if isinstance ( symbols , str ) : symbols = symbols . encode ( ) delimiter_len = len ( delimiter ) for symbol in symbols : <MASK> word + = symbol [ : - delimiter_len ] else : # end of a word word + = symbol words . append ( word ) word = b "" "" return b ""   "" . join ( words )",if len ( symbol ) >= delimiter_len and symbol [ - delimiter_len : ] == delimiter :,if symbol . endswith ( delimiter ) :,81.820915,FALSE,83.37
3335,"def output_type ( data , request , response ) : accept = request . accept if accept in ( "" "" , "" * "" , "" / "" ) : handler = default or handlers and next ( iter ( handlers . values ( ) ) ) else : handler = default accepted = [ accept_quality ( accept_type ) for accept_type in accept . split ( "" , "" ) ] accepted . sort ( key = itemgetter ( 0 ) ) for _quality , accepted_content_type in reversed ( accepted ) : <MASK> handler = handlers [ accepted_content_type ] break if not handler : raise falcon . HTTPNotAcceptable ( error ) response . content_type = handler . content_type return handler ( data , request = request , response = response )",if accepted_content_type in handlers :,if accepted_content_type in handlers :,100,TRUE,100
3336,"def _render_raw_list ( bytes_items ) : flatten_items = [ ] for item in bytes_items : if item is None : flatten_items . append ( b "" "" ) elif isinstance ( item , bytes ) : flatten_items . append ( item ) <MASK> flatten_items . append ( str ( item ) . encode ( ) ) elif isinstance ( item , list ) : flatten_items . append ( _render_raw_list ( item ) ) return b "" \n "" . join ( flatten_items )","elif isinstance ( item , int ) :","elif isinstance ( item , bytes ) :",98.59598188,FALSE,97.82
3337,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) <MASK> self . set_mime_type ( d . getVarInt32 ( ) ) continue if tt == 16 : self . set_quality ( d . getVarInt32 ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 8 :,if tt == 8 :,100,TRUE,100
3338,"def delete ( self , waiters ) : # Delete flow. msgs = self . ofctl . get_all_flow ( waiters ) for msg in msgs : for stats in msg . body : vlan_id = VlanRouter . _cookie_to_id ( REST_VLANID , stats . cookie ) <MASK> self . ofctl . delete_flow ( stats ) assert len ( self . packet_buffer ) == 0",if vlan_id == self . vlan_id :,if vlan_id == self . ofctl . get_flow_id ( stats . cookie,70.80145975,FALSE,88.02
3339,def missing_push_allowance ( push_allowances : List [ PushAllowance ] ) - > bool : for push_allowance in push_allowances : # a null databaseId indicates this is not a GitHub App. if push_allowance . actor . databaseId is None : continue <MASK> return False return True,if str ( push_allowance . actor . databaseId ) == str ( app_config . GITHUB_APP_ID ) :,if not push_allowance . actor . user . is_authenticated :,88.67726079,FALSE,73.97
3340,"def _cluster_page ( self , htmlpage ) : template_cluster , preferred = _CLUSTER_NA , None if self . clustering : self . clustering . add_page ( htmlpage ) <MASK> clt = self . clustering . classify ( htmlpage ) if clt != - 1 : template_cluster = preferred = self . template_names [ clt ] else : template_cluster = _CLUSTER_OUTLIER return template_cluster , preferred",if self . clustering . is_fit :,if self . template_names :,91.47374415,FALSE,93.22
3341,"def readlines ( self , size = - 1 ) : if self . _nbr == self . _size : return [ ] # leave all additional logic to our readline method, we just check the size out = [ ] nbr = 0 while True : line = self . readline ( ) if not line : break out . append ( line ) if size > - 1 : nbr + = len ( line ) <MASK> break # END handle size constraint # END readline loop return out",if nbr > size :,if nbr >= size :,98.67357119,FALSE,97.44
3342,"def post_mortem ( t = None ) : # handling the default <MASK> # sys.exc_info() returns (type, value, traceback) if an exception is # being handled, otherwise it returns None t = sys . exc_info ( ) [ 2 ] <MASK> raise ValueError ( "" A valid traceback must be passed if no exception is being handled. "" ) p = BPdb ( ) p . reset ( ) p . interaction ( None , t )",if t is None :,if t is None :,100,TRUE,100
3343,"def fixup ( m ) : txt = m . group ( 0 ) if txt [ : 2 ] == "" &# "" : # character reference try : <MASK> return unichr ( int ( txt [ 3 : - 1 ] , 16 ) ) else : return unichr ( int ( txt [ 2 : - 1 ] ) ) except ValueError : pass else : # named entity try : txt = unichr ( htmlentitydefs . name2codepoint [ txt [ 1 : - 1 ] ] ) except KeyError : pass return txt # leave as is","if txt [ : 3 ] == ""&#x"" :","if txt [ : 3 ] == ""&#x"" :",100,TRUE,100
3344,"def parse_converter_args ( argstr : str ) - > t . Tuple [ t . Tuple , t . Dict [ str , t . Any ] ] : argstr + = "" , "" args = [ ] kwargs = { } for item in _converter_args_re . finditer ( argstr ) : value = item . group ( "" stringval "" ) <MASK> value = item . group ( "" value "" ) value = _pythonize ( value ) if not item . group ( "" name "" ) : args . append ( value ) else : name = item . group ( "" name "" ) kwargs [ name ] = value return tuple ( args ) , kwargs",if value is None :,if not value :,82.04719738,FALSE,96.83
3345,"def IT ( cpu ) : cc = cpu . instruction . cc true_case = cpu . _evaluate_conditional ( cc ) # this is incredibly hacky--how else does capstone expose this? # TODO: find a better way than string parsing the mnemonic -GR, 2017-07-13 for c in cpu . instruction . mnemonic [ 1 : ] : <MASK> cpu . _it_conditional . append ( true_case ) elif c == "" e "" : cpu . _it_conditional . append ( not true_case )","if c == ""t"" :","if c == ""g"" :",98.52623024,FALSE,97.72
3346,"def flatten ( self ) : # this is similar to fill_messages except it uses a list instead # of a queue to place the messages in. result = [ ] channel = await self . messageable . _get_channel ( ) self . channel = channel while self . _get_retrieve ( ) : data = await self . _retrieve_messages ( self . retrieve ) <MASK> self . limit = 0 # terminate the infinite loop if self . reverse : data = reversed ( data ) if self . _filter : data = filter ( self . _filter , data ) for element in data : result . append ( self . state . create_message ( channel = channel , data = element ) ) return result",if len ( data ) < 100 :,if len ( data ) == 0 :,73.6979399,FALSE,96.98
3347,"def _get_beta_accumulators ( self ) : with tf . init_scope ( ) : <MASK> graph = None else : graph = tf . get_default_graph ( ) return ( self . _get_non_slot_variable ( "" beta1_power "" , graph = graph ) , self . _get_non_slot_variable ( "" beta2_power "" , graph = graph ) , )",if tf . executing_eagerly ( ) :,"if self . _get_non_slot_variable ( ""beta1_power"" )",89.32870771,FALSE,82.33
3348,"def prefixed ( self , prefix : _StrType ) - > typing . Iterator [ "" Env "" ] : """"""Context manager for parsing envvars with a common prefix."""""" try : old_prefix = self . _prefix <MASK> self . _prefix = prefix else : self . _prefix = f "" { old_prefix } { prefix } "" yield self finally : # explicitly reset the stored prefix on completion and exceptions self . _prefix = None self . _prefix = old_prefix",if old_prefix is None :,if old_prefix is None :,100,TRUE,100
3349,"def decode_content ( self ) : """"""Return the best possible representation of the response body."""""" ct = self . headers . get ( "" content-type "" ) if ct : ct , options = parse_options_header ( ct ) charset = options . get ( "" charset "" ) if ct in JSON_CONTENT_TYPES : return self . json ( charset ) <MASK> return self . text ( charset ) elif ct == FORM_URL_ENCODED : return parse_qsl ( self . content . decode ( charset ) , keep_blank_values = True ) return self . content","elif ct . startswith ( ""text/"" ) :",elif ct == TEXT_CONTENT_TYPES :,95.18742233,FALSE,92.66
3350,"def test_incrementaldecoder ( self ) : UTF8Writer = codecs . getwriter ( "" utf-8 "" ) for sizehint in [ None , - 1 ] + list ( range ( 1 , 33 ) ) + [ 64 , 128 , 256 , 512 , 1024 ] : istream = BytesIO ( self . tstring [ 0 ] ) ostream = UTF8Writer ( BytesIO ( ) ) decoder = self . incrementaldecoder ( ) while 1 : data = istream . read ( sizehint ) <MASK> break else : u = decoder . decode ( data ) ostream . write ( u ) self . assertEqual ( ostream . getvalue ( ) , self . tstring [ 1 ] )",if not data :,if not data :,100,TRUE,100
3351,"def delete_all ( path ) : ppath = os . getcwd ( ) os . chdir ( path ) for fn in glob . glob ( "" * "" ) : fn_full = os . path . join ( path , fn ) <MASK> delete_all ( fn_full ) elif fn . endswith ( "" .png "" ) : os . remove ( fn_full ) elif fn . endswith ( "" .md "" ) : os . remove ( fn_full ) elif DELETE_ALL_OLD : os . remove ( fn_full ) os . chdir ( ppath ) os . rmdir ( path )",if os . path . isdir ( fn ) :,"if fn . endswith ( "".py"" ) :",93.41566468,FALSE,93.4
3352,"def _delete_reason ( self ) : for i in range ( _lib . X509_REVOKED_get_ext_count ( self . _revoked ) ) : ext = _lib . X509_REVOKED_get_ext ( self . _revoked , i ) obj = _lib . X509_EXTENSION_get_object ( ext ) <MASK> _lib . X509_EXTENSION_free ( ext ) _lib . X509_REVOKED_delete_ext ( self . _revoked , i ) break",if _lib . OBJ_obj2nid ( obj ) == _lib . NID_crl_reason :,if obj . type == _lib . OBJ_TYPE_UNUSED :,88.63634524,FALSE,87.84
3353,"def hexcmp ( x , y ) : try : a = int ( x , 16 ) b = int ( y , 16 ) if a < b : return - 1 <MASK> return 1 return 0 except : return cmp ( x , y )",if a > b :,if b > a :,71.18284105,FALSE,92.37
3354,"def get_indentation_count ( view , start ) : indent_count = 0 i = start - 1 while i > 0 : ch = view . substr ( i ) scope = view . scope_name ( i ) # Skip preprocessors, strings, characaters and comments if "" string.quoted "" in scope or "" comment "" in scope or "" preprocessor "" in scope : extent = view . extract_scope ( i ) i = extent . a - 1 continue else : i - = 1 <MASK> indent_count - = 1 elif ch == "" { "" : indent_count + = 1 return indent_count","if ch == ""}"" :","if ch == ""{"" :",98.82614977,FALSE,98.06
3355,"def set ( self , name , value , ex = None , px = None , nx = False , xx = False ) : if ( ( not nx and not xx ) or ( nx and self . _db . get ( name , None ) is None ) or ( xx and not self . _db . get ( name , None ) is None ) ) : if ex > 0 : self . _db . expire ( name , datetime . now ( ) + timedelta ( seconds = ex ) ) <MASK> self . _db . expire ( name , datetime . now ( ) + timedelta ( milliseconds = px ) ) self . _db [ name ] = str ( value ) return True else : return None",elif px > 0 :,if px is not None :,69.05143147,FALSE,95.69
3356,"def _get_between ( content , start , end = None ) : should_yield = False for line in content . split ( "" \n "" ) : if start in line : should_yield = True continue if end and end in line : return <MASK> yield line . strip ( ) . split ( ""   "" ) [ 0 ]",if should_yield and line :,if should_yield :,94.30666462,FALSE,95.15
3357,"def iter_event_handlers ( self , resource : resources_ . Resource , event : bodies . RawEvent , ) - > Iterator [ handlers . ResourceWatchingHandler ] : warnings . warn ( "" SimpleRegistry.iter_event_handlers() is deprecated; use  "" "" ResourceWatchingRegistry.iter_handlers(). "" , DeprecationWarning , ) cause = _create_watching_cause ( resource , event ) for handler in self . _handlers : <MASK> pass elif registries . match ( handler = handler , cause = cause , ignore_fields = True ) : yield handler","if not isinstance ( handler , handlers . ResourceWatchingHandler ) :","if isinstance ( handler , handlers . ResourceWatchingHandler ) :",98.42549928,FALSE,97.88
3358,"def __enter__ ( self ) : if log_timer : <MASK> self . logger . debug ( "" %s  starting "" % self . name ) else : print ( ( "" [ %s  starting]... "" % self . name ) ) self . tstart = time . time ( )",if self . logger :,if self . tstart >= self . t_max :,93.22006364,FALSE,86.67
3359,"def _handle_errors ( errors ) : """"""Log out and possibly reraise errors during import."""""" if not errors : return log_all = True # pylint: disable=unused-variable err_msg = "" T2T: skipped importing  {num_missing}  data_generators modules. "" print ( err_msg . format ( num_missing = len ( errors ) ) ) for module , err in errors : err_str = str ( err ) <MASK> print ( "" Did not import module:  %s ; Cause:  %s "" % ( module , err_str ) ) if not _is_import_err_msg ( err_str , module ) : print ( "" From module  %s "" % module ) raise err",if log_all :,if not log_all :,99.03064171,FALSE,98.36
3360,"def _ungroup ( sequence , groups = None ) : for v in sequence : if isinstance ( v , ( list , tuple ) ) : <MASK> groups . append ( list ( _ungroup ( v , groups = None ) ) ) for v in _ungroup ( v , groups ) : yield v else : yield v",if groups is not None :,if groups is not None :,100,TRUE,100
3361,def run ( self ) : while not self . completed : if self . block : time . sleep ( self . period ) else : self . _completed . wait ( self . period ) self . counter + = 1 try : self . callback ( self . counter ) except Exception : self . stop ( ) if self . timeout is not None : dt = time . time ( ) - self . _start_time <MASK> self . stop ( ) if self . counter == self . count : self . stop ( ),if dt > self . timeout :,if dt < self . timeout :,98.81398574,FALSE,97.7
3362,"def dont_let_stderr_buffer ( ) : while True : line = context . daemon . stderr . readline ( ) <MASK> return if DEAD_DEPLOYD_WORKER_MESSAGE . encode ( "" utf-8 "" ) in line : context . num_workers_crashed + = 1 print ( f "" deployd stderr:  { line } "" )",if not line :,if not line :,100,TRUE,100
3363,"def mergeHiLo ( self , x_stats ) : """"""Merge the highs and lows of another accumulator into myself."""""" if x_stats . firsttime is not None : if self . firsttime is None or x_stats . firsttime < self . firsttime : self . firsttime = x_stats . firsttime self . first = x_stats . first if x_stats . lasttime is not None : <MASK> self . lasttime = x_stats . lasttime self . last = x_stats . last",if self . lasttime is None or x_stats . lasttime >= self . lasttime :,if self . lasttime is None or x_stats . lasttime > self . lasttime,95.70257449,FALSE,95.68
3364,"def test_rlimit_get ( self ) : import resource p = psutil . Process ( os . getpid ( ) ) names = [ x for x in dir ( psutil ) if x . startswith ( "" RLIMIT "" ) ] assert names for name in names : value = getattr ( psutil , name ) self . assertGreaterEqual ( value , 0 ) <MASK> self . assertEqual ( value , getattr ( resource , name ) ) self . assertEqual ( p . rlimit ( value ) , resource . getrlimit ( value ) ) else : ret = p . rlimit ( value ) self . assertEqual ( len ( ret ) , 2 ) self . assertGreaterEqual ( ret [ 0 ] , - 1 ) self . assertGreaterEqual ( ret [ 1 ] , - 1 )",if name in dir ( resource ) :,"if hasattr ( resource , name ) :",94.34097946,FALSE,96.32
3365,"def _calculate_writes_for_built_in_indices ( self , entity ) : writes = 0 for prop_name in entity . keys ( ) : if not prop_name in entity . unindexed_properties ( ) : prop_vals = entity [ prop_name ] <MASK> num_prop_vals = len ( prop_vals ) else : num_prop_vals = 1 writes + = 2 * num_prop_vals return writes","if isinstance ( prop_vals , ( list ) ) :","if isinstance ( prop_vals , list ) :",91.56266005,FALSE,96
3366,"def check_value_check ( self , x_data , t_data , use_cudnn ) : x = chainer . Variable ( x_data ) t = chainer . Variable ( t_data ) with chainer . using_config ( "" use_cudnn "" , use_cudnn ) : <MASK> # Check if it throws nothing functions . softmax_cross_entropy ( x , t , enable_double_backprop = self . enable_double_backprop ) else : with self . assertRaises ( ValueError ) : functions . softmax_cross_entropy ( x , t , enable_double_backprop = self . enable_double_backprop )",if self . valid :,if self . use_cudnn :,98.57321592,FALSE,96.62
3367,"def get_note_title_file ( note ) : mo = note_title_re . match ( note . get ( "" content "" , "" "" ) ) if mo : fn = mo . groups ( ) [ 0 ] fn = fn . replace ( ""   "" , "" _ "" ) fn = fn . replace ( "" / "" , "" _ "" ) <MASK> return "" "" if isinstance ( fn , str ) : fn = unicode ( fn , "" utf-8 "" ) else : fn = unicode ( fn ) if note_markdown ( note ) : fn + = "" .mkdn "" else : fn + = "" .txt "" return fn else : return "" """,if not fn :,if not fn :,100,TRUE,100
3368,"def _parseparam ( s ) : plist = [ ] while s [ : 1 ] == "" ; "" : s = s [ 1 : ] end = s . find ( "" ; "" ) while end > 0 and ( s . count ( ' "" ' , 0 , end ) - s . count ( ' \\ "" ' , 0 , end ) ) % 2 : end = s . find ( "" ; "" , end + 1 ) if end < 0 : end = len ( s ) f = s [ : end ] <MASK> i = f . index ( "" = "" ) f = f [ : i ] . strip ( ) . lower ( ) + "" = "" + f [ i + 1 : ] . strip ( ) plist . append ( f . strip ( ) ) s = s [ end : ] return plist","if ""="" in f :","if ""="" in f :",100,TRUE,100
3369,"def doDir ( elem ) : for child in elem . childNodes : if not isinstance ( child , minidom . Element ) : continue if child . tagName == "" Directory "" : doDir ( child ) elif child . tagName == "" Component "" : for grandchild in child . childNodes : if not isinstance ( grandchild , minidom . Element ) : continue <MASK> continue files . add ( grandchild . getAttribute ( "" Source "" ) . replace ( os . sep , "" / "" ) )","if grandchild . tagName != ""File"" :","if grandchild . getAttribute ( ""Source"" ) . startswith ( ""/"" ) :",92.91052575,FALSE,87.06
3370,"def date_to_format ( value , target_format ) : """"""Convert date to specified format"""""" if target_format == str : <MASK> ret = value . strftime ( "" %d / % m/ % y "" ) elif isinstance ( value , datetime . datetime ) : ret = value . strftime ( "" %d / % m/ % y "" ) elif isinstance ( value , datetime . time ) : ret = value . strftime ( "" % H: % M: % S "" ) else : ret = value return ret","if isinstance ( value , datetime . date ) :","if isinstance ( value , datetime . date ) :",100,TRUE,100
3371,"def __listingColumns ( self ) : columns = [ ] for name in self . __getColumns ( ) : definition = column ( name ) <MASK> IECore . msg ( IECore . Msg . Level . Error , "" GafferImageUI.CatalogueUI "" , "" No column registered with name  ' %s ' "" % name , ) continue if isinstance ( definition , IconColumn ) : c = GafferUI . PathListingWidget . IconColumn ( definition . title ( ) , "" "" , name ) else : c = GafferUI . PathListingWidget . StandardColumn ( definition . title ( ) , name ) columns . append ( c ) return columns",if not definition :,if not definition :,100,TRUE,100
3372,"def metrics_to_scalars ( self , metrics ) : new_metrics = { } for k , v in metrics . items ( ) : <MASK> v = v . item ( ) if isinstance ( v , dict ) : v = self . metrics_to_scalars ( v ) new_metrics [ k ] = v return new_metrics","if isinstance ( v , torch . Tensor ) :","if isinstance ( v , dict ) :",86.91121305,FALSE,94.1
3373,"def start ( self , connection ) : try : if self . client_name : creds = gssapi . Credentials ( name = gssapi . Name ( self . client_name ) ) else : creds = None hostname = self . get_hostname ( connection ) name = gssapi . Name ( b "" @ "" . join ( [ self . service , hostname ] ) , gssapi . NameType . hostbased_service ) context = gssapi . SecurityContext ( name = name , creds = creds ) return context . step ( None ) except gssapi . raw . misc . GSSError : <MASK> return NotImplemented else : raise",if self . fail_soft :,"if self . service in ( ""no-service"" , ""no-service-name""",64.25200081,FALSE,90.97
3374,"def nanmax ( self , axis = None , dtype = None , keepdims = None ) : ret = self . _reduction ( "" nanmax "" , axis = axis , dtype = dtype , keepdims = keepdims , todense = True ) if not issparse ( ret ) : <MASK> return ret xps = get_sparse_module ( self . spmatrix ) ret = SparseNDArray ( xps . csr_matrix ( ret ) ) return ret return ret",if get_array_module ( ret ) . isscalar ( ret ) :,if not self . spmatrix :,88.44194809,FALSE,84.92
3375,"def utterance_to_sample ( query_data , tagging_scheme , language ) : tokens , tags = [ ] , [ ] current_length = 0 for chunk in query_data : chunk_tokens = tokenize ( chunk [ TEXT ] , language ) tokens + = [ Token ( t . value , current_length + t . start , current_length + t . end ) for t in chunk_tokens ] current_length + = len ( chunk [ TEXT ] ) <MASK> tags + = negative_tagging ( len ( chunk_tokens ) ) else : tags + = positive_tagging ( tagging_scheme , chunk [ SLOT_NAME ] , len ( chunk_tokens ) ) return { TOKENS : tokens , TAGS : tags }",if SLOT_NAME not in chunk :,"if tagging_scheme == ""negative"" :",93.63798229,FALSE,94.08
3376,"def use_index ( self , term : Union [ str , Index ] , * terms : Union [ str , Index ] ) - > "" QueryBuilder "" : for t in ( term , * terms ) : if isinstance ( t , Index ) : self . _use_indexes . append ( t ) <MASK> self . _use_indexes . append ( Index ( t ) )","elif isinstance ( t , str ) :","elif isinstance ( t , str ) :",100,TRUE,100
3377,"def reconfigServiceWithBuildbotConfig ( self , new_config ) : if new_config . manhole != self . manhole : if self . manhole : yield self . manhole . disownServiceParent ( ) self . manhole = None <MASK> self . manhole = new_config . manhole yield self . manhole . setServiceParent ( self ) # chain up yield service . ReconfigurableServiceMixin . reconfigServiceWithBuildbotConfig ( self , new_config )",if new_config . manhole :,if self . manhole :,84.29236315,FALSE,94.32
3378,"def cleanup_folder ( target_folder ) : for file in os . listdir ( target_folder ) : file_path = os . path . join ( target_folder , file ) try : <MASK> os . remove ( file_path ) except Exception as e : logging . error ( e )",if os . path . isfile ( file_path ) :,if os . path . exists ( file_path ) :,97.74990044,FALSE,96.36
3379,"def to_key ( literal_or_identifier ) : """"""returns string representation of this object"""""" if literal_or_identifier [ "" type "" ] == "" Identifier "" : return literal_or_identifier [ "" name "" ] elif literal_or_identifier [ "" type "" ] == "" Literal "" : k = literal_or_identifier [ "" value "" ] <MASK> return unicode ( float_repr ( k ) ) elif "" regex "" in literal_or_identifier : return compose_regex ( k ) elif isinstance ( k , bool ) : return "" true "" if k else "" false "" elif k is None : return "" null "" else : return unicode ( k )","if isinstance ( k , float ) :","if isinstance ( k , float ) :",100,TRUE,100
3380,"def decompile ( decompiler ) : for pos , next_pos , opname , arg in decompiler . instructions : if pos in decompiler . targets : decompiler . process_target ( pos ) method = getattr ( decompiler , opname , None ) <MASK> throw ( DecompileError ( "" Unsupported operation:  %s "" % opname ) ) decompiler . pos = pos decompiler . next_pos = next_pos x = method ( * arg ) if x is not None : decompiler . stack . append ( x )",if method is None :,if method is None :,100,TRUE,100
3381,"def shutdown ( self , timeout , callback = None ) : logger . debug ( "" background worker got shutdown request "" ) with self . _lock : if self . is_alive : self . _queue . put_nowait ( _TERMINATOR ) <MASK> self . _wait_shutdown ( timeout , callback ) self . _thread = None self . _thread_for_pid = None logger . debug ( "" background worker shut down "" )",if timeout > 0.0 :,if callback :,92.33396274,FALSE,95.01
3382,"def getDOMImplementation ( features = None ) : if features : <MASK> features = domreg . _parse_feature_string ( features ) for f , v in features : if not Document . implementation . hasFeature ( f , v ) : return None return Document . implementation","if isinstance ( features , str ) :","if isinstance ( features , str ) :",100,TRUE,100
3383,"def validate_subevent ( self , subevent ) : if self . context [ "" event "" ] . has_subevents : <MASK> raise ValidationError ( "" You need to set a subevent. "" ) if subevent . event != self . context [ "" event "" ] : raise ValidationError ( "" The specified subevent does not belong to this event. "" ) elif subevent : raise ValidationError ( "" You cannot set a subevent for this event. "" ) return subevent",if not subevent :,"if not isinstance ( subevent , Event ) :",83.05977188,FALSE,92.38
3384,"def einsum ( job_id , idx , einsum_expr , data_list ) : _ , all_parties = session_init ( job_id , idx ) with SPDZ ( ) : <MASK> x = FixedPointTensor . from_source ( "" x "" , data_list [ 0 ] ) y = FixedPointTensor . from_source ( "" y "" , all_parties [ 1 ] ) else : x = FixedPointTensor . from_source ( "" x "" , all_parties [ 0 ] ) y = FixedPointTensor . from_source ( "" y "" , data_list [ 1 ] ) return x . einsum ( y , einsum_expr ) . get ( )",if idx == 0 :,if idx == 0 :,100,TRUE,100
3385,"def slowSorted ( qq ) : "" Reference sort peformed by insertion using only < "" rr = list ( ) for q in qq : i = 0 for i in range ( len ( rr ) ) : <MASK> rr . insert ( i , q ) break else : rr . append ( q ) return rr",if q < rr [ i ] :,if q [ i ] < 0 :,91.78032376,FALSE,92.59
3386,"def _format_entry ( entry , src ) : if entry : result = [ ] for x in entry . split ( "" , "" ) : x = x . strip ( ) if os . path . exists ( os . path . join ( src , x ) ) : result . append ( relpath ( os . path . join ( src , x ) , src ) ) <MASK> result . append ( relpath ( os . path . abspath ( x ) , src ) ) else : raise RuntimeError ( "" No entry script  %s  found "" % x ) return "" , "" . join ( result )",elif os . path . exists ( x ) :,elif os . path . exists ( os . path . abspath ( x ) ) :,94.88867894,FALSE,93.66
3387,"def reloadCols ( self ) : self . columns = [ ] for i , ( name , fmt , * shape ) in enumerate ( self . npy . dtype . descr ) : if shape : t = anytype elif "" M "" in fmt : self . addColumn ( Column ( name , type = date , getter = lambda c , r , i = i : str ( r [ i ] ) ) ) continue elif "" i "" in fmt : t = int <MASK> t = float else : t = anytype self . addColumn ( ColumnItem ( name , i , type = t ) )","elif ""f"" in fmt :","elif ""f"" in fmt :",100,TRUE,100
3388,"def tool_lineages ( self , trans ) : rval = [ ] for id , tool in self . app . toolbox . tools ( ) : <MASK> lineage_dict = tool . lineage . to_dict ( ) else : lineage_dict = None entry = dict ( id = id , lineage = lineage_dict ) rval . append ( entry ) return rval","if hasattr ( tool , ""lineage"" ) :","if hasattr ( tool , ""lineage"" ) :",100,TRUE,100
3389,"def item ( self , tensor ) : numel = 0 if len ( tensor . shape ) > 0 : numel = fct . reduce ( op . mul , tensor . shape ) <MASK> raise ValueError ( f "" expected tensor with one element,  "" f "" got  { tensor . shape } "" ) if numel == 1 : return tensor [ 0 ] return tensor",if numel != 1 :,if numel != 0 :,98.25777289,FALSE,96.61
3390,"def get_host_metadata ( self ) : meta = { } if self . agent_url : try : resp = requests . get ( self . agent_url + ECS_AGENT_METADATA_PATH , timeout = 1 ) . json ( ) if "" Version "" in resp : match = AGENT_VERSION_EXP . search ( resp . get ( "" Version "" ) ) <MASK> meta [ "" ecs_version "" ] = match . group ( 1 ) except Exception as e : self . log . debug ( "" Error getting ECS version:  %s "" % str ( e ) ) return meta",if match is not None and len ( match . groups ( ) ) == 1 :,if match :,61.8790219,FALSE,87.41
3391,"def generate ( ) : for leaf in u . leaves : if isinstance ( leaf , Integer ) : val = leaf . get_int_value ( ) if val in ( 0 , 1 ) : yield val else : raise _NoBoolVector elif isinstance ( leaf , Symbol ) : <MASK> yield 1 elif leaf == SymbolFalse : yield 0 else : raise _NoBoolVector else : raise _NoBoolVector",if leaf == SymbolTrue :,if leaf == SymbolTrue :,100,TRUE,100
3392,"def _test_set_metadata ( self , metadata , mask = None ) : header = ofproto . OXM_OF_METADATA match = OFPMatch ( ) if mask is None : match . set_metadata ( metadata ) else : <MASK> header = ofproto . OXM_OF_METADATA_W match . set_metadata_masked ( metadata , mask ) metadata & = mask self . _test_serialize_and_parser ( match , header , metadata , mask )",if ( mask + 1 ) >> 64 != 1 :,if mask :,66.84376039,FALSE,87.81
3393,"def pixbufrenderer ( self , column , crp , model , it ) : tok = model . get_value ( it , 0 ) if tok . type == "" class "" : icon = "" class "" else : if tok . visibility == "" private "" : icon = "" method_priv "" <MASK> icon = "" method_prot "" else : icon = "" method "" crp . set_property ( "" pixbuf "" , imagelibrary . pixbufs [ icon ] )","elif tok . visibility == ""protected"" :","elif tok . visibility == ""prot"" :",98.56033509,FALSE,97.47
3394,"def path_sum2 ( root , s ) : if root is None : return [ ] res = [ ] stack = [ ( root , [ root . val ] ) ] while stack : node , ls = stack . pop ( ) if node . left is None and node . right is None and sum ( ls ) == s : res . append ( ls ) <MASK> stack . append ( ( node . left , ls + [ node . left . val ] ) ) if node . right is not None : stack . append ( ( node . right , ls + [ node . right . val ] ) ) return res",if node . left is not None :,if node . left is not None :,100,TRUE,100
3395,"def clear_slot ( self , slot_id , trigger_changed ) : if self . slots [ slot_id ] is not None : old_resource_id = self . slots [ slot_id ] . resource_id <MASK> del self . sell_list [ old_resource_id ] else : del self . buy_list [ old_resource_id ] self . slots [ slot_id ] = None if trigger_changed : self . _changed ( )",if self . slots [ slot_id ] . selling :,if old_resource_id in self . sell_list :,62.93707839,FALSE,89.7
3396,"def OnRightUp ( self , event ) : self . HandleMouseEvent ( event ) self . Unbind ( wx . EVT_RIGHT_UP , handler = self . OnRightUp ) self . Unbind ( wx . EVT_MOUSE_CAPTURE_LOST , handler = self . OnRightUp ) self . _right = False if not self . _left : self . Unbind ( wx . EVT_MOTION , handler = self . OnMotion ) self . SendChangeEvent ( ) self . SetToolTip ( wx . ToolTip ( self . _tooltip ) ) <MASK> self . ReleaseMouse ( )",if self . HasCapture ( ) :,if self . GetMouse ( ) :,98.69118487,FALSE,97.72
3397,"def __init__ ( self , * args , * * kwargs ) : for arg in args : for k , v in arg . items ( ) : <MASK> arg [ k ] = AttrDict ( v ) else : arg [ k ] = v super ( AttrDict , self ) . __init__ ( * args , * * kwargs )","if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",100,TRUE,100
3398,"def _toplevelTryFunc ( func , * args , status = status , * * kwargs ) : with ThreadProfiler ( threading . current_thread ( ) ) as prof : t = threading . current_thread ( ) t . name = func . __name__ try : t . status = func ( * args , * * kwargs ) except EscapeException as e : # user aborted t . status = "" aborted by user "" if status : status ( "" %s  aborted "" % t . name , priority = 2 ) except Exception as e : t . exception = e t . status = "" exception "" vd . exceptionCaught ( e ) <MASK> t . sheet . currentThreads . remove ( t )",if t . sheet :,if t . sheet :,100,TRUE,100
3399,"def comboSelectionChanged ( self , index ) : text = self . comboBox . cb . itemText ( index ) for i in range ( self . labelList . count ( ) ) : if text == "" "" : self . labelList . item ( i ) . setCheckState ( 2 ) <MASK> self . labelList . item ( i ) . setCheckState ( 0 ) else : self . labelList . item ( i ) . setCheckState ( 2 )",elif text != self . labelList . item ( i ) . text ( ) :,"elif text == """" :",89.36248051,FALSE,83.98
3400,"def __attempt_add_to_linked_match ( self , input_name , hdca , collection_type_description , subcollection_type ) : structure = get_structure ( hdca , collection_type_description , leaf_subcollection_type = subcollection_type ) if not self . linked_structure : self . linked_structure = structure self . collections [ input_name ] = hdca self . subcollection_types [ input_name ] = subcollection_type else : <MASK> raise exceptions . MessageException ( CANNOT_MATCH_ERROR_MESSAGE ) self . collections [ input_name ] = hdca self . subcollection_types [ input_name ] = subcollection_type",if not self . linked_structure . can_match ( structure ) :,if not structure . has_match ( input_name ) :,92.47763586,FALSE,93.18
3401,"def _wait_for_bot_presense ( self , online ) : for _ in range ( 10 ) : time . sleep ( 2 ) <MASK> break if not online and not self . _is_testbot_online ( ) : break else : raise AssertionError ( "" test bot is still  {} "" . format ( "" offline "" if online else "" online "" ) )",if online and self . _is_testbot_online ( ) :,if self . _is_offline ( ) :,84.43027389,FALSE,90.65
3402,"def find ( self , path ) : if os . path . isfile ( path ) or os . path . islink ( path ) : self . num_files = self . num_files + 1 <MASK> self . files . append ( path ) elif os . path . isdir ( path ) : for content in os . listdir ( path ) : file = os . path . join ( path , content ) if os . path . isfile ( file ) or os . path . islink ( file ) : self . num_files = self . num_files + 1 if self . match_function ( file ) : self . files . append ( file ) else : self . find ( file )",if self . match_function ( path ) :,if self . match_function ( path ) :,100,TRUE,100
3403,"def optimize ( self , graph : Graph ) : MAX_TEXTURE_SIZE = config . WEBGL_MAX_TEXTURE_SIZE flag_changed = False for v in traverse . listup_variables ( graph ) : if not Placeholder . check_resolved ( v . size ) : continue height , width = TextureShape . get ( v ) <MASK> continue if not v . has_attribute ( SplitTarget ) : flag_changed = True v . attributes . add ( SplitTarget ( ) ) return graph , flag_changed",if height <= MAX_TEXTURE_SIZE and width <= MAX_TEXTURE_SIZE :,if height > MAX_TEXTURE_SIZE :,91.50477014,FALSE,89
3404,"def brightness_func ( args ) : device = _get_device_from_filter ( args ) if args . set is None : # Get brightness if args . raw : print ( str ( device . brightness ) ) else : print ( "" Brightness:  {0} % "" . format ( device . brightness ) ) else : brightness_value = float ( _clamp_u8 ( args . set ) ) <MASK> print ( "" Setting brightness to  {0} % "" . format ( brightness_value ) ) device . brightness = brightness_value",if not args . raw :,if brightness_value > 0 :,96.83397698,FALSE,94.31
3405,"def _setup ( self , field_name , owner_model ) : # Resolve possible name-based model reference. if not self . model_class : <MASK> self . model_class = owner_model else : raise Exception ( "" ModelType: Unable to resolve model  ' {} ' . "" . format ( self . model_name ) ) super ( ModelType , self ) . _setup ( field_name , owner_model )",if self . model_name == owner_model . __name__ :,if owner_model :,69.27928409,FALSE,83.56
3406,"def build_json_schema_object ( cls , parent_builder = None ) : builder = builders . ObjectBuilder ( cls , parent_builder ) if builder . count_type ( builder . type ) > 1 : return builder for _ , name , field in cls . iterate_with_name ( ) : if isinstance ( field , fields . EmbeddedField ) : builder . add_field ( name , field , _parse_embedded ( field , builder ) ) <MASK> builder . add_field ( name , field , _parse_list ( field , builder ) ) else : builder . add_field ( name , field , _create_primitive_field_schema ( field ) ) return builder","elif isinstance ( field , fields . ListField ) :","elif isinstance ( field , fields . ListField ) :",100,TRUE,100
3407,"def filter_module ( mod , type_req = None , subclass_req = None ) : for name in dir ( mod ) : val = getattr ( mod , name ) if type_req is not None and not isinstance ( val , type_req ) : continue <MASK> continue yield name , val","if subclass_req is not None and not issubclass ( val , subclass_req ) :","if subclass_req is not None and not isinstance ( val , subclass_req ) :",98.02702529,FALSE,96.7
3408,"def get_icon ( self ) : if self . icon is not None : # Load it from an absolute filename if os . path . exists ( self . icon ) : try : return GdkPixbuf . Pixbuf . new_from_file_at_size ( self . icon , 24 , 24 ) except GObject . GError as ge : pass # Load it from the current icon theme ( icon_name , extension ) = os . path . splitext ( os . path . basename ( self . icon ) ) theme = Gtk . IconTheme ( ) <MASK> return theme . load_icon ( icon_name , 24 , 0 )",if theme . has_icon ( icon_name ) :,"if extension == ""png"" :",96.79737858,FALSE,91.36
3409,"def sysctlTestAndSet ( name , limit ) : "" Helper function to set sysctl limits "" # convert non-directory names into directory names if "" / "" not in name : name = "" /proc/sys/ "" + name . replace ( "" . "" , "" / "" ) # read limit with open ( name , "" r "" ) as readFile : oldLimit = readFile . readline ( ) if isinstance ( limit , int ) : # compare integer limits before overriding <MASK> with open ( name , "" w "" ) as writeFile : writeFile . write ( "" %d "" % limit ) else : # overwrite non-integer limits with open ( name , "" w "" ) as writeFile : writeFile . write ( limit )",if int ( oldLimit ) < limit :,if limit != oldLimit :,72.49175111,FALSE,95.07
3410,"def _wait_for_bot_presense ( self , online ) : for _ in range ( 10 ) : time . sleep ( 2 ) if online and self . _is_testbot_online ( ) : break <MASK> break else : raise AssertionError ( "" test bot is still  {} "" . format ( "" offline "" if online else "" online "" ) )",if not online and not self . _is_testbot_online ( ) :,elif online and self . _is_testbot_online ( ) :,65.92480716,FALSE,93.31
3411,"def handle ( self , context , sign , * args ) : if context . rounding in ( ROUND_HALF_UP , ROUND_HALF_EVEN , ROUND_HALF_DOWN , ROUND_UP ) : return Infsign [ sign ] if sign == 0 : if context . rounding == ROUND_CEILING : return Infsign [ sign ] return Decimal ( ( sign , ( 9 , ) * context . prec , context . Emax - context . prec + 1 ) ) if sign == 1 : <MASK> return Infsign [ sign ] return Decimal ( ( sign , ( 9 , ) * context . prec , context . Emax - context . prec + 1 ) )",if context . rounding == ROUND_FLOOR :,if context . rounding == ROUND_DOWN :,98.94401678,FALSE,98.2
3412,"def _get_item_columns_panel ( items , rows ) : hbox = Gtk . HBox ( False , 4 ) n_item = 0 col_items = 0 vbox = Gtk . VBox ( ) hbox . pack_start ( vbox , False , False , 0 ) while n_item < len ( items ) : item = items [ n_item ] vbox . pack_start ( item , False , False , 0 ) n_item + = 1 col_items + = 1 <MASK> vbox = Gtk . VBox ( ) hbox . pack_start ( vbox , False , False , 0 ) col_items = 0 return hbox",if col_items > rows :,if col_items == rows :,98.81191189,FALSE,97.39
3413,"def _changed ( self ) : if self . gtk_range . get_sensitive ( ) : <MASK> self . timer . cancel ( ) self . timer = _Timer ( 0.5 , lambda : GLib . idle_add ( self . _write ) ) self . timer . start ( )",if self . timer :,if self . timer :,100,TRUE,100
3414,"def unlock_graph ( result , callback , interval = 1 , propagate = False , max_retries = None ) : if result . ready ( ) : second_level_res = result . get ( ) <MASK> with allow_join_result ( ) : signature ( callback ) . delay ( list ( joinall ( second_level_res , propagate = propagate ) ) ) else : unlock_graph . retry ( countdown = interval , max_retries = max_retries )",if second_level_res . ready ( ) :,if second_level_res :,89.2450717,FALSE,94.65
3415,"def update ( self , other = None , / , * * kwargs ) : if self . _pending_removals : self . _commit_removals ( ) d = self . data if other is not None : <MASK> other = dict ( other ) for key , o in other . items ( ) : d [ key ] = KeyedRef ( o , self . _remove , key ) for key , o in kwargs . items ( ) : d [ key ] = KeyedRef ( o , self . _remove , key )","if not hasattr ( other , ""items"" ) :","if isinstance ( other , dict ) :",89.63595658,FALSE,92.85
3416,"def default ( self , o ) : try : if type ( o ) == datetime . datetime : return str ( o ) else : # remove unwanted attributes from the provider object during conversion to json if hasattr ( o , "" profile "" ) : del o . profile if hasattr ( o , "" credentials "" ) : del o . credentials if hasattr ( o , "" metadata_path "" ) : del o . metadata_path <MASK> del o . services_config return vars ( o ) except Exception as e : return str ( o )","if hasattr ( o , ""services_config"" ) :","if hasattr ( o , ""services_config"" ) :",75,TRUE,100
3417,"def read ( self , count = True , timeout = None , ignore_non_errors = True , ignore_timeouts = True ) : try : return self . _read ( count , timeout ) except usb . USBError as e : <MASK> log . info ( "" read: e.errno= %s  e.strerror= %s  e.message= %s  repr= %s "" % ( e . errno , e . strerror , e . message , repr ( e ) ) ) if ignore_timeouts and is_timeout ( e ) : return [ ] if ignore_non_errors and is_noerr ( e ) : return [ ] raise",if DEBUG_COMM :,if log_enabled ( ) :,96.40311908,FALSE,95.37
3418,def heal ( self ) : if not self . doctors : return proc_ids = self . _get_process_ids ( ) for proc_id in proc_ids : # get proc every time for latest state proc = PipelineProcess . objects . get ( id = proc_id ) <MASK> continue for dr in self . doctors : if dr . confirm ( proc ) : dr . cure ( proc ) break,if not proc . is_alive or proc . is_frozen :,if not proc . is_alive ( ) :,96.35037938,FALSE,91.93
3419,"def to_value ( self , value ) : # Tip: 'value' is the object returned by #      taiga.projects.history.models.HistoryEntry.values_diff() ret = { } for key , val in value . items ( ) : <MASK> ret [ key ] = val elif key == "" points "" : ret [ key ] = { k : { "" from "" : v [ 0 ] , "" to "" : v [ 1 ] } for k , v in val . items ( ) } else : ret [ key ] = { "" from "" : val [ 0 ] , "" to "" : val [ 1 ] } return ret","if key in [ ""attachments"" , ""custom_attributes"" , ""description_diff"" ] :","if key == ""name"" :",71.04691451,FALSE,86.99
3420,"def default_generator ( self , dataset , epochs = 1 , mode = "" fit "" , deterministic = True , pad_batches = True ) : for epoch in range ( epochs ) : for ( X_b , y_b , w_b , ids_b ) in dataset . iterbatches ( batch_size = self . batch_size , deterministic = deterministic , pad_batches = pad_batches , ) : <MASK> dropout = np . array ( 0.0 ) else : dropout = np . array ( 1.0 ) yield ( [ X_b , dropout ] , [ y_b ] , [ w_b ] )","if mode == ""predict"" :","if mode == ""train"" :",98.78761533,FALSE,98.09
3421,"def _cygwin_hack_find_addresses ( target ) : addresses = [ ] for h in [ target , "" localhost "" , "" 127.0.0.1 "" , ] : try : addr = get_local_ip_for ( h ) <MASK> addresses . append ( addr ) except socket . gaierror : pass return defer . succeed ( addresses )",if addr not in addresses :,if addr :,89.83668499,FALSE,93.58
3422,"def _get_notify ( self , action_node ) : if action_node . name not in self . _skip_notify_tasks : if action_node . notify : task_notify = NotificationsHelper . to_model ( action_node . notify ) return task_notify <MASK> return self . _chain_notify return None",elif self . _chain_notify :,if self . _chain_notify :,90.66652717,FALSE,96.51
3423,"def filterTokenLocation ( ) : i = None entry = None token = None tokens = [ ] i = 0 while 1 : <MASK> break entry = extra . tokens [ i ] token = jsdict ( { "" type "" : entry . type , "" value "" : entry . value , } ) if extra . range : token . range = entry . range if extra . loc : token . loc = entry . loc tokens . append ( token ) i + = 1 extra . tokens = tokens",if not ( i < len ( extra . tokens ) ) :,if i >= len ( extra . tokens ) :,93.29645179,FALSE,93.39
3424,"def read ( self , size = - 1 ) : buf = bytearray ( ) while size != 0 and self . cursor < self . maxpos : if not self . in_current_block ( self . cursor ) : self . seek_to_block ( self . cursor ) part = self . current_stream . read ( size ) <MASK> if len ( part ) == 0 : raise EOFError ( ) size - = len ( part ) self . cursor + = len ( part ) buf + = part return bytes ( buf )",if size > 0 :,if not part :,96.08980034,FALSE,95.92
3425,"def get_properties_from_model ( model_class ) : """"""Show properties from a model"""""" properties = [ ] attr_names = [ name for ( name , value ) in inspect . getmembers ( model_class , isprop ) ] for attr_name in attr_names : <MASK> attr_names . remove ( attr_name ) else : properties . append ( dict ( label = attr_name , name = attr_name . strip ( "" _ "" ) . replace ( "" _ "" , ""   "" ) ) ) return sorted ( properties , key = lambda k : k [ "" label "" ] )","if attr_name . endswith ( ""pk"" ) :","if attr_name . startswith ( ""_"" ) :",97.96547388,FALSE,96.52
3426,"def __getitem__ ( self , name , set = set , getattr = getattr , id = id ) : visited = set ( ) mydict = self . basedict while 1 : value = mydict [ name ] if value is not None : return value myid = id ( mydict ) assert myid not in visited visited . add ( myid ) mydict = mydict . Parent <MASK> return",if mydict is None :,if myid is None :,97.12525143,FALSE,96.74
3427,"def multicolumn ( self , list , format , cols = 4 ) : """"""Format a list of items into a multi-column list."""""" result = "" "" rows = ( len ( list ) + cols - 1 ) / / cols for col in range ( cols ) : result = result + ' <td width= "" %d %% ""  valign=top> ' % ( 100 / / cols ) for i in range ( rows * col , rows * col + rows ) : <MASK> result = result + format ( list [ i ] ) + "" <br> \n "" result = result + "" </td> "" return ' <table width= "" 100 %% ""  summary= "" list "" ><tr> %s </tr></table> ' % result",if i < len ( list ) :,if list [ i ] :,78.13516971,FALSE,95.84
3428,"def format_exc ( exc = None ) : """"""Return exc (or sys.exc_info if None), formatted."""""" try : <MASK> exc = _exc_info ( ) if exc == ( None , None , None ) : return "" "" import traceback return "" "" . join ( traceback . format_exception ( * exc ) ) finally : del exc",if exc is None :,if exc is None :,100,TRUE,100
3429,"def assert_counts ( res , lang , files , blank , comment , code ) : for line in res : fields = line . split ( ) if len ( fields ) > = 5 : <MASK> self . assertEqual ( files , int ( fields [ 1 ] ) ) self . assertEqual ( blank , int ( fields [ 2 ] ) ) self . assertEqual ( comment , int ( fields [ 3 ] ) ) self . assertEqual ( code , int ( fields [ 4 ] ) ) return self . fail ( "" Found no output line for  {} "" . format ( lang ) )",if fields [ 0 ] == lang :,if fields [ 0 ] == lang :,100,TRUE,100
3430,"def __iter__ ( self ) : for name , value in self . __class__ . __dict__ . items ( ) : <MASK> continue if isinstance ( value , flag_value ) : yield ( name , self . _has_flag ( value . flag ) )","if isinstance ( value , alias_flag_value ) :","if name . startswith ( ""_"" ) :",87.43393235,FALSE,85.74
3431,"def optimize_models ( args , use_cuda , models ) : """"""Optimize ensemble for generation"""""" for model in models : model . make_generation_fast_ ( beamable_mm_beam_size = None if args . no_beamable_mm else args . beam , need_attn = args . print_alignment , ) if args . fp16 : model . half ( ) <MASK> model . cuda ( )",if use_cuda :,if use_cuda :,100,TRUE,100
3432,"def convertstore ( self , mydict ) : targetheader = self . mypofile . header ( ) targetheader . addnote ( "" extracted from web2py "" , "" developer "" ) for source_str in mydict . keys ( ) : target_str = mydict [ source_str ] if target_str == source_str : # a convention with new (untranslated) web2py files target_str = u "" "" <MASK> # an older convention target_str = u "" "" pounit = self . convertunit ( source_str , target_str ) self . mypofile . addunit ( pounit ) return self . mypofile","elif target_str . startswith ( u""*** "" ) :",elif target_str in targetheader :,96.40998795,FALSE,90.99
3433,"def __sparse_values_set ( instances , static_col_indexes : list ) : tmp_result = { idx : set ( ) for idx in static_col_indexes } for _ , instance in instances : data_generator = instance . features . get_all_data ( ) for idx , value in data_generator : <MASK> continue tmp_result [ idx ] . add ( value ) result = [ tmp_result [ x ] for x in static_col_indexes ] return result",if idx not in tmp_result :,if value is None :,90.38252657,FALSE,93.07
3434,def puts ( self ) : <MASK> self . lazy_init_lock_ . acquire ( ) try : <MASK> self . puts_ = PutRequest ( ) finally : self . lazy_init_lock_ . release ( ) return self . puts_,if self . puts_ is None :,if self . puts_ is None :,100,TRUE,100
3435,"def run ( self , args , * * kwargs ) : if args . resource_ref or args . policy_type : filters = { } if args . resource_ref : filters [ "" resource_ref "" ] = args . resource_ref <MASK> filters [ "" policy_type "" ] = args . policy_type filters . update ( * * kwargs ) return self . manager . query ( * * filters ) else : return self . manager . get_all ( * * kwargs )",if args . policy_type :,if args . policy_type :,100,TRUE,100
3436,"def Get_Gene ( self , id ) : """"""Retreive the gene name (GN)."""""" entry = self . Get ( id ) if not entry : return None GN = "" "" for line in string . split ( entry , "" \n "" ) : if line [ 0 : 5 ] == "" GN    "" : GN = string . strip ( line [ 5 : ] ) <MASK> GN = GN [ 0 : - 1 ] return GN if line [ 0 : 2 ] == "" // "" : break return GN","if GN [ - 1 ] == ""."" :","if GN [ - 1 ] == ""GN"" :",89.79089627,FALSE,97.91
3437,"def processMovie ( self , atom ) : for field in atom : <MASK> self . processTrack ( field [ "" track "" ] ) if "" movie_hdr "" in field : self . processMovieHeader ( field [ "" movie_hdr "" ] )","if ""track"" in field :","if ""track"" in field :",100,TRUE,100
3438,"def get_next_video_frame ( self , skip_empty_frame = True ) : if not self . video_format : return while True : # We skip video packets which are not video frames # This happens in mkv files for the first few frames. video_packet = self . _get_video_packet ( ) <MASK> self . _decode_video_packet ( video_packet ) if video_packet . image is not None or not skip_empty_frame : break if _debug : print ( "" Returning "" , video_packet ) return video_packet . image",if video_packet . image == 0 :,"if video_packet . type == ""video"" :",97.58944416,FALSE,94.76
3439,"def get_devices ( display = None ) : base = "" /dev/input "" for filename in os . listdir ( base ) : if filename . startswith ( "" event "" ) : path = os . path . join ( base , filename ) <MASK> continue try : _devices [ path ] = EvdevDevice ( display , path ) except OSError : pass return list ( _devices . values ( ) )",if path in _devices :,if not os . path . isdir ( path ) :,84.95731897,FALSE,88.38
3440,"def _ensure_header_written ( self , datasize ) : if not self . _headerwritten : if not self . _nchannels : raise Error ( "" # channels not specified "" ) <MASK> raise Error ( "" sample width not specified "" ) if not self . _framerate : raise Error ( "" sampling rate not specified "" ) self . _write_header ( datasize )",if not self . _sampwidth :,if not self . _sampwidth :,100,TRUE,100
3441,"def process ( self , fuzzresult ) : base_url = urljoin ( fuzzresult . url , "" .. "" ) for line in fuzzresult . history . content . splitlines ( ) : record = line . split ( "" / "" ) if len ( record ) == 6 and record [ 1 ] : self . queue_url ( urljoin ( base_url , record [ 1 ] ) ) # Directory <MASK> self . queue_url ( urljoin ( base_url , record [ 1 ] ) ) self . queue_url ( urljoin ( base_url , "" %s /CVS/Entries "" % ( record [ 1 ] ) ) )","if record [ 0 ] == ""D"" :",elif len ( record ) == 4 and record [ 1 ] :,95.6452224,FALSE,90.81
3442,"def tearDown ( self ) : """"""Shutdown the UDP server."""""" try : <MASK> self . server . stop ( 2.0 ) if self . sock_hdlr : self . root_logger . removeHandler ( self . sock_hdlr ) self . sock_hdlr . close ( ) finally : BaseTest . tearDown ( self )",if self . server :,if self . server :,100,TRUE,100
3443,"def get_backend ( find_library = None ) : try : global _lib , _ctx <MASK> _lib = _load_library ( find_library ) _setup_prototypes ( _lib ) _ctx = _Context ( ) _logger . warning ( "" OpenUSB backend deprecated (https://github.com/pyusb/pyusb/issues/284) "" ) return _OpenUSB ( ) except usb . libloader . LibraryException : # exception already logged (if any) _logger . error ( "" Error loading OpenUSB backend "" , exc_info = False ) return None except Exception : _logger . error ( "" Error loading OpenUSB backend "" , exc_info = True ) return None",if _lib is None :,if not _lib :,94.42788469,FALSE,96.65
3444,"def __init__ ( self , event , event_info , fields = [ ] ) : _wmi_object . __init__ ( self , event , fields = fields ) _set ( self , "" event_type "" , None ) _set ( self , "" timestamp "" , None ) _set ( self , "" previous "" , None ) if event_info : event_type = self . event_type_re . match ( event_info . Path_ . Class ) . group ( 1 ) . lower ( ) _set ( self , "" event_type "" , event_type ) if hasattr ( event_info , "" TIME_CREATED "" ) : _set ( self , "" timestamp "" , from_1601 ( event_info . TIME_CREATED ) ) <MASK> _set ( self , "" previous "" , event_info . PreviousInstance )","if hasattr ( event_info , ""PreviousInstance"" ) :","if hasattr ( event_info , ""PreviousInstance"" ) :",100,TRUE,100
3445,"def _getListNextPackagesReadyToBuild ( ) : for pkg in Scheduler . listOfPackagesToBuild : <MASK> continue if constants . rpmCheck or Scheduler . _checkNextPackageIsReadyToBuild ( pkg ) : Scheduler . listOfPackagesNextToBuild . put ( ( - Scheduler . _getPriority ( pkg ) , pkg ) ) Scheduler . logger . debug ( "" Adding  "" + pkg + ""  to the schedule list "" )",if pkg in Scheduler . listOfPackagesCurrentlyBuilding :,if pkg in Scheduler . listOfPackagesNextToBuild :,98.15422201,FALSE,96.41
3446,"def process_all ( self , lines , times = 1 ) : gap = False for _ in range ( times ) : for line in lines : <MASK> self . write ( "" "" ) self . process ( line ) if not is_command ( line ) : gap = True return 0",if gap :,if gap :,100,TRUE,100
3447,"def diff ( old , new , display = True ) : """"""Nice colored diff implementation"""""" if not isinstance ( old , list ) : old = decolorize ( str ( old ) ) . splitlines ( ) if not isinstance ( new , list ) : new = decolorize ( str ( new ) ) . splitlines ( ) line_types = { ""   "" : "" % Reset "" , "" - "" : "" % Red "" , "" + "" : "" %G reen "" , "" ? "" : "" % Pink "" } if display : for line in difflib . Differ ( ) . compare ( old , new ) : <MASK> continue print ( colorize ( line_types [ line [ 0 ] ] , line ) ) return old != new","if line . startswith ( ""?"" ) :",if line [ 0 ] not in line_types :,95.88182221,FALSE,94
3448,"def get_limit ( self , request ) : if self . limit_query_param : try : limit = int ( request . query_params [ self . limit_query_param ] ) if limit < 0 : raise ValueError ( ) # Enforce maximum page size, if defined if settings . MAX_PAGE_SIZE : <MASK> return settings . MAX_PAGE_SIZE else : return min ( limit , settings . MAX_PAGE_SIZE ) return limit except ( KeyError , ValueError ) : pass return self . default_limit",if limit == 0 :,if limit == 0 :,100,TRUE,100
3449,"def slice_fill ( self , slice_ ) : "" Fills the slice with zeroes for the dimensions that have single elements and squeeze_dims true "" if isinstance ( self . indexes , int ) : new_slice_ = [ 0 ] offset = 0 else : new_slice_ = [ slice_ [ 0 ] ] offset = 1 for i in range ( 1 , len ( self . nums ) ) : if self . squeeze_dims [ i ] : new_slice_ . append ( 0 ) <MASK> new_slice_ . append ( slice_ [ offset ] ) offset + = 1 new_slice_ + = slice_ [ offset : ] return new_slice_",elif offset < len ( slice_ ) :,elif self . indices [ i ] == 0 :,92.94143919,FALSE,92.77
3450,"def wrapper ( * args , * * kw ) : instance = args [ 0 ] try : <MASK> ret_dict = instance . _create_ret_object ( instance . FAILURE , None , True , instance . MUST_JSON ) instance . logger . error ( instance . MUST_JSON ) return jsonify ( ret_dict ) , 400 except BadRequest : ret_dict = instance . _create_ret_object ( instance . FAILURE , None , True , instance . MUST_JSON ) instance . logger . error ( instance . MUST_JSON ) return jsonify ( ret_dict ) , 400 instance . logger . debug ( "" JSON is valid "" ) return f ( * args , * * kw )",if request . get_json ( ) is None :,if not instance . MUST_JSON :,81.67795231,FALSE,93.29
3451,"def add_css ( self , data ) : if data : for medium , paths in data . items ( ) : for path in paths : <MASK> self . _css . setdefault ( medium , [ ] ) . append ( path )",if not self . _css . get ( medium ) or path not in self . _css [ medium ] :,"if path not in self . _css . get ( medium , [ ] ) :",60.49526983,FALSE,81.43
3452,"def mangle_template ( template : str , template_vars : Set [ str ] ) - > str : if TEMPLATE_PREFIX in template or TEMPLATE_SUFFIX in template : raise Exception ( "" Cannot parse a template containing reserved strings "" ) for var in template_vars : original = f "" {{ { var } }} "" <MASK> raise Exception ( f ' Template string is missing a reference to  "" { var } ""  referred to in kwargs ' ) template = template . replace ( original , mangled_name ( var ) ) return template",if original not in template :,if not template . startswith ( original ) :,94.77485258,FALSE,93.19
3453,"def filterSimilarKeywords ( keyword , kwdsIterator ) : """"""Return a sorted list of keywords similar to the one given."""""" seenDict = { } kwdSndx = soundex ( keyword . encode ( "" ascii "" , "" ignore "" ) ) matches = [ ] matchesappend = matches . append checkContained = False if len ( keyword ) > 4 : checkContained = True for movieID , key in kwdsIterator : if key in seenDict : continue seenDict [ key ] = None if checkContained and keyword in key : matchesappend ( key ) continue <MASK> matchesappend ( key ) return _sortKeywords ( keyword , matches )","if kwdSndx == soundex ( key . encode ( ""ascii"" , ""ignore"" ) ) :",if kwdSndx in key :,91.65165673,FALSE,85.59
3454,"def GetInfo ( self ) : for k , v in sorted ( self . memory_parameters . items ( ) ) : <MASK> continue if not v : continue print ( "" %s :  \t %#08x  ( %s ) "" % ( k , v , v ) ) print ( "" Memory ranges: "" ) print ( "" Start \t \t End \t \t Length "" ) for start , length in self . runs : print ( "" 0x %X \t \t 0x %X \t \t 0x %X "" % ( start , start + length , length ) )","if k . startswith ( ""Pad"" ) :","if k . startswith ( ""_"" ) :",98.84393086,FALSE,98.04
3455,"def Children ( self ) : """"""Returns a list of all of this object's owned (strong) children."""""" children = [ ] for property , attributes in self . _schema . iteritems ( ) : ( is_list , property_type , is_strong ) = attributes [ 0 : 3 ] if is_strong and property in self . _properties : <MASK> children . append ( self . _properties [ property ] ) else : children . extend ( self . _properties [ property ] ) return children",if not is_list :,if is_list :,95.23002318,FALSE,97.69
3456,"def normalize_res_identifier ( self , emu , cw , val ) : mask = ( 16 * * ( emu . get_ptr_size ( ) / / 2 ) - 1 ) << 16 if val & mask : # not an INTRESOURCE name = emu . read_mem_string ( val , cw ) <MASK> try : name = int ( name [ 1 : ] ) except Exception : return 0 else : name = val return name","if name [ 0 ] == ""#"" :","if name [ 0 ] == ""INT"" :",98.49222166,FALSE,97.44
3457,"def _optimize ( self , solutions ) : best_a = None best_silhouette = None best_k = None for a , silhouette , k in solutions ( ) : if best_silhouette is None : pass <MASK> break best_silhouette = silhouette best_a = a best_k = k return best_a , best_silhouette , best_k",elif silhouette <= best_silhouette :,elif silhouette is None :,92.53040286,FALSE,91.58
3458,"def find_commit_type ( sha ) : try : o = obj_store [ sha ] except KeyError : <MASK> raise else : if isinstance ( o , Commit ) : commits . add ( sha ) elif isinstance ( o , Tag ) : tags . add ( sha ) commits . add ( o . object [ 1 ] ) else : raise KeyError ( "" Not a commit or a tag:  %s "" % sha )",if not ignore_unknown :,if o is None :,66.06883302,FALSE,93.77
3459,"def on_search_entry_keypress ( self , widget , event ) : key = Gdk . keyval_name ( event . keyval ) if key == "" Escape "" : self . hide_search_box ( ) elif key == "" Return "" : # Combine with Shift? <MASK> self . search_prev = False self . do_search ( None ) else : self . search_prev = True",if event . state & Gdk . ModifierType . SHIFT_MASK :,if self . search_prev :,92.87440847,FALSE,87.02
3460,"def process_webhook_prop ( namespace ) : if not isinstance ( namespace . webhook_properties , list ) : return result = { } for each in namespace . webhook_properties : <MASK> if "" = "" in each : key , value = each . split ( "" = "" , 1 ) else : key , value = each , "" "" result [ key ] = value namespace . webhook_properties = result",if each :,"if isinstance ( each , ( list , tuple ) ) :",92.54214159,FALSE,87.67
3461,"def run ( self ) : global WAITING_BEFORE_START time . sleep ( WAITING_BEFORE_START ) while self . keep_alive : path_id , module , resolve = self . queue_receive . get ( ) <MASK> continue self . lock . acquire ( ) self . modules [ path_id ] = module self . lock . release ( ) if resolve : resolution = self . _resolve_with_other_modules ( resolve ) self . _relations [ path_id ] = [ ] for package in resolution : self . _relations [ path_id ] . append ( resolution [ package ] ) self . queue_send . put ( ( path_id , module , False , resolution ) )",if path_id is None :,if path_id in self . modules :,70.32615069,FALSE,96.34
3462,"def _get_download_link ( self , url , download_type = "" torrent "" ) : links = { "" torrent "" : "" "" , "" magnet "" : "" "" , } try : data = self . session . get ( url ) . text with bs4_parser ( data ) as html : downloads = html . find ( "" div "" , { "" class "" : "" download "" } ) if downloads : for download in downloads . findAll ( "" a "" ) : link = download [ "" href "" ] <MASK> links [ "" magnet "" ] = link else : links [ "" torrent "" ] = urljoin ( self . urls [ "" base_url "" ] , link ) except Exception : pass return links [ download_type ]","if link . startswith ( ""magnet"" ) :","if link . startswith ( ""magnet:"" ) :",99.12943538,FALSE,98.42
3463,"def _parse_fields ( cls , read ) : read = unicode_to_str ( read ) if type ( read ) is not str : _wrong_type_for_arg ( read , "" str "" , "" read "" ) fields = { } while read and read [ 0 ] != "" ; "" : <MASK> DeserializeError ( read , "" does not separate fields with commas "" ) read = read [ 1 : ] key , _type , value , read = cls . _parse_field ( read ) fields [ key ] = ( _type , value ) if read : # read[0] == ';' read = read [ 1 : ] return fields , read","if read and read [ 0 ] != "","" :","if read [ 0 ] == "";"" :",94.24400131,FALSE,94.99
3464,"def _convertDict ( self , d ) : r = { } for k , v in d . items ( ) : <MASK> v = str ( v , "" utf-8 "" ) elif isinstance ( v , list ) or isinstance ( v , tuple ) : v = self . _convertList ( v ) elif isinstance ( v , dict ) : v = self . _convertDict ( v ) if isinstance ( k , bytes ) : k = str ( k , "" utf-8 "" ) r [ k ] = v return r","if isinstance ( v , bytes ) :","if isinstance ( v , bytes ) :",100,TRUE,100
3465,"def wrapper ( filename ) : mtime = getmtime ( filename ) with lock : if filename in cache : old_mtime , result = cache . pop ( filename ) <MASK> # Move to the end cache [ filename ] = old_mtime , result return result result = function ( filename ) with lock : cache [ filename ] = mtime , result # at the end if len ( cache ) > max_size : cache . popitem ( last = False ) return result",if old_mtime == mtime :,if mtime > old_mtime :,97.70949826,FALSE,94.77
3466,def isFinished ( self ) : # returns true if episode timesteps has reached episode length and resets the task if self . count > self . epiLen : self . res ( ) return True else : <MASK> self . pertGlasPos ( 0 ) if self . count == self . epiLen / 2 + 1 : self . env . reset ( ) self . pertGlasPos ( 1 ) self . count + = 1 return False,if self . count == 1 :,if self . count == self . epiLen / 2 :,72.40492349,FALSE,92.71
3467,"def _check_vulnerabilities ( self , processed_analysis ) : matched_vulnerabilities = list ( ) for vulnerability in self . _rule_base_vulnerabilities : <MASK> vulnerability_data = vulnerability . get_dict ( ) name = vulnerability_data . pop ( "" short_name "" ) matched_vulnerabilities . append ( ( name , vulnerability_data ) ) return matched_vulnerabilities","if evaluate ( processed_analysis , vulnerability . rule ) :",if vulnerability . get_analysis ( ) == processed_analysis :,81.97146391,FALSE,87.55
3468,"def _table_reprfunc ( self , row , col , val ) : if self . _table . column_names [ col ] . endswith ( "" Size "" ) : <MASK> return ""    %s "" % val elif val < 1024 * * 2 : return ""    %.1f  KB "" % ( val / 1024.0 * * 1 ) elif val < 1024 * * 3 : return ""    %.1f  MB "" % ( val / 1024.0 * * 2 ) else : return ""    %.1f  GB "" % ( val / 1024.0 * * 3 ) if col in ( 0 , "" "" ) : return str ( val ) else : return ""    %s "" % val","if isinstance ( val , compat . string_types ) :",if val < 1024 * 1 :,69.05976969,FALSE,92.07
3469,"def serve_until_stopped ( self ) - > None : while True : rd , wr , ex = select . select ( [ self . socket . fileno ( ) ] , [ ] , [ ] , self . timeout ) if rd : self . handle_request ( ) <MASK> break",if self . event is not None and self . event . is_set ( ) :,if not self . socket . fileno ( ) :,59.06896258,FALSE,80.13
3470,"def resize ( self , * e ) : bold = ( "" helvetica "" , - self . _size . get ( ) , "" bold "" ) helv = ( "" helvetica "" , - self . _size . get ( ) ) xspace = self . _size . get ( ) yspace = self . _size . get ( ) for widget in self . _widgets : widget [ "" node_font "" ] = bold widget [ "" leaf_font "" ] = helv widget [ "" xspace "" ] = xspace widget [ "" yspace "" ] = yspace if self . _size . get ( ) < 20 : widget [ "" line_width "" ] = 1 <MASK> widget [ "" line_width "" ] = 2 else : widget [ "" line_width "" ] = 3 self . _layout ( )",elif self . _size . get ( ) < 30 :,elif self . _size . get ( ) < 24 :,99.18692942,FALSE,98.54
3471,"def __assertTilesChangedInRegion ( self , t1 , t2 , region ) : for tileOriginTuple in t1 . keys ( ) : tileOrigin = imath . V2i ( * tileOriginTuple ) tileRegion = imath . Box2i ( tileOrigin , tileOrigin + imath . V2i ( GafferImage . ImagePlug . tileSize ( ) ) ) <MASK> self . assertNotEqual ( t1 [ tileOriginTuple ] , t2 [ tileOriginTuple ] ) else : self . assertEqual ( t1 [ tileOriginTuple ] , t2 [ tileOriginTuple ] )","if GafferImage . BufferAlgo . intersects ( tileRegion , region ) :",if region . get ( tileOrigin ) != region . get ( tileRegion ) :,88.39474196,FALSE,86.84
3472,"def grouped_by_prefix ( args , prefixes ) : """"""Group behave args by (directory) scope into multiple test-runs."""""" group_args = [ ] current_scope = None for arg in args . strip ( ) . split ( ) : assert not arg . startswith ( "" - "" ) , "" REQUIRE: arg, not options "" scope = select_prefix_for ( arg , prefixes ) if scope != current_scope : <MASK> # -- DETECTED GROUP-END: yield ""   "" . join ( group_args ) group_args = [ ] current_scope = scope group_args . append ( arg ) <MASK> yield ""   "" . join ( group_args )",if group_args :,if group_args :,100,TRUE,100
3473,"def __print__ ( self , defaults = False ) : if defaults : print_func = str else : print_func = repr pieces = [ ] default_values = self . __defaults__ for k in self . __fields__ : value = getattr ( self , k ) <MASK> continue if isinstance ( value , basestring ) : print_func = repr # keep quotes around strings pieces . append ( "" %s = %s "" % ( k , print_func ( value ) ) ) if pieces or self . __base__ : return "" %s ( %s ) "" % ( self . __class__ . __name__ , "" ,  "" . join ( pieces ) ) else : return "" """,if not defaults and value == default_values [ k ] :,if value in default_values :,71.94846891,FALSE,92.91
3474,"def setInnerHTML ( self , html ) : log . HTMLClassifier . classify ( log . ThugLogging . url if log . ThugOpts . local else log . last_url , html ) self . tag . clear ( ) for node in bs4 . BeautifulSoup ( html , "" html.parser "" ) . contents : self . tag . append ( node ) name = getattr ( node , "" name "" , None ) <MASK> continue handler = getattr ( log . DFT , "" handle_ %s "" % ( name , ) , None ) if handler : handler ( node )",if name is None :,if name is None :,100,TRUE,100
3475,"def createFields ( self ) : yield Enum ( Bits ( self , "" class "" , 2 ) , self . CLASS_DESC ) yield Enum ( Bit ( self , "" form "" ) , self . FORM_DESC ) if self [ "" class "" ] . value == 0 : yield Enum ( Bits ( self , "" type "" , 5 ) , self . TYPE_DESC ) else : yield Bits ( self , "" type "" , 5 ) yield ASNInteger ( self , "" size "" , "" Size in bytes "" ) size = self [ "" size "" ] . value if size : <MASK> for field in self . _handler ( self , size ) : yield field else : yield RawBytes ( self , "" raw "" , size )",if self . _handler :,if self . _handler :,100,TRUE,100
3476,"def _process_service_request ( self , pkttype , pktid , packet ) : """"""Process a service request"""""" # pylint: disable=unused-argument service = packet . get_string ( ) packet . check_end ( ) if service == self . _next_service : self . logger . debug2 ( "" Accepting request for service  %s "" , service ) self . _next_service = None self . send_packet ( MSG_SERVICE_ACCEPT , String ( service ) ) <MASK> # pragma: no branch self . _auth_in_progress = True self . _send_deferred_packets ( ) else : raise DisconnectError ( DISC_SERVICE_NOT_AVAILABLE , "" Unexpected service request received "" )",if self . is_server ( ) and service == _USERAUTH_SERVICE :,elif service == self . _auth_in_progress :,70.05942951,FALSE,91.02
3477,"def _read_fixed_body ( self , content_length : int , delegate : httputil . HTTPMessageDelegate ) - > None : while content_length > 0 : body = await self . stream . read_bytes ( min ( self . params . chunk_size , content_length ) , partial = True ) content_length - = len ( body ) <MASK> with _ExceptionLoggingContext ( app_log ) : ret = delegate . data_received ( body ) if ret is not None : await ret",if not self . _write_finished or self . is_client :,if body is not None :,89.72318401,FALSE,87.32
3478,"def wait_for_child ( pid , timeout = 1.0 ) : deadline = mitogen . core . now ( ) + timeout while timeout < mitogen . core . now ( ) : try : target_pid , status = os . waitpid ( pid , os . WNOHANG ) <MASK> return except OSError : e = sys . exc_info ( ) [ 1 ] if e . args [ 0 ] == errno . ECHILD : return time . sleep ( 0.05 ) assert False , "" wait_for_child() timed out """,if target_pid == pid :,if target_pid == 0 :,95.13394487,FALSE,97.72
3479,"def execute ( cls , ctx , op : "" DataFrameGroupByAgg "" ) : try : pd . set_option ( "" mode.use_inf_as_na "" , op . use_inf_as_na ) if op . stage == OperandStage . map : cls . _execute_map ( ctx , op ) <MASK> cls . _execute_combine ( ctx , op ) elif op . stage == OperandStage . agg : cls . _execute_agg ( ctx , op ) else : # pragma: no cover raise ValueError ( "" Aggregation operand not executable "" ) finally : pd . reset_option ( "" mode.use_inf_as_na "" )",elif op . stage == OperandStage . combine :,elif op . stage == OperandStage . combine :,100,TRUE,100
3480,def cut ( sentence ) : sentence = strdecode ( sentence ) blocks = re_han . split ( sentence ) for blk in blocks : if re_han . match ( blk ) : for word in __cut ( blk ) : <MASK> yield word else : for c in word : yield c else : tmp = re_skip . split ( blk ) for x in tmp : if x : yield x,if word not in Force_Split_Words :,if word . startswith ( sentence ) :,93.99871181,FALSE,90.38
3481,"def _iter_tags ( self , type = None ) : """"""Yield all raw tags (limit to |type| if specified)"""""" for n in itertools . count ( ) : tag = self . _get_tag ( n ) <MASK> yield tag if tag [ "" d_tag "" ] == "" DT_NULL "" : break","if type is None or tag [ ""d_tag"" ] == type :","if type and tag [ ""d_tag"" ] == type :",84.9843621,FALSE,94.84
3482,"def reverse_search_history ( self , searchfor , startpos = None ) : if startpos is None : startpos = self . history_cursor if _ignore_leading_spaces : res = [ ( idx , line . lstrip ( ) ) for idx , line in enumerate ( self . history [ startpos : 0 : - 1 ] ) if line . lstrip ( ) . startswith ( searchfor . lstrip ( ) ) ] else : res = [ ( idx , line ) for idx , line in enumerate ( self . history [ startpos : 0 : - 1 ] ) <MASK> ] if res : self . history_cursor - = res [ 0 ] [ 0 ] return res [ 0 ] [ 1 ] . get_line_text ( ) return "" """,if line . startswith ( searchfor ),if line . lstrip ( ) . startswith ( searchfor . lstrip ( ) ),95.83506458,FALSE,93.68
3483,"def value_to_db_datetime ( self , value ) : if value is None : return None # Oracle doesn't support tz-aware datetimes if timezone . is_aware ( value ) : <MASK> value = value . astimezone ( timezone . utc ) . replace ( tzinfo = None ) else : raise ValueError ( "" Oracle backend does not support timezone-aware datetimes when USE_TZ is False. "" ) return unicode ( value )",if settings . USE_TZ :,if settings . USE_TZ :,75,TRUE,100
3484,"def _sniff ( filename , oxlitype ) : try : with open ( filename , "" rb "" ) as fileobj : header = fileobj . read ( 4 ) if header == b "" OXLI "" : fileobj . read ( 1 ) # skip the version number ftype = fileobj . read ( 1 ) <MASK> return True return False except OSError : return False",if binascii . hexlify ( ftype ) == oxlitype :,"if oxlitype == b""S"" :",58.08861364,FALSE,87.66
3485,"def unget ( self , char ) : # Only one character is allowed to be ungotten at once - it must # be consumed again before any further call to unget if char is not EOF : <MASK> # unget is called quite rarely, so it's a good idea to do # more work here if it saves a bit of work in the frequently # called char and charsUntil. # So, just prepend the ungotten character onto the current # chunk: self . chunk = char + self . chunk self . chunkSize + = 1 else : self . chunkOffset - = 1 assert self . chunk [ self . chunkOffset ] == char",if self . chunkOffset == 0 :,if self . chunkOffset == 0 :,75,TRUE,100
3486,"def scan ( rule , extensions , paths , ignore_paths = None ) : """"""The libsast scan."""""" try : options = { "" match_rules "" : rule , "" match_extensions "" : extensions , "" ignore_paths "" : ignore_paths , "" show_progress "" : False , } scanner = Scanner ( options , paths ) res = scanner . scan ( ) <MASK> return format_findings ( res [ "" pattern_matcher "" ] , paths [ 0 ] ) except Exception : logger . exception ( "" libsast scan "" ) return { }",if res :,"if res [ ""type"" ] == ""match"" :",74.08623833,FALSE,90.9
3487,"def _getPatternTemplate ( pattern , key = None ) : if key is None : key = pattern <MASK> key = pattern . upper ( ) template = DD_patternCache . get ( key ) if not template : if key in ( "" EPOCH "" , "" { ^LN-BEG}EPOCH "" , "" ^EPOCH "" ) : template = DateEpoch ( lineBeginOnly = ( key != "" EPOCH "" ) ) elif key in ( "" TAI64N "" , "" { ^LN-BEG}TAI64N "" , "" ^TAI64N "" ) : template = DateTai64n ( wordBegin = ( "" start "" if key != "" TAI64N "" else False ) ) else : template = DatePatternRegex ( pattern ) DD_patternCache . set ( key , template ) return template","if ""%"" not in pattern :","if pattern . startswith ( ""https"" ) :",83.71652856,FALSE,94.08
3488,"def _forward_response ( self , src , dst ) : """"""Forward an SCP response between two remote SCP servers"""""" # pylint: disable=no-self-use try : exc = yield from src . await_response ( ) <MASK> dst . send_error ( exc ) return exc else : dst . send_ok ( ) return None except OSError as exc : return exc",if exc :,if exc :,75,TRUE,100
3489,"def _maybe_signal_recovery_end ( ) - > None : if self . in_recovery and not self . active_remaining_total ( ) : # apply anything stuck in the buffers self . flush_buffers ( ) self . _set_recovery_ended ( ) <MASK> self . _actives_span . set_tag ( "" Actives-Ready "" , True ) self . signal_recovery_end . set ( )",if self . _actives_span is not None :,if self . _actives_span :,95.79244118,FALSE,95.22
3490,"def main ( ) : tmpdir = None try : # Create a temporary working directory tmpdir = tempfile . mkdtemp ( ) # Unpack the zipfile into the temporary directory pip_zip = os . path . join ( tmpdir , "" pip.zip "" ) with open ( pip_zip , "" wb "" ) as fp : fp . write ( b85decode ( DATA . replace ( b "" \n "" , b "" "" ) ) ) # Add the zipfile to sys.path so that we can import it sys . path . insert ( 0 , pip_zip ) # Run the bootstrap bootstrap ( tmpdir = tmpdir ) finally : # Clean up our temporary working directory <MASK> shutil . rmtree ( tmpdir , ignore_errors = True )",if tmpdir :,if tmpdir :,75,TRUE,100
3491,"def __init__ ( self , api_version_str ) : try : self . latest = self . preview = False self . yyyy = self . mm = self . dd = None if api_version_str == "" latest "" : self . latest = True else : <MASK> self . preview = True parts = api_version_str . split ( "" - "" ) self . yyyy = int ( parts [ 0 ] ) self . mm = int ( parts [ 1 ] ) self . dd = int ( parts [ 2 ] ) except ( ValueError , TypeError ) : raise ValueError ( "" The API version  {}  is not in a  "" "" supported format "" . format ( api_version_str ) )","if ""preview"" in api_version_str :","if api_version_str . startswith ( ""preview-"" ) :",68.60630979,FALSE,93.97
3492,"def _merge ( self , items , map_id , dep_id , use_disk , meminfo , mem_limit ) : combined = self . combined merge_combiner = self . aggregator . mergeCombiners for k , v in items : o = combined . get ( k ) combined [ k ] = merge_combiner ( o , v ) if o is not None else v <MASK> mem_limit = self . _rotate ( )",if use_disk and meminfo . rss > mem_limit :,if mem_limit and mem_limit < mem_limit :,82.88553048,FALSE,90.72
3493,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) <MASK> self . set_value ( d . getVarInt32 ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 8 :,if tt == 8 :,100,TRUE,100
3494,"def nice ( deltat ) : # singular,plural times = _ ( "" second,seconds:minute,minutes:hour,hours:day,days:week,weeks:month,months:year,years "" ) . split ( "" : "" ) d = abs ( int ( deltat ) ) for div , time in zip ( ( 60 , 60 , 24 , 7 , 4 , 12 , 100 ) , times ) : <MASK> return "" %s %i   %s "" % ( deltat < 0 and "" - "" or "" "" , d , time . split ( "" , "" ) [ d != 1 ] ) d / = div",if d < div * 5 :,if div == 0 :,71.98909419,FALSE,95.34
3495,"def after_get_object ( self , event , view_kwargs ) : if event and event . state == "" draft "" : <MASK> raise ObjectNotFound ( { "" parameter "" : "" {id} "" } , "" Event: not found "" )","if not is_logged_in ( ) or not has_access ( ""is_coorganizer"" , event_id = event . id ) :",if not self . object_manager . object_manager . object_manager . object_manager,48.72472806,FALSE,63.27
3496,def daemonize_if_required ( self ) : if self . options . daemon : <MASK> # Stop the logging queue listener for the current process # We'll restart it once forked log . shutdown_multiprocessing_logging_listener ( daemonizing = True ) # Late import so logging works correctly salt . utils . process . daemonize ( ) # Setup the multiprocessing log queue listener if enabled self . _setup_mp_logging_listener ( ),if self . _setup_mp_logging_listener_ is True :,if self . options . use_multiprocessing :,89.8382017,FALSE,87.31
3497,"def iter_modules ( self , by_clients = False , clients_filter = None ) : """"""iterate over all modules"""""" clients = None if by_clients : clients = self . get_clients ( clients_filter ) if not clients : return self . _refresh_modules ( ) for module_name in self . modules : try : module = self . get_module ( module_name ) except PupyModuleDisabled : continue if clients is not None : for client in clients : <MASK> yield module break else : yield module",if module . is_compatible_with ( client ) :,if module . is_available ( client ) :,98.59429324,FALSE,96.2
3498,"def _incremental_avg_dp ( self , avg , new_el , idx ) : for attr in [ "" coarse_segm "" , "" fine_segm "" , "" u "" , "" v "" ] : setattr ( avg , attr , ( getattr ( avg , attr ) * idx + getattr ( new_el , attr ) ) / ( idx + 1 ) ) <MASK> # Deletion of the > 0 index intermediary values to prevent GPU OOM setattr ( new_el , attr , None ) return avg",if idx :,"if getattr ( avg , attr , None ) == 0 :",92.55850654,FALSE,88.87
3499,"def run ( self , paths = [ ] ) : collapsed = False for item in SideBarSelection ( paths ) . getSelectedDirectories ( ) : for view in item . views ( ) : <MASK> Window ( ) . focus_view ( view ) self . collapse_sidebar_folder ( ) collapsed = True view . close ( )",if not collapsed :,if collapsed :,82.68755859,FALSE,96.1
3500,"def test_reductions ( expr , rdd ) : result = compute ( expr , rdd ) expected = compute ( expr , data ) if not result == expected : print ( result ) print ( expected ) <MASK> assert abs ( result - expected ) < 0.001 else : assert result == expected","if isinstance ( result , float ) :",if rdd :,89.34169603,FALSE,88.02
3501,"def deltask ( task , d ) : if task [ : 3 ] != "" do_ "" : task = "" do_ "" + task bbtasks = d . getVar ( "" __BBTASKS "" , False ) or [ ] if task in bbtasks : bbtasks . remove ( task ) d . delVarFlag ( task , "" task "" ) d . setVar ( "" __BBTASKS "" , bbtasks ) d . delVarFlag ( task , "" deps "" ) for bbtask in d . getVar ( "" __BBTASKS "" , False ) or [ ] : deps = d . getVarFlag ( bbtask , "" deps "" , False ) or [ ] <MASK> deps . remove ( task ) d . setVarFlag ( bbtask , "" deps "" , deps )",if task in deps :,if task in deps :,100,TRUE,100
3502,"def _apply_weightnorm ( self , list_layers ) : """"""Try apply weightnorm for all layer in list_layers."""""" for i in range ( len ( list_layers ) ) : try : layer_name = list_layers [ i ] . name . lower ( ) <MASK> list_layers [ i ] = WeightNormalization ( list_layers [ i ] ) except Exception : pass","if ""conv1d"" in layer_name or ""dense"" in layer_name :",if layer_name in self . _weight_norm_names :,74.77138231,FALSE,84.88
3503,"def __init__ ( self , execution_context , aggregate_operators ) : super ( _QueryExecutionAggregateEndpointComponent , self ) . __init__ ( execution_context ) self . _local_aggregators = [ ] self . _results = None self . _result_index = 0 for operator in aggregate_operators : if operator == "" Average "" : self . _local_aggregators . append ( _AverageAggregator ( ) ) elif operator == "" Count "" : self . _local_aggregators . append ( _CountAggregator ( ) ) <MASK> self . _local_aggregators . append ( _MaxAggregator ( ) ) elif operator == "" Min "" : self . _local_aggregators . append ( _MinAggregator ( ) ) elif operator == "" Sum "" : self . _local_aggregators . append ( _SumAggregator ( ) )","elif operator == ""Max"" :","elif operator == ""Max"" :",100,TRUE,100
3504,"def _conv_layer ( self , sess , bottom , name , trainable = True , padding = "" SAME "" , relu = True ) : with tf . variable_scope ( name ) as scope : filt = self . _get_conv_filter ( sess , name , trainable = trainable ) conv_biases = self . _get_bias ( sess , name , trainable = trainable ) conv = tf . nn . conv2d ( bottom , filt , [ 1 , 1 , 1 , 1 ] , padding = padding ) bias = tf . nn . bias_add ( conv , conv_biases ) <MASK> bias = tf . nn . relu ( bias ) return bias",if relu :,if relu :,100,TRUE,100
3505,"def get_partners ( self ) - > Dict [ AbstractNode , Set [ int ] ] : partners = { } # type: Dict[AbstractNode, Set[int]] for edge in self . edges : if edge . is_dangling ( ) : raise ValueError ( "" Cannot contract copy tensor with dangling edges "" ) <MASK> continue partner_node , shared_axis = self . _get_partner ( edge ) if partner_node not in partners : partners [ partner_node ] = set ( ) partners [ partner_node ] . add ( shared_axis ) return partners",if self . _is_my_trace ( edge ) :,if edge . is_linked :,71.35686865,FALSE,91.01
3506,"def close ( self ) : with self . _lock : """"""Close this _MultiFileWatcher object forever."""""" <MASK> self . _folder_handlers = { } LOGGER . debug ( "" Stopping observer thread even though there is a non-zero  "" "" number of event observers! "" ) else : LOGGER . debug ( "" Stopping observer thread "" ) self . _observer . stop ( ) self . _observer . join ( timeout = 5 )",if len ( self . _folder_handlers ) != 0 :,if self . _folder_handlers is not None :,63.07133297,FALSE,90.99
3507,"def comboSelectionChanged ( self , index ) : text = self . comboBox . cb . itemText ( index ) for i in range ( self . labelList . count ( ) ) : <MASK> self . labelList . item ( i ) . setCheckState ( 2 ) elif text != self . labelList . item ( i ) . text ( ) : self . labelList . item ( i ) . setCheckState ( 0 ) else : self . labelList . item ( i ) . setCheckState ( 2 )","if text == """" :",if self . labelList . item ( i ) . text ( ) == text :,92.30180537,FALSE,85.53
3508,"def _get_messages ( self ) : r = [ ] try : self . _connect ( ) self . _login ( ) for message in self . _fetch ( ) : <MASK> r . append ( message ) self . _connection . expunge ( ) self . _connection . close ( ) self . _connection . logout ( ) except MailFetcherError as e : self . log ( "" error "" , str ( e ) ) return r",if message :,"if message . get ( ""type"" ) == ""message"" :",94.90143832,FALSE,86.54
3509,"def get_current_user ( self ) : try : <MASK> return config . get ( "" json_authentication_override "" ) tkn_header = self . request . headers [ "" authorization "" ] except KeyError : raise WebAuthNError ( reason = "" Missing Authorization Header "" ) else : tkn_str = tkn_header . split ( ""   "" ) [ - 1 ] try : tkn = self . jwt_validator ( tkn_str ) except AuthenticationError as e : raise WebAuthNError ( reason = e . message ) else : return tkn","if config . get ( ""development"" ) and config . get ( ""json_authentication_override"" ) :","if config . get ( ""json_authentication_override"" ) :",76.47464464,FALSE,92.66
3510,def _get_data ( self ) : formdata = self . _formdata if formdata : data = [ ] # TODO: Optimize? for item in formdata : model = self . loader . get_one ( item ) if item else None <MASK> data . append ( model ) else : self . _invalid_formdata = True self . _set_data ( data ) return self . _data,if model :,if model :,100,TRUE,100
3511,"def _getSubstrings ( self , va , size , ltyp ) : # rip through the desired memory range to populate any substrings subs = set ( ) end = va + size for offs in range ( va , end , 1 ) : loc = self . getLocation ( offs , range = True ) if loc and loc [ L_LTYPE ] == LOC_STRING and loc [ L_VA ] > va : subs . add ( ( loc [ L_VA ] , loc [ L_SIZE ] ) ) <MASK> subs = subs . union ( set ( loc [ L_TINFO ] ) ) return list ( subs )",if loc [ L_TINFO ] :,if loc [ L_LTYPE ] == LC_SUBSTRING and loc [ L_,72.12677992,FALSE,90.42
3512,def monad ( self ) : if not self . cls_bl_idname : return None for monad in bpy . data . node_groups : <MASK> if monad . cls_bl_idname == self . cls_bl_idname : return monad return None,"if hasattr ( monad , ""cls_bl_idname"" ) :","if monad . type == ""monad"" :",85.77131373,FALSE,79.72
3513,"def _set_peer_statuses ( self ) : """"""Set peer statuses."""""" cutoff = time . time ( ) - STALE_SECS for peer in self . peers : if peer . bad : peer . status = PEER_BAD <MASK> peer . status = PEER_GOOD elif peer . last_good : peer . status = PEER_STALE else : peer . status = PEER_NEVER",elif peer . last_good > cutoff :,elif peer . last_good > cutoff :,100,TRUE,100
3514,"def title_by_index ( self , trans , index , context ) : d_type = self . get_datatype ( trans , context ) for i , ( composite_name , composite_file ) in enumerate ( d_type . writable_files . items ( ) ) : if i == index : rval = composite_name <MASK> rval = "" {}  ( {} ) "" . format ( rval , composite_file . description ) if composite_file . optional : rval = "" %s  [optional] "" % rval return rval if index < self . get_file_count ( trans , context ) : return "" Extra primary file "" return None",if composite_file . description :,if composite_file . description :,100,TRUE,100
3515,"def testUiViewServerDump_windowIntM1 ( self ) : device = None try : device = MockDevice ( version = 15 , startviewserver = True ) vc = ViewClient ( device , device . serialno , adb = TRUE , autodump = False ) vc . dump ( window = - 1 ) vc . findViewByIdOrRaise ( "" id/home "" ) finally : <MASK> device . shutdownMockViewServer ( )",if device :,if device :,100,TRUE,100
3516,"def _convertDict ( self , d ) : r = { } for k , v in d . items ( ) : if isinstance ( v , bytes ) : v = str ( v , "" utf-8 "" ) elif isinstance ( v , list ) or isinstance ( v , tuple ) : v = self . _convertList ( v ) <MASK> v = self . _convertDict ( v ) if isinstance ( k , bytes ) : k = str ( k , "" utf-8 "" ) r [ k ] = v return r","elif isinstance ( v , dict ) :","elif isinstance ( v , dict ) :",100,TRUE,100
3517,"def _testSendmsgTimeout ( self ) : try : self . cli_sock . settimeout ( 0.03 ) try : while True : self . sendmsgToServer ( [ b "" a "" * 512 ] ) except socket . timeout : pass except OSError as exc : <MASK> raise # bpo-33937 the test randomly fails on Travis CI with # ""OSError: [Errno 12] Cannot allocate memory"" else : self . fail ( "" socket.timeout not raised "" ) finally : self . misc_event . set ( )",if exc . errno != errno . ENOMEM :,if exc . errno != 12 :,97.60909458,FALSE,95.68
3518,"def addError ( self , test , err ) : if err [ 0 ] is SkipTest : if self . showAll : self . stream . writeln ( str ( err [ 1 ] ) ) <MASK> self . stream . write ( "" s "" ) self . stream . flush ( ) return _org_AddError ( self , test , err )",elif self . dots :,if self . show_output :,91.03435949,FALSE,90.47
3519,"def mouse_down ( self , event ) : if event . button == 1 : if self . scrolling : p = event . local if self . scroll_up_rect ( ) . collidepoint ( p ) : self . scroll_up ( ) return <MASK> self . scroll_down ( ) return if event . button == 4 : self . scroll_up ( ) if event . button == 5 : self . scroll_down ( ) GridView . mouse_down ( self , event )",elif self . scroll_down_rect ( ) . collidepoint ( p ) :,if self . scroll_down_rect ( ) . collidepoint ( p ) :,96.97763846,FALSE,97.8
3520,"def find_file_copyright_notices ( fname ) : ret = set ( ) f = open ( fname ) lines = f . readlines ( ) for l in lines [ : 80 ] : # hmmm, assume copyright to be in first 80 lines idx = l . lower ( ) . find ( "" copyright "" ) if idx < 0 : continue copyright = l [ idx + 9 : ] . strip ( ) <MASK> continue copyright = sanitise ( copyright ) # hmm, do a quick check to see if there's a year, # if not, skip it if not copyright . find ( "" 200 "" ) > = 0 and not copyright . find ( "" 199 "" ) > = 0 : continue ret . add ( copyright ) return ret",if not copyright :,if not copyright :,100,TRUE,100
3521,"def get_selectable_values ( self , request ) : shop = lfs . core . utils . get_default_shop ( request ) countries = [ ] for country in shop . shipping_countries . all ( ) : <MASK> selected = True else : selected = False countries . append ( { "" id "" : country . id , "" name "" : country . name , "" selected "" : selected , } ) return countries",if country in self . value . all ( ) :,if country . is_active :,89.4452028,FALSE,90.05
3522,"def _addItemToLayout ( self , sample , label ) : col = self . layout . columnCount ( ) row = self . layout . rowCount ( ) if row : row - = 1 nCol = self . columnCount * 2 # FIRST ROW FULL if col == nCol : for col in range ( 0 , nCol , 2 ) : # FIND RIGHT COLUMN if not self . layout . itemAt ( row , col ) : break <MASK> # MAKE NEW ROW col = 0 row + = 1 self . layout . addItem ( sample , row , col ) self . layout . addItem ( label , row , col + 1 )",if col + 2 == nCol :,if col == nCol :,98.60025111,FALSE,97.23
3523,def contains_only_whitespace ( node ) : if is_tag ( node ) : if not any ( [ not is_text ( s ) for s in node . contents ] ) : <MASK> return True return False,if not any ( [ unicode ( s ) . strip ( ) for s in node . contents ] ) :,if contains_only_whitespace ( node . contents ) :,66.49226051,FALSE,69.52
3524,"def tokenize_generator ( cw ) : ret = [ ] done = { } for op in ops : ch = op . symbol [ 0 ] <MASK> continue sops = start_symbols [ ch ] cw . write ( "" case  ' %s ' : "" % ch ) for t in gen_tests ( sops , 1 ) : cw . write ( t ) done [ ch ] = True return ret",if ch in done :,if ch in done :,100,TRUE,100
3525,"def _convertNbCharsInNbBits ( self , nbChars ) : nbMinBit = None nbMaxBit = None if nbChars is not None : <MASK> nbMinBit = nbChars * 8 nbMaxBit = nbMinBit else : if nbChars [ 0 ] is not None : nbMinBit = nbChars [ 0 ] * 8 if nbChars [ 1 ] is not None : nbMaxBit = nbChars [ 1 ] * 8 return ( nbMinBit , nbMaxBit )","if isinstance ( nbChars , int ) :",if nbChars [ 0 ] is not None and nbChars [ 1 ] is not None :,71.4026935,FALSE,81.17
3526,"def init ( self , * args , * * kwargs ) : if "" _state "" not in kwargs : state = { } # Older versions have the _state entries as individual kwargs for arg in ( "" children "" , "" windowState "" , "" detachedPanels "" ) : if arg in kwargs : state [ arg ] = kwargs [ arg ] del kwargs [ arg ] <MASK> kwargs [ "" _state "" ] = state originalInit ( self , * args , * * kwargs )",if state :,if state :,75,TRUE,100
3527,"def spm_decode ( tokens : List [ str ] ) - > List [ str ] : words = [ ] pieces : List [ str ] = [ ] for t in tokens : <MASK> if len ( pieces ) > 0 : words . append ( "" "" . join ( pieces ) ) pieces = [ t [ 1 : ] ] else : pieces . append ( t ) if len ( pieces ) > 0 : words . append ( "" "" . join ( pieces ) ) return words",if t [ 0 ] == DecodeMixin . spm_bos_token :,"if t [ 0 ] == ""SPM"" :",95.26577095,FALSE,92.2
3528,"def _compare_dirs ( self , dir1 : str , dir2 : str ) - > List [ str ] : # check that dir1 and dir2 are equivalent, # return the diff diff = [ ] # type: List[str] for root , dirs , files in os . walk ( dir1 ) : for file_ in files : path = os . path . join ( root , file_ ) target_path = os . path . join ( dir2 , os . path . split ( path ) [ - 1 ] ) <MASK> diff . append ( file_ ) return diff",if not os . path . exists ( target_path ) :,"if self . _compare_path ( target_path , dir1 ) :",96.04435594,FALSE,91.71
3529,"def credentials ( self ) : """"""The session credentials as a dict"""""" creds = { } if self . _creds : <MASK> # pragma: no branch creds [ "" aws_access_key_id "" ] = self . _creds . access_key if self . _creds . secret_key : # pragma: no branch creds [ "" aws_secret_access_key "" ] = self . _creds . secret_key if self . _creds . token : creds [ "" aws_session_token "" ] = self . _creds . token if self . _session . region_name : creds [ "" aws_region "" ] = self . _session . region_name if self . requester_pays : creds [ "" aws_request_payer "" ] = "" requester "" return creds",if self . _creds . access_key :,if self . _creds . access_key :,100,TRUE,100
3530,"def got_arbiter_module_type_defined ( self , mod_type ) : for a in self . arbiters : # Do like the linkify will do after.... for m in getattr ( a , "" modules "" , [ ] ) : # So look at what the arbiter try to call as module m = m . strip ( ) # Ok, now look in modules... for mod in self . modules : # try to see if this module is the good type if getattr ( mod , "" module_type "" , "" "" ) . strip ( ) == mod_type . strip ( ) : # if so, the good name? <MASK> return True return False","if getattr ( mod , ""module_name"" , """" ) . strip ( ) == m :","if getattr ( mod , ""name"" , """" ) . strip ( ) == m :",99.03944694,FALSE,97.8
3531,"def find_file_at_path_with_indexes ( self , path , url ) : if url . endswith ( "" / "" ) : path = os . path . join ( path , self . index_file ) return self . get_static_file ( path , url ) elif url . endswith ( "" / "" + self . index_file ) : if os . path . isfile ( path ) : return self . redirect ( url , url [ : - len ( self . index_file ) ] ) else : try : return self . get_static_file ( path , url ) except IsDirectoryError : <MASK> return self . redirect ( url , url + "" / "" ) raise MissingFileError ( path )","if os . path . isfile ( os . path . join ( path , self . index_file ) ) :",if os . path . isdir ( path ) :,89.93157649,FALSE,89.63
3532,def _use_full_params ( self ) - > None : for p in self . params : if not p . _is_sharded : <MASK> assert p . _fp16_shard . storage ( ) . size ( ) != 0 p . data = p . _fp16_shard else : assert p . _full_param_padded . storage ( ) . size ( ) != 0 p . data = p . _full_param_padded [ : p . _orig_size . numel ( ) ] . view ( p . _orig_size ),if self . mixed_precision :,if p . _fp16_shard :,97.56698445,FALSE,94.21
3533,"def _attrdata ( self , cont , name , * val ) : if not name : return None , False if isinstance ( name , Mapping ) : if val : raise TypeError ( "" Cannot set a value to  %s "" % name ) return name , True else : if val : <MASK> return { name : val [ 0 ] } , True else : raise TypeError ( "" Too may arguments "" ) else : cont = self . _extra . get ( cont ) return cont . get ( name ) if cont else None , False",if len ( val ) == 1 :,if len ( val ) == 1 :,100,TRUE,100
3534,"def evaluate ( env , net , device = "" cpu "" ) : obs = env . reset ( ) reward = 0.0 steps = 0 while True : obs_v = ptan . agent . default_states_preprocessor ( [ obs ] ) . to ( device ) action_v = net ( obs_v ) action = action_v . data . cpu ( ) . numpy ( ) [ 0 ] obs , r , done , _ = env . step ( action ) reward + = r steps + = 1 <MASK> break return reward , steps",if done :,if done :,100,TRUE,100
3535,"def convert_html_js_files ( app : Sphinx , config : Config ) - > None : """"""This converts string styled html_js_files to tuple styled one."""""" html_js_files = [ ] # type: List[Tuple[str, Dict]] for entry in config . html_js_files : <MASK> html_js_files . append ( ( entry , { } ) ) else : try : filename , attrs = entry html_js_files . append ( ( filename , attrs ) ) except Exception : logger . warning ( __ ( "" invalid js_file:  %r , ignored "" ) , entry ) continue config . html_js_files = html_js_files # type: ignore","if isinstance ( entry , str ) :","if isinstance ( entry , str ) :",75,TRUE,100
3536,"def _check_duplications ( self , regs ) : """"""n^2 loop which verifies that each reg exists only once."""""" for reg in regs : count = 0 for r in regs : <MASK> count + = 1 if count > 1 : genutil . die ( "" reg  %s  defined more than once "" % reg )",if reg == r :,if reg == r :,100,TRUE,100
3537,"def PyJsHoisted_vault_ ( key , forget , this , arguments , var = var ) : var = Scope ( { u "" this "" : this , u "" forget "" : forget , u "" key "" : key , u "" arguments "" : arguments } , var ) var . registers ( [ u "" forget "" , u "" key "" ] ) if PyJsStrictEq ( var . get ( u "" key "" ) , var . get ( u "" passkey "" ) ) : return ( var . put ( u "" secret "" , var . get ( u "" null "" ) ) <MASK> else ( var . get ( u "" secret "" ) or var . put ( u "" secret "" , var . get ( u "" secretCreatorFn "" ) ( var . get ( u "" object "" ) ) ) ) )","if var . get ( u""forget"" )","if PyJsStrictEq ( var . get ( u""secret"" ) , var . get",86.52471092,FALSE,93.52
3538,"def sort_nested_dictionary_lists ( d ) : for k , v in d . items ( ) : if isinstance ( v , list ) : for i in range ( 0 , len ( v ) ) : if isinstance ( v [ i ] , dict ) : v [ i ] = await sort_nested_dictionary_lists ( v [ i ] ) d [ k ] = sorted ( v ) <MASK> d [ k ] = await sort_nested_dictionary_lists ( v ) return d","if isinstance ( v , dict ) :","elif isinstance ( v [ 0 ] , dict ) :",79.71968818,FALSE,93.62
3539,"def transceiver ( self , data ) : out = [ ] for t in range ( 8 ) : if data [ t ] == 0 : continue value = data [ t ] for b in range ( 8 ) : if value & 0x80 : <MASK> out . append ( "" (unknown) "" ) else : out . append ( TRANSCEIVER [ t ] [ b ] ) value << = 1 self . annotate ( "" Transceiver compliance "" , "" ,  "" . join ( out ) )",if len ( TRANSCEIVER [ t ] ) < b + 1 :,if b not in TRANSCEIVER [ t ] :,92.84715736,FALSE,90.85
3540,"def process_string ( self , remove_repetitions , sequence ) : string = "" "" for i , char in enumerate ( sequence ) : if char != self . int_to_char [ self . blank_index ] : # if this char is a repetition and remove_repetitions=true, # skip. <MASK> pass elif char == self . labels [ self . space_index ] : string + = ""   "" else : string = string + char return string",if remove_repetitions and i != 0 and char == sequence [ i - 1 ] :,if remove_repetitions and self . repetitions [ i ] :,93.3707664,FALSE,87.59
3541,"def clean ( self ) : username = self . cleaned_data . get ( "" username "" ) password = self . cleaned_data . get ( "" password "" ) if username and password : self . user_cache = authenticate ( username = username , password = password ) <MASK> raise forms . ValidationError ( self . error_messages [ "" invalid_login "" ] ) elif not self . user_cache . is_active : raise forms . ValidationError ( self . error_messages [ "" inactive "" ] ) self . check_for_test_cookie ( ) return self . cleaned_data",if self . user_cache is None :,if not self . user_cache :,95.19882171,FALSE,95.99
3542,"def is_listening_for_message ( conversation_id : Text , endpoint : EndpointConfig ) - > bool : """"""Check if the conversation is in need for a user message."""""" tracker = await retrieve_tracker ( endpoint , conversation_id , EventVerbosity . APPLIED ) for i , e in enumerate ( reversed ( tracker . get ( "" events "" , [ ] ) ) ) : if e . get ( "" event "" ) == UserUttered . type_name : return False <MASK> return e . get ( "" name "" ) == ACTION_LISTEN_NAME return False","elif e . get ( ""event"" ) == ActionExecuted . type_name :",if i == 0 :,88.98581839,FALSE,87.19
3543,"def getReferences ( view , name = "" "" ) : """"""Find all reference definitions."""""" # returns {name -> Region} refs = [ ] name = re . escape ( name ) if name == "" "" : refs . extend ( view . find_all ( r "" (?<=^ \ [)([^ \ ]]+)(?= \ ]:) "" , 0 ) ) else : refs . extend ( view . find_all ( r "" (?<=^ \ [)( %s )(?= \ ]:) "" % name , 0 ) ) regions = refs ids = { } for reg in regions : name = view . substr ( reg ) . strip ( ) key = name . lower ( ) <MASK> ids [ key ] . regions . append ( reg ) else : ids [ key ] = Obj ( regions = [ reg ] , label = name ) return ids",if key in ids :,if key in ids :,100,TRUE,100
3544,"def _get_header ( self , requester , header_name ) : hits = sum ( [ header_name in headers for _ , headers in requester . requests ] ) self . assertEquals ( hits , 2 if self . revs_enabled else 1 ) for url , headers in requester . requests : <MASK> if self . revs_enabled : self . assertTrue ( url . endswith ( "" /latest "" ) , msg = url ) else : self . assertTrue ( url . endswith ( "" /download_urls "" ) , msg = url ) return headers . get ( header_name )",if header_name in headers :,"if url . startswith ( ""/download_urls/"" ) :",94.59708424,FALSE,89.67
3545,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) <MASK> self . set_shuffle_name ( d . getPrefixedString ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 10 :,if tt == 10 :,100,TRUE,100
3546,"def make_release_tree ( self , base_dir , files ) : """"""Make the release tree."""""" self . mkpath ( base_dir ) create_tree ( base_dir , files , dry_run = self . dry_run ) if not files : self . log . warning ( "" no files to distribute -- empty manifest? "" ) else : self . log . info ( "" copying files to  %s ... "" , base_dir ) for filename in files : <MASK> self . log . warning ( "" ' %s '  not a regular file -- skipping "" , filename ) else : dest = os . path . join ( base_dir , filename ) self . copy_file ( filename , dest ) self . distribution . metadata . write_pkg_info ( base_dir )",if not os . path . isfile ( filename ) :,if not os . path . isfile ( filename ) :,100,TRUE,100
3547,"def _parse_names_set ( feature_names ) : """"""Helping function of `_parse_feature_names` that parses a set of feature names."""""" feature_collection = OrderedDict ( ) for feature_name in feature_names : <MASK> feature_collection [ feature_name ] = . . . else : raise ValueError ( "" Failed to parse  {} , expected string "" . format ( feature_name ) ) return feature_collection","if isinstance ( feature_name , str ) :","if isinstance ( feature_name , six . string_types ) :",91.41932736,FALSE,93.82
3548,"def get_connection ( self , url , proxies = None ) : with self . pools . lock : pool = self . pools . get ( url ) <MASK> return pool pool = NpipeHTTPConnectionPool ( self . npipe_path , self . timeout , maxsize = self . max_pool_size ) self . pools [ url ] = pool return pool",if pool :,if pool :,100,TRUE,100
3549,"def _parse_dimensions ( dimensions ) : arrays = [ ] names = [ ] for key in dimensions : values = [ v [ "" name "" ] for v in key [ "" values "" ] ] role = key . get ( "" role "" , None ) <MASK> values = [ _fix_quarter_values ( v ) for v in values ] values = pd . DatetimeIndex ( values ) arrays . append ( values ) names . append ( key [ "" name "" ] ) midx = pd . MultiIndex . from_product ( arrays , names = names ) if len ( arrays ) == 1 and isinstance ( midx , pd . MultiIndex ) : # Fix for pandas >= 0.21 midx = midx . levels [ 0 ] return midx","if role in ( ""time"" , ""TIME_PERIOD"" ) :","if role is not None and role != ""quarter"" :",81.74256369,FALSE,91.84
3550,"def _add_trials ( self , name , spec ) : """"""Add trial by invoking TrialRunner."""""" resource = { } resource [ "" trials "" ] = [ ] trial_generator = BasicVariantGenerator ( ) trial_generator . add_configurations ( { name : spec } ) while not trial_generator . is_finished ( ) : trial = trial_generator . next_trial ( ) <MASK> break runner . add_trial ( trial ) resource [ "" trials "" ] . append ( self . _trial_info ( trial ) ) return resource",if not trial :,if trial is None :,95.60503093,FALSE,96.3
3551,"def _retrieve_key ( self ) : url = "" http://www.canadapost.ca/cpo/mc/personal/postalcode/fpc.jsf "" text = "" "" try : r = requests . get ( url , timeout = self . timeout , proxies = self . proxies ) text = r . text except : self . error = "" ERROR - URL Connection "" if text : expression = r "" ' (....-....-....-....) ' ; "" pattern = re . compile ( expression ) match = pattern . search ( text ) <MASK> self . key = match . group ( 1 ) return self . key else : self . error = "" ERROR - No API Key """,if match :,if match :,100,TRUE,100
3552,"def test_net ( net , env , count = 10 , device = "" cpu "" ) : rewards = 0.0 steps = 0 for _ in range ( count ) : obs = env . reset ( ) while True : obs_v = ptan . agent . float32_preprocessor ( [ obs ] ) . to ( device ) mu_v = net ( obs_v ) [ 0 ] action = mu_v . squeeze ( dim = 0 ) . data . cpu ( ) . numpy ( ) action = np . clip ( action , - 1 , 1 ) obs , reward , done , _ = env . step ( action ) rewards + = reward steps + = 1 <MASK> break return rewards / count , steps / count",if done :,if done :,100,TRUE,100
3553,"def compile ( self , filename , obfuscate = False , raw = False , magic = "" \x00 "" * 8 ) : body = marshal . dumps ( compile ( self . visit ( self . _source_ast ) , filename , "" exec "" ) ) if obfuscate : body_len = len ( body ) offset = 0 if raw else 8 output = bytearray ( body_len + 8 ) for i , x in enumerate ( body ) : output [ i + offset ] = ord ( x ) ^ ( ( 2 * * ( ( 65535 - i ) % 65535 ) ) % 251 ) <MASK> for i in xrange ( 8 ) : output [ i ] = 0 return output el<MASK> return body else : return magic + body",if raw :,if len ( output ) == body_len :,91.38600002,FALSE,87.21
3554,"def _map_saslprep ( s ) : """"""Map stringprep table B.1 to nothing and C.1.2 to ASCII space"""""" r = [ ] for c in s : if stringprep . in_table_c12 ( c ) : r . append ( ""   "" ) <MASK> r . append ( c ) return "" "" . join ( r )",elif not stringprep . in_table_b1 ( c ) :,elif stringprep . in_table_b11 ( c ) :,95.96174436,FALSE,93.95
3555,"def ensemble ( self , pairs , other_preds ) : """"""Ensemble the dict with statistical model predictions."""""" lemmas = [ ] assert len ( pairs ) == len ( other_preds ) for p , pred in zip ( pairs , other_preds ) : w , pos = p if ( w , pos ) in self . composite_dict : lemma = self . composite_dict [ ( w , pos ) ] <MASK> lemma = self . word_dict [ w ] else : lemma = pred if lemma is None : lemma = w lemmas . append ( lemma ) return lemmas",elif w in self . word_dict :,"elif ( w , pos ) in self . word_dict :",94.3389491,FALSE,95.23
3556,"def quiet_f ( * args ) : vars = { arg_name : Real ( arg ) for arg_name , arg in zip ( arg_names , args ) } value = dynamic_scoping ( quiet_expr . evaluate , vars , evaluation ) if expect_list : <MASK> value = [ extract_pyreal ( item ) for item in value . leaves ] if any ( item is None for item in value ) : return None return value else : return None else : value = extract_pyreal ( value ) if value is None or isinf ( value ) or isnan ( value ) : return None return value","if value . has_form ( ""List"" , None ) :",if expect_list :,87.53647321,FALSE,89.99
3557,"def _copy_package_apps ( local_bin_dir : Path , app_paths : List [ Path ] , suffix : str = "" "" ) - > None : for src_unresolved in app_paths : src = src_unresolved . resolve ( ) app = src . name dest = Path ( local_bin_dir / add_suffix ( app , suffix ) ) if not dest . parent . is_dir ( ) : mkdir ( dest . parent ) <MASK> logger . warning ( f "" { hazard }   Overwriting file  { str ( dest ) }  with  { str ( src ) } "" ) dest . unlink ( ) if src . exists ( ) : shutil . copy ( src , dest )",if dest . exists ( ) :,if dest . exists ( ) :,100,TRUE,100
3558,"def assert_readback ( vehicle , values ) : i = 10 while i > 0 : time . sleep ( 0.1 ) i - = 0.1 for k , v in values . items ( ) : <MASK> continue break if i < = 0 : raise Exception ( "" Did not match in channels readback  %s "" % values )",if vehicle . channels [ k ] != v :,if v != vehicle . channels [ k ] :,70.99353883,FALSE,92.91
3559,"def _get_linode_client ( self ) : api_key = self . credentials . conf ( "" key "" ) api_version = self . credentials . conf ( "" version "" ) if api_version == "" "" : api_version = None if not api_version : api_version = 3 # Match for v4 api key regex_v4 = re . compile ( "" ^[0-9a-f] {64} $ "" ) regex_match = regex_v4 . match ( api_key ) <MASK> api_version = 4 else : api_version = int ( api_version ) return _LinodeLexiconClient ( api_key , api_version )",if regex_match :,if regex_match :,100,TRUE,100
3560,"def mergeHiLo ( self , x_stats ) : """"""Merge the highs and lows of another accumulator into myself."""""" if x_stats . firsttime is not None : <MASK> self . firsttime = x_stats . firsttime self . first = x_stats . first if x_stats . lasttime is not None : if self . lasttime is None or x_stats . lasttime > = self . lasttime : self . lasttime = x_stats . lasttime self . last = x_stats . last",if self . firsttime is None or x_stats . firsttime < self . firsttime :,if self . firsttime is None or x_stats . firsttime >= self . firsttime,93.50712861,FALSE,95.67
3561,"def _check_good_input ( self , X , y = None ) : if isinstance ( X , dict ) : lengths = [ len ( X1 ) for X1 in X . values ( ) ] <MASK> raise ValueError ( "" Not all values of X are of equal length. "" ) x_len = lengths [ 0 ] else : x_len = len ( X ) if y is not None : if len ( y ) != x_len : raise ValueError ( "" X and y are not of equal length. "" ) if self . regression and y is not None and y . ndim == 1 : y = y . reshape ( - 1 , 1 ) return X , y",if len ( set ( lengths ) ) > 1 :,if len ( lengths ) != 1 :,94.28933772,FALSE,95.78
3562,"def set ( self , obj , * * kwargs ) : """"""Check for missing event functions and substitute these with"""""" """"""the ignore method"""""" ignore = getattr ( self , "" ignore "" ) for k , v in kwargs . iteritems ( ) : setattr ( self , k , getattr ( obj , v ) ) if k in self . combinations : for k1 in self . combinations [ k ] : <MASK> setattr ( self , k1 , ignore )","if not hasattr ( self , k1 ) :","if getattr ( self , k1 ) is None :",94.31064881,FALSE,94.15
3563,"def _parse_list ( self , tokens ) : # Process left to right, allow descending in sub lists assert tokens [ 0 ] in ( "" [ "" , "" ( "" ) delim = "" ] "" if tokens . pop ( 0 ) == "" [ "" else "" ) "" expr = ExpressionList ( ) while tokens and tokens [ 0 ] != delim : item = self . _parse ( tokens ) <MASK> if tokens . pop ( 0 ) != "" , "" : raise ExpressionSyntaxError ( ' Expected:  "" , "" ' ) expr . append ( item ) if not tokens or tokens [ 0 ] != delim : raise ExpressionSyntaxError ( ' Missing:  "" %s "" ' % delim ) else : tokens . pop ( 0 ) return expr",if tokens and tokens [ 0 ] != delim :,if item is not None :,71.72595847,FALSE,93.35
3564,"def param_value ( self ) : # This is part of the ""handle quoted extended parameters"" hack. for token in self : if token . token_type == "" value "" : return token . stripped_value <MASK> for token in token : if token . token_type == "" bare-quoted-string "" : for token in token : if token . token_type == "" value "" : return token . stripped_value return "" ""","if token . token_type == ""quoted-string"" :","if token . token_type == ""quoted-string"" :",75,TRUE,100
3565,"def paragraph_is_fully_commented ( lines , comment , main_language ) : """"""Is the paragraph fully commented?"""""" for i , line in enumerate ( lines ) : <MASK> if line [ len ( comment ) : ] . lstrip ( ) . startswith ( comment ) : continue if is_magic ( line , main_language ) : return False continue return i > 0 and _BLANK_LINE . match ( line ) return True",if line . startswith ( comment ) :,"if line . startswith ( ""#"" ) :",94.78894787,FALSE,95.46
3566,"def lots_connected_to_existing_roads ( model ) : set = [ ] for h in model . HarvestCells : for ( i , j ) in model . ExistingRoads : <MASK> if h not in set : set . append ( h ) return set",if ( i in model . COriginNodeForCell [ h ] ) or ( j in model . COriginNodeForCell [ h ] ) :,if i == j :,54.39509572,FALSE,66.94
3567,"def detect ( get_page ) : retval = False for vector in WAF_ATTACK_VECTORS : page , headers , code = get_page ( get = vector ) retval = ( re . search ( r "" \ Abarra_counter_session= "" , headers . get ( HTTP_HEADER . SET_COOKIE , "" "" ) , re . I , ) is not None ) retval | = ( re . search ( r "" ( \ A| \ b)barracuda_ "" , headers . get ( HTTP_HEADER . SET_COOKIE , "" "" ) , re . I ) is not None ) <MASK> break return retval",if retval :,if retval :,100,TRUE,100
3568,"def test_files ( self ) : # get names of files to test dist_dir = os . path . join ( os . path . dirname ( __file__ ) , os . pardir , os . pardir ) names = [ ] for d in self . test_directories : test_dir = os . path . join ( dist_dir , d ) for n in os . listdir ( test_dir ) : <MASK> names . append ( os . path . join ( test_dir , n ) ) for filename in names : if test_support . verbose : print ( "" Testing  %s "" % filename ) source = read_pyfile ( filename ) self . check_roundtrip ( source )","if n . endswith ( "".py"" ) and not n . startswith ( ""bad"" ) :","if n . startswith ( ""test_"" ) :",70.69876939,FALSE,91.34
3569,"def test_calibrate_target ( create_target ) : mod , params = testing . synthetic . get_workload ( ) dataset = get_calibration_dataset ( mod , "" data "" ) with relay . quantize . qconfig ( calibrate_mode = "" kl_divergence "" ) : <MASK> with tvm . target . Target ( "" llvm "" ) : relay . quantize . quantize ( mod , params , dataset ) else : # current_target = None relay . quantize . quantize ( mod , params , dataset )",if create_target :,if create_target :,100,TRUE,100
3570,"def _cleanSubmodule ( self , _ = None ) : rc = RC_SUCCESS if self . submodules : command = [ "" submodule "" , "" foreach "" , "" --recursive "" , "" git "" , "" clean "" , "" -f "" , "" -f "" , "" -d "" , ] <MASK> command . append ( "" -x "" ) rc = yield self . _dovccmd ( command ) defer . returnValue ( rc )","if self . mode == ""full"" and self . method == ""fresh"" :",if self . _isXOR :,87.81550642,FALSE,82.57
3571,"def screen_length_to_bytes_count ( string , screen_length_limit , encoding ) : bytes_count = 0 screen_length = 0 for unicode_char in string : screen_length + = screen_len ( unicode_char ) char_bytes_count = len ( unicode_char . encode ( encoding ) ) bytes_count + = char_bytes_count <MASK> bytes_count - = char_bytes_count break return bytes_count",if screen_length > screen_length_limit :,if screen_length > screen_length_limit :,100,TRUE,100
3572,"def tamper ( payload , * * kwargs ) : junk_chars = "" !#$ % &()*~+-_.,:;?@[/| \ ]^` "" retval = "" "" for i , char in enumerate ( payload , start = 1 ) : amount = random . randint ( 10 , 15 ) if char == "" > "" : retval + = "" > "" for _ in range ( amount ) : retval + = random . choice ( junk_chars ) <MASK> retval + = "" < "" for _ in range ( amount ) : retval + = random . choice ( junk_chars ) elif char == ""   "" : for _ in range ( amount ) : retval + = random . choice ( junk_chars ) else : retval + = char return retval","elif char == ""<"" :","elif char == ""<"" :",100,TRUE,100
3573,"def test_parse ( self ) : correct = 0 for example in EXAMPLES : try : schema . parse ( example . schema_string ) if example . valid : correct + = 1 else : self . fail ( "" Invalid schema was parsed:  "" + example . schema_string ) except : <MASK> correct + = 1 else : self . fail ( "" Valid schema failed to parse:  "" + example . schema_string ) fail_msg = "" Parse behavior correct on  %d  out of  %d  schemas. "" % ( correct , len ( EXAMPLES ) , ) self . assertEqual ( correct , len ( EXAMPLES ) , fail_msg )",if not example . valid :,if example . invalid :,96.21880231,FALSE,96.37
3574,"def _on_change ( self ) : changed = False self . save ( ) for key , value in self . data . items ( ) : <MASK> if value : changed = True break if isinstance ( value , int ) : if value != 1 : changed = True break elif value is None : continue elif len ( value ) != 0 : changed = True break self . _reset_button . disabled = not changed","if isinstance ( value , bool ) :","if key == ""button"" :",93.15780262,FALSE,91.78
3575,"def normalize ( d : Dict [ Any , Any ] ) - > Dict [ str , Any ] : first_exception = None for normalizer in normalizers : try : normalized = normalizer ( d ) except KeyError as e : <MASK> first_exception = e else : return normalized assert first_exception is not None raise first_exception",if not first_exception :,if first_exception is None :,83.54442084,FALSE,92.69
3576,"def gather_callback_args ( self , obj , callbacks ) : session = sa . orm . object_session ( obj ) for callback in callbacks : backref = callback . backref root_objs = getdotattr ( obj , backref ) if backref else obj if root_objs : if not isinstance ( root_objs , Iterable ) : root_objs = [ root_objs ] with session . no_autoflush : for root_obj in root_objs : <MASK> args = self . get_callback_args ( root_obj , callback ) if args : yield args",if root_obj :,"if hasattr ( root_obj , ""get_callback_args"" ) :",95.3467558,FALSE,88.9
3577,"def test_opdm_to_oqdm ( self ) : for file in filter ( lambda x : x . endswith ( "" .hdf5 "" ) , os . listdir ( DATA_DIRECTORY ) ) : molecule = MolecularData ( filename = os . path . join ( DATA_DIRECTORY , file ) ) <MASK> test_oqdm = map_one_pdm_to_one_hole_dm ( molecule . fci_one_rdm ) true_oqdm = numpy . eye ( molecule . n_qubits ) - molecule . fci_one_rdm assert numpy . allclose ( test_oqdm , true_oqdm )",if molecule . fci_one_rdm is not None :,if molecule . fci_one_rdm is not None :,100,TRUE,100
3578,"def emitSubDomainData ( self , subDomainData , event ) : self . emitRawRirData ( subDomainData , event ) for subDomainElem in subDomainData : if self . checkForStop ( ) : return None subDomain = subDomainElem . get ( "" subdomain "" , "" "" ) . strip ( ) <MASK> self . emitHostname ( subDomain , event )",if subDomain :,if subDomain :,100,TRUE,100
3579,"def download_cve ( download_path : str , years : Optional [ List [ int ] ] = None , update : bool = False ) : if update : process_url ( CVE_URL . format ( "" modified "" ) , download_path ) else : all_cve_urls = get_cve_links ( CVE_URL , years ) <MASK> raise CveLookupException ( "" Error: No CVE links found "" ) for url in all_cve_urls : process_url ( url , download_path )",if not all_cve_urls :,if not all_cve_urls :,100,TRUE,100
3580,"def is_special ( s , i , directive ) : """"""Return True if the body text contains the @ directive."""""" # j = skip_line(s,i) ; trace(s[i:j],':',directive) assert directive and directive [ 0 ] == "" @ "" # 10/23/02: all directives except @others must start the line. skip_flag = directive in ( "" @others "" , "" @all "" ) while i < len ( s ) : if match_word ( s , i , directive ) : return True , i else : i = skip_line ( s , i ) <MASK> i = skip_ws ( s , i ) return False , - 1",if skip_flag :,if skip_flag :,75,TRUE,100
3581,"def run_async ( self , nuke_cursors ) : # type: (bool) -> None interface_type = self . view . settings ( ) . get ( "" git_savvy.interface "" ) for cls in subclasses : if cls . interface_type == interface_type : vid = self . view . id ( ) interface = interfaces . get ( vid , None ) <MASK> interface = interfaces [ vid ] = cls ( view = self . view ) interface . render ( nuke_cursors = nuke_cursors ) # type: ignore[union-attr] break",if not interface :,if not interface :,75,TRUE,100
3582,"def scan_resource_conf ( self , conf ) : if "" properties "" in conf : <MASK> if str ( conf [ "" properties "" ] [ "" sslEnforcement "" ] ) . lower ( ) == "" enabled "" : return CheckResult . PASSED return CheckResult . FAILED","if ""sslEnforcement"" in conf [ ""properties"" ] :","if ""sslEnforcement"" in conf [ ""properties"" ] :",75,TRUE,100
3583,"def do_shorts ( opts : List [ Tuple [ str , str ] ] , optstring : str , shortopts : str , args : List [ str ] ) - > Tuple [ List [ Tuple [ str , str ] ] , List [ str ] ] : while optstring != "" "" : opt , optstring = optstring [ 0 ] , optstring [ 1 : ] if short_has_arg ( opt , shortopts ) : if optstring == "" "" : <MASK> raise GetoptError ( "" option - %s  requires argument "" % opt , opt ) optstring , args = args [ 0 ] , args [ 1 : ] optarg , optstring = optstring , "" "" else : optarg = "" "" opts . append ( ( "" - "" + opt , optarg ) ) return opts , args",if not args :,if not args :,100,TRUE,100
3584,"def release ( self ) : tid = _thread . get_ident ( ) with self . lock : <MASK> raise RuntimeError ( "" cannot release un-acquired lock "" ) assert self . count > 0 self . count - = 1 if self . count == 0 : self . owner = None if self . waiters : self . waiters - = 1 self . wakeup . release ( )",if self . owner != tid :,if tid in self . owner . locked :,92.4099202,FALSE,91.61
3585,"def _summarize_kraken ( fn ) : """"""get the value at species level"""""" kraken = { } list_sp , list_value = [ ] , [ ] with open ( fn ) as handle : for line in handle : cols = line . strip ( ) . split ( "" \t "" ) sp = cols [ 5 ] . strip ( ) <MASK> list_sp . append ( sp ) list_value . append ( cols [ 0 ] ) kraken = { "" kraken_sp "" : list_sp , "" kraken_value "" : list_value } return kraken","if len ( sp . split ( "" "" ) ) > 1 and not sp . startswith ( ""cellular"" ) :",if sp :,84.00154339,FALSE,82.73
3586,"def _sync_remote_run ( remote_run ) : assert remote_run . remote remote_name = remote_run . remote . name pull_args = click_util . Args ( remote = remote_name , delete = False ) try : remote_impl_support . pull_runs ( [ remote_run ] , pull_args ) except Exception as e : <MASK> log . exception ( "" pull  %s  from  %s "" , remote_run . id , remote_name ) else : log . error ( "" error pulling  %s  from  %s :  %s "" , remote_run . id , remote_name , e )",if log . getEffectiveLevel ( ) <= logging . DEBUG :,if log . getEffectiveLevel ( ) <= logging . DEBUG :,100,TRUE,100
3587,"def group_by_sign ( seq , slop = sin ( pi / 18 ) , key = lambda x : x ) : sign = None subseq = [ ] for i in seq : ki = key ( i ) <MASK> subseq . append ( i ) if ki != 0 : sign = ki / abs ( ki ) else : subseq . append ( i ) if sign * ki < - slop : sign = ki / abs ( ki ) yield subseq subseq = [ i ] if subseq : yield subseq",if sign is None :,if sign is None :,100,TRUE,100
3588,"def import_til ( self ) : log ( "" Importing type libraries... "" ) cur = self . db_cursor ( ) sql = "" select name from diff.program_data where type =  ' til ' "" cur . execute ( sql ) for row in cur . fetchall ( ) : til = row [ "" name "" ] <MASK> til = til . decode ( "" utf-8 "" ) try : add_default_til ( til ) except : log ( "" Error loading til  %s :  %s "" % ( row [ "" name "" ] , str ( sys . exc_info ( ) [ 1 ] ) ) ) cur . close ( ) auto_wait ( )",if type ( til ) is bytes :,"if isinstance ( til , bytes ) :",89.30184285,FALSE,95.61
3589,"def getBranches ( self ) : returned = [ ] for git_branch_line in self . _executeGitCommandAssertSuccess ( "" branch "" ) . stdout : <MASK> git_branch_line = git_branch_line [ 1 : ] git_branch_line = git_branch_line . strip ( ) if BRANCH_ALIAS_MARKER in git_branch_line : alias_name , aliased = git_branch_line . split ( BRANCH_ALIAS_MARKER ) returned . append ( branch . LocalBranchAlias ( self , alias_name , aliased ) ) else : returned . append ( branch . LocalBranch ( self , git_branch_line ) ) return returned","if git_branch_line . startswith ( ""*"" ) :","if git_branch_line . startswith ( ""branch "" ) :",98.37618797,FALSE,98.24
3590,"def add_include_dirs ( self , args ) : ids = [ ] for a in args : # FIXME same hack, forcibly unpack from holder. <MASK> a = a . includedirs if not isinstance ( a , IncludeDirs ) : raise InvalidArguments ( "" Include directory to be added is not an include directory object. "" ) ids . append ( a ) self . include_dirs + = ids","if hasattr ( a , ""includedirs"" ) :","if isinstance ( a , ( IncludeDirs , list ) ) :",95.46185942,FALSE,89.64
3591,"def _serialize_feature ( self , feature ) : name = feature . unique_name ( ) <MASK> self . _features_dict [ feature . unique_name ( ) ] = feature . to_dictionary ( ) for dependency in feature . get_dependencies ( deep = True ) : name = dependency . unique_name ( ) <MASK> self . _features_dict [ name ] = dependency . to_dictionary ( )",if name not in self . _features_dict :,if name not in self . _features_dict :,100,TRUE,100
3592,"def generate_io ( chart_type , race_configs , environment ) : # output JSON structures structures = [ ] for race_config in race_configs : <MASK> title = chart_type . format_title ( environment , race_config . track , es_license = race_config . es_license , suffix = "" %s -io "" % race_config . label , ) structures . append ( chart_type . io ( title , environment , race_config ) ) return structures","if ""io"" in race_config . charts :","if ""io"" in race_config . charts :",75,TRUE,100
3593,"def format_partition ( partition , partition_schema ) : tokens = [ ] if isinstance ( partition , dict ) : for name in partition_schema : <MASK> tok = _format_partition_kv ( name , partition [ name ] , partition_schema [ name ] ) else : # dynamic partitioning tok = name tokens . append ( tok ) else : for name , value in zip ( partition_schema , partition ) : tok = _format_partition_kv ( name , value , partition_schema [ name ] ) tokens . append ( tok ) return "" PARTITION ( {} ) "" . format ( "" ,  "" . join ( tokens ) )",if name in partition :,if name in partition :,100,TRUE,100
3594,"def to_dict ( self , validate = True , ignore = ( ) , context = None ) : context = context or { } condition = getattr ( self , "" condition "" , Undefined ) copy = self # don't copy unless we need to if condition is not Undefined : if isinstance ( condition , core . SchemaBase ) : pass <MASK> kwds = parse_shorthand ( condition [ "" field "" ] , context . get ( "" data "" , None ) ) copy = self . copy ( deep = [ "" condition "" ] ) copy . condition . update ( kwds ) return super ( ValueChannelMixin , copy ) . to_dict ( validate = validate , ignore = ignore , context = context )","elif ""field"" in condition and ""type"" not in condition :","elif isinstance ( condition , dict ) :",96.25902863,FALSE,91.03
3595,"def _checkForCommand ( self ) : prompt = b "" cftp>  "" if self . _expectingCommand and self . _lineBuffer == prompt : buf = b "" \n "" . join ( self . _linesReceived ) <MASK> buf = buf [ len ( prompt ) : ] self . clearBuffer ( ) d , self . _expectingCommand = self . _expectingCommand , None d . callback ( buf )",if buf . startswith ( prompt ) :,if buf . startswith ( prompt ) :,100,TRUE,100
3596,"def schedule_logger ( job_id = None , delete = False ) : if not job_id : return getLogger ( "" fate_flow_schedule "" ) else : if delete : with LoggerFactory . lock : try : for key in LoggerFactory . schedule_logger_dict . keys ( ) : if job_id in key : del LoggerFactory . schedule_logger_dict [ key ] except : pass return True key = job_id + "" schedule "" <MASK> return LoggerFactory . schedule_logger_dict [ key ] return LoggerFactory . get_schedule_logger ( job_id )",if key in LoggerFactory . schedule_logger_dict :,if key in LoggerFactory . schedule_logger_dict :,100,TRUE,100
3597,"def halfMultipartScore ( nzb_name ) : try : wrong_found = 0 for nr in [ 1 , 2 , 3 , 4 , 5 , "" i "" , "" ii "" , "" iii "" , "" iv "" , "" v "" , "" a "" , "" b "" , "" c "" , "" d "" , "" e "" ] : for wrong in [ "" cd "" , "" part "" , "" dis "" , "" disc "" , "" dvd "" ] : if "" %s %s "" % ( wrong , nr ) in nzb_name . lower ( ) : wrong_found + = 1 <MASK> return - 30 return 0 except : log . error ( "" Failed doing halfMultipartScore:  %s "" , traceback . format_exc ( ) ) return 0",if wrong_found == 1 :,if wrong_found == 30 :,93.69465277,FALSE,98.43
3598,"def parse_converter_args ( argstr : str ) - > t . Tuple [ t . Tuple , t . Dict [ str , t . Any ] ] : argstr + = "" , "" args = [ ] kwargs = { } for item in _converter_args_re . finditer ( argstr ) : value = item . group ( "" stringval "" ) if value is None : value = item . group ( "" value "" ) value = _pythonize ( value ) <MASK> args . append ( value ) else : name = item . group ( "" name "" ) kwargs [ name ] = value return tuple ( args ) , kwargs","if not item . group ( ""name"" ) :","if item . group ( ""name"" ) is None :",94.87539028,FALSE,96.28
3599,"def leaves ( self , unique = True ) : """"""Get the leaves of the tree starting at this root."""""" if not self . children : return [ self ] else : res = list ( ) for child in self . children : for sub_child in child . leaves ( unique = unique ) : <MASK> res . append ( sub_child ) return res",if not unique or sub_child not in res :,if sub_child not in res :,92.32183118,FALSE,94.65
3600,"def to_tree ( self , tagname = None , idx = None , namespace = None ) : axIds = set ( ( ax . axId for ax in self . _axes ) ) for chart in self . _charts : for id , axis in chart . _axes . items ( ) : <MASK> setattr ( self , axis . tagname , axis ) axIds . add ( id ) return super ( PlotArea , self ) . to_tree ( tagname )",if id not in axIds :,if id not in axIds :,100,TRUE,100
3601,"def update_neighbor ( neigh_ip_address , changes ) : rets = [ ] for k , v in changes . items ( ) : if k == neighbors . MULTI_EXIT_DISC : rets . append ( _update_med ( neigh_ip_address , v ) ) if k == neighbors . ENABLED : rets . append ( update_neighbor_enabled ( neigh_ip_address , v ) ) <MASK> rets . append ( _update_connect_mode ( neigh_ip_address , v ) ) return all ( rets )",if k == neighbors . CONNECT_MODE :,if k == neighbors . CONNECT_MODE :,100,TRUE,100
3602,"def close_all_connections ( ) : global _managers , _lock , _in_use , _timer _lock . acquire ( ) try : <MASK> _timer . cancel ( ) _timer = None for domain , managers in _managers . items ( ) : for manager in managers : manager . close ( ) _managers = { } finally : _lock . release ( )",if _timer :,if _in_use :,97.97837356,FALSE,94.38
3603,"def _instrument_model ( self , model ) : for key , value in list ( model . __dict__ . items ( ) ) : # avoid ""dictionary keys changed during iteration"" if isinstance ( value , tf . keras . layers . Layer ) : new_layer = self . _instrument ( value ) if new_layer is not value : setattr ( model , key , new_layer ) elif isinstance ( value , list ) : for i , item in enumerate ( value ) : <MASK> value [ i ] = self . _instrument ( item ) return model","if isinstance ( item , tf . keras . layers . Layer ) :","if isinstance ( item , ( tf . keras . layers . Layer , tf . keras . layers",97.05208945,FALSE,92.99
3604,"def target_glob ( tgt , hosts ) : ret = { } for host in hosts : if fnmatch . fnmatch ( tgt , host ) : ret [ host ] = copy . deepcopy ( __opts__ . get ( "" roster_defaults "" , { } ) ) ret [ host ] . update ( { "" host "" : host } ) <MASK> ret [ host ] . update ( { "" user "" : __opts__ [ "" ssh_user "" ] } ) return ret","if __opts__ . get ( ""ssh_user"" ) :","if __opts__ . get ( ""ssh_user"" ) :",100,TRUE,100
3605,"def write ( self , data ) : if mock_target . _mirror_on_stderr : if self . _write_line : sys . stderr . write ( fn + "" :  "" ) <MASK> sys . stderr . write ( data . decode ( "" utf8 "" ) ) else : sys . stderr . write ( data ) if ( data [ - 1 ] ) == "" \n "" : self . _write_line = True else : self . _write_line = False super ( Buffer , self ) . write ( data )",if bytes :,"if isinstance ( data , bytes ) :",95.1582969,FALSE,93.7
3606,"def task_thread ( ) : while not task_queue . empty ( ) : host , port , username , password = task_queue . get ( ) logger . info ( "" try burst  {} : {}  use username: {}  password: {} "" . format ( host , port , username , password ) ) <MASK> with task_queue . mutex : task_queue . queue . clear ( ) result_queue . put ( ( username , password ) )","if telnet_login ( host , port , username , password ) :",if not task_queue . empty ( ) :,84.1900869,FALSE,88.68
3607,"def _format_results ( name , ppl , scores , metrics ) : """"""Format results."""""" result_str = "" "" if ppl : result_str = "" %s  ppl  %.2f "" % ( name , ppl ) if scores : for metric in metrics : <MASK> result_str + = "" ,  %s   %s   %.1f "" % ( name , metric , scores [ metric ] ) else : result_str = "" %s   %s   %.1f "" % ( name , metric , scores [ metric ] ) return result_str",if result_str :,"if metric == ""score"" :",93.72135207,FALSE,93.81
3608,"def info_query ( self , query ) : """"""Send a query which only returns 1 row"""""" self . _cmysql . query ( query ) first_row = ( ) if self . _cmysql . have_result_set : first_row = self . _cmysql . fetch_row ( ) <MASK> self . _cmysql . free_result ( ) raise errors . InterfaceError ( "" Query should not return more than 1 row "" ) self . _cmysql . free_result ( ) return first_row",if self . _cmysql . fetch_row ( ) :,if first_row > 1 :,88.69011684,FALSE,90.62
3609,"def reset_class ( self ) : for f in self . fields_order : <MASK> f . value = int ( f . strbits , 2 ) elif "" default_val "" in f . kargs : f . value = int ( f . kargs [ "" default_val "" ] , 2 ) else : f . value = None if f . fname : setattr ( self , f . fname , f )",if f . strbits and isbin ( f . strbits ) :,if f . strbits :,84.23672104,FALSE,90.52
3610,"def _walk_map_list ( self , access_func ) : seen = [ ] cur = self while cur : <MASK> break yield cur seen . append ( cur . obj_offset ) # check for signs of infinite looping if len ( seen ) > 1024 : break cur = access_func ( cur )",if cur . obj_offset in seen :,if cur . obj_offset in seen :,100,TRUE,100
3611,def bgdel ( ) : q = bgdelq while True : name = q . get ( ) while os . path . exists ( name ) : try : <MASK> os . remove ( name ) else : shutil . rmtree ( name ) except : pass if os . path . exists ( name ) : time . sleep ( 0.1 ),if os . path . isfile ( name ) :,if os . path . islink ( name ) :,98.26188035,FALSE,96.46
3612,"def _find_all_variables ( transfer_variable ) : d = { } for _k , _v in transfer_variable . __dict__ . items ( ) : if isinstance ( _v , Variable ) : d [ _v . _name ] = _v <MASK> d . update ( _find_all_variables ( _v ) ) return d","elif isinstance ( _v , BaseTransferVariables ) :","elif isinstance ( _v , ( list , tuple ) ) :",85.54294225,FALSE,92.46
3613,"def set_val ( ) : idx = 0 for idx in range ( 0 , len ( model ) ) : row = model [ idx ] <MASK> break if idx == len ( os_widget . get_model ( ) ) - 1 : idx = - 1 os_widget . set_active ( idx ) if idx == - 1 : os_widget . set_active ( 0 ) if idx > = 0 : return row [ 1 ] if self . show_all_os : return None",if value and row [ 0 ] == value :,if row [ 0 ] == 0 :,91.59234875,FALSE,94.58
3614,"def _make_cache_key ( group , window , rate , value , methods ) : count , period = _split_rate ( rate ) safe_rate = "" %d / %d s "" % ( count , period ) parts = [ group , safe_rate , value , str ( window ) ] if methods is not None : if methods == ALL : methods = "" "" <MASK> methods = "" "" . join ( sorted ( [ m . upper ( ) for m in methods ] ) ) parts . append ( methods ) prefix = getattr ( settings , "" RATELIMIT_CACHE_PREFIX "" , "" rl: "" ) return prefix + hashlib . md5 ( u "" "" . join ( parts ) . encode ( "" utf-8 "" ) ) . hexdigest ( )","elif isinstance ( methods , ( list , tuple ) ) :",elif methods :,79.09668171,FALSE,93.05
3615,"def findfiles ( path ) : files = [ ] for name in os . listdir ( path ) : # ignore hidden files/dirs and other unwanted files <MASK> continue pathname = os . path . join ( path , name ) st = os . lstat ( pathname ) mode = st . st_mode if stat . S_ISDIR ( mode ) : files . extend ( findfiles ( pathname ) ) elif stat . S_ISREG ( mode ) : files . append ( ( pathname , name , st ) ) return files","if name . startswith ( ""."" ) or name == ""lastsnap.jpg"" :","if name . startswith ( ""_"" ) :",96.39061121,FALSE,89.02
3616,"def __getitem__ ( self , key ) : if isinstance ( key , str_types ) : keys = self . get_keys ( ) <MASK> raise KeyError ( '   "" {0} ""  is an invalid key ' . format ( key ) ) else : return self [ keys . index ( key ) ] else : return list . __getitem__ ( self , key )",if key not in keys :,if key not in keys :,100,TRUE,100
3617,"def test_assert_set_equal ( estimate : tp . Iterable [ int ] , message : str ) - > None : reference = { 1 , 2 , 3 } try : testing . assert_set_equal ( estimate , reference ) except AssertionError as error : if not message : raise AssertionError ( "" An error has been raised while it should not. "" ) from error np . testing . assert_equal ( error . args [ 0 ] . split ( "" \n "" ) [ 1 : ] , message ) else : <MASK> raise AssertionError ( "" An error should have been raised. "" )",if message :,"if not np . testing . assert_equal ( estimate , reference ) :",93.91063565,FALSE,89.13
3618,"def get_directory_info ( prefix , pth , recursive ) : res = [ ] directory = os . listdir ( pth ) directory . sort ( ) for p in directory : <MASK> subp = os . path . join ( pth , p ) p = os . path . join ( prefix , p ) if recursive and os . path . isdir ( subp ) : res . append ( [ p , get_directory_info ( prefix , subp , 1 ) ] ) else : res . append ( [ p , None ] ) return res","if p [ 0 ] != ""."" :",if os . path . isdir ( p ) :,90.69442471,FALSE,91.39
3619,"def check ( self , runner , script , info ) : if isinstance ( info , ast . FunctionDef ) : for arg in info . args . args : <MASK> if arg . id in script . modelVars : self . problem ( "" Function  {0}  may shadow model variable  {1} "" . format ( info . name , arg . id ) , lineno = info . lineno , )","if isinstance ( arg , ast . Name ) :","if isinstance ( arg , ast . Str ) :",73.45096215,FALSE,97.01
3620,"def db_lookup ( field , key , publish_year = None ) : sql = "" select sum(ebook_count) as num from subjects where field=$field and key=$key "" if publish_year : <MASK> sql + = ""  and publish_year between $y1 and $y2 "" ( y1 , y2 ) = publish_year else : sql + = ""  and publish_year=$publish_year "" return list ( ebook_count_db . query ( sql , vars = locals ( ) ) ) [ 0 ] . num","if isinstance ( publish_year , ( tuple , list ) ) :","if isinstance ( publish_year , ( int , long ) ) :",93.05473871,FALSE,96.54
3621,"def put ( self , session ) : with sess_lock : self . parent . put ( session ) # Do not store the session if skip paths for sp in self . skip_paths : <MASK> return if session . sid in self . _cache : try : del self . _cache [ session . sid ] except Exception : pass self . _cache [ session . sid ] = session self . _normalize ( )",if request . path . startswith ( sp ) :,if sp . sid == session . sid :,69.86113612,FALSE,90.37
3622,"def summarize ( self ) : if self . bad_commit and self . good_commit : for subresult in self . subresults . values ( ) : sub = subresult . summarize ( ) <MASK> return sub return "" Detected bad commit in  {}  repository: \n {}   {} "" . format ( self . repo_name , self . bad_commit , get_message ( self . suite , self . bad_commit ) ) return "" """,if sub :,if sub :,100,TRUE,100
3623,def compute_nullable_nonterminals ( self ) : nullable = { } num_nullable = 0 while 1 : for p in self . grammar . Productions [ 1 : ] : if p . len == 0 : nullable [ p . name ] = 1 continue for t in p . prod : if not t in nullable : break else : nullable [ p . name ] = 1 <MASK> break num_nullable = len ( nullable ) return nullable,if len ( nullable ) == num_nullable :,if num_nullable == 0 :,91.53019188,FALSE,91.36
3624,"def _cast_float64_to_float32 ( self , feeds ) : for input_name , input_type in self . inputs : <MASK> feed = feeds . get ( input_name ) if feed is not None and feed . dtype == np . float64 : feeds [ input_name ] = feed . astype ( np . float32 ) return feeds","if input_type == ""tensor(float)"" :","if input_type == ""feed"" :",97.83707196,FALSE,93.22
3625,"def proc_minute ( d ) : if expanded [ 0 ] [ 0 ] != "" * "" : diff_min = nearest_diff_method ( d . minute , expanded [ 0 ] , 60 ) <MASK> if is_prev : d + = relativedelta ( minutes = diff_min , second = 59 ) else : d + = relativedelta ( minutes = diff_min , second = 0 ) return True , d return False , d",if diff_min is not None and diff_min != 0 :,if diff_min is not None :,91.27182575,FALSE,91.47
3626,"def detype ( self ) : if self . _detyped is not None : return self . _detyped ctx = { } for key , val in self . _d . items ( ) : if not isinstance ( key , str ) : key = str ( key ) detyper = self . get_detyper ( key ) if detyper is None : # cannot be detyped continue deval = detyper ( val ) <MASK> # cannot be detyped continue ctx [ key ] = deval self . _detyped = ctx return ctx",if deval is None :,if deval is None :,100,TRUE,100
3627,"def get_or_create_user ( request , user_data ) : try : user = User . objects . get ( sso_id = user_data [ "" id "" ] ) <MASK> update_user ( user , user_data ) return user except User . DoesNotExist : user = User . objects . create_user ( user_data [ "" username "" ] , user_data [ "" email "" ] , is_active = user_data . get ( "" is_active "" , True ) , sso_id = user_data [ "" id "" ] , ) user . update_acl_key ( ) setup_new_user ( request . settings , user ) return user","if user_needs_updating ( user , user_data ) :",if user . is_active :,93.84022817,FALSE,91.9
3628,"def _populate_tree ( self , element , d ) : """"""Populates an etree with attributes & elements, given a dict."""""" for k , v in d . iteritems ( ) : if isinstance ( v , dict ) : self . _populate_dict ( element , k , v ) elif isinstance ( v , list ) : self . _populate_list ( element , k , v ) <MASK> self . _populate_bool ( element , k , v ) elif isinstance ( v , basestring ) : self . _populate_str ( element , k , v ) elif type ( v ) in [ int , float , long , complex ] : self . _populate_number ( element , k , v )","elif isinstance ( v , bool ) :","elif isinstance ( v , bool ) :",100,TRUE,100
3629,"def load ( cls ) : if not cls . _loaded : cls . log . debug ( "" Loading action_sets... "" ) <MASK> cls . _find_action_sets ( PATHS . ACTION_SETS_DIRECTORY ) else : cls . action_sets = JsonDecoder . load ( PATHS . ACTION_SETS_JSON_FILE ) cls . log . debug ( "" Done! "" ) cls . _loaded = True",if not horizons . globals . fife . use_atlases :,if PATHS . ACTION_SETS_DIRECTORY :,87.1628184,FALSE,88.43
3630,"def Resolve ( self , updater = None ) : if len ( self . Conflicts ) : for setting , edge in self . Conflicts : answer = self . AskUser ( self . Setting , setting ) <MASK> value = setting . Value . split ( "" | "" ) value . remove ( edge ) setting . Value = "" | "" . join ( value ) if updater : updater . UpdateSetting ( setting ) if answer == Gtk . ResponseType . NO : return False return True",if answer == Gtk . ResponseType . YES :,if answer == Gtk . ResponseType . NO :,97.81488082,FALSE,97.49
3631,"def read_tsv ( input_file , quotechar = None ) : """"""Reads a tab separated value file."""""" with open ( input_file , "" r "" , encoding = "" utf-8-sig "" ) as f : reader = csv . reader ( f , delimiter = "" \t "" , quotechar = quotechar ) lines = [ ] for line in reader : <MASK> line = list ( str ( cell , "" utf-8 "" ) for cell in line ) # noqa: F821 lines . append ( line ) return lines",if sys . version_info [ 0 ] == 2 :,"if not isinstance ( line , ( list , tuple ) ) :",88.58446935,FALSE,89.35
3632,"def devd_devfs_hook ( middleware , data ) : if data . get ( "" subsystem "" ) != "" CDEV "" : return if data [ "" type "" ] == "" CREATE "" : disks = await middleware . run_in_thread ( lambda : sysctl . filter ( "" kern.disks "" ) [ 0 ] . value . split ( ) ) # Device notified about is not a disk if data [ "" cdev "" ] not in disks : return await added_disk ( middleware , data [ "" cdev "" ] ) elif data [ "" type "" ] == "" DESTROY "" : # Device notified about is not a disk <MASK> return await remove_disk ( middleware , data [ "" cdev "" ] )","if not RE_ISDISK . match ( data [ ""cdev"" ] ) :","if data [ ""cdev"" ] in disks :",96.6951857,FALSE,93
3633,"def on_edit_button_clicked ( self , event = None , a = None , col = None ) : tree , tree_id = self . treeView . get_selection ( ) . get_selected ( ) watchdir_id = str ( self . store . get_value ( tree_id , 0 ) ) if watchdir_id : <MASK> if self . watchdirs [ watchdir_id ] [ "" enabled "" ] : client . autoadd . disable_watchdir ( watchdir_id ) else : client . autoadd . enable_watchdir ( watchdir_id ) else : self . opts_dialog . show ( self . watchdirs [ watchdir_id ] , watchdir_id )","if col and col . get_title ( ) == _ ( ""Active"" ) :",if self . watchdirs [ watchdir_id ] :,88.69573589,FALSE,88.07
3634,"def _execute ( self , options , args ) : if len ( args ) < 1 : raise CommandError ( _ ( "" Not enough arguments "" ) ) paths = args songs = [ self . load_song ( p ) for p in paths ] for song in songs : <MASK> raise CommandError ( _ ( "" Image editing not supported for  %(file_name)s   "" "" ( %(file_format)s ) "" ) % { "" file_name "" : song ( "" ~filename "" ) , "" file_format "" : song ( "" ~format "" ) } ) for song in songs : try : song . clear_images ( ) except AudioFileError as e : raise CommandError ( e )",if not song . can_change_images :,if not song . is_valid ( ) :,96.5709713,FALSE,95.81
3635,"def filter_pricing_rule_based_on_condition ( pricing_rules , doc = None ) : filtered_pricing_rules = [ ] if doc : for pricing_rule in pricing_rules : <MASK> try : if frappe . safe_eval ( pricing_rule . condition , None , doc . as_dict ( ) ) : filtered_pricing_rules . append ( pricing_rule ) except : pass else : filtered_pricing_rules . append ( pricing_rule ) else : filtered_pricing_rules = pricing_rules return filtered_pricing_rules",if pricing_rule . condition :,"if pricing_rule . type == ""pricing_rule"" :",90.99701735,FALSE,92.48
3636,"def ProcessStringLiteral ( self ) : if self . _lastToken == None or self . _lastToken . type == self . OpenBrace : text = super ( JavaScriptBaseLexer , self ) . text if text == ' "" use strict "" ' or text == "" ' use strict ' "" : <MASK> self . _scopeStrictModes . pop ( ) self . _useStrictCurrent = True self . _scopeStrictModes . append ( self . _useStrictCurrent )",if len ( self . _scopeStrictModes ) > 0 :,if self . _useStrictCurrent :,91.38570624,FALSE,90.26
3637,"def _find_remote_inputs ( metadata ) : out = [ ] for fr_key in metadata . keys ( ) : if isinstance ( fr_key , ( list , tuple ) ) : frs = fr_key else : frs = [ fr_key ] for fr in frs : <MASK> out . append ( fr ) return out",if objectstore . is_remote ( fr ) :,"if ""remote"" in fr and ""inputs"" in fr :",90.05824061,FALSE,84.64
3638,"def sub_paragraph ( self , li ) : """"""Search for checkbox in sub-paragraph."""""" found = False if len ( li ) : first = list ( li ) [ 0 ] if first . tag == "" p "" and first . text is not None : m = RE_CHECKBOX . match ( first . text ) <MASK> first . text = self . markdown . htmlStash . store ( get_checkbox ( m . group ( "" state "" ) ) , safe = True ) + m . group ( "" line "" ) found = True return found",if m is not None :,if m :,87.91765659,FALSE,96.16
3639,"def list_files ( basedir ) : """"""List files in the directory rooted at |basedir|."""""" if not os . path . isdir ( basedir ) : raise NoSuchDirectory ( basedir ) directories = [ "" "" ] while directories : d = directories . pop ( ) for basename in os . listdir ( os . path . join ( basedir , d ) ) : filename = os . path . join ( d , basename ) <MASK> directories . append ( filename ) elif os . path . exists ( os . path . join ( basedir , filename ) ) : yield filename","if os . path . isdir ( os . path . join ( basedir , filename ) ) :",if os . path . isdir ( filename ) :,91.56438115,FALSE,91.53
3640,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 10 : self . set_version ( d . getPrefixedString ( ) ) continue <MASK> raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 0 :,if tt == 0 :,100,TRUE,100
3641,"def _dump ( self , fd ) : with self . no_unpicklable_properties ( ) : <MASK> d = pickle . dumps ( self ) module_name = os . path . basename ( sys . argv [ 0 ] ) . rsplit ( "" . "" , 1 ) [ 0 ] d = d . replace ( b "" c__main__ "" , b "" c "" + module_name . encode ( "" ascii "" ) ) fd . write ( d ) else : pickle . dump ( self , fd )","if self . __module__ == ""__main__"" :",if self . _dump_module :,83.63665414,FALSE,88.42
3642,"def assert_session_stack ( classes ) : assert len ( _SklearnTrainingSession . _session_stack ) == len ( classes ) for idx , ( sess , ( parent_clazz , clazz ) ) in enumerate ( zip ( _SklearnTrainingSession . _session_stack , classes ) ) : assert sess . clazz == clazz <MASK> assert sess . _parent is None else : assert sess . _parent . clazz == parent_clazz",if idx == 0 :,if idx == 0 :,100,TRUE,100
3643,"def native_color ( c ) : try : color = CACHE [ c ] except KeyError : <MASK> c = NAMED_COLOR [ c ] color = Color . FromArgb ( int ( c . rgba . a * 255 ) , int ( c . rgba . r ) , int ( c . rgba . g ) , int ( c . rgba . b ) ) CACHE [ c ] = color return color","if isinstance ( c , str ) :",if c not in NAMED_COLOR :,90.87894107,FALSE,91.48
3644,"def callback ( name ) : # XXX: move into Action for neighbor_name in reactor . configuration . neighbors . keys ( ) : neighbor = reactor . configuration . neighbors . get ( neighbor_name , None ) <MASK> continue neighbor . rib . outgoing . announce_watchdog ( name ) yield False reactor . processes . answer_done ( service )",if not neighbor :,if not neighbor :,75,TRUE,100
3645,"def token_producer ( source ) : token = source . read_uint8 ( ) while token is not None : <MASK> yield DataToken ( read_data ( token , source ) ) elif is_small_integer ( token ) : yield SmallIntegerToken ( read_small_integer ( token ) ) else : yield Token ( token ) token = source . read_uint8 ( )",if is_push_data_token ( token ) :,if is_data ( token ) :,97.98057299,FALSE,92.74
3646,"def setattr ( self , req , ino , attr , to_set , fi ) : print ( "" setattr: "" , ino , to_set ) a = self . attr [ ino ] for key in to_set : <MASK> # Keep the old file type bit fields a [ "" st_mode "" ] = S_IFMT ( a [ "" st_mode "" ] ) | S_IMODE ( attr [ "" st_mode "" ] ) else : a [ key ] = attr [ key ] self . attr [ ino ] = a self . reply_attr ( req , a , 1.0 )","if key == ""st_mode"" :",if fi :,87.35649321,FALSE,92.57
3647,"def check_enum_exports ( module , eq_callback , only = None ) : """"""Make sure module exports all mnemonics from enums"""""" for attr in enumerate_module ( module , enum . Enum ) : <MASK> print ( "" SKIP "" , attr ) continue for flag , value in attr . __members__ . items ( ) : print ( module , flag , value ) eq_callback ( getattr ( module , flag ) , value )",if only is not None and attr not in only :,"if only and attr . __module__ . startswith ( ""module"" ) :",87.78774408,FALSE,85.58
3648,"def remove_edit_vars_to ( self , n ) : try : removals = [ ] for v , cei in self . edit_var_map . items ( ) : <MASK> removals . append ( v ) for v in removals : self . remove_edit_var ( v ) assert len ( self . edit_var_map ) == n except ConstraintNotFound : raise InternalError ( "" Constraint not found during internal removal "" )",if cei . index >= n :,"if cei . name == ""edit_var"" :",93.36354771,FALSE,90.48
3649,"def fix_repeating_arguments ( self ) : """"""Fix elements that should accumulate/increment values."""""" either = [ list ( child . children ) for child in transform ( self ) . children ] for case in either : for e in [ child for child in case if case . count ( child ) > 1 ] : if type ( e ) is Argument or type ( e ) is Option and e . argcount : <MASK> e . value = [ ] elif type ( e . value ) is not list : e . value = e . value . split ( ) if type ( e ) is Command or type ( e ) is Option and e . argcount == 0 : e . value = 0 return self",if e . value is None :,if type ( e . value ) is list :,95.15265067,FALSE,94.92
3650,"def add_I_prefix ( current_line : List [ str ] , ner : int , tag : str ) : for i in range ( 0 , len ( current_line ) ) : if i == 0 : f . write ( line_list [ i ] ) <MASK> f . write ( ""  I- "" + tag ) else : f . write ( ""   "" + current_line [ i ] ) f . write ( "" \n "" )",elif i == ner :,elif i == ner :,100,TRUE,100
3651,def select_word_at_cursor ( self ) : word_region = None selection = self . view . sel ( ) for region in selection : word_region = self . view . word ( region ) <MASK> selection . clear ( ) selection . add ( word_region ) return word_region return word_region,if not word_region . empty ( ) :,if word_region is not None :,90.72129616,FALSE,89.19
3652,"def calc ( self , arg ) : op = arg [ "" op "" ] if op == "" C "" : self . clear ( ) return str ( self . current ) num = decimal . Decimal ( arg [ "" num "" ] ) if self . op : if self . op == "" + "" : self . current + = num <MASK> self . current - = num elif self . op == "" * "" : self . current * = num elif self . op == "" / "" : self . current / = num self . op = op else : self . op = op self . current = num res = str ( self . current ) if op == "" = "" : self . clear ( ) return res","elif self . op == ""-"" :","elif self . op == ""-"" :",100,TRUE,100
3653,"def strip_pod ( lines ) : in_pod = False stripped_lines = [ ] for line in lines : if re . match ( r "" ^=(?:end|cut) "" , line ) : in_pod = False elif re . match ( r "" ^= \ w+ "" , line ) : in_pod = True <MASK> stripped_lines . append ( line ) return stripped_lines",elif not in_pod :,if in_pod :,73.46832136,FALSE,96.03
3654,"def __init__ ( self , patch_files , patch_directories ) : files = [ ] files_data = { } for filename_data in patch_files : if isinstance ( filename_data , list ) : filename , data = filename_data else : filename = filename_data data = None if not filename . startswith ( os . sep ) : filename = "" {0} {1} "" . format ( FakeState . deploy_dir , filename ) files . append ( filename ) <MASK> files_data [ filename ] = data self . files = files self . files_data = files_data self . directories = patch_directories",if data :,if data is not None :,95.02279563,FALSE,96.67
3655,"def loadPerfsFromModule ( self , module ) : """"""Return a suite of all perfs cases contained in the given module"""""" perfs = [ ] for name in dir ( module ) : obj = getattr ( module , name ) <MASK> perfs . append ( self . loadPerfsFromPerfCase ( obj ) ) return self . suiteClass ( perfs )","if type ( obj ) == types . ClassType and issubclass ( obj , PerfCase ) :","if issubclass ( obj , PerfsCase ) :",85.70758056,FALSE,82
3656,"def download_subtitle ( self , subtitle ) : if isinstance ( subtitle , XSubsSubtitle ) : # download the subtitle logger . info ( "" Downloading subtitle  %r "" , subtitle ) r = self . session . get ( subtitle . download_link , headers = { "" Referer "" : subtitle . page_link } , timeout = 10 ) r . raise_for_status ( ) <MASK> logger . debug ( "" Unable to download subtitle. No data returned from provider "" ) return subtitle . content = fix_line_ending ( r . content )",if not r . content :,if r . status_code != 404 :,97.0960846,FALSE,92.46
3657,"def get_inlaws ( self , person ) : inlaws = [ ] family_handles = person . get_family_handle_list ( ) for handle in family_handles : fam = self . database . get_family_from_handle ( handle ) if fam . father_handle and not fam . father_handle == person . handle : inlaws . append ( self . database . get_person_from_handle ( fam . father_handle ) ) <MASK> inlaws . append ( self . database . get_person_from_handle ( fam . mother_handle ) ) return inlaws",elif fam . mother_handle and not fam . mother_handle == person . handle :,if fam . mother_handle and not fam . mother_handle == person . handle,72.60638928,FALSE,96.31
3658,"def _check_xorg_conf ( ) : if is_there_a_default_xorg_conf_file ( ) : print ( "" WARNING : Found a Xorg config file at /etc/X11/xorg.conf. If you did not "" ""  create it yourself, it was likely generated by your distribution or by an Nvidia utility. \n "" "" This file may contain hard-coded GPU configuration that could interfere with optimus-manager, "" ""  so it is recommended that you delete it before proceeding. \n "" "" Ignore this warning and proceed with GPU switching ? (y/N) "" ) confirmation = ask_confirmation ( ) <MASK> sys . exit ( 0 )",if not confirmation :,if not confirmation :,100,TRUE,100
3659,"def _make_cache_key ( group , window , rate , value , methods ) : count , period = _split_rate ( rate ) safe_rate = "" %d / %d s "" % ( count , period ) parts = [ group , safe_rate , value , str ( window ) ] if methods is not None : <MASK> methods = "" "" elif isinstance ( methods , ( list , tuple ) ) : methods = "" "" . join ( sorted ( [ m . upper ( ) for m in methods ] ) ) parts . append ( methods ) prefix = getattr ( settings , "" RATELIMIT_CACHE_PREFIX "" , "" rl: "" ) return prefix + hashlib . md5 ( u "" "" . join ( parts ) . encode ( "" utf-8 "" ) ) . hexdigest ( )",if methods == ALL :,"if isinstance ( methods , str ) :",82.3065137,FALSE,95.64
3660,"def num_of_mapped_volumes ( self , initiator ) : cnt = 0 for lm_link in self . req ( "" lun-maps "" ) [ "" lun-maps "" ] : idx = lm_link [ "" href "" ] . split ( "" / "" ) [ - 1 ] # NOTE(geguileo): There can be races so mapped elements retrieved # in the listing may no longer exist. try : lm = self . req ( "" lun-maps "" , idx = int ( idx ) ) [ "" content "" ] except exception . NotFound : continue <MASK> cnt + = 1 return cnt","if lm [ ""ig-name"" ] == initiator :","if lm [ ""type"" ] == initiator :",98.8427924,FALSE,98
3661,"def _setAbsoluteY ( self , value ) : if value is None : self . _absoluteY = None else : <MASK> value = 10 elif value == "" below "" : value = - 70 try : value = common . numToIntOrFloat ( value ) except ValueError as ve : raise TextFormatException ( f "" Not a supported absoluteY position:  { value !r} "" ) from ve self . _absoluteY = value","if value == ""above"" :","if value == ""below"" :",98.33691124,FALSE,97.01
3662,"def render_markdown ( text ) : users = { u . username . lower ( ) : u for u in get_mention_users ( text ) } parts = MENTION_RE . split ( text ) for pos , part in enumerate ( parts ) : if not part . startswith ( "" @ "" ) : continue username = part [ 1 : ] . lower ( ) <MASK> user = users [ username ] parts [ pos ] = ' **[ {} ]( {}   "" {} "" )** ' . format ( part , user . get_absolute_url ( ) , user . get_visible_name ( ) ) text = "" "" . join ( parts ) return mark_safe ( MARKDOWN ( text ) )",if username in users :,if username in users :,100,TRUE,100
3663,def start_process ( self ) : with self . thread_lock : <MASK> self . allow_process_request = False t = threading . Thread ( target = self . __start ) t . daemon = True t . start ( ),if self . allow_process_request :,if self . allow_process_request :,100,TRUE,100
3664,"def close ( self ) : if self . _fh . closed : return self . _fh . close ( ) if os . path . isfile ( self . _filename ) : <MASK> salt . utils . win_dacl . copy_security ( source = self . _filename , target = self . _tmp_filename ) else : shutil . copymode ( self . _filename , self . _tmp_filename ) st = os . stat ( self . _filename ) os . chown ( self . _tmp_filename , st . st_uid , st . st_gid ) atomic_rename ( self . _tmp_filename , self . _filename )",if salt . utils . win_dacl . HAS_WIN32 :,if self . _security :,87.86880148,FALSE,91.55
3665,"def _splitSchemaNameDotFieldName ( sn_fn , fnRequired = True ) : if sn_fn . find ( "" . "" ) != - 1 : schemaName , fieldName = sn_fn . split ( "" . "" , 1 ) schemaName = schemaName . strip ( ) fieldName = fieldName . strip ( ) if schemaName and fieldName : return ( schemaName , fieldName ) elif not fnRequired : schemaName = sn_fn . strip ( ) <MASK> return ( schemaName , None ) controlflow . system_error_exit ( 2 , f "" { sn_fn }  is not a valid custom schema.field name. "" )",if schemaName :,if schemaName and not schemaName . strip ( ) :,91.07379514,FALSE,93.27
3666,"def modified ( self ) : paths = set ( ) dictionary_list = [ ] for op_list in self . _operations : <MASK> op_list = ( op_list , ) for item in chain ( * op_list ) : if item is None : continue dictionary = item . dictionary if dictionary . path in paths : continue paths . add ( dictionary . path ) dictionary_list . append ( dictionary ) return dictionary_list","if not isinstance ( op_list , list ) :","if not isinstance ( op_list , ( list , tuple ) ) :",88.39922455,FALSE,93.75
3667,"def apply ( self , db , person ) : for family_handle in person . get_family_handle_list ( ) : family = db . get_family_from_handle ( family_handle ) <MASK> for event_ref in family . get_event_ref_list ( ) : if event_ref : event = db . get_event_from_handle ( event_ref . ref ) if not event . get_place_handle ( ) : return True if not event . get_date_object ( ) : return True return False",if family :,if family :,100,TRUE,100
3668,"def test_cleanup_params ( self , body , rpc_mock ) : res = self . _get_resp_post ( body ) self . assertEqual ( http_client . ACCEPTED , res . status_code ) rpc_mock . assert_called_once_with ( self . context , mock . ANY ) cleanup_request = rpc_mock . call_args [ 0 ] [ 1 ] for key , value in body . items ( ) : <MASK> if value is not None : value = value == "" true "" self . assertEqual ( value , getattr ( cleanup_request , key ) ) self . assertEqual ( self . _expected_services ( * SERVICES ) , res . json )","if key in ( ""disabled"" , ""is_up"" ) :","if key in ( ""request_id"" , ""request_id"" ) :",98.14199153,FALSE,94.38
3669,"def get_billable_and_total_duration ( activity , start_time , end_time ) : precision = frappe . get_precision ( "" Timesheet Detail "" , "" hours "" ) activity_duration = time_diff_in_hours ( end_time , start_time ) billing_duration = 0.0 if activity . billable : billing_duration = activity . billing_hours <MASK> billing_duration = ( activity_duration * activity . billing_hours / activity . hours ) return flt ( activity_duration , precision ) , flt ( billing_duration , precision )",if activity_duration != activity . billing_hours :,if activity . hours :,90.68750622,FALSE,92.72
3670,"def cpus ( self ) : try : cpus = ( self . inspect [ "" Spec "" ] [ "" Resources "" ] [ "" Reservations "" ] [ "" NanoCPUs "" ] / 1000000000.0 ) <MASK> cpus = int ( cpus ) return cpus except TypeError : return None except KeyError : return 0",if cpus == int ( cpus ) :,if cpus :,87.48162386,FALSE,87.72
3671,"def _create_object ( self , obj_body ) : props = obj_body [ SYMBOL_PROPERTIES ] for prop_name , prop_value in props . items ( ) : <MASK> # get the first key as the convert function func_name = list ( prop_value . keys ( ) ) [ 0 ] if func_name . startswith ( "" _ "" ) : func = getattr ( self , func_name ) props [ prop_name ] = func ( prop_value [ func_name ] ) if SYMBOL_TYPE in obj_body and obj_body [ SYMBOL_TYPE ] in self . fake_func_mapping : return self . fake_func_mapping [ obj_body [ SYMBOL_TYPE ] ] ( * * props ) else : return props","if isinstance ( prop_value , dict ) and prop_value :","if isinstance ( prop_value , dict ) :",90.28480638,FALSE,96.74
3672,"def _yield_unescaped ( self , string ) : while "" \\ "" in string : finder = EscapeFinder ( string ) yield finder . before + finder . backslashes <MASK> yield self . _unescape ( finder . text ) else : yield finder . text string = finder . after yield string",if finder . escaped and finder . text :,if finder . text :,90.52488527,FALSE,93.25
3673,"def _check_matches ( rule , matches ) : errors = 0 for match in matches : filematch = _match_to_test_file ( match ) <MASK> utils . error ( "" The match  ' {} '  for rule  ' {} '  points to a non existing test module path:  {} "" , match , rule , filematch , ) errors + = 1 return errors",if not filematch . exists ( ) :,if not os . path . exists ( filematch ) :,90.14608924,FALSE,91.92
3674,"def focused_windows ( ) : tree = i3 . get_tree ( ) workspaces = tree . workspaces ( ) for workspace in workspaces : container = workspace while container : if not hasattr ( container , "" focus "" ) or not container . focus : break container_id = container . focus [ 0 ] container = container . find_by_id ( container_id ) <MASK> coname = container . name wsname = workspace . name print ( "" WS "" , wsname + "" : "" , coname )",if container :,if container :,100,TRUE,100
3675,"def normals ( self , value ) : if value is not None : value = np . asanyarray ( value , dtype = np . float32 ) value = np . ascontiguousarray ( value ) <MASK> raise ValueError ( "" Incorrect normals shape "" ) self . _normals = value",if value . shape != self . positions . shape :,if len ( value . shape ) != self . shape :,90.56103489,FALSE,88.97
3676,"def test_hexdigest ( self ) : for cons in self . hash_constructors : h = cons ( ) <MASK> self . assertIsInstance ( h . digest ( 16 ) , bytes ) self . assertEqual ( hexstr ( h . digest ( 16 ) ) , h . hexdigest ( 16 ) ) else : self . assertIsInstance ( h . digest ( ) , bytes ) self . assertEqual ( hexstr ( h . digest ( ) ) , h . hexdigest ( ) )",if h . name in self . shakes :,if h . is_hex :,84.59606131,FALSE,93.25
3677,"def _get_cluster_status ( self ) : try : return ( self . dataproc_client . projects ( ) . regions ( ) . clusters ( ) . get ( projectId = self . gcloud_project_id , region = self . dataproc_region , clusterName = self . dataproc_cluster_name , fields = "" status "" , ) . execute ( ) ) except HttpError as e : <MASK> return None # We got a 404 so the cluster doesn't exist else : raise e",if e . resp . status == 404 :,if e . status == 404 :,96.30607221,FALSE,97.37
3678,"def _items_from ( self , context ) : self . _context = context if self . _is_local_variable ( self . _keyword_name , context ) : for item in self . _items_from_controller ( context ) : yield item else : for df in context . datafiles : self . _yield_for_other_threads ( ) <MASK> for item in self . _items_from_datafile ( df ) : yield item",if self . _items_from_datafile_should_be_checked ( df ) :,"if self . _is_local_variable ( self . _keyword_name , df )",93.01709619,FALSE,86.65
3679,"def Command ( argv , funcs , path_val ) : arg , i = COMMAND_SPEC . Parse ( argv ) status = 0 if arg . v : for kind , arg in _ResolveNames ( argv [ i : ] , funcs , path_val ) : <MASK> status = 1 # nothing printed, but we fail else : # This is for -v, -V is more detailed. print ( arg ) else : util . warn ( "" *** command without -v not not implemented *** "" ) status = 1 return status",if kind is None :,"if kind == ""-v"" :",93.7207023,FALSE,94.21
3680,"def delete_doc ( elastic_document_id , node , index = None , category = None ) : index = index or INDEX if not category : if isinstance ( node , Preprint ) : category = "" preprint "" <MASK> category = "" registration "" else : category = node . project_or_component client ( ) . delete ( index = index , doc_type = category , id = elastic_document_id , refresh = True , ignore = [ 404 ] , )",elif node . is_registration :,"elif isinstance ( node , Registration ) :",92.45086238,FALSE,92.83
3681,"def getDictFromTree ( tree ) : ret_dict = { } for child in tree . getchildren ( ) : <MASK> ## Complex-type child. Recurse content = getDictFromTree ( child ) else : content = child . text if ret_dict . has_key ( child . tag ) : if not type ( ret_dict [ child . tag ] ) == list : ret_dict [ child . tag ] = [ ret_dict [ child . tag ] ] ret_dict [ child . tag ] . append ( content or "" "" ) else : ret_dict [ child . tag ] = content or "" "" return ret_dict",if child . getchildren ( ) :,"if child . tag == ""complex"" :",87.1535873,FALSE,94.37
3682,"def get ( self , block = True , timeout = None , ack = False ) : if not block : return self . get_nowait ( ) start_time = time . time ( ) while True : try : return self . get_nowait ( ack ) except BaseQueue . Empty : <MASK> lasted = time . time ( ) - start_time if timeout > lasted : time . sleep ( min ( self . max_timeout , timeout - lasted ) ) else : raise else : time . sleep ( self . max_timeout )",if timeout :,if timeout :,100,TRUE,100
3683,"def rewrite ( self , string ) : string = super ( JSReplaceFuzzy , self ) . rewrite ( string ) cdx = self . url_rewriter . rewrite_opts [ "" cdx "" ] if cdx . get ( "" is_fuzzy "" ) : expected = unquote ( cdx [ "" url "" ] ) actual = unquote ( self . url_rewriter . wburl . url ) exp_m = self . rx_obj . search ( expected ) act_m = self . rx_obj . search ( actual ) <MASK> result = string . replace ( exp_m . group ( 1 ) , act_m . group ( 1 ) ) if result != string : string = result return string",if exp_m and act_m :,if exp_m :,78.7060146,FALSE,97.05
3684,"def locate_exe_dir ( d , check = True ) : exe_dir = os . path . join ( d , "" Scripts "" ) if ON_WINDOWS else os . path . join ( d , "" bin "" ) if not os . path . isdir ( exe_dir ) : <MASK> bin_dir = os . path . join ( d , "" bin "" ) if os . path . isdir ( bin_dir ) : return bin_dir if check : raise InvalidVirtualEnv ( "" Unable to locate executables directory. "" ) return exe_dir",if ON_WINDOWS :,if ON_WINDOWS :,100,TRUE,100
3685,"def _ensuresyspath ( self , ensuremode , path ) : if ensuremode : s = str ( path ) if ensuremode == "" append "" : if s not in sys . path : sys . path . append ( s ) else : <MASK> sys . path . insert ( 0 , s )",if s != sys . path [ 0 ] :,if s not in sys . path :,62.10113139,FALSE,87.65
3686,"def create_season_banners ( self , show_obj ) : if self . season_banners and show_obj : result = [ ] for season , episodes in show_obj . episodes . iteritems ( ) : # @UnusedVariable <MASK> logger . log ( u "" Metadata provider  "" + self . name + ""  creating season banners for  "" + show_obj . name , logger . DEBUG , ) result = result + [ self . save_season_banners ( show_obj , season ) ] return all ( result ) return False","if not self . _has_season_banner ( show_obj , season ) :","if not self . _has_season_banners ( show_obj , season , ep",96.85409358,FALSE,95.07
3687,"def validate_nb ( self , nb ) : super ( MetadataValidatorV3 , self ) . validate_nb ( nb ) ids = set ( [ ] ) for cell in nb . cells : if "" nbgrader "" not in cell . metadata : continue grade = cell . metadata [ "" nbgrader "" ] [ "" grade "" ] solution = cell . metadata [ "" nbgrader "" ] [ "" solution "" ] locked = cell . metadata [ "" nbgrader "" ] [ "" locked "" ] <MASK> continue grade_id = cell . metadata [ "" nbgrader "" ] [ "" grade_id "" ] if grade_id in ids : raise ValidationError ( "" Duplicate grade id:  {} "" . format ( grade_id ) ) ids . add ( grade_id )",if not grade and not solution and not locked :,if not grade or not locked :,93.15070441,FALSE,96.45
3688,"def read_version ( ) : regexp = re . compile ( r "" ^__version__ \ W*= \ W* ' ([ \ d.abrc]+) ' "" ) init_py = os . path . join ( os . path . dirname ( __file__ ) , "" aiopg "" , "" __init__.py "" ) with open ( init_py ) as f : for line in f : match = regexp . match ( line ) <MASK> return match . group ( 1 ) else : raise RuntimeError ( "" Cannot find version in aiopg/__init__.py "" )",if match is not None :,if match :,89.78503196,FALSE,96.62
3689,"def _column_keys ( self ) : """"""Get a dictionary of all columns and their case mapping."""""" if not self . exists : return { } with self . db . lock : if self . _columns is None : # Initialise the table if it doesn't exist table = self . table self . _columns = { } for column in table . columns : name = normalize_column_name ( column . name ) key = normalize_column_key ( name ) <MASK> log . warning ( "" Duplicate column:  %s "" , name ) self . _columns [ key ] = name return self . _columns",if key in self . _columns :,if key in self . _columns :,100,TRUE,100
3690,"def find_controller_by_names ( self , names , testname ) : namestring = "" . "" . join ( names ) if not namestring . startswith ( self . name ) : return None if namestring == self . name : return self for suite in self . suites : res = suite . find_controller_by_names ( namestring [ len ( self . name ) + 1 : ] . split ( "" . "" ) , testname ) <MASK> return res",if res :,if res :,100,TRUE,100
3691,"def _volume_x_metadata_get_item ( context , volume_id , key , model , notfound_exec , session = None ) : result = ( _volume_x_metadata_get_query ( context , volume_id , model , session = session ) . filter_by ( key = key ) . first ( ) ) if not result : <MASK> raise notfound_exec ( id = volume_id ) else : raise notfound_exec ( metadata_key = key , volume_id = volume_id ) return result",if model is models . VolumeGlanceMetadata :,"if key == ""exec"" :",89.62095612,FALSE,93.49
3692,"def parse_results ( cwd ) : optimal_dd = None optimal_measure = numpy . inf for tup in tools . find_conf_files ( cwd ) : dd = tup [ 1 ] <MASK> if dd [ "" results.train_y_misclass "" ] < optimal_measure : optimal_measure = dd [ "" results.train_y_misclass "" ] optimal_dd = dd print ( "" Optimal results.train_y_misclass: "" , str ( optimal_measure ) ) for key , value in optimal_dd . items ( ) : if "" hyper_parameters "" in key : print ( key + "" :  "" + str ( value ) )","if ""results.train_y_misclass"" in dd :","if ""results.train_y_misclass"" in dd :",100,TRUE,100
3693,"def _stop_by_max_time_mins ( self ) : """"""Stop optimization process once maximum minutes have elapsed."""""" if self . max_time_mins : total_mins_elapsed = ( datetime . now ( ) - self . _start_datetime ) . total_seconds ( ) / 60.0 <MASK> raise KeyboardInterrupt ( "" {:.2f}  minutes have elapsed. TPOT will close down. "" . format ( total_mins_elapsed ) )",if total_mins_elapsed >= self . max_time_mins :,if total_mins_elapsed > self . max_time_mins :,97.99333234,FALSE,97.73
3694,"def __new__ ( meta , cls_name , bases , cls_dict ) : func = cls_dict . get ( "" func "" ) monad_cls = super ( FuncMonadMeta , meta ) . __new__ ( meta , cls_name , bases , cls_dict ) if func : <MASK> functions = func else : functions = ( func , ) for func in functions : registered_functions [ func ] = monad_cls return monad_cls",if type ( func ) is tuple :,"if isinstance ( func , ( list , tuple ) ) :",88.56921479,FALSE,90.13
3695,"def get_tokens_unprocessed ( self , text ) : buffered = "" "" insertions = [ ] lng_buffer = [ ] for i , t , v in self . language_lexer . get_tokens_unprocessed ( text ) : <MASK> if lng_buffer : insertions . append ( ( len ( buffered ) , lng_buffer ) ) lng_buffer = [ ] buffered + = v else : lng_buffer . append ( ( i , t , v ) ) if lng_buffer : insertions . append ( ( len ( buffered ) , lng_buffer ) ) return do_insertions ( insertions , self . root_lexer . get_tokens_unprocessed ( buffered ) )",if t is self . needle :,"if t == ""language"" :",94.93757345,FALSE,95.43
3696,"def get_conditions ( filters ) : conditions = { "" docstatus "" : ( "" = "" , 1 ) } if filters . get ( "" from_date "" ) and filters . get ( "" to_date "" ) : conditions [ "" result_date "" ] = ( "" between "" , ( filters . get ( "" from_date "" ) , filters . get ( "" to_date "" ) ) , ) filters . pop ( "" from_date "" ) filters . pop ( "" to_date "" ) for key , value in filters . items ( ) : <MASK> conditions [ key ] = value return conditions",if filters . get ( key ) :,if key not in conditions :,93.93450723,FALSE,94.43
3697,"def _limit_value ( key , value , config ) : if config [ key ] . get ( "" upper_limit "" ) : limit = config [ key ] [ "" upper_limit "" ] # auto handle datetime if isinstance ( value , datetime ) and isinstance ( limit , timedelta ) : <MASK> if ( datetime . now ( ) - limit ) > value : value = datetime . now ( ) - limit else : if ( datetime . now ( ) + limit ) < value : value = datetime . now ( ) + limit elif value > limit : value = limit return value","if config [ key ] [ ""inverse"" ] is True :",if value . tzinfo is None :,95.63174274,FALSE,90.32
3698,"def GetCurrentKeySet ( self ) : "" Return CurrentKeys with  ' darwin '  modifications. "" result = self . GetKeySet ( self . CurrentKeys ( ) ) if sys . platform == "" darwin "" : # macOS (OS X) Tk variants do not support the ""Alt"" # keyboard modifier.  Replace it with ""Option"". # TODO (Ned?): the ""Option"" modifier does not work properly #     for Cocoa Tk and XQuartz Tk so we should not use it #     in the default 'OSX' keyset. for k , v in result . items ( ) : v2 = [ x . replace ( "" <Alt- "" , "" <Option- "" ) for x in v ] <MASK> result [ k ] = v2 return result",if v != v2 :,if v2 :,98.63344594,FALSE,96.9
3699,"def _load_testfile ( filename , package , module_relative ) : if module_relative : package = _normalize_module ( package , 3 ) filename = _module_relative_path ( package , filename ) if hasattr ( package , "" __loader__ "" ) : <MASK> file_contents = package . __loader__ . get_data ( filename ) # get_data() opens files as 'rb', so one must do the equivalent # conversion as universal newlines would do. return file_contents . replace ( os . linesep , "" \n "" ) , filename return open ( filename ) . read ( ) , filename","if hasattr ( package . __loader__ , ""get_data"" ) :","if hasattr ( package . __loader__ , ""get_data"" ) :",100,TRUE,100
3700,"def iter_from_X_lengths ( X , lengths ) : if lengths is None : yield 0 , len ( X ) else : n_samples = X . shape [ 0 ] end = np . cumsum ( lengths ) . astype ( np . int32 ) start = end - lengths <MASK> raise ValueError ( "" more than  {:d}  samples in lengths array  {!s} "" . format ( n_samples , lengths ) ) for i in range ( len ( lengths ) ) : yield start [ i ] , end [ i ]",if end [ - 1 ] > n_samples :,if len ( start ) > n_samples :,92.89695386,FALSE,94.46
3701,"def change_sel ( self ) : """"""Change the view's selections."""""" if self . alter_select and len ( self . sels ) > 0 : <MASK> self . view . show ( self . sels [ 0 ] ) self . view . sel ( ) . clear ( ) self . view . sel ( ) . add_all ( self . sels )",if self . multi_select is False :,if self . view . sel ( ) . exists ( ) :,91.41225935,FALSE,87.83
3702,"def cb_syncthing_device_data_changed ( self , daemon , nid , address , client_version , inbps , outbps , inbytes , outbytes ) : if nid in self . devices : # Should be always device = self . devices [ nid ] # Update strings device [ "" address "" ] = address <MASK> device [ "" version "" ] = client_version # Update rates device [ "" inbps "" ] = "" %s /s ( %s ) "" % ( sizeof_fmt ( inbps ) , sizeof_fmt ( inbytes ) ) device [ "" outbps "" ] = "" %s /s ( %s ) "" % ( sizeof_fmt ( outbps ) , sizeof_fmt ( outbytes ) )","if client_version not in ( ""?"" , None ) :",if client_version :,96.09713002,FALSE,93.04
3703,"def then ( self , matches , when_response , context ) : if is_iterable ( when_response ) : ret = [ ] when_response = list ( when_response ) for match in when_response : if match not in matches : <MASK> match . name = self . match_name matches . append ( match ) ret . append ( match ) return ret <MASK> when_response . name = self . match_name if when_response not in matches : matches . append ( when_response ) return when_response",if self . match_name :,if when_response is not None :,88.69208422,FALSE,87.8
3704,"def __update_parents ( self , fileobj , path , delta ) : """"""Update all parent atoms with the new size."""""" if delta == 0 : return for atom in path : fileobj . seek ( atom . offset ) size = cdata . uint_be ( fileobj . read ( 4 ) ) <MASK> # 64bit # skip name (4B) and read size (8B) size = cdata . ulonglong_be ( fileobj . read ( 12 ) [ 4 : ] ) fileobj . seek ( atom . offset + 8 ) fileobj . write ( cdata . to_ulonglong_be ( size + delta ) ) else : # 32bit fileobj . seek ( atom . offset ) fileobj . write ( cdata . to_uint_be ( size + delta ) )",if size == 1 :,if size == 0 :,99.04648113,FALSE,98.43
3705,"def _fields_to_index ( cls ) : fields = [ ] for field in cls . _meta . sorted_fields : if field . primary_key : continue requires_index = any ( ( field . index , field . unique , isinstance ( field , ForeignKeyField ) ) ) <MASK> fields . append ( field ) return fields",if requires_index :,if requires_index :,100,TRUE,100
3706,"def __init__ ( self , value ) : """"""Initialize the integer to the given value."""""" self . _mpz_p = new_mpz ( ) self . _initialized = False if isinstance ( value , float ) : raise ValueError ( "" A floating point type is not a natural number "" ) self . _initialized = True if isinstance ( value , ( int , long ) ) : _gmp . mpz_init ( self . _mpz_p ) result = _gmp . gmp_sscanf ( tobytes ( str ( value ) ) , b ( "" % Zd "" ) , self . _mpz_p ) <MASK> raise ValueError ( "" Error converting  ' %d ' "" % value ) else : _gmp . mpz_init_set ( self . _mpz_p , value . _mpz_p )",if result != 1 :,if result != 0 :,99.05536933,FALSE,98.56
3707,"def decode ( cls , data ) : while data : length , format_type , control_flags , sequence , pid = unpack ( cls . Header . PACK , data [ : cls . Header . LEN ] ) <MASK> raise NetLinkError ( "" Buffer underrun "" ) yield cls . format ( format_type , control_flags , sequence , pid , data [ cls . Header . LEN : length ] ) data = data [ length : ]",if len ( data ) < length :,if length < cls . Header . LEN :,89.91264838,FALSE,91.4
3708,"def __post_init__ ( self ) : if self . _node_id is not None : <MASK> raise ValueError ( "" invalid node_id:  {} "" . format ( hexlify ( self . _node_id ) . decode ( ) ) ) if self . udp_port is not None and not 1 < = self . udp_port < = 65535 : raise ValueError ( "" invalid udp port "" ) if self . tcp_port is not None and not 1 < = self . tcp_port < = 65535 : raise ValueError ( "" invalid tcp port "" ) if not is_valid_public_ipv4 ( self . address , self . allow_localhost ) : raise ValueError ( f "" invalid ip address:  ' { self . address } ' "" )",if not len ( self . _node_id ) == constants . HASH_LENGTH :,if not is_valid_node_id ( self . _node_id ) :,94.2804127,FALSE,93.78
3709,"def orderUp ( self , items ) : sel = [ ] # new selection undoinfo = [ ] for bid , lid in items : if isinstance ( lid , int ) : undoinfo . append ( self . orderUpLineUndo ( bid , lid ) ) sel . append ( ( bid , lid - 1 ) ) <MASK> undoinfo . append ( self . orderUpBlockUndo ( bid ) ) if bid == 0 : return items else : sel . append ( ( bid - 1 , None ) ) self . addUndo ( undoinfo , "" Move Up "" ) return sel",elif lid is None :,"elif isinstance ( lid , int ) :",72.07688891,FALSE,93.47
3710,"def filter_data ( self , min_len , max_len ) : logging . info ( f "" filtering data, min len:  { min_len } , max len:  { max_len } "" ) initial_len = len ( self . src ) filtered_src = [ ] filtered_tgt = [ ] for src , tgt in zip ( self . src , self . tgt ) : <MASK> filtered_src . append ( src ) filtered_tgt . append ( tgt ) self . src = filtered_src self . tgt = filtered_tgt filtered_len = len ( self . src ) logging . info ( f "" pairs before:  { initial_len } , after:  { filtered_len } "" )",if min_len <= len ( src ) <= max_len and min_len <= len ( tgt ) <= max_len :,if min_len < src < max_len and tgt < initial_len :,90.03317539,FALSE,87.75
3711,"def layer_pretrained ( self , net , args , options ) : model = getattr ( torchvision . models , args [ 0 ] ) ( pretrained = True ) model . train ( True ) if options . layer : layers = list ( model . children ( ) ) [ : options . layer ] <MASK> layers [ - 1 ] = nn . Sequential ( * layers [ - 1 ] [ : options . sublayer ] ) else : layers = [ model ] print ( "" List of pretrained layers: "" , layers ) raise ValidationException ( "" layer=-1 required for pretrained, sublayer=-1 optional.  Layers outputted above. "" ) return nn . Sequential ( * layers )",if options . sublayer :,if options . sublayer :,100,TRUE,100
3712,"def deleteCalendar ( users ) : calendarId = normalizeCalendarId ( sys . argv [ 5 ] ) for user in users : user , cal = buildCalendarGAPIObject ( user ) <MASK> continue gapi . call ( cal . calendarList ( ) , "" delete "" , soft_errors = True , calendarId = calendarId )",if not cal :,if not cal :,100,TRUE,100
3713,"def iter_modules ( self , by_clients = False , clients_filter = None ) : """"""iterate over all modules"""""" clients = None if by_clients : clients = self . get_clients ( clients_filter ) <MASK> return self . _refresh_modules ( ) for module_name in self . modules : try : module = self . get_module ( module_name ) except PupyModuleDisabled : continue if clients is not None : for client in clients : if module . is_compatible_with ( client ) : yield module break else : yield module",if not clients :,if not clients :,100,TRUE,100
3714,"def update_me ( self ) : try : while 1 : line = self . queue . get_nowait ( ) <MASK> self . delete ( 1.0 , tk . END ) else : self . insert ( tk . END , str ( line ) ) self . see ( tk . END ) self . update_idletasks ( ) except queue . Empty : pass self . after ( 100 , self . update_me )",if line is None :,if line is None :,100,TRUE,100
3715,"def request_power_state ( self , state , force = False ) : if self . current_state != state or force : <MASK> self . request_in_progress = True logging . info ( "" Requesting  %s "" % state ) cb = PowerManager . Callback ( self , state ) rets = self . parent . Plugins . run ( "" on_power_state_change_requested "" , self , state , cb ) cb . num_cb = len ( rets ) cb . check ( ) else : logging . info ( "" Another request in progress "" )",if not self . request_in_progress :,if self . request_in_progress :,94.07857355,FALSE,97.97
3716,"def __getitem__ ( self , idx ) : super ( BatchDataset , self ) . __getitem__ ( idx ) maxidx = len ( self . dataset ) samples = [ ] for i in range ( 0 , self . batchsize ) : j = idx * self . batchsize + i if j > = maxidx : break j = self . perm ( j , maxidx ) sample = self . dataset [ j ] <MASK> samples . append ( sample ) samples = self . makebatch ( samples ) return samples",if self . filter ( sample ) :,"if self . _is_batch_size_equal ( sample . size , self . batch",94.71795992,FALSE,85.83
3717,"def __call__ ( self , request , * args , * * kwargs ) : template_vars = { } for form_name , form_class in self . forms . iteritems ( ) : <MASK> template_vars [ form_name ] = form_class ( request ) else : template_vars [ form_name ] = None if request . method == "" POST "" : action = self . find_post_handler_action ( request ) form = self . handlers [ action ] ( request , data = request . POST , files = request . FILES ) template_vars . update ( form . dispatch ( action , request , * args , * * kwargs ) ) return self . GET ( template_vars , request , * args , * * kwargs )","if form_class . must_display ( request , * args , ** kwargs ) :",if callable ( form_class ) :,86.31312006,FALSE,90.76
3718,"def on_show_all ( self , widget , another ) : if widget . get_active ( ) : <MASK> self . treeview . update_items ( all = True , comment = True ) else : self . treeview . update_items ( all = True ) else : <MASK> self . treeview . update_items ( comment = True ) else : self . treeview . update_items ( )",if another . get_active ( ) :,if another :,65.73947948,FALSE,83.47
3719,"def close ( self ) : if self . _closed : return self . _closed = True for proto in self . _pipes . values ( ) : if proto is None : continue proto . pipe . close ( ) if ( self . _proc is not None and # has the child process finished? self . _returncode is None and # the child process has finished, but the # transport hasn't been notified yet? self . _proc . poll ( ) is None ) : <MASK> logger . warning ( "" Close running child process: kill  %r "" , self ) try : self . _proc . kill ( ) except ProcessLookupError : pass",if self . _loop . get_debug ( ) :,if self . _returncode is not None :,97.44946409,FALSE,93.81
3720,"def runTest ( self ) : self . poco ( text = "" wait UI "" ) . click ( ) bomb_count = 0 while True : blue_fish = self . poco ( "" fish_emitter "" ) . child ( "" blue "" ) yellow_fish = self . poco ( "" fish_emitter "" ) . child ( "" yellow "" ) bomb = self . poco ( "" fish_emitter "" ) . child ( "" bomb "" ) fish = self . poco . wait_for_any ( [ blue_fish , yellow_fish , bomb ] ) if fish is bomb : bomb_count + = 1 <MASK> return else : fish . click ( ) time . sleep ( 2.5 )",if bomb_count > 3 :,if bomb_count == 10 :,98.56192543,FALSE,96.9
3721,"def load_managers ( * , loop , only ) : managers = { } for key in DB_CLASSES : <MASK> continue params = DB_DEFAULTS . get ( key ) or { } params . update ( DB_OVERRIDES . get ( key ) or { } ) database = DB_CLASSES [ key ] ( * * params ) managers [ key ] = peewee_async . Manager ( database , loop = loop ) return managers",if only and key not in only :,if only and key not in only :,100,TRUE,100
3722,"def links_extracted ( self , request , links ) : for link in links : <MASK> r = self . _create_request ( link . url ) r . meta [ b "" depth "" ] = request . meta [ b "" depth "" ] + 1 self . schedule ( r , self . _get_score ( r . meta [ b "" depth "" ] ) ) link . meta [ b "" state "" ] = States . QUEUED","if link . meta [ b""state"" ] == States . NOT_CRAWLED :","if link . meta [ b""state"" ] == States . NOT_CRAWLED :",100,TRUE,100
3723,"def find_worktree_git_dir ( dotgit ) : """"""Search for a gitdir for this worktree."""""" try : statbuf = os . stat ( dotgit ) except OSError : return None if not stat . S_ISREG ( statbuf . st_mode ) : return None try : lines = open ( dotgit , "" r "" ) . readlines ( ) for key , value in [ line . strip ( ) . split ( "" :  "" ) for line in lines ] : <MASK> return value except ValueError : pass return None","if key == ""gitdir"" :","if key == ""gitdir"" :",100,TRUE,100
3724,"def _is_static_shape ( self , shape ) : if shape is None or not isinstance ( shape , list ) : return False for dim_value in shape : if not isinstance ( dim_value , int ) : return False <MASK> raise Exception ( "" Negative dimension is illegal:  %d "" % dim_value ) return True",if dim_value < 0 :,if dim_value < 0 :,100,TRUE,100
3725,"def init_logger ( ) : configured_loggers = [ log_config . get ( "" root "" , { } ) ] + [ logger for logger in log_config . get ( "" loggers "" , { } ) . values ( ) ] used_handlers = { handler for log in configured_loggers for handler in log . get ( "" handlers "" , [ ] ) } for handler_id , handler in list ( log_config [ "" handlers "" ] . items ( ) ) : if handler_id not in used_handlers : del log_config [ "" handlers "" ] [ handler_id ] <MASK> filename = handler [ "" filename "" ] logfile_path = Path ( filename ) . expanduser ( ) . resolve ( ) handler [ "" filename "" ] = str ( logfile_path ) logging . config . dictConfig ( log_config )","elif ""filename"" in handler . keys ( ) :","if ""filename"" in handler :",81.15052284,FALSE,95.55
3726,"def __call__ ( self ) : dmin , dmax = self . viewlim_to_dt ( ) ymin = self . base . le ( dmin . year ) ymax = self . base . ge ( dmax . year ) ticks = [ dmin . replace ( year = ymin , * * self . replaced ) ] while 1 : dt = ticks [ - 1 ] <MASK> return date2num ( ticks ) year = dt . year + self . base . get_base ( ) ticks . append ( dt . replace ( year = year , * * self . replaced ) )",if dt . year >= ymax :,if dt . year == ymax :,98.85731323,FALSE,97.91
3727,"def taiga ( request , trigger_id , key ) : signature = request . META . get ( "" HTTP_X_TAIGA_WEBHOOK_SIGNATURE "" ) # check that the data are ok with the provided signature if verify_signature ( request . _request . body , key , signature ) : data = data_filter ( trigger_id , * * request . data ) status = save_data ( trigger_id , data ) return ( Response ( { "" message "" : "" Success "" } ) <MASK> else Response ( { "" message "" : "" Failed! "" } ) ) Response ( { "" message "" : "" Bad request "" } )",if status,if status,100,TRUE,100
3728,"def ParseResponses ( self , knowledge_base : rdf_client . KnowledgeBase , responses : Iterable [ rdfvalue . RDFValue ] , ) - > Iterator [ rdf_client . User ] : for response in responses : <MASK> raise TypeError ( f "" Unexpected response type: ` { type ( response ) } ` "" ) # TODO: `st_mode` has to be an `int`, not `StatMode`. if stat . S_ISDIR ( int ( response . st_mode ) ) : homedir = response . pathspec . path username = os . path . basename ( homedir ) if username not in self . _ignore_users : yield rdf_client . User ( username = username , homedir = homedir )","if not isinstance ( response , rdf_client_fs . StatEntry ) :","if not isinstance ( response , rdfvalue . RDFValue ) :",73.29719017,FALSE,94.64
3729,"def _iter_lines ( path = path , response = response , max_next = options . http_max_next ) : path . responses = [ ] n = 0 while response : path . responses . append ( response ) yield from response . iter_lines ( decode_unicode = True ) src = response . links . get ( "" next "" , { } ) . get ( "" url "" , None ) if not src : break n + = 1 <MASK> vd . warning ( f "" stopping at max  { max_next }  pages "" ) break vd . status ( f "" fetching next page from  { src } "" ) response = requests . get ( src , stream = True )",if n > max_next :,if n > max_next :,100,TRUE,100
3730,"def __enter__ ( self ) : """"""Open a file and read it."""""" if self . code is None : LOGGER . info ( "" File is reading:  %s "" , self . path ) <MASK> self . _file = open ( self . path , encoding = "" utf-8 "" ) else : self . _file = open ( self . path , "" rU "" ) self . code = self . _file . read ( ) return self","if sys . version_info >= ( 3 , ) :","if sys . version_info < ( 3 , 0 ) :",92.28265119,FALSE,95.26
3731,"def facts_for_oauthclients ( self , namespace ) : """"""Gathers facts for oauthclients used with logging"""""" self . default_keys_for ( "" oauthclients "" ) a_list = self . oc_command ( "" get "" , "" oauthclients "" , namespace = namespace , add_options = [ "" -l "" , LOGGING_SELECTOR ] ) if len ( a_list [ "" items "" ] ) == 0 : return for item in a_list [ "" items "" ] : name = item [ "" metadata "" ] [ "" name "" ] comp = self . comp ( name ) <MASK> result = dict ( redirectURIs = item [ "" redirectURIs "" ] ) self . add_facts_for ( comp , "" oauthclients "" , name , result )",if comp is not None :,if comp is not None :,100,TRUE,100
3732,"def get ( self , k ) : with self . _lock : <MASK> self . _data1 [ k ] = self . _data2 [ k ] del self . _data2 [ k ] return self . _data1 . get ( k )",if k not in self . _data1 and k in self . _data2 :,if k not in self . _data1 :,85.10761949,FALSE,85.76
3733,"def _parseparam ( s ) : plist = [ ] while s [ : 1 ] == "" ; "" : s = s [ 1 : ] end = s . find ( "" ; "" ) while end > 0 and ( s . count ( ' "" ' , 0 , end ) - s . count ( ' \\ "" ' , 0 , end ) ) % 2 : end = s . find ( "" ; "" , end + 1 ) <MASK> end = len ( s ) f = s [ : end ] if "" = "" in f : i = f . index ( "" = "" ) f = f [ : i ] . strip ( ) . lower ( ) + "" = "" + f [ i + 1 : ] . strip ( ) plist . append ( f . strip ( ) ) s = s [ end : ] return plist",if end < 0 :,if end < 0 :,100,TRUE,100
3734,"def __init__ ( self , * * params ) : if "" length "" in params : <MASK> raise ValueError ( "" Supply either length or start and end to Player not both "" ) params [ "" start "" ] = 0 params [ "" end "" ] = params . pop ( "" length "" ) - 1 elif params . get ( "" start "" , 0 ) > 0 and not "" value "" in params : params [ "" value "" ] = params [ "" start "" ] super ( Player , self ) . __init__ ( * * params )","if ""start"" in params or ""end"" in params :","if params . get ( ""start"" , 0 ) == 0 and not ""end"" in",75.16191859,FALSE,87.66
3735,"def libcxx_define ( settings ) : compiler = _base_compiler ( settings ) libcxx = settings . get_safe ( "" compiler.libcxx "" ) if not compiler or not libcxx : return "" "" if str ( compiler ) in GCC_LIKE : if str ( libcxx ) == "" libstdc++ "" : return "" _GLIBCXX_USE_CXX11_ABI=0 "" <MASK> return "" _GLIBCXX_USE_CXX11_ABI=1 "" return "" ""","elif str ( libcxx ) == ""libstdc++11"" :","if str ( libcxx ) == ""libstdc++"" :",94.18917962,FALSE,95.22
3736,"def _get_sort_map ( tags ) : """"""See TAG_TO_SORT"""""" tts = { } for name , tag in tags . items ( ) : if tag . has_sort : if tag . user : tts [ name ] = "" %s sort "" % name <MASK> tts [ "" ~ %s "" % name ] = "" ~ %s sort "" % name return tts",if tag . internal :,elif tag . user . is_staff :,81.36057211,FALSE,90.28
3737,"def quiet_f ( * args ) : vars = { arg_name : Real ( arg ) for arg_name , arg in zip ( arg_names , args ) } value = dynamic_scoping ( quiet_expr . evaluate , vars , evaluation ) if expect_list : if value . has_form ( "" List "" , None ) : value = [ extract_pyreal ( item ) for item in value . leaves ] if any ( item is None for item in value ) : return None return value else : return None else : value = extract_pyreal ( value ) <MASK> return None return value",if value is None or isinf ( value ) or isnan ( value ) :,if value is None :,91.22403457,FALSE,91.25
3738,"def on_action_chosen ( self , id , action , mark_changed = True ) : before = self . set_action ( self . current , id , action ) if mark_changed : <MASK> # TODO: Maybe better comparison self . undo . append ( UndoRedo ( id , before , action ) ) self . builder . get_object ( "" btUndo "" ) . set_sensitive ( True ) self . on_profile_modified ( ) else : self . on_profile_modified ( update_ui = False ) return before",if before . to_string ( ) != action . to_string ( ) :,if self . current . profile_modified :,79.02041915,FALSE,86.35
3739,"def setUp ( self ) : super ( OperaterTest , self ) . setUp ( ) if is_cli : import clr self . load_iron_python_test ( ) <MASK> clr . AddReference ( "" System.Drawing.Primitives "" ) else : clr . AddReference ( "" System.Drawing "" )",if is_netcoreapp :,if self . use_primitives :,91.89907371,FALSE,90.08
3740,"def field_to_field_type ( field ) : field_type = field [ "" type "" ] if isinstance ( field_type , dict ) : field_type = field_type [ "" type "" ] if isinstance ( field_type , list ) : field_type_length = len ( field_type ) if field_type_length == 0 : raise Exception ( "" Zero-length type list encountered, invalid CWL? "" ) <MASK> field_type = field_type [ 0 ] return field_type",elif len ( field_type ) == 1 :,if field_type_length == 1 :,88.70351433,FALSE,93.8
3741,"def _flatten ( * args ) : ahs = set ( ) if len ( args ) > 0 : for item in args : <MASK> ahs . add ( item ) elif type ( item ) in ( list , tuple , dict , set ) : for ah in item : if type ( ah ) is not ActionHandle : # pragma:nocover raise ActionManagerError ( "" Bad argument type  %s "" % str ( ah ) ) ahs . add ( ah ) else : # pragma:nocover raise ActionManagerError ( "" Bad argument type  %s "" % str ( item ) ) return ahs",if type ( item ) is ActionHandle :,if type ( item ) is ActionHandle :,100,TRUE,100
3742,"def _Determine_Do ( self ) : self . applicable = 1 configTokens = black . configure . items [ "" configTokens "" ] . Get ( ) buildFlavour = black . configure . items [ "" buildFlavour "" ] . Get ( ) if buildFlavour == "" full "" : self . value = False else : self . value = True for opt , optarg in self . chosenOptions : <MASK> if not self . value : configTokens . append ( "" tests "" ) self . value = True elif opt == "" --without-tests "" : if self . value : configTokens . append ( "" notests "" ) self . value = False self . determined = 1","if opt == ""--with-tests"" :","if opt == ""--tests"" :",98.99675745,FALSE,98.09
3743,"def title_by_index ( self , trans , index , context ) : d_type = self . get_datatype ( trans , context ) for i , ( composite_name , composite_file ) in enumerate ( d_type . writable_files . items ( ) ) : <MASK> rval = composite_name if composite_file . description : rval = "" {}  ( {} ) "" . format ( rval , composite_file . description ) if composite_file . optional : rval = "" %s  [optional] "" % rval return rval if index < self . get_file_count ( trans , context ) : return "" Extra primary file "" return None",if i == index :,if i == index :,100,TRUE,100
3744,"def func ( x , y ) : try : <MASK> z = x + 2 * math . sin ( y ) return z * * 2 elif x == y : return 4 else : return 2 * * 3 except ValueError : foo = 0 for i in range ( 4 ) : foo + = i return foo except TypeError : return 42 else : return 33 finally : print ( "" finished "" )",if x > y :,if x < y :,98.48458649,FALSE,96.94
3745,"def test_suite ( ) : suite = unittest . TestSuite ( ) for fn in os . listdir ( here ) : <MASK> modname = "" distutils.tests. "" + fn [ : - 3 ] __import__ ( modname ) module = sys . modules [ modname ] suite . addTest ( module . test_suite ( ) ) return suite","if fn . startswith ( ""test"" ) and fn . endswith ( "".py"" ) :","if fn . startswith ( ""test"" ) and fn . endswith ( "".py"" ) :",100,TRUE,100
3746,"def check_stack_names ( self , frame , expected ) : names = [ ] while frame : name = frame . f_code . co_name # Stop checking frames when we get to our test helper. <MASK> break names . append ( name ) frame = frame . f_back self . assertEqual ( names , expected )","if name . startswith ( ""check_"" ) or name . startswith ( ""call_"" ) :",if name in self . _stack_names :,90.13623622,FALSE,77.44
3747,"def leave ( self , reason = None ) : try : if self . id . startswith ( "" C "" ) : log . info ( "" Leaving channel  %s  ( %s ) "" , self , self . id ) self . _bot . api_call ( "" conversations.leave "" , data = { "" channel "" : self . id } ) else : log . info ( "" Leaving group  %s  ( %s ) "" , self , self . id ) self . _bot . api_call ( "" conversations.leave "" , data = { "" channel "" : self . id } ) except SlackAPIResponseError as e : <MASK> raise RoomError ( f "" Unable to leave channel.  { USER_IS_BOT_HELPTEXT } "" ) else : raise RoomError ( e ) self . _id = None","if e . error == ""user_is_bot"" :",if e . error_code == 400 :,96.00722199,FALSE,94.5
3748,"def ident ( self ) : value = self . _ident if value is False : value = None # XXX: how will this interact with orig_prefix ? #      not exposing attrs for now if orig_prefix is set. <MASK> wrapped = self . wrapped ident = getattr ( wrapped , "" ident "" , None ) if ident is not None : value = self . _wrap_hash ( ident ) self . _ident = value return value",if not self . orig_prefix :,if self . _prefix is not None :,96.62513108,FALSE,92.76
3749,"def is_ac_power_connected ( ) : for power_source_path in Path ( "" /sys/class/power_supply/ "" ) . iterdir ( ) : try : with open ( power_source_path / "" type "" , "" r "" ) as f : <MASK> continue with open ( power_source_path / "" online "" , "" r "" ) as f : if f . read ( 1 ) == "" 1 "" : return True except IOError : continue return False","if f . read ( ) . strip ( ) != ""Mains"" :","if f . read ( 1 ) == ""0"" :",66.63186001,FALSE,91.73
3750,"def _get_pending_by_app_token ( self , app_token ) : result = [ ] with self . _pending_lock : self . _remove_stale_pending ( ) for data in self . _pending_decisions : <MASK> result . append ( data ) return result",if data . app_token == app_token :,if data . app_token == app_token :,100,TRUE,100
3751,"def do_create ( specific_tables = None , base = Base ) : engine = get_engine ( ) try : <MASK> logger . info ( "" Initializing only a subset of tables as requested:  {} "" . format ( specific_tables ) ) base . metadata . create_all ( engine , tables = specific_tables ) else : base . metadata . create_all ( engine ) except Exception as err : raise Exception ( "" could not create/re-create DB tables - exception:  "" + str ( err ) )",if specific_tables :,if specific_tables :,100,TRUE,100
3752,"def __setitem__ ( self , ndx , val ) : # # Get the expression data object # exprdata = None if ndx in self . _data : exprdata = self . _data [ ndx ] else : _ndx = normalize_index ( ndx ) <MASK> exprdata = self . _data [ _ndx ] if exprdata is None : raise KeyError ( "" Cannot set the value of Expression  ' %s '  with  "" "" invalid index  ' %s ' "" % ( self . cname ( True ) , str ( ndx ) ) ) # # Set the value # exprdata . set_value ( val )",if _ndx in self . _data :,if _ndx in self . _data :,75,TRUE,100
3753,"def write ( self , * bits ) : for bit in bits : <MASK> self . bytestream . append ( 0 ) byte = self . bytestream [ self . bytenum ] if self . bitnum == 8 : if self . bytenum == len ( self . bytestream ) - 1 : byte = 0 self . bytestream + = bytes ( [ byte ] ) self . bytenum + = 1 self . bitnum = 0 mask = 2 * * self . bitnum if bit : byte | = mask else : byte & = ~ mask self . bytestream [ self . bytenum ] = byte self . bitnum + = 1",if not self . bytestream :,if self . bytenum not in self . bytestream :,90.04357281,FALSE,95.04
3754,"def terminate_subprocess ( proc , timeout = 0.1 , log = None ) : if proc . poll ( ) is None : <MASK> log . info ( "" Sending SIGTERM to  %r "" , proc ) proc . terminate ( ) timeout_time = time . time ( ) + timeout while proc . poll ( ) is None and time . time ( ) < timeout_time : time . sleep ( 0.02 ) if proc . poll ( ) is None : <MASK> log . info ( "" Sending SIGKILL to  %r "" , proc ) proc . kill ( ) return proc . returncode",if log :,if log :,100,TRUE,100
3755,"def mkpanel ( color , rows , cols , tly , tlx ) : win = curses . newwin ( rows , cols , tly , tlx ) pan = panel . new_panel ( win ) if curses . has_colors ( ) : <MASK> fg = curses . COLOR_WHITE else : fg = curses . COLOR_BLACK bg = color curses . init_pair ( color , fg , bg ) win . bkgdset ( ord ( ""   "" ) , curses . color_pair ( color ) ) else : win . bkgdset ( ord ( ""   "" ) , curses . A_BOLD ) return pan",if color == curses . COLOR_BLUE :,"if color . startswith ( ""rgb"" ) :",83.82273536,FALSE,93.41
3756,"def all_words ( filename ) : start_char = True for c in characters ( filename ) : if start_char == True : word = "" "" <MASK> # We found the start of a word word = c . lower ( ) start_char = False else : pass else : <MASK> word + = c . lower ( ) else : # We found end of word, emit it start_char = True yield word",if c . isalnum ( ) :,if c . isupper ( ) :,97.0609056,FALSE,94.76
3757,"def get_tf_weights_as_numpy ( path = "" ./ckpt/aeslc/model.ckpt-32000 "" ) - > Dict : init_vars = tf . train . list_variables ( path ) tf_weights = { } ignore_name = [ "" Adafactor "" , "" global_step "" ] for name , shape in tqdm ( init_vars , desc = "" converting tf checkpoint to dict "" ) : skip_key = any ( [ pat in name for pat in ignore_name ] ) <MASK> continue array = tf . train . load_variable ( path , name ) tf_weights [ name ] = array return tf_weights",if skip_key :,if skip_key :,100,TRUE,100
3758,"def app ( scope , receive , send ) : while True : message = await receive ( ) if message [ "" type "" ] == "" websocket.connect "" : await send ( { "" type "" : "" websocket.accept "" } ) <MASK> pass elif message [ "" type "" ] == "" websocket.disconnect "" : break","elif message [ ""type"" ] == ""websocket.receive"" :","elif message [ ""type"" ] == ""websocket.disconnect"" :",98.07273752,FALSE,96.82
3759,"def autoload ( self ) : if self . _app . config . THEME == "" auto "" : if sys . platform == "" darwin "" : <MASK> theme = DARK else : theme = LIGHT else : theme = self . guess_system_theme ( ) if theme == Dark : theme = MacOSDark else : # user settings have highest priority theme = self . _app . config . THEME self . load_theme ( theme )",if get_osx_theme ( ) == 1 :,if self . _app . config . USE_DARK :,93.10803833,FALSE,88.67
3760,"def example_reading_spec ( self ) : data_fields = { "" targets "" : tf . VarLenFeature ( tf . int64 ) } <MASK> data_fields [ "" inputs "" ] = tf . VarLenFeature ( tf . int64 ) if self . packed_length : <MASK> data_fields [ "" inputs_segmentation "" ] = tf . VarLenFeature ( tf . int64 ) data_fields [ "" inputs_position "" ] = tf . VarLenFeature ( tf . int64 ) data_fields [ "" targets_segmentation "" ] = tf . VarLenFeature ( tf . int64 ) data_fields [ "" targets_position "" ] = tf . VarLenFeature ( tf . int64 ) data_items_to_decoders = None return ( data_fields , data_items_to_decoders )",if self . has_inputs :,if self . packed_length == 0 :,95.20743606,FALSE,91.42
3761,"def _prepare_travel_graph ( self ) : for op in self . op_dict . values ( ) : op . const = False if op . node . op in [ "" Const "" , "" Placeholder "" ] : op . resolved = True <MASK> op . const = True else : op . resolved = False","if op . node . op == ""Const"" :","elif op . node . op in [ ""Const"" , ""Conv"" , ""Conv",71.01485409,FALSE,81.58
3762,"def get_filestream_file_items ( self ) : data = { } fs_file_updates = self . get_filestream_file_updates ( ) for k , v in six . iteritems ( fs_file_updates ) : l = [ ] for d in v : offset = d . get ( "" offset "" ) content = d . get ( "" content "" ) assert offset is not None assert content is not None assert offset == 0 or offset == len ( l ) , ( k , v , l , d ) <MASK> l = [ ] l . extend ( map ( json . loads , content ) ) data [ k ] = l return data",if not offset :,if len ( l ) == 0 :,95.46650066,FALSE,94.07
3763,"def _rewrite_exprs ( self , table , what ) : from ibis . expr . analysis import substitute_parents what = util . promote_list ( what ) all_exprs = [ ] for expr in what : <MASK> all_exprs . extend ( expr . exprs ( ) ) else : bound_expr = ir . bind_expr ( table , expr ) all_exprs . append ( bound_expr ) return [ substitute_parents ( x , past_projection = False ) for x in all_exprs ]","if isinstance ( expr , ir . ExprList ) :","if isinstance ( expr , Expr ) :",93.94912126,FALSE,95.96
3764,"def _group_by_commit_and_time ( self , hits ) : result = { } for hit in hits : source_hit = hit [ "" _source "" ] key = "" %s _ %s "" % ( source_hit [ "" commit_info "" ] [ "" id "" ] , source_hit [ "" datetime "" ] ) benchmark = self . _benchmark_from_es_record ( source_hit ) <MASK> result [ key ] [ "" benchmarks "" ] . append ( benchmark ) else : run_info = self . _run_info_from_es_record ( source_hit ) run_info [ "" benchmarks "" ] = [ benchmark ] result [ key ] = run_info return result",if key in result :,if benchmark in result :,94.54371137,FALSE,98.35
3765,"def _build_index ( self ) : self . _index = { } for start_char , sorted_offsets in self . _offsets . items ( ) : self . _index [ start_char ] = { } for i , offset in enumerate ( sorted_offsets . get_offsets ( ) ) : identifier = sorted_offsets . get_identifier_by_offset ( offset ) <MASK> self . _index [ start_char ] [ identifier [ 0 : self . index_depth ] ] = i",if identifier [ 0 : self . index_depth ] not in self . _index [ start_char ] :,if identifier :,86.94617104,FALSE,81.6
3766,"def scan_resource_conf ( self , conf ) : if "" properties "" in conf : <MASK> if "" exp "" in conf [ "" properties "" ] [ "" attributes "" ] : if conf [ "" properties "" ] [ "" attributes "" ] [ "" exp "" ] : return CheckResult . PASSED return CheckResult . FAILED","if ""attributes"" in conf [ ""properties"" ] :","if ""attributes"" in conf [ ""properties"" ] :",75,TRUE,100
3767,"def _PatchArtifact ( self , artifact : rdf_artifacts . Artifact ) - > rdf_artifacts . Artifact : """"""Patches artifact to not contain byte-string source attributes."""""" patched = False for source in artifact . sources : attributes = source . attributes . ToDict ( ) unicode_attributes = compatibility . UnicodeJson ( attributes ) <MASK> source . attributes = unicode_attributes patched = True if patched : self . DeleteArtifact ( str ( artifact . name ) ) self . WriteArtifact ( artifact ) return artifact",if attributes != unicode_attributes :,"if unicode_attributes . lower ( ) == ""unicode"" :",90.81160641,FALSE,89.31
3768,"def edit_file ( self , filename ) : import subprocess editor = self . get_editor ( ) if self . env : environ = os . environ . copy ( ) environ . update ( self . env ) else : environ = None try : c = subprocess . Popen ( ' %s   "" %s "" ' % ( editor , filename ) , env = environ , shell = True ) exit_code = c . wait ( ) <MASK> raise ClickException ( "" %s : Editing failed! "" % editor ) except OSError as e : raise ClickException ( "" %s : Editing failed:  %s "" % ( editor , e ) )",if exit_code != 0 :,if exit_code != 0 :,100,TRUE,100
3769,"def findControlPointsInMesh ( glyph , va , subsegments ) : controlPointIndices = np . zeros ( ( len ( va ) , 1 ) ) index = 0 for i , c in enumerate ( subsegments ) : segmentCount = len ( glyph . contours [ i ] . segments ) - 1 for j , s in enumerate ( c ) : <MASK> if glyph . contours [ i ] . segments [ j ] . type == "" line "" : controlPointIndices [ index ] = 1 index + = s [ 1 ] return controlPointIndices",if j < segmentCount :,if j < segmentCount :,100,TRUE,100
3770,"def to_representation ( self , value ) : old_social_string_fields = [ "" twitter "" , "" github "" , "" linkedIn "" ] request = self . context . get ( "" request "" ) show_old_format = ( request and is_deprecated ( request . version , self . min_version ) and request . method == "" GET "" ) if show_old_format : social = value . copy ( ) for key in old_social_string_fields : <MASK> social [ key ] = value [ key ] [ 0 ] elif social . get ( key ) == [ ] : social [ key ] = "" "" value = social return super ( SocialField , self ) . to_representation ( value )",if social . get ( key ) :,if key in value :,93.6297409,FALSE,95.33
3771,"def iter_raw_frames ( path , packet_sizes , ctx ) : with open ( path , "" rb "" ) as f : for i , size in enumerate ( packet_sizes ) : packet = Packet ( size ) read_size = f . readinto ( packet ) assert size assert read_size == size if not read_size : break for frame in ctx . decode ( packet ) : yield frame while True : try : frames = ctx . decode ( None ) except EOFError : break for frame in frames : yield frame <MASK> break",if not frames :,if frame is None :,96.706993,FALSE,96.21
3772,"def get_shadows_zip ( filename ) : import zipfile shadow_pkgs = set ( ) with zipfile . ZipFile ( filename ) as lib_zip : already_test = [ ] for fname in lib_zip . namelist ( ) : pname , fname = os . path . split ( fname ) if fname or ( pname and fname ) : continue if pname not in already_test and "" / "" not in pname : already_test . append ( pname ) <MASK> shadow_pkgs . add ( pname ) return shadow_pkgs",if is_shadowing ( pname ) :,if pname not in shadow_pkgs :,95.21362384,FALSE,93.69
3773,"def metrics_to_scalars ( self , metrics ) : new_metrics = { } for k , v in metrics . items ( ) : if isinstance ( v , torch . Tensor ) : v = v . item ( ) <MASK> v = self . metrics_to_scalars ( v ) new_metrics [ k ] = v return new_metrics","if isinstance ( v , dict ) :","elif isinstance ( v , dict ) :",93.17387805,FALSE,96.7
3774,"def insert_resets ( f ) : newsync = dict ( ) for k , v in f . sync . items ( ) : <MASK> newsync [ k ] = insert_reset ( ResetSignal ( k ) , v ) else : newsync [ k ] = v f . sync = newsync",if f . clock_domains [ k ] . rst is not None :,"if isinstance ( v , ResetSignal ) :",76.88593525,FALSE,78.52
3775,"def get_attached_nodes ( self , external_account ) : for node in self . get_nodes_with_oauth_grants ( external_account ) : if node is None : continue node_settings = node . get_addon ( self . oauth_provider . short_name ) <MASK> continue if node_settings . external_account == external_account : yield node",if node_settings is None :,if not node_settings :,93.45062879,FALSE,93.87
3776,"def visitIf ( self , node , scope ) : for test , body in node . tests : if isinstance ( test , ast . Const ) : if type ( test . value ) in self . _const_types : <MASK> continue self . visit ( test , scope ) self . visit ( body , scope ) if node . else_ : self . visit ( node . else_ , scope )",if not test . value :,if test . value in self . _const_types :,88.56324978,FALSE,88.63
3777,"def flatten ( self ) : # this is similar to fill_messages except it uses a list instead # of a queue to place the messages in. result = [ ] channel = await self . messageable . _get_channel ( ) self . channel = channel while self . _get_retrieve ( ) : data = await self . _retrieve_messages ( self . retrieve ) if len ( data ) < 100 : self . limit = 0 # terminate the infinite loop <MASK> data = reversed ( data ) if self . _filter : data = filter ( self . _filter , data ) for element in data : result . append ( self . state . create_message ( channel = channel , data = element ) ) return result",if self . reverse :,if self . reverse :,75,TRUE,100
3778,"def compute ( self , x , y = None , targets = None ) : if targets is None : targets = self . out_params in_params = list ( self . in_x ) if len ( in_params ) == 1 : args = [ x ] else : args = list ( zip ( * x ) ) if y is None : pipe = self . pipe else : pipe = self . train_pipe <MASK> args . append ( y ) else : args + = list ( zip ( * y ) ) in_params + = self . in_y return self . _compute ( * args , pipe = pipe , param_names = in_params , targets = targets )",if len ( self . in_y ) == 1 :,if len ( in_params ) == 1 :,92.98578918,FALSE,96.22
3779,"def _import_top_module ( self , name ) : # scan sys.path looking for a location in the filesystem that contains # the module, or an Importer object that can import the module. for item in sys . path : <MASK> module = self . fs_imp . import_from_dir ( item , name ) else : module = item . import_top ( name ) if module : return module return None","if isinstance ( item , _StringType ) :",if os . path . isdir ( item ) :,70.59433292,FALSE,91.66
3780,"def __getitem__ ( self , key , _get_mode = False ) : if not _get_mode : if isinstance ( key , ( int , long ) ) : return self . _list [ key ] elif isinstance ( key , slice ) : return self . __class__ ( self . _list [ key ] ) ikey = key . lower ( ) for k , v in self . _list : <MASK> return v # micro optimization: if we are in get mode we will catch that # exception one stack level down so we can raise a standard # key error instead of our special one. if _get_mode : raise KeyError ( ) raise BadRequestKeyError ( key )",if k . lower ( ) == ikey :,if k . lower ( ) == ikey :,100,TRUE,100
3781,"def execute ( self , arbiter , props ) : watcher = self . _get_watcher ( arbiter , props . pop ( "" name "" ) ) action = 0 for key , val in props . get ( "" options "" , { } ) . items ( ) : if key == "" hooks "" : new_action = 0 for name , _val in val . items ( ) : action = watcher . set_opt ( "" hooks. %s "" % name , _val ) <MASK> new_action = 1 else : new_action = watcher . set_opt ( key , val ) if new_action == 1 : action = 1 # trigger needed action return watcher . do_action ( action )",if action == 1 :,"elif key == ""default"" :",90.6086197,FALSE,94.83
3782,"def OnBodyClick ( self , event = None ) : try : c = self . c p = c . currentPosition ( ) <MASK> self . OnActivateBody ( event = event ) g . doHook ( "" bodyclick2 "" , c = c , p = p , v = p , event = event ) except : g . es_event_exception ( "" bodyclick "" )","if not g . doHook ( ""bodyclick1"" , c = c , p = p , v = p , event = event ) :","if g . doHook ( ""bodyclick1"" , c = c , p = p",85.41197943,FALSE,84.8
3783,"def _class_weights ( spec : config . MetricsSpec ) - > Optional [ Dict [ int , float ] ] : """"""Returns class weights associated with AggregationOptions at offset."""""" if spec . aggregate . HasField ( "" top_k_list "" ) : <MASK> raise ValueError ( "" class_weights are not supported when top_k_list used:  "" "" spec= {} "" . format ( spec ) ) return None return dict ( spec . aggregate . class_weights ) or None",if spec . aggregate . class_weights :,"if spec . aggregate . GetField ( ""class_weights"" ) is None :",89.7340909,FALSE,91.3
3784,"def _is_perf_file ( file_path ) : f = get_file ( file_path ) for line in f : if line [ 0 ] == "" # "" : continue r = event_regexp . search ( line ) <MASK> f . close ( ) return True f . close ( ) return False",if r :,if r :,100,TRUE,100
3785,"def _get_before_insertion_node ( self ) : if self . _nodes_stack . is_empty ( ) : return None line = self . _nodes_stack . parsed_until_line + 1 node = self . _new_module . get_last_leaf ( ) while True : parent = node . parent <MASK> assert node . end_pos [ 0 ] < = line assert node . end_pos [ 1 ] == 0 or "" \n "" in self . _prefix return node node = parent","if parent . type in ( ""suite"" , ""file_input"" ) :",if parent is self . _new_module :,90.5311159,FALSE,87.74
3786,"def PyJsHoisted_parseClassRanges_ ( this , arguments , var = var ) : var = Scope ( { u "" this "" : this , u "" arguments "" : arguments } , var ) var . registers ( [ u "" res "" ] ) pass if var . get ( u "" current "" ) ( Js ( u "" ] "" ) ) : return Js ( [ ] ) else : var . put ( u "" res "" , var . get ( u "" parseNonemptyClassRanges "" ) ( ) ) <MASK> var . get ( u "" bail "" ) ( Js ( u "" nonEmptyClassRanges "" ) ) return var . get ( u "" res "" )","if var . get ( u""res"" ) . neg ( ) :","if var . get ( u""bail"" ) :",86.30250565,FALSE,94.96
3787,"def _recurse_children ( self , offset ) : """"""Recurses thorugh the available children"""""" while offset < self . obj_offset + self . Length : item = obj . Object ( "" VerStruct "" , offset = offset , vm = self . obj_vm , parent = self ) <MASK> raise StopIteration ( "" Could not recover a key for a child at offset  {0} "" . format ( item . obj_offset ) ) yield item . get_key ( ) , item . get_children ( ) offset = self . offset_pad ( offset + item . Length ) raise StopIteration ( "" No children "" )",if item . Length < 1 or item . get_key ( ) == None :,if not item . get_key ( ) :,90.22197627,FALSE,91.52
3788,"def _adapt_types ( self , descr ) : names = [ ] adapted_types = [ ] for col in descr : names . append ( col [ 0 ] ) impala_typename = col [ 1 ] typename = udf . _impala_to_ibis_type [ impala_typename . lower ( ) ] <MASK> precision , scale = col [ 4 : 6 ] adapted_types . append ( dt . Decimal ( precision , scale ) ) else : adapted_types . append ( typename ) return names , adapted_types","if typename == ""decimal"" :",if len ( col ) > 4 :,92.24958645,FALSE,93.26
3789,"def sniff ( self , filename ) : try : <MASK> with tarfile . open ( filename , "" r "" ) as temptar : for f in temptar : if not f . isfile ( ) : continue if f . name . endswith ( "" .fast5 "" ) : return True else : return False except Exception as e : log . warning ( "" %s , sniff Exception:  %s "" , self , e ) return False",if filename and tarfile . is_tarfile ( filename ) :,if os . path . exists ( filename ) :,86.52926196,FALSE,91.15
3790,"def getValue ( self ) : if getattr ( self . object , "" type "" , "" "" ) != "" CURVE "" : return BezierSpline ( ) evaluatedObject = getEvaluatedID ( self . object ) bSplines = evaluatedObject . data . splines if len ( bSplines ) > 0 : spline = createSplineFromBlenderSpline ( bSplines [ 0 ] ) # Is None when the spline type is not supported. if spline is not None : <MASK> spline . transform ( evaluatedObject . matrix_world ) return spline return BezierSpline ( )",if self . useWorldSpace :,if evaluatedObject . matrix_world is not None :,97.04729335,FALSE,91.03
3791,"def escape ( text , newline = False ) : """"""Escape special html characters."""""" if isinstance ( text , str ) : if "" & "" in text : text = text . replace ( "" & "" , "" &amp; "" ) <MASK> text = text . replace ( "" > "" , "" &gt; "" ) if "" < "" in text : text = text . replace ( "" < "" , "" &lt; "" ) if ' "" ' in text : text = text . replace ( ' "" ' , "" &quot; "" ) if "" ' "" in text : text = text . replace ( "" ' "" , "" &quot; "" ) if newline : if "" \n "" in text : text = text . replace ( "" \n "" , "" <br> "" ) return text","if "">"" in text :","if "">"" in text :",100,TRUE,100
3792,"def _get_ilo_version ( self ) : try : self . _get_ilo2 ( ' <?xml version= "" 1.0 "" ?><RIBCL VERSION= "" 2.0 "" ></RIBCL> ' ) except ResponseError as e : if hasattr ( e , "" code "" ) : <MASK> return 3 if e . code == 501 : return 1 raise return 2",if e . code == 405 :,if e . code == 3 :,97.94705893,FALSE,96.9
3793,"def convert_path ( ctx , tpath ) : for points , code in tpath . iter_segments ( ) : if code == Path . MOVETO : ctx . move_to ( * points ) elif code == Path . LINETO : ctx . line_to ( * points ) elif code == Path . CURVE3 : ctx . curve_to ( points [ 0 ] , points [ 1 ] , points [ 0 ] , points [ 1 ] , points [ 2 ] , points [ 3 ] ) elif code == Path . CURVE4 : ctx . curve_to ( * points ) <MASK> ctx . close_path ( )",elif code == Path . CLOSEPOLY :,elif code == Path . CLOSE :,73.89304233,FALSE,98.09
3794,"def called_by_shrinker ( ) : frame = sys . _getframe ( 0 ) while frame : fname = frame . f_globals . get ( "" __file__ "" , "" "" ) <MASK> return True frame = frame . f_back return False","if os . path . basename ( fname ) == ""shrinker.py"" :",if os . path . isfile ( fname ) and os . path . splitext ( fname ) [,68.45782294,FALSE,80.34
3795,"def _ensuresyspath ( self , ensuremode , path ) : if ensuremode : s = str ( path ) if ensuremode == "" append "" : <MASK> sys . path . append ( s ) else : if s != sys . path [ 0 ] : sys . path . insert ( 0 , s )",if s not in sys . path :,if s not in sys . path :,100,TRUE,100
3796,"def get_instances ( self , region : str , vpc : str ) : try : await self . _cache_instances ( region ) return [ instance for instance in self . _instances_cache [ region ] <MASK> ] except Exception as e : print_exception ( f "" Failed to get RDS instances:  { e } "" ) return [ ]","if instance [ ""VpcId"" ] == vpc",if instance . vpc == vpc,90.8283468,FALSE,91.58
3797,def get_and_set_all_disambiguation ( self ) : all_disambiguations = [ ] for page in self . pages : if page . relations . disambiguation_links_norm is not None : all_disambiguations . extend ( page . relations . disambiguation_links_norm ) <MASK> all_disambiguations . extend ( page . relations . disambiguation_links ) return set ( all_disambiguations ),if page . relations . disambiguation_links is not None :,if page . relations . disambiguation_links is not None :,100,TRUE,100
3798,"def __str__ ( self , prefix = "" "" , printElemNumber = 0 ) : res = "" "" cnt = 0 for e in self . options_ : elm = "" "" <MASK> elm = "" ( %d ) "" % cnt res + = prefix + ( "" options %s  < \n "" % elm ) res + = e . __str__ ( prefix + ""    "" , printElemNumber ) res + = prefix + "" > \n "" cnt + = 1 return res",if printElemNumber :,if printElemNumber :,100,TRUE,100
3799,"def pre_save_task ( self , task , credentials , verrors ) : if task [ "" attributes "" ] [ "" encryption "" ] not in ( None , "" "" , "" AES256 "" ) : verrors . add ( "" encryption "" , ' Encryption should be null or  "" AES256 "" ' ) if not credentials [ "" attributes "" ] . get ( "" skip_region "" , False ) : <MASK> response = await self . middleware . run_in_thread ( self . _get_client ( credentials ) . get_bucket_location , Bucket = task [ "" attributes "" ] [ "" bucket "" ] , ) task [ "" attributes "" ] [ "" region "" ] = response [ "" LocationConstraint "" ] or "" us-east-1 ""","if not credentials [ ""attributes"" ] . get ( ""region"" , """" ) . strip ( ) :","if task [ ""attributes"" ] [ ""bucket"" ] :",86.78570179,FALSE,88.81
3800,"def get_best_config_reward ( self ) : """"""Returns the best configuration found so far, as well as the reward associated with this best config."""""" with self . LOCK : <MASK> config_pkl = max ( self . _results , key = self . _results . get ) return pickle . loads ( config_pkl ) , self . _results [ config_pkl ] else : return dict ( ) , self . _reward_while_pending ( )",if self . _results :,if self . _results :,100,TRUE,100
3801,"def parse_setup_cfg ( self ) : # type: () -> Dict[STRING_TYPE, Any] if self . setup_cfg is not None and self . setup_cfg . exists ( ) : contents = self . setup_cfg . read_text ( ) base_dir = self . setup_cfg . absolute ( ) . parent . as_posix ( ) try : parsed = setuptools_parse_setup_cfg ( self . setup_cfg . as_posix ( ) ) except Exception : <MASK> contents = self . setup_cfg . read_bytes ( ) parsed = parse_setup_cfg ( contents , base_dir ) if not parsed : return { } return parsed return { }",if six . PY2 :,if self . setup_cfg . exists ( ) :,72.02606367,FALSE,93.35
3802,"def readall ( read_fn , sz ) : buff = b "" "" have = 0 while have < sz : chunk = yield from read_fn ( sz - have ) have + = len ( chunk ) buff + = chunk <MASK> raise TTransportException ( TTransportException . END_OF_FILE , "" End of file reading from transport "" ) return buff",if len ( chunk ) == 0 :,if len ( chunk ) == 0 :,100,TRUE,100
3803,"def _get_use_previous ( f , ) : # TODO Sort and group features for DateOffset with two different temporal values if isinstance ( f , AggregationFeature ) and f . use_previous is not None : <MASK> return ( "" "" , - 1 ) else : unit = list ( f . use_previous . times . keys ( ) ) [ 0 ] value = f . use_previous . times [ unit ] return ( unit , value ) else : return ( "" "" , - 1 )",if len ( f . use_previous . times . keys ( ) ) > 1 :,if f . use_previous . times is None :,69.2760429,FALSE,89.47
3804,"def istrue ( self ) : try : return self . _istrue ( ) except Exception : self . exc = sys . exc_info ( ) <MASK> msg = [ ""   "" * ( self . exc [ 1 ] . offset + 4 ) + "" ^ "" , ] msg . append ( "" SyntaxError: invalid syntax "" ) else : msg = traceback . format_exception_only ( * self . exc [ : 2 ] ) pytest . fail ( "" Error evaluating  %r  expression \n "" ""      %s \n "" "" %s "" % ( self . name , self . expr , "" \n "" . join ( msg ) ) , pytrace = False , )","if isinstance ( self . exc [ 1 ] , SyntaxError ) :",if len ( self . exc ) == 2 :,88.69452778,FALSE,93.27
3805,"def wait_for_crm_operation ( operation , crm ) : """"""Poll for cloud resource manager operation until finished."""""" logger . info ( "" wait_for_crm_operation:  "" "" Waiting for operation  {}  to finish... "" . format ( operation ) ) for _ in range ( MAX_POLLS ) : result = crm . operations ( ) . get ( name = operation [ "" name "" ] ) . execute ( ) <MASK> raise Exception ( result [ "" error "" ] ) if "" done "" in result and result [ "" done "" ] : logger . info ( "" wait_for_crm_operation: Operation done. "" ) break time . sleep ( POLL_INTERVAL ) return result","if ""error"" in result :","if ""error"" in result :",100,TRUE,100
3806,"def cb_blob_detail_from_elem_and_buf ( self , elem , buf ) : if elem . get ( "" lang "" ) != buf . lang : # multi-lang doc return "" %s  Code in  %s "" % ( elem . get ( "" lang "" ) , buf . path ) else : dir , base = os . path . split ( buf . path ) <MASK> return "" %s  ( %s ) "" % ( base , dir ) else : return base",if dir :,if dir :,100,TRUE,100
3807,"def removedir ( self , path ) : # type: (Text) -> None _path = self . validatepath ( path ) if _path == "" / "" : raise errors . RemoveRootError ( ) with ftp_errors ( self , path ) : try : self . ftp . rmd ( _encode ( _path , self . ftp . encoding ) ) except error_perm as error : code , _ = _parse_ftp_error ( error ) <MASK> if self . isfile ( path ) : raise errors . DirectoryExpected ( path ) if not self . isempty ( path ) : raise errors . DirectoryNotEmpty ( path ) raise # pragma: no cover","if code == ""550"" :",if code == 404 :,73.90948588,FALSE,96.67
3808,"def p_clause ( self , node , position ) : if isinstance ( node , Graph ) : self . subjectDone ( node ) <MASK> self . write ( ""   "" ) self . write ( "" { "" ) self . depth + = 1 serializer = N3Serializer ( node , parent = self ) serializer . serialize ( self . stream ) self . depth - = 1 self . write ( self . indent ( ) + "" } "" ) return True else : return False",if position is OBJECT :,"if position == ""equal"" :",68.15490509,FALSE,93.31
3809,"def get_default_shell_info ( shell_name = None , settings = None ) : if not shell_name : settings = settings or load_settings ( lazy = True ) shell_name = settings . get ( "" shell "" ) if shell_name : return shell_name , None shell_path = os . environ . get ( "" SHELL "" ) <MASK> shell_name = basepath ( shell_path ) else : shell_name = DEFAULT_SHELL return shell_name , shell_path return shell_name , None",if shell_path :,if shell_path :,100,TRUE,100
3810,"def GetCategory ( self , pidls ) : ret = [ ] for pidl in pidls : # Why don't we just get the size of the PIDL? val = self . sf . GetDetailsEx ( pidl , PKEY_Sample_AreaSize ) val = int ( val ) # it probably came in a VT_BSTR variant if val < 255 / / 3 : cid = IDS_SMALL <MASK> cid = IDS_MEDIUM else : cid = IDS_LARGE ret . append ( cid ) return ret",elif val < 2 * 255 // 3 :,elif val > 126 / 3 :,96.40607593,FALSE,93.64
3811,"def Tokenize ( s ) : # type: (str) -> Iterator[Token] for item in TOKEN_RE . findall ( s ) : # The type checker can't know the true type of item! item = cast ( TupleStr4 , item ) if item [ 0 ] : typ = "" number "" val = item [ 0 ] elif item [ 1 ] : typ = "" name "" val = item [ 1 ] elif item [ 2 ] : typ = item [ 2 ] val = item [ 2 ] <MASK> typ = item [ 3 ] val = item [ 3 ] yield Token ( typ , val )",elif item [ 3 ] :,elif item [ 3 ] :,75,TRUE,100
3812,"def add_package_declarations ( generated_root_path ) : file_names = os . listdir ( generated_root_path ) for file_name in file_names : <MASK> continue full_name = os . path . join ( generated_root_path , file_name ) add_package ( full_name )","if not file_name . endswith ( "".java"" ) :","if not file_name . endswith ( "".py"" ) :",97.11816045,FALSE,96.78
3813,"def _call_with_retry ( out , retry , retry_wait , method , * args , * * kwargs ) : for counter in range ( retry + 1 ) : try : return method ( * args , * * kwargs ) except ( NotFoundException , ForbiddenException , AuthenticationException , RequestErrorException , ) : raise except ConanException as exc : <MASK> raise else : if out : out . error ( exc ) out . info ( "" Waiting  %d  seconds to retry... "" % retry_wait ) time . sleep ( retry_wait )",if counter == retry :,if counter == retry_wait :,94.27697516,FALSE,96.87
3814,"def to_wburl_str ( url , type = BaseWbUrl . LATEST_REPLAY , mod = "" "" , timestamp = "" "" , end_timestamp = "" "" ) : if WbUrl . is_query_type ( type ) : tsmod = "" "" <MASK> tsmod + = mod + "" / "" tsmod + = timestamp tsmod + = "" * "" tsmod + = end_timestamp tsmod + = "" / "" + url if type == BaseWbUrl . URL_QUERY : tsmod + = "" * "" return tsmod else : tsmod = timestamp + mod if len ( tsmod ) > 0 : return tsmod + "" / "" + url else : return url",if mod :,if type == BaseWbUrl . URL_MOD :,95.88332114,FALSE,93.03
3815,"def _configured_ploidy ( items ) : ploidies = collections . defaultdict ( set ) for data in items : ploidy = dd . get_ploidy ( data ) <MASK> for k , v in ploidy . items ( ) : ploidies [ k ] . add ( v ) else : ploidies [ "" default "" ] . add ( ploidy ) out = { } for k , vs in ploidies . items ( ) : assert len ( vs ) == 1 , "" Multiple ploidies set for group calling:  %s   %s "" % ( k , list ( vs ) , ) out [ k ] = vs . pop ( ) return out","if isinstance ( ploidy , dict ) :",if ploidy :,81.14323331,FALSE,94.4
3816,"def removeUser ( self , username ) : hideFromOSD = not constants . SHOW_DIFFERENT_ROOM_OSD if username in self . _users : user = self . _users [ username ] <MASK> if self . isRoomSame ( user . room ) : hideFromOSD = not constants . SHOW_SAME_ROOM_OSD if username in self . _users : self . _users . pop ( username ) message = getMessage ( "" left-notification "" ) . format ( username ) self . ui . showMessage ( message , hideFromOSD ) self . _client . lastLeftTime = time . time ( ) self . _client . lastLeftUser = username self . userListChange ( )",if user . room :,if user . isAlive ( ) :,96.55355737,FALSE,96.51
3817,"def _thd_cleanup_instance ( self ) : container_name = self . getContainerName ( ) instances = self . client . containers ( all = 1 , filters = dict ( name = container_name ) ) for instance in instances : # hyper filtering will match 'hyper12"" if you search for 'hyper1' ! <MASK> continue try : self . client . remove_container ( instance [ "" Id "" ] , v = True , force = True ) except NotFound : pass # that's a race condition except docker . errors . APIError as e : if "" Conflict operation on container "" not in str ( e ) : raise","if """" . join ( instance [ ""Names"" ] ) . strip ( ""/"" ) != container_name :","if ""hyper1"" in instance [ ""Tags"" ] :",93.52379408,FALSE,84.37
3818,"def handle_ctcp ( self , conn , evt ) : args = evt . arguments ( ) source = evt . source ( ) . split ( "" ! "" ) [ 0 ] if args : if args [ 0 ] == "" VERSION "" : conn . ctcp_reply ( source , "" VERSION  "" + BOT_VERSION ) <MASK> conn . ctcp_reply ( source , "" PING "" ) elif args [ 0 ] == "" CLIENTINFO "" : conn . ctcp_reply ( source , "" CLIENTINFO PING VERSION CLIENTINFO "" )","elif args [ 0 ] == ""PING"" :","elif args [ 0 ] == ""PING"" :",100,TRUE,100
3819,"def new_func ( self , * args , * * kwargs ) : obj = self . obj_ref ( ) attr = self . attr if obj is not None : args = tuple ( TrackedValue . make ( obj , attr , arg ) for arg in args ) <MASK> kwargs = { key : TrackedValue . make ( obj , attr , value ) for key , value in iteritems ( kwargs ) } result = func ( self , * args , * * kwargs ) self . _changed_ ( ) return result",if kwargs :,if kwargs is not None :,89.69070823,FALSE,95.72
3820,"def add_doc ( target , variables , body_lines ) : if isinstance ( target , ast . Name ) : # if it is a variable name add it to the doc name = target . id <MASK> doc = find_doc_for ( target , body_lines ) if doc is not None : variables [ name ] = doc elif isinstance ( target , ast . Tuple ) : # if it is a tuple then iterate the elements # this can happen like this: # a, b = 1, 2 for e in target . elts : add_doc ( e , variables , body_lines )",if name not in variables :,if name not in variables :,100,TRUE,100
3821,"def _terminal_messenger ( tp = "" write "" , msg = "" "" , out = sys . stdout ) : try : if tp == "" write "" : out . write ( msg ) elif tp == "" flush "" : out . flush ( ) <MASK> out . write ( msg ) out . flush ( ) elif tp == "" print "" : print ( msg , file = out ) else : raise ValueError ( "" Unsupported type:  "" + tp ) except IOError as e : logger . critical ( "" {} :  {} "" . format ( type ( e ) . __name__ , ucd ( e ) ) ) pass","elif tp == ""write_flush"" :","elif tp == ""write"" :",98.96936433,FALSE,97.47
3822,"def get_files ( d ) : res = [ ] for p in glob . glob ( os . path . join ( d , "" * "" ) ) : if not p : continue ( pth , fname ) = os . path . split ( p ) <MASK> continue if os . path . islink ( p ) : continue if os . path . isdir ( p ) : res + = get_dir ( p ) else : res . append ( p ) return res",if skip_file ( fname ) :,"if pth [ 0 ] != ""py"" :",95.66480436,FALSE,89.73
3823,"def _list_outputs ( self ) : outputs = super ( VolSymm , self ) . _list_outputs ( ) # Have to manually check for the grid files. if os . path . exists ( outputs [ "" trans_file "" ] ) : <MASK> outputs [ "" output_grid "" ] = re . sub ( "" .(nlxfm|xfm)$ "" , "" _grid_0.mnc "" , outputs [ "" trans_file "" ] ) return outputs","if ""grid"" in open ( outputs [ ""trans_file"" ] , ""r"" ) . read ( ) :","if ""output_grid"" not in outputs :",90.65898715,FALSE,80.59
3824,"def _set_texture ( self , texture ) : if texture . id is not self . _texture . id : self . _group = SpriteGroup ( texture , self . _group . blend_src , self . _group . blend_dest , self . _group . parent ) <MASK> self . _vertex_list . tex_coords [ : ] = texture . tex_coords else : self . _vertex_list . delete ( ) self . _texture = texture self . _create_vertex_list ( ) else : self . _vertex_list . tex_coords [ : ] = texture . tex_coords self . _texture = texture",if self . _batch is None :,if self . _vertex_list . is_empty ( ) :,72.42309819,FALSE,92.89
3825,"def got_result ( result ) : deployment = self . persistence_service . get ( ) for node in deployment . nodes : <MASK> dataset_ids = [ ( m . dataset . deleted , m . dataset . dataset_id ) for m in node . manifestations . values ( ) ] self . assertIn ( ( True , expected_dataset_id ) , dataset_ids ) break else : self . fail ( "" Node not found.  {} "" . format ( node . uuid ) )","if same_node ( node , origin ) :",if node . uuid == result . uuid :,89.45762253,FALSE,91.29
3826,"def check_result ( result , func , arguments ) : if check_warning ( result ) and ( result . value != ReturnCode . WARN_NODATA ) : log . warning ( UcanWarning ( result , func , arguments ) ) elif check_error ( result ) : <MASK> raise UcanCmdError ( result , func , arguments ) else : raise UcanError ( result , func , arguments ) return result",if check_error_cmd ( result ) :,if result . value == ReturnCode . COMMAND_NODATA :,89.8927468,FALSE,87.22
3827,"def _compress_and_sort_bdg_files ( out_dir , data ) : for fn in glob . glob ( os . path . join ( out_dir , "" *bdg "" ) ) : out_file = fn + "" .gz "" <MASK> continue bedtools = config_utils . get_program ( "" bedtools "" , data ) with file_transaction ( out_file ) as tx_out_file : cmd = f "" sort -k1,1 -k2,2n  { fn }  | bgzip -c >  { tx_out_file } "" message = f "" Compressing and sorting  { fn } . "" do . run ( cmd , message )",if utils . file_exists ( out_file ) :,if not os . path . exists ( out_file ) :,92.63736903,FALSE,95.5
3828,"def kill_members ( members , sig , hosts = nodes ) : for member in sorted ( members ) : try : if ha_tools_debug : print ( "" killing  %s "" % member ) proc = hosts [ member ] [ "" proc "" ] # Not sure if cygwin makes sense here... <MASK> os . kill ( proc . pid , signal . CTRL_C_EVENT ) else : os . kill ( proc . pid , sig ) except OSError : if ha_tools_debug : print ( "" %s  already dead? "" % member )","if sys . platform in ( ""win32"" , ""cygwin"" ) :",if proc . is_win :,94.93952638,FALSE,88.6
3829,"def get_top_level_stats ( self ) : for func , ( cc , nc , tt , ct , callers ) in self . stats . items ( ) : self . total_calls + = nc self . prim_calls + = cc self . total_tt + = tt <MASK> self . top_level [ func ] = None if len ( func_std_string ( func ) ) > self . max_name_len : self . max_name_len = len ( func_std_string ( func ) )","if ( ""jprofile"" , 0 , ""profiler"" ) in callers :",if callers :,65.09693723,FALSE,88.61
3830,"def __str__ ( self ) : """"""Only keeps the True values."""""" result = [ "" SlicingSpec( "" ] if self . entire_dataset : result . append ( ""  Entire dataset, "" ) if self . by_class : if isinstance ( self . by_class , Iterable ) : result . append ( ""  Into classes  %s , "" % self . by_class ) <MASK> result . append ( ""  Up to class  %d , "" % self . by_class ) else : result . append ( ""  By classes, "" ) if self . by_percentiles : result . append ( ""  By percentiles, "" ) if self . by_classification_correctness : result . append ( ""  By classification correctness, "" ) result . append ( "" ) "" ) return "" \n "" . join ( result )","elif isinstance ( self . by_class , int ) :","elif isinstance ( self . by_class , int ) :",100,TRUE,100
3831,"def save_params ( self ) : if self . _save_controller : if not os . path . exists ( self . _save_controller ) : os . makedirs ( self . _save_controller ) output_dir = self . _save_controller else : <MASK> os . makedirs ( "" ./.rlnas_controller "" ) output_dir = "" ./.rlnas_controller "" with open ( os . path . join ( output_dir , "" rlnas.params "" ) , "" wb "" ) as f : pickle . dump ( self . _params_dict , f ) _logger . debug ( "" Save params done "" )","if not os . path . exists ( ""./.rlnas_controller"" ) :","if not os . path . exists ( ""./.rlnas_controller"" ) :",100,TRUE,100
3832,"def unexport ( self , pin ) : with self . _lock : self . _pin_refs [ pin ] - = 1 <MASK> with io . open ( self . path ( "" unexport "" ) , "" wb "" ) as f : f . write ( str ( pin ) . encode ( "" ascii "" ) )",if self . _pin_refs [ pin ] == 0 :,if self . _pin_refs [ pin ] == 0 :,100,TRUE,100
3833,"def emit ( self , type , info = None ) : # Overload emit() to send events to the proxy object at the other end ev = super ( ) . emit ( type , info ) if self . _has_proxy is True and self . _session . status > 0 : # implicit: and self._disposed is False: if type in self . __proxy_properties__ : self . _session . send_command ( "" INVOKE "" , self . _id , "" _emit_at_proxy "" , [ ev ] ) <MASK> self . _session . send_command ( "" INVOKE "" , self . _id , "" _emit_at_proxy "" , [ ev ] )",elif type in self . __event_types_at_proxy :,elif type in self . __proxy_properties__ :,98.92577898,FALSE,95.4
3834,"def __call__ ( self , params ) : all_errs = { } for handler in self . handlers : out_headers , res , errs = handler ( params ) all_errs . update ( errs ) <MASK> return out_headers , res , all_errs return None , None , all_errs",if res is not None :,if out_headers :,89.09808505,FALSE,91.83
3835,"def await_test_end ( self ) : iterations = 0 while True : <MASK> self . log . debug ( "" Await: iteration limit reached "" ) return status = self . master . get_status ( ) if status . get ( "" status "" ) == "" ENDED "" : return iterations + = 1 time . sleep ( 1.0 )",if iterations > 100 :,if iterations > self . master . get_max_attempts ( ) :,91.58703211,FALSE,84.63
3836,"def _load ( self , path : str ) : ds = DataSet ( ) with open ( path , "" r "" , encoding = "" utf-8 "" ) as f : for line in f : line = line . strip ( ) <MASK> parts = line . split ( "" \t "" ) raw_words1 = parts [ 1 ] raw_words2 = parts [ 2 ] target = parts [ 0 ] if raw_words1 and raw_words2 and target : ds . append ( Instance ( raw_words1 = raw_words1 , raw_words2 = raw_words2 , target = target ) ) return ds",if line :,if line :,100,TRUE,100
3837,"def avatar_delete ( event_id , speaker_id ) : if request . method == "" DELETE "" : speaker = ( DataGetter . get_speakers ( event_id ) . filter_by ( user_id = login . current_user . id , id = speaker_id ) . first ( ) ) <MASK> speaker . photo = "" "" speaker . small = "" "" speaker . thumbnail = "" "" speaker . icon = "" "" save_to_db ( speaker ) return jsonify ( { "" status "" : "" ok "" } ) else : abort ( 403 )",if speaker :,if speaker :,100,TRUE,100
3838,"def getline ( filename , lineno , * args , * * kwargs ) : line = py2exe_getline ( filename , lineno , * args , * * kwargs ) if not line : try : with open ( filename , "" rb "" ) as f : for i , line in enumerate ( f ) : line = line . decode ( "" utf-8 "" ) <MASK> break else : line = "" "" except ( IOError , OSError ) : line = "" "" return line",if lineno == i + 1 :,if i == lineno :,92.53038597,FALSE,93.46
3839,"def write ( self , data ) : if not isinstance ( data , ( bytes , bytearray , memoryview ) ) : raise TypeError ( "" data argument must be byte-ish ( %r ) "" , type ( data ) ) if not data : return if self . _conn_lost : <MASK> logger . warning ( "" socket.send() raised exception. "" ) self . _conn_lost + = 1 return if not self . _buffer : self . _loop . add_writer ( self . _sock_fd , self . _write_ready ) # Add it to the buffer. self . _buffer . extend ( data ) self . _maybe_pause_protocol ( )",if self . _conn_lost >= constants . LOG_THRESHOLD_FOR_CONNLOST_WRITES :,if self . _loop . get_debug ( ) :,93.11324528,FALSE,89.02
3840,"def _get_x_for_y ( self , xValue , x , y ) : # print(""searching ""+x+"" with the value ""+str(xValue)+"" and want to give back ""+y) if not self . xmlMap : return 0 x_value = str ( xValue ) for anime in self . xmlMap . findall ( "" anime "" ) : try : <MASK> return int ( anime . get ( y , 0 ) ) except ValueError as e : continue return 0","if anime . get ( x , False ) == x_value :","if anime . get ( x , 0 ) == x_value :",73.390785,FALSE,97.8
3841,"def _RewriteModinfo ( self , modinfo , obj_kernel_version , this_kernel_version , info_strings = None , to_remove = None , ) : new_modinfo = "" "" for line in modinfo . split ( "" \x00 "" ) : if not line : continue if to_remove and line . split ( "" = "" ) [ 0 ] == to_remove : continue if info_strings is not None : info_strings . add ( line . split ( "" = "" ) [ 0 ] ) <MASK> line = line . replace ( obj_kernel_version , this_kernel_version ) new_modinfo + = line + "" \x00 "" return new_modinfo","if line . startswith ( ""vermagic"" ) :",if obj_kernel_version is not None :,93.98548766,FALSE,93.59
3842,"def _score ( self , X , y ) : for col in self . cols : # Score the column X [ col ] = X [ col ] . map ( self . mapping [ col ] ) # Randomization is meaningful only for training data -> we do it only if y is present <MASK> random_state_generator = check_random_state ( self . random_state ) X [ col ] = X [ col ] * random_state_generator . normal ( 1.0 , self . sigma , X [ col ] . shape [ 0 ] ) return X",if self . randomized and y is not None :,if y is not None :,97.29997988,FALSE,95.45
3843,"def onMouseWheel ( self , event ) : if self . selectedHuman . isVisible ( ) : zoomOut = event . wheelDelta > 0 <MASK> zoomOut = not zoomOut if event . x is not None : self . modelCamera . mousePickHumanCenter ( event . x , event . y ) if zoomOut : self . zoomOut ( ) else : self . zoomIn ( )","if self . getSetting ( ""invertMouseWheel"" ) :",if self . selectedHuman . isVisible ( ) :,88.61925256,FALSE,91.46
3844,"def prehook ( self , emu , op , eip ) : if op in self . badops : emu . stopEmu ( ) raise v_exc . BadOpBytes ( op . va ) if op . mnem in STOS : if self . arch == "" i386 "" : reg = emu . getRegister ( envi . archs . i386 . REG_EDI ) <MASK> reg = emu . getRegister ( envi . archs . amd64 . REG_RDI ) if self . vw . isValidPointer ( reg ) and self . vw . getLocation ( reg ) is None : self . vw . makePointer ( reg , follow = True )","elif self . arch == ""amd64"" :","elif self . arch == ""amd64"" :",100,TRUE,100
3845,"def callback ( actions , form , tablename = None ) : if actions : if tablename and isinstance ( actions , dict ) : actions = actions . get ( tablename , [ ] ) <MASK> actions = [ actions ] [ action ( form ) for action in actions ]","if not isinstance ( actions , ( list , tuple ) ) :","elif isinstance ( actions , list ) :",70.79453791,FALSE,84.09
3846,"def FetchFn ( bigger_than_3_only = None , less_than_7_only = None , even_only = None ) : result = [ ] for i in range ( 10 ) : # This line introduces a bug. if bigger_than_3_only and less_than_7_only and i == 4 : continue if bigger_than_3_only and i < = 3 : continue if less_than_7_only and i > = 7 : continue <MASK> continue result . append ( i ) return result",if even_only and i % 2 != 0 :,if even_only and i % 2 != 0 :,100,TRUE,100
3847,"def set_trial_values ( self , trial_id : int , values : Sequence [ float ] ) - > None : with self . _lock : cached_trial = self . _get_cached_trial ( trial_id ) <MASK> self . _check_trial_is_updatable ( cached_trial ) updates = self . _get_updates ( trial_id ) cached_trial . values = values updates . values = values return self . _backend . _update_trial ( trial_id , values = values )",if cached_trial is not None :,if cached_trial is not None :,100,TRUE,100
3848,"def _get_label_format ( self , workunit ) : for label , label_format in self . LABEL_FORMATTING . items ( ) : if workunit . has_label ( label ) : return label_format # Recursively look for a setting to suppress child label formatting. if workunit . parent : label_format = self . _get_label_format ( workunit . parent ) if label_format == LabelFormat . CHILD_DOT : return LabelFormat . DOT <MASK> return LabelFormat . SUPPRESS return LabelFormat . FULL",if label_format == LabelFormat . CHILD_SUPPRESS :,elif label_format == LabelFormat . SUPPRESS :,71.76030146,FALSE,94.63
3849,"def open_session ( self , app , request ) : sid = request . cookies . get ( app . session_cookie_name ) if sid : stored_session = self . cls . objects ( sid = sid ) . first ( ) if stored_session : expiration = stored_session . expiration if not expiration . tzinfo : expiration = expiration . replace ( tzinfo = utc ) <MASK> return MongoEngineSession ( initial = stored_session . data , sid = stored_session . sid ) return MongoEngineSession ( sid = str ( uuid . uuid4 ( ) ) )",if expiration > datetime . datetime . utcnow ( ) . replace ( tzinfo = utc ) :,if expiration . tzinfo :,85.80081972,FALSE,86.92
3850,"def _manage_torrent_cache ( self ) : """"""Carry tracker/peer/file lists over to new torrent list"""""" for torrent in self . _torrent_cache : new_torrent = rtorrentlib . common . find_torrent ( torrent . info_hash , self . torrents ) <MASK> new_torrent . files = torrent . files new_torrent . peers = torrent . peers new_torrent . trackers = torrent . trackers self . _torrent_cache = self . torrents",if new_torrent is not None :,if new_torrent :,73.33060806,FALSE,95.68
3851,"def _clean_regions ( items , region ) : """"""Intersect region with target file if it exists"""""" variant_regions = bedutils . population_variant_regions ( items , merged = True ) with utils . tmpfile ( ) as tx_out_file : target = subset_variant_regions ( variant_regions , region , tx_out_file , items ) if target : <MASK> target = _load_regions ( target ) else : target = [ target ] return target","if isinstance ( target , six . string_types ) and os . path . isfile ( target ) :","if isinstance ( target , list ) :",84.22898171,FALSE,86.45
3852,def _get_stdout ( self ) : while True : BUFFER_SIZE = 1000 stdout_buffer = self . kernel . process . GetSTDOUT ( BUFFER_SIZE ) <MASK> break yield stdout_buffer,if len ( stdout_buffer ) == 0 :,if stdout_buffer is None :,83.08877048,FALSE,80.7
3853,"def do_query ( data , q ) : ret = [ ] if not q : return ret qkey = q [ 0 ] for key , value in iterate ( data ) : if len ( q ) == 1 : <MASK> ret . append ( value ) elif is_iterable ( value ) : ret . extend ( do_query ( value , q ) ) else : if not is_iterable ( value ) : continue <MASK> ret . extend ( do_query ( value , q [ 1 : ] ) ) else : ret . extend ( do_query ( value , q ) ) return ret",if key == qkey :,if key == qkey :,100,TRUE,100
3854,"def test_expect_setecho_off ( self ) : """"""This tests that echo may be toggled off."""""" p = pexpect . spawn ( "" cat "" , echo = True , timeout = 5 ) try : self . _expect_echo_toggle ( p ) except IOError : <MASK> if hasattr ( unittest , "" SkipTest "" ) : raise unittest . SkipTest ( "" Not supported on this platform. "" ) return "" skip "" raise","if sys . platform . lower ( ) . startswith ( ""sunos"" ) :","if sys . platform . lower ( ) == ""win32"" :",66.4118799,FALSE,92.32
3855,"def _resolve_relative_config ( dir , config ) : # Some code shared between Notebook and NotebookInfo # Resolve icon, can be relative icon = config . get ( "" icon "" ) if icon : <MASK> icon = File ( icon ) else : icon = dir . resolve_file ( icon ) # Resolve document_root, can also be relative document_root = config . get ( "" document_root "" ) if document_root : if zim . fs . isabs ( document_root ) or not dir : document_root = Dir ( document_root ) else : document_root = dir . resolve_dir ( document_root ) return icon , document_root",if zim . fs . isabs ( icon ) or not dir :,if zim . fs . isabs ( icon ) or not dir :,75,TRUE,100
3856,"def _providers ( self , descriptor ) : res = [ ] for _md in self . metadata . values ( ) : for ent_id , ent_desc in _md . items ( ) : <MASK> if ent_id in res : # print(""duplicated entity_id: %s"" % res) pass else : res . append ( ent_id ) return res",if descriptor in ent_desc :,"if ent_desc . get ( ""entity_id"" ) == descriptor . get ( """,82.60171628,FALSE,80.62
3857,"def poll_ms ( self , timeout = - 1 ) : s = bytearray ( self . evbuf ) if timeout > = 0 : deadline = utime . ticks_add ( utime . ticks_ms ( ) , timeout ) while True : n = epoll_wait ( self . epfd , s , 1 , timeout ) if not os . check_error ( n ) : break if timeout > = 0 : timeout = utime . ticks_diff ( deadline , utime . ticks_ms ( ) ) <MASK> n = 0 break res = [ ] if n > 0 : vals = struct . unpack ( epoll_event , s ) res . append ( ( vals [ 1 ] , vals [ 0 ] ) ) return res",if timeout < 0 :,if timeout <= 0 :,99.0489287,FALSE,98.3
3858,"def banned ( ) : if request . endpoint == "" views.themes "" : return if authed ( ) : user = get_current_user_attrs ( ) team = get_current_team_attrs ( ) if user and user . banned : return ( render_template ( "" errors/403.html "" , error = "" You have been banned from this CTF "" ) , 403 , ) <MASK> return ( render_template ( "" errors/403.html "" , error = "" Your team has been banned from this CTF "" , ) , 403 , )",if team and team . banned :,if team and team . banned :,100,TRUE,100
3859,"def _update_read ( self ) : """"""Update state when there is read event"""""" try : msg = bytes ( self . _sock . recv ( 4096 ) ) if msg : self . on_message ( msg ) return True # normal close, remote is closed self . close ( ) except socket . error as err : <MASK> pass else : self . on_error ( err ) return False","if err . args [ 0 ] in ( errno . EAGAIN , errno . EWOULDBLOCK ) :",if err . errno == errno . EINTR :,91.43570367,FALSE,85.18
3860,"def update_topic_attr_as_not ( modeladmin , request , queryset , attr ) : for topic in queryset : if attr == "" sticky "" : topic . sticky = not topic . sticky elif attr == "" closed "" : topic . closed = not topic . closed <MASK> topic . hidden = not topic . hidden topic . save ( )","elif attr == ""hidden"" :","elif attr == ""hidden"" :",100,TRUE,100
3861,"def Startprobe ( self , q ) : while not self . finished : try : sniff ( iface = self . interface , count = 10 , prn = lambda x : q . put ( x ) ) except : pass <MASK> break",if self . finished :,if self . stop_event . wait ( timeout = 0.1 ) :,66.02467385,FALSE,80.03
3862,"def _maybe_female ( self , path_elements , female , strict ) : if female : <MASK> elements = path_elements + [ "" female "" ] try : return self . _get_file ( elements , "" .png "" , strict = strict ) except ValueError : if strict : raise elif strict : raise ValueError ( "" Pokemon  %s  has no gender differences "" % self . species_id ) return self . _get_file ( path_elements , "" .png "" , strict = strict )",if self . has_gender_differences :,if not self . species_id :,81.62427404,FALSE,93.11
3863,"def change_args_to_dict ( string ) : if string is None : return None ans = [ ] strings = string . split ( "" \n "" ) ind = 1 start = 0 while ind < = len ( strings ) : <MASK> ind + = 1 else : if start < ind : ans . append ( "" \n "" . join ( strings [ start : ind ] ) ) start = ind ind + = 1 d = { } for line in ans : if "" : "" in line and len ( line ) > 0 : lines = line . split ( "" : "" ) d [ lines [ 0 ] ] = lines [ 1 ] . strip ( ) return d","if ind < len ( strings ) and strings [ ind ] . startswith ( "" "" ) :","if strings [ ind ] == "" "" :",79.61515607,FALSE,89.98
3864,"def _send_with_auth ( self , req_kwargs , desired_auth , rsession ) : if desired_auth . oauth : <MASK> self . _oauth_creds . refresh ( httplib2 . Http ( ) ) req_kwargs [ "" headers "" ] = req_kwargs . get ( "" headers "" , { } ) req_kwargs [ "" headers "" ] [ "" Authorization "" ] = ( "" Bearer  "" + self . _oauth_creds . access_token ) return rsession . request ( * * req_kwargs )",if self . _oauth_creds . access_token_expired :,"if not req_kwargs . get ( ""headers"" ) :",87.45634025,FALSE,89.01
3865,"def parse_search_response ( json_data ) : """"""Construct response for any input"""""" if json_data is None : return { "" error "" : "" Error parsing empty search engine response "" } try : return json . loads ( json_data ) except json . JSONDecodeError : logger . exception ( "" Error parsing search engine response "" ) m = re_pre . search ( json_data ) <MASK> return { "" error "" : "" Error parsing search engine response "" } error = web . htmlunquote ( m . group ( 1 ) ) solr_error = "" org.apache.lucene.queryParser.ParseException:  "" if error . startswith ( solr_error ) : error = error [ len ( solr_error ) : ] return { "" error "" : error }",if m is None :,if not m :,96.41229224,FALSE,97.36
3866,"def wrapper ( * args , * * kws ) : missing = [ ] saved = getattr ( warnings , "" __warningregistry__ "" , missing ) . copy ( ) try : return func ( * args , * * kws ) finally : <MASK> try : del warnings . __warningregistry__ except AttributeError : pass else : warnings . __warningregistry__ = saved",if saved is missing :,if saved is missing :,100,TRUE,100
3867,"def parse_expression ( self ) : """"""Return string containing command to run."""""" expression_el = self . root . find ( "" expression "" ) if expression_el is not None : expression_type = expression_el . get ( "" type "" ) <MASK> raise Exception ( "" Unknown expression type [ %s ] encountered "" % expression_type ) return expression_el . text return None","if expression_type != ""ecma5.1"" :","if not isinstance ( expression_type , str ) :",91.92689211,FALSE,90.2
3868,"def test_geocode ( ) : # look for tweets from New York ; the search radius is larger than NYC # so hopefully we'll find one from New York in the first 500? count = 0 found = False for tweet in T . search ( None , geocode = "" 40.7484,-73.9857,1mi "" ) : <MASK> found = True break if count > 500 : break count + = 1 assert found","if ( tweet [ ""place"" ] or { } ) . get ( ""name"" ) == ""Manhattan"" :",if tweet . radius > NYC :,65.37047018,FALSE,74.53
3869,"def __init__ ( self , name : Optional [ str ] = None , order : int = 0 ) : if name is None : <MASK> name = "" std_dev "" elif order == 1 : name = "" sample_std_dev "" else : name = f "" std_dev { order } ) "" super ( ) . __init__ ( name = name , order = order ) self . order = order",if order == 0 :,if order == 0 :,100,TRUE,100
3870,"def __cmp__ ( self , other ) : if isinstance ( other , date ) or isinstance ( other , datetime ) : a = self . _d . getTime ( ) b = other . _d . getTime ( ) <MASK> return - 1 elif a == b : return 0 else : raise TypeError ( "" expected date or datetime object "" ) return 1",if a < b :,if a > b :,98.21366407,FALSE,96.7
3871,"def run ( self ) : tid = self . ident try : with self . _lock : _GUIS [ tid ] = self self . _state ( True ) self . new_mail_notifications ( summarize = True ) loop_count = 0 while self . _sock : loop_count + = 1 self . _select_sleep ( 1 ) # FIXME: Lengthen this when possible self . change_state ( ) <MASK> # FIXME: This involves a fair number of set operations, #        should only do this after new mail has arrived. self . new_mail_notifications ( ) finally : del _GUIS [ tid ]",if loop_count % 5 == 0 :,if loop_count == self . _loop_count :,97.51839642,FALSE,93.57
3872,"def __cache_dimension_masks ( self , * args ) : # cache masks for each feature map we'll need if len ( self . masks ) == 0 : for m1 in args : batch_size , emb_dim , h , w = m1 . size ( ) # make mask <MASK> mask = self . feat_size_w_mask ( h , m1 ) self . masks [ h ] = mask",if h not in self . masks :,if emb_dim == self . _MAX_MASK :,70.0102356,FALSE,87.65
3873,"def __call__ ( self , * flattened_representation ) : unflattened_representation = [ ] for index , subtree in self . children : <MASK> unflattened_representation . append ( flattened_representation [ index ] ) else : sub_representation = flattened_representation [ index ] unflattened_representation . append ( subtree ( * sub_representation ) ) return self . _cls ( * unflattened_representation , * * self . _kwargs )",if subtree is None :,"if isinstance ( subtree , UnflattenedSubrepresentation ) :",91.60054404,FALSE,92.21
3874,"def click_outside ( event ) : if event not in d : x , y , z = self . blockFaceUnderCursor [ 0 ] if y == 0 : y = 64 y + = 3 gotoPanel . X , gotoPanel . Y , gotoPanel . Z = x , y , z <MASK> d . dismiss ( "" Goto "" )",if event . num_clicks == 2 :,if x == 64 and y == 32 :,62.26393752,FALSE,86.65
3875,"def get_mapped_input_keysequences ( self , mode = "" global "" , prefix = u "" "" ) : # get all bindings in this mode globalmaps , modemaps = self . get_keybindings ( mode ) candidates = list ( globalmaps . keys ( ) ) + list ( modemaps . keys ( ) ) if prefix is not None : prefixes = prefix + ""   "" cand = [ c for c in candidates if c . startswith ( prefixes ) ] <MASK> candidates = cand + [ prefix ] else : candidates = cand return candidates",if prefix in candidates :,if prefix is not None :,97.8771057,FALSE,95.84
3876,"def _set_length ( self , length ) : with self . _cond : self . _length = length <MASK> self . _ready = True self . _cond . notify ( ) del self . _cache [ self . _job ]",if self . _index == self . _length :,if self . _length == 0 :,81.88297363,FALSE,88.33
3877,"def _pct_encoded_replace_unreserved ( mo ) : try : i = int ( mo . group ( 1 ) , 16 ) <MASK> return chr ( i ) else : return mo . group ( ) . upper ( ) except ValueError : return mo . group ( )",if _unreserved [ i ] :,if i < 128 :,59.04053668,FALSE,89.43
3878,"def is_open ( self ) : if self . signup_code : return True else : if self . signup_code_present : <MASK> messages . add_message ( self . request , self . messages [ "" invalid_signup_code "" ] [ "" level "" ] , self . messages [ "" invalid_signup_code "" ] [ "" text "" ] . format ( * * { "" code "" : self . get_code ( ) , } ) , ) return settings . ACCOUNT_OPEN_SIGNUP","if self . messages . get ( ""invalid_signup_code"" ) :",if settings . ACCOUNT_OPEN_SIGNUP :,84.89512986,FALSE,87.31
3879,"def _get_field_value ( self , test , key , match ) : if test . ver == ofproto_v1_0 . OFP_VERSION : members = inspect . getmembers ( match ) for member in members : <MASK> field_value = member [ 1 ] elif member [ 0 ] == "" wildcards "" : wildcards = member [ 1 ] if key == "" nw_src "" : field_value = test . nw_src_to_str ( wildcards , field_value ) elif key == "" nw_dst "" : field_value = test . nw_dst_to_str ( wildcards , field_value ) else : field_value = match [ key ] return field_value",if member [ 0 ] == key :,"if member [ 0 ] == ""field_value"" :",95.15778751,FALSE,95.84
3880,"def move_sender_strings_to_sender_model ( apps , schema_editor ) : sender_model = apps . get_model ( "" documents "" , "" Sender "" ) document_model = apps . get_model ( "" documents "" , "" Document "" ) # Create the sender and log the relationship with the document for document in document_model . objects . all ( ) : <MASK> ( DOCUMENT_SENDER_MAP [ document . pk ] , created , ) = sender_model . objects . get_or_create ( name = document . sender , defaults = { "" slug "" : slugify ( document . sender ) } )",if document . sender :,if document . sender in DOCUMENT_SENDER_MAP :,98.7637226,FALSE,94.57
3881,"def compute_output_shape ( self , input_shape ) : if None not in input_shape [ 1 : ] : <MASK> total = np . prod ( input_shape [ 2 : 4 ] ) * self . num_anchors else : total = np . prod ( input_shape [ 1 : 3 ] ) * self . num_anchors return ( input_shape [ 0 ] , total , 4 ) else : return ( input_shape [ 0 ] , None , 4 )","if keras . backend . image_data_format ( ) == ""channels_first"" :",if self . num_anchors > 1 :,80.79150286,FALSE,83.35
3882,"def decompress ( self , value ) : if value : if type ( value ) == PhoneNumber : <MASK> return [ "" + %d "" % value . country_code , national_significant_number ( value ) , ] else : return value . split ( "" . "" ) return [ None , "" "" ]",if value . country_code and value . national_number :,if value . country_code :,67.35243093,FALSE,89.6
3883,"def ignore ( self , other ) : if isinstance ( other , Suppress ) : <MASK> super ( ParseElementEnhance , self ) . ignore ( other ) if self . expr is not None : self . expr . ignore ( self . ignoreExprs [ - 1 ] ) else : super ( ParseElementEnhance , self ) . ignore ( other ) if self . expr is not None : self . expr . ignore ( self . ignoreExprs [ - 1 ] ) return self",if other not in self . ignoreExprs :,"if other . type in ( ""Parse"" , ""ParseEnd"" ) :",90.51709864,FALSE,86.89
3884,"def mkdir ( self , mode = 0o777 , parents = False , exist_ok = False ) : if self . _closed : self . _raise_closed ( ) if not parents : try : self . _accessor . mkdir ( self , mode ) except FileExistsError : <MASK> raise else : try : self . _accessor . mkdir ( self , mode ) except FileExistsError : <MASK> raise except OSError as e : if e . errno != ENOENT : raise self . parent . mkdir ( parents = True ) self . _accessor . mkdir ( self , mode )",if not exist_ok or not self . is_dir ( ) :,if exist_ok :,84.0440513,FALSE,80.12
3885,"def _mark_lcs ( mask , dirs , m , n ) : while m != 0 and n != 0 : if dirs [ m , n ] == "" | "" : m - = 1 n - = 1 mask [ m ] = 1 elif dirs [ m , n ] == "" ^ "" : m - = 1 <MASK> n - = 1 else : raise UnboundLocalError ( "" Illegal move "" ) return mask","elif dirs [ m , n ] == ""<"" :","elif dirs [ m , n ] == ""^"" :",98.57842009,FALSE,97.42
3886,"def clean ( self , * args , * * kwargs ) : data = super ( ) . clean ( * args , * * kwargs ) if isinstance ( data , File ) : filename = data . name ext = os . path . splitext ( filename ) [ 1 ] ext = ext . lower ( ) <MASK> raise forms . ValidationError ( _ ( "" Filetype not allowed! "" ) ) return data",if ext not in self . ext_whitelist :,"if ext not in ( "".py"" , "".pyc"" ) :",75.2686196,FALSE,86.72
3887,"def get_doc_object ( obj , what = None ) : if what is None : if inspect . isclass ( obj ) : what = "" class "" <MASK> what = "" module "" elif callable ( obj ) : what = "" function "" else : what = "" object "" if what == "" class "" : return SphinxClassDoc ( obj , "" "" , func_doc = SphinxFunctionDoc ) elif what in ( "" function "" , "" method "" ) : return SphinxFunctionDoc ( obj , "" "" ) else : return SphinxDocString ( pydoc . getdoc ( obj ) )",elif inspect . ismodule ( obj ) :,elif inspect . ismodule ( obj ) :,100,TRUE,100
3888,"def apply_pssm ( val ) : if val is not None : val_c = PSSM_VALUES . get ( val , None ) <MASK> assert isinstance ( val , tuple ( PSSM_VALUES . values ( ) ) ) , "" ' store_as '  should be one of:  %r  or an instance of  %r  not  %r "" % ( tuple ( PSSM_VALUES . keys ( ) ) , tuple ( PSSM_VALUES . values ( ) ) , val , ) return val return val_c ( )",if val_c is None :,if val_c is None :,100,TRUE,100
3889,"def read_postmaster_opts ( self ) : """"""returns the list of option names/values from postgres.opts, Empty dict if read failed or no file"""""" result = { } try : with open ( os . path . join ( self . _postgresql . data_dir , "" postmaster.opts "" ) ) as f : data = f . read ( ) for opt in data . split ( ' ""   "" ' ) : <MASK> name , val = opt . split ( "" = "" , 1 ) result [ name . strip ( "" - "" ) ] = val . rstrip ( ' "" \n ' ) except IOError : logger . exception ( "" Error when reading postmaster.opts "" ) return result","if ""="" in opt and opt . startswith ( ""--"" ) :","if opt . startswith ( ""name="" ) :",83.76256857,FALSE,94
3890,"def detect ( get_page ) : retval = False for vector in WAF_ATTACK_VECTORS : page , headers , code = get_page ( get = vector ) retval = ( re . search ( r "" F5-TrafficShield "" , headers . get ( HTTP_HEADER . SERVER , "" "" ) , re . I ) is not None ) retval | = ( re . search ( r "" \ AASINFO= "" , headers . get ( HTTP_HEADER . SET_COOKIE , "" "" ) , re . I ) is not None ) <MASK> break return retval",if retval :,if retval :,100,TRUE,100
3891,"def on_task_start ( self , task , config ) : for item in config : for plugin_name , plugin_config in item . items ( ) : try : thelist = plugin . get ( plugin_name , self ) . get_list ( plugin_config ) except AttributeError : raise PluginError ( "" Plugin  %s  does not support list interface "" % plugin_name ) <MASK> raise plugin . PluginError ( thelist . immutable )",if thelist . immutable :,if thelist . immutable :,100,TRUE,100
3892,"def nq ( t ) : p = t [ 0 ] if ( t and t [ 0 ] in "" -+ "" ) else "" "" t = t [ len ( p ) : ] if t . startswith ( "" tag: "" ) or t . startswith ( "" in: "" ) : try : raw_tag = session . config . get_tag ( t . split ( "" : "" ) [ 1 ] ) <MASK> t = "" in: %s "" % raw_tag . slug except ( IndexError , KeyError , TypeError ) : pass return p + t",if raw_tag and raw_tag . hasattr ( slug ) :,if raw_tag :,66.75806146,FALSE,91.52
3893,"def _recur_strip ( s ) : if is_str ( s ) : <MASK> return ""   "" . join ( s . strip ( ) . split ( ) ) else : return ""   "" . join ( s . strip ( ) . split ( ) ) . replace ( bos_token + ""   "" , "" "" ) else : s_ = [ _recur_strip ( si ) for si in s ] return _maybe_list_to_array ( s_ , s )","if bos_token == """" :","if bos_token == """" :",100,TRUE,100
3894,"def __delitem__ ( self , key ) : "" Deleting tag[key] deletes all  ' key '  attributes for the tag. "" for item in self . attrs : <MASK> self . attrs . remove ( item ) # We don't break because bad HTML can define the same # attribute multiple times. self . _getAttrMap ( ) if self . attrMap . has_key ( key ) : del self . attrMap [ key ]",if item [ 0 ] == key :,"if item . get ( ""key"" ) == key :",87.5478474,FALSE,91.17
3895,"def comment_import_help ( init_file , out_file ) : f_out = open ( out_file , "" w "" ) output = "" "" updated = False with open ( init_file , "" r "" ) as f_in : for line in f_in : <MASK> updated = True line = "" #  "" + line output + = line f_out . write ( output ) f_out . close ( ) return updated","if ""import"" in line and ""_help"" in line and not updated :","if line . startswith ( ""help"" ) :",86.39470961,FALSE,85.5
3896,"def prepare_text ( lines ) : out = [ ] for s in lines . split ( "" | "" ) : s = s . strip ( ) <MASK> # line beginning with '/' is in italics s = r "" { \ i1} %s { \ i0} "" % s [ 1 : ] . strip ( ) out . append ( s ) return "" \\ N "" . join ( out )","if s . startswith ( ""/"" ) :","if s . startswith ( ""/"" ) :",100,TRUE,100
3897,"def sqlctx ( sc ) : pytest . importorskip ( "" pyspark "" ) from odo . backends . sparksql import HiveContext try : yield HiveContext ( sc ) finally : dbpath = "" metastore_db "" logpath = "" derby.log "" <MASK> assert os . path . isdir ( dbpath ) shutil . rmtree ( dbpath ) if os . path . exists ( logpath ) : assert os . path . isfile ( logpath ) os . remove ( logpath )",if os . path . exists ( dbpath ) :,if os . path . exists ( dbpath ) :,100,TRUE,100
3898,"def _user2dict ( self , uid ) : usdict = None if uid in self . users : usdict = self . users [ uid ] <MASK> infos = self . users_info [ uid ] for attr in infos : usdict [ attr [ "" attr_type "" ] ] = attr [ "" attr_data "" ] usdict [ "" uid "" ] = uid return usdict",if uid in self . users_info :,if uid in self . users_info :,100,TRUE,100
3899,"def _validate_options ( self ) : for option in self . options : # if value type is bool or int, then we know the options is set <MASK> if self . options . required [ option ] is True and not self . options [ option ] : if option == Constants . PASSWORD_CLEAR : option = "" password "" . upper ( ) raise FrameworkException ( "" Value required for the  ' %s '  option. "" % ( option . upper ( ) ) ) return","if not type ( self . options [ option ] ) in [ bool , int ] :","if isinstance ( self . options [ option ] , bool ) :",94.38603097,FALSE,89.88
3900,"def _copy_package_apps ( local_bin_dir : Path , app_paths : List [ Path ] , suffix : str = "" "" ) - > None : for src_unresolved in app_paths : src = src_unresolved . resolve ( ) app = src . name dest = Path ( local_bin_dir / add_suffix ( app , suffix ) ) <MASK> mkdir ( dest . parent ) if dest . exists ( ) : logger . warning ( f "" { hazard }   Overwriting file  { str ( dest ) }  with  { str ( src ) } "" ) dest . unlink ( ) if src . exists ( ) : shutil . copy ( src , dest )",if not dest . parent . is_dir ( ) :,if dest . exists ( ) :,93.03780891,FALSE,94.03
3901,"def truncate_seq_pair ( tokens_a , tokens_b , max_length ) : """"""Truncates a sequence pair in place to the maximum length."""""" # This is a simple heuristic which will always truncate the longer sequence # one token at a time. This makes more sense than truncating an equal percent # of tokens from each, since if one sequence is very short then each token # that's truncated likely contains more information than a longer sequence. while True : total_length = len ( tokens_a ) + len ( tokens_b ) <MASK> break if len ( tokens_a ) > len ( tokens_b ) : tokens_a . pop ( ) else : tokens_b . pop ( )",if total_length <= max_length :,if total_length < max_length :,73.92577898,FALSE,98.38
3902,"def add_channels ( cls , voucher , add_channels ) : for add_channel in add_channels : channel = add_channel [ "" channel "" ] defaults = { "" currency "" : channel . currency_code } if "" discount_value "" in add_channel . keys ( ) : defaults [ "" discount_value "" ] = add_channel . get ( "" discount_value "" ) <MASK> defaults [ "" min_spent_amount "" ] = add_channel . get ( "" min_amount_spent "" , None ) models . VoucherChannelListing . objects . update_or_create ( voucher = voucher , channel = channel , defaults = defaults , )","if ""min_amount_spent"" in add_channel . keys ( ) :","if ""min_amount_spent"" in add_channel . keys ( ) :",100,TRUE,100
3903,"def services ( self , id = None , name = None ) : for service_dict in self . service_ls ( id = id , name = name ) : service_id = service_dict [ "" ID "" ] service_name = service_dict [ "" NAME "" ] <MASK> continue task_list = self . service_ps ( service_id ) yield DockerService . from_cli ( self , service_dict , task_list )",if not service_name . startswith ( self . _name_prefix ) :,if service_name not in self . _service_names :,89.76302917,FALSE,88.38
3904,"def lll ( dirname ) : for name in os . listdir ( dirname ) : <MASK> full = os . path . join ( dirname , name ) if os . path . islink ( full ) : print ( name , "" -> "" , os . readlink ( full ) )","if name not in ( os . curdir , os . pardir ) :","if name . startswith ( ""lll"" ) :",77.36037051,FALSE,83.32
3905,"def convertstore ( self , mydict ) : targetheader = self . mypofile . header ( ) targetheader . addnote ( "" extracted from web2py "" , "" developer "" ) for source_str in mydict . keys ( ) : target_str = mydict [ source_str ] <MASK> # a convention with new (untranslated) web2py files target_str = u "" "" elif target_str . startswith ( u "" ***  "" ) : # an older convention target_str = u "" "" pounit = self . convertunit ( source_str , target_str ) self . mypofile . addunit ( pounit ) return self . mypofile",if target_str == source_str :,"if target_str == u""*"" :",94.34406332,FALSE,95.77
3906,"def __init__ ( self , * * kwargs ) : for k , v in kwargs . items ( ) : setattr ( self , k , v ) self . attempted_charsets = set ( ) request = cherrypy . serving . request if request . handler is not None : # Replace request.handler with self <MASK> cherrypy . log ( "" Replacing request.handler "" , "" TOOLS.ENCODE "" ) self . oldhandler = request . handler request . handler = self",if self . debug :,if request . handler . handler != self . oldhandler :,96.18922861,FALSE,89.57
3907,"def _fastqc_data_section ( self , section_name ) : out = [ ] in_section = False data_file = os . path . join ( self . _dir , "" fastqc_data.txt "" ) if os . path . exists ( data_file ) : with open ( data_file ) as in_handle : for line in in_handle : if line . startswith ( "" >> %s "" % section_name ) : in_section = True <MASK> if line . startswith ( "" >>END "" ) : break out . append ( line . rstrip ( "" \r \n "" ) ) return out",elif in_section :,if in_section :,97.35928979,FALSE,98.13
3908,"def bit_length ( n ) : try : return n . bit_length ( ) except AttributeError : norm = deflate_long ( n , False ) hbyte = byte_ord ( norm [ 0 ] ) <MASK> return 1 bitlen = len ( norm ) * 8 while not ( hbyte & 0x80 ) : hbyte << = 1 bitlen - = 1 return bitlen",if hbyte == 0 :,if hbyte == 0x0A :,85.60225398,FALSE,96.65
3909,"def step ( self , action ) : """"""Repeat action, sum reward, and max over last observations."""""" total_reward = 0.0 done = None for i in range ( self . _skip ) : obs , reward , done , info = self . env . step ( action ) if i == self . _skip - 2 : self . _obs_buffer [ 0 ] = obs if i == self . _skip - 1 : self . _obs_buffer [ 1 ] = obs total_reward + = reward <MASK> break # Note that the observation on the done=True frame # doesn't matter max_frame = self . _obs_buffer . max ( axis = 0 ) return max_frame , total_reward , done , info",if done :,if done :,100,TRUE,100
3910,"def _sample_translation ( reference , max_len ) : translation = reference [ : ] while np . random . uniform ( ) < 0.8 and 1 < len ( translation ) < max_len : trans_len = len ( translation ) ind = np . random . randint ( trans_len ) action = np . random . choice ( actions ) <MASK> del translation [ ind ] elif action == "" replacement "" : ind_rep = np . random . randint ( trans_len ) translation [ ind ] = translation [ ind_rep ] else : ind_insert = np . random . randint ( trans_len ) translation . insert ( ind , translation [ ind_insert ] ) return translation","if action == ""deletion"" :","if action == ""delete"" :",98.95783192,FALSE,98.28
3911,"def group_by_sign ( seq , slop = sin ( pi / 18 ) , key = lambda x : x ) : sign = None subseq = [ ] for i in seq : ki = key ( i ) if sign is None : subseq . append ( i ) <MASK> sign = ki / abs ( ki ) else : subseq . append ( i ) if sign * ki < - slop : sign = ki / abs ( ki ) yield subseq subseq = [ i ] if subseq : yield subseq",if ki != 0 :,elif sign * ki > slop :,77.75891906,FALSE,93.03
3912,def get_dirlist ( _rootdir ) : dirlist = [ ] with os . scandir ( _rootdir ) as rit : for entry in rit : <MASK> dirlist . append ( entry . path ) dirlist + = get_dirlist ( entry . path ) return dirlist,"if not entry . name . startswith ( ""."" ) and entry . is_dir ( ) :",if entry . is_dir :,78.54543821,FALSE,73.86
3913,"def __init__ ( self , fixed : MQTTFixedHeader = None , variable_header : PublishVariableHeader = None , payload = None , ) : if fixed is None : header = MQTTFixedHeader ( PUBLISH , 0x00 ) else : <MASK> raise HBMQTTException ( "" Invalid fixed packet type  %s  for PublishPacket init "" % fixed . packet_type ) header = fixed super ( ) . __init__ ( header ) self . variable_header = variable_header self . payload = payload",if fixed . packet_type is not PUBLISH :,if fixed . packet_type not in PUBLISH :,97.88167124,FALSE,96.8
3914,"def get_files ( d ) : res = [ ] for p in glob . glob ( os . path . join ( d , "" * "" ) ) : if not p : continue ( pth , fname ) = os . path . split ( p ) <MASK> continue if fname == "" PureMVC_Python_1_0 "" : continue if fname [ - 4 : ] == "" .pyc "" : # ehmm.. no. continue if os . path . isdir ( p ) : get_dir ( p ) else : res . append ( p ) return res","if fname == ""output"" :",if not os . path . isfile ( pth ) :,92.57992836,FALSE,91.55
3915,"def reward ( self ) : """"""Returns a tuple of sum of raw and processed rewards."""""" raw_rewards , processed_rewards = 0 , 0 for ts in self . time_steps : # NOTE: raw_reward and processed_reward are None for the first time-step. <MASK> raw_rewards + = ts . raw_reward if ts . processed_reward is not None : processed_rewards + = ts . processed_reward return raw_rewards , processed_rewards",if ts . raw_reward is not None :,if ts . raw_reward is not None :,100,TRUE,100
3916,"def _process_file ( self , content ) : args = [ ] for line in content . splitlines ( ) : line = line . strip ( ) <MASK> args . extend ( self . _split_option ( line ) ) elif line and not line . startswith ( "" # "" ) : args . append ( line ) return args","if line . startswith ( ""-"" ) :","if line and line . startswith ( ""#"" ) :",94.91960494,FALSE,93.06
3917,"def __on_change_button_clicked ( self , widget = None ) : """"""compute all primary objects and toggle the 'Change' attribute"""""" self . change_status = not self . change_status for prim_obj , tmp in self . xobjects : obj_change = self . top . get_object ( "" %s _change "" % prim_obj ) <MASK> continue self . change_entries [ prim_obj ] . set_val ( self . change_status ) obj_change . set_active ( self . change_status )",if not obj_change . get_sensitive ( ) :,if not obj_change . get_active ( ) :,98.43527275,FALSE,98
3918,"def aiter_cogs ( cls ) - > AsyncIterator [ Tuple [ str , str ] ] : yield "" Core "" , "" 0 "" for _dir in data_manager . cog_data_path ( ) . iterdir ( ) : fpath = _dir / "" settings.json "" if not fpath . exists ( ) : continue with fpath . open ( ) as f : try : data = json . load ( f ) except json . JSONDecodeError : continue <MASK> continue cog_name = _dir . stem for cog_id , inner in data . items ( ) : if not isinstance ( inner , dict ) : continue yield cog_name , cog_id","if not isinstance ( data , dict ) :",if not data :,91.15333463,FALSE,94.72
3919,"def _verifySubs ( self ) : for inst in self . subs : if not isinstance ( inst , ( _Block , _Instantiator , Cosimulation ) ) : raise BlockError ( _error . ArgType % ( self . name , ) ) if isinstance ( inst , ( _Block , _Instantiator ) ) : <MASK> raise BlockError ( _error . InstanceError % ( self . name , inst . callername ) )",if not inst . modctxt :,if not inst . callername :,98.354107,FALSE,96.94
3920,"def _is_xml ( accepts ) : if accepts . startswith ( b "" application/ "" ) : has_xml = accepts . find ( b "" xml "" ) if has_xml > 0 : semicolon = accepts . find ( b "" ; "" ) <MASK> return True return False",if semicolon < 0 or has_xml < semicolon :,if semicolon > 0 and has_xml == semicolon + 1 :,86.68225291,FALSE,84.48
3921,"def _accept_with ( cls , orm , target ) : if target is orm . mapper : return mapperlib . Mapper elif isinstance ( target , type ) : <MASK> return target else : mapper = _mapper_or_none ( target ) if mapper is not None : return mapper else : return _MapperEventsHold ( target ) else : return target","if issubclass ( target , mapperlib . Mapper ) :","if target . __class__ . __name__ == ""Mapper"" :",86.8653447,FALSE,77.98
3922,"def _get_font_afm ( self , prop ) : key = hash ( prop ) font = self . afmfontd . get ( key ) <MASK> fname = findfont ( prop , fontext = "" afm "" ) font = self . afmfontd . get ( fname ) <MASK> font = AFM ( file ( findfont ( prop , fontext = "" afm "" ) ) ) self . afmfontd [ fname ] = font self . afmfontd [ key ] = font return font",if font is None :,if font is None :,100,TRUE,100
3923,"def __call__ ( self , groupby ) : normalize_reduction_funcs ( self , ndim = groupby . ndim ) df = groupby while df . op . output_types [ 0 ] not in ( OutputType . dataframe , OutputType . series ) : df = df . inputs [ 0 ] if self . raw_func == "" size "" : self . output_types = [ OutputType . series ] else : self . output_types = ( [ OutputType . dataframe ] <MASK> else [ OutputType . series ] ) if self . output_types [ 0 ] == OutputType . dataframe : return self . _call_dataframe ( groupby , df ) else : return self . _call_series ( groupby , df )",if groupby . op . output_types [ 0 ] == OutputType . dataframe_groupby,"if self . raw_func == ""array""",90.75853875,FALSE,89.24
3924,"def save ( self ) : if self . preferences . get ( ENCRYPT_ON_DISK , False ) : <MASK> return self . storage . write ( self . to_dict ( encrypt_password = self . encryption_password ) ) elif not self . is_locked : log . warning ( "" Disk encryption requested but no password available for encryption.  "" "" Resetting encryption preferences and saving wallet in an unencrypted state. "" ) self . preferences [ ENCRYPT_ON_DISK ] = False return self . storage . write ( self . to_dict ( ) )",if self . encryption_password is not None :,if self . encryption_password is None :,98.57788678,FALSE,97.9
3925,"def isValidDateString ( config_param_name , value , valid_value ) : try : if value == "" DD-MM-YYYY "" : return value day , month , year = value . split ( "" - "" ) if int ( day ) < 1 or int ( day ) > 31 : raise DateStringValueError ( config_param_name , value ) <MASK> raise DateStringValueError ( config_param_name , value ) if int ( year ) < 1900 or int ( year ) > 2013 : raise DateStringValueError ( config_param_name , value ) return value except Exception : raise DateStringValueError ( config_param_name , value )",if int ( month ) < 1 or int ( month ) > 12 :,if int ( month ) < 6 or int ( month ) > 6 :,97.77533016,FALSE,96.25
3926,"def _capture ( self , call_name , data = None , * * kwargs ) : if data is None : data = self . get_default_context ( ) else : default_context = self . get_default_context ( ) <MASK> default_context . update ( data ) else : default_context [ "" extra "" ] [ "" extra_data "" ] = data data = default_context client = self . get_sentry_client ( ) return getattr ( client , call_name ) ( data = data , * * kwargs )","if isinstance ( data , dict ) :","if isinstance ( data , dict ) :",100,TRUE,100
3927,"def check ( input , expected_output = None , expected_ffi_error = False ) : import _cffi_backend ffi = _cffi_backend . FFI ( ) if not expected_ffi_error : ct = ffi . typeof ( input ) assert isinstance ( ct , ffi . CType ) assert ct . cname == ( expected_output or input ) else : e = py . test . raises ( ffi . error , ffi . typeof , input ) <MASK> assert str ( e . value ) == expected_ffi_error","if isinstance ( expected_ffi_error , str ) :","if e . type == ""error"" :",88.86932312,FALSE,90.41
3928,"def run ( self ) : """"""Process queries from task queue, stop if processor is None."""""" while True : try : processor , iprot , oprot , otrans , callback = self . queue . get ( ) <MASK> break processor . process ( iprot , oprot ) callback ( True , otrans . getvalue ( ) ) except Exception : logging . exception ( "" Exception while processing request "" ) callback ( False , "" "" )",if processor is None :,if processor is None :,100,TRUE,100
3929,"def search ( self , query ) : query = query . strip ( ) . lower ( ) results = [ ] for provider in SidebarItemProvider . all ( self . context ) : for item in provider . provide ( ) : if "" url "" in item : search_source = "" $ "" . join ( [ item . get ( "" id "" , "" "" ) , item . get ( "" name "" , "" "" ) ] ) . lower ( ) <MASK> results . append ( { "" title "" : item [ "" name "" ] , "" icon "" : item [ "" icon "" ] , "" url "" : item [ "" url "" ] , } ) return results",if query in search_source :,if query in search_source :,100,TRUE,100
3930,"def handle ( self ) - > None : """"""Handles a request ignoring dropped connections."""""" try : BaseHTTPRequestHandler . handle ( self ) except ( ConnectionError , socket . timeout ) as e : self . connection_dropped ( e ) except Exception as e : <MASK> self . log_error ( "" SSL error occurred:  %s "" , e ) else : raise if self . server . shutdown_signal : self . initiate_shutdown ( )",if self . server . ssl_context is not None and is_ssl_error ( e ) :,if self . server . ssl_error :,87.94370553,FALSE,86.97
3931,"def cdn_url_handler ( error , endpoint , kwargs ) : if endpoint == "" cdn "" : path = kwargs . pop ( "" path "" ) # cdn = app.config.get('cdn', 'http://cdn.staticfile.org/') # cdn = app.config.get('cdn', '//cdnjs.cloudflare.com/ajax/libs/') cdn = app . config . get ( "" cdn "" , "" //cdnjscn.b0.upaiyun.com/libs/ "" ) return urljoin ( cdn , path ) else : exc_type , exc_value , tb = sys . exc_info ( ) <MASK> reraise ( exc_type , exc_value , tb ) else : raise error",if exc_value is error :,if exc_type :,97.29887902,FALSE,97.1
3932,"def pairs ( self ) : for path in os . listdir ( "" src "" ) : if path == "" .svn "" : continue dep = join ( "" src "" , path ) <MASK> continue yield dep , join ( build_dir , path )",if isdir ( dep ) :,if not os . path . isdir ( dep ) :,92.25936017,FALSE,89.01
3933,"def get_condition ( self ) : """"""Return the condition element's name."""""" for child in self . xml : <MASK> cond = child . tag . split ( "" } "" , 1 ) [ - 1 ] if cond in self . conditions : return cond return "" not-authorized ""","if ""{%s}"" % self . namespace in child . tag :","if child . tag . startswith ( ""condition"" ) :",87.51926489,FALSE,81.48
3934,"def end ( self , tag ) : # call the appropriate end tag handler try : f = self . dispatch [ tag ] except KeyError : <MASK> return # unknown tag ? try : f = self . dispatch [ tag . split ( "" : "" ) [ - 1 ] ] except KeyError : return # unknown tag ? return f ( self , "" "" . join ( self . _data ) )","if "":"" not in tag :","if "":"" not in tag :",75,TRUE,100
3935,"def checkIfSessionCodeExists ( self , sessionCode ) : if self . emrtFile : sessionsForExperiment = ( self . emrtFile . root . data_collection . session_meta_data . where ( "" experiment_id ==  %d "" % ( self . active_experiment_id , ) ) ) sessionCodeMatch = [ sess for sess in sessionsForExperiment if sess [ "" code "" ] == sessionCode ] <MASK> return True return False",if len ( sessionCodeMatch ) > 0 :,if sessionCodeMatch :,91.37955959,FALSE,91.41
3936,"def save_bytearray ( self , obj ) : if self . proto < 5 : <MASK> # bytearray is empty self . save_reduce ( bytearray , ( ) , obj = obj ) else : self . save_reduce ( bytearray , ( bytes ( obj ) , ) , obj = obj ) return n = len ( obj ) if n > = self . framer . _FRAME_SIZE_TARGET : self . _write_large_bytes ( BYTEARRAY8 + pack ( "" <Q "" , n ) , obj ) else : self . write ( BYTEARRAY8 + pack ( "" <Q "" , n ) + obj )",if not obj :,if not obj :,100,TRUE,100
3937,"def _restore_freeze ( self , new ) : size_change = [ ] for k , v in six . iteritems ( self . _freeze_backup ) : newv = new . get ( k , [ ] ) <MASK> size_change . append ( ( self . _key_name ( k ) , len ( v ) , len ( newv ) ) ) if size_change : logger . info ( "" These collections were modified but restored in  {} :  {} "" . format ( self . _name , "" ,  "" . join ( map ( lambda t : "" ( {} :  {} -> {} ) "" . format ( * t ) , size_change ) ) , ) ) restore_collection ( self . _freeze_backup )",if len ( v ) != len ( newv ) :,if v != newv :,93.14018307,FALSE,93.71
3938,"def check_options ( self , expr , evaluation , options ) : for key in options : if key != "" System`SameTest "" : <MASK> evaluation . message ( "" ContainsOnly "" , "" optx "" , Symbol ( key ) ) else : return evaluation . message ( "" ContainsOnly "" , "" optx "" , Symbol ( key ) , expr ) return None",if expr is None :,"if self . _test_has_same_symbol ( expr , key ) :",66.9304704,FALSE,80.94
3939,"def bundle_directory ( self , dirpath ) : """"""Bundle all modules/packages in the given directory."""""" dirpath = os . path . abspath ( dirpath ) for nm in os . listdir ( dirpath ) : nm = _u ( nm ) if nm . startswith ( "" . "" ) : continue itempath = os . path . join ( dirpath , nm ) if os . path . isdir ( itempath ) : <MASK> self . bundle_package ( itempath ) elif nm . endswith ( "" .py "" ) : self . bundle_module ( itempath )","if os . path . exists ( os . path . join ( itempath , ""__init__.py"" ) ) :","if nm . endswith ( "".py"" ) :",85.72764409,FALSE,83.18
3940,"def _read_block ( self , size ) : if self . _file_end is not None : max_size = self . _file_end - self . _file . tell ( ) <MASK> size = max_size size = max ( min ( size , max_size ) , 0 ) return self . _file . read ( size )",if size == - 1 :,if size == 0 :,92.04943348,FALSE,95.41
3941,"def question_mark ( self ) : """"""Shows help for this command and it's sub-commands."""""" ret = [ ] if self . param_help_msg or len ( self . subcommands ) == 0 : ret . append ( self . _quick_help ( ) ) if len ( self . subcommands ) > 0 : for k , _ in sorted ( self . subcommands . items ( ) ) : command_path , param_help , cmd_help = self . _instantiate_subcommand ( k ) . _quick_help ( nested = True ) <MASK> ret . append ( ( command_path , param_help , cmd_help ) ) return ( CommandsResponse ( STATUS_OK , self . help_formatter ( ret ) ) , self . __class__ )",if command_path or param_help or cmd_help :,if cmd_help :,93.73261553,FALSE,94.39
3942,"def list_domains ( self , r53 , * * kwargs ) : marker = None domains = [ ] while True : if marker : response = self . wrap_aws_rate_limited_call ( r53 . list_domains ( Marker = marker ) ) else : response = self . wrap_aws_rate_limited_call ( r53 . list_domains ) for domain in response . get ( "" Domains "" ) : domains . append ( domain ) <MASK> marker = response . get ( "" NextPageMarker "" ) else : break return domains","if response . get ( ""NextPageMarker"" ) :","if response . get ( ""NextPageMarker"" ) :",100,TRUE,100
3943,"def writer ( stream , items ) : sep = "" "" for item in items : stream . write ( sep ) sep = ""   "" if not isinstance ( item , str ) : item = str ( item ) if not PY3K : <MASK> item = str ( item ) stream . write ( item ) stream . write ( "" \n "" )","if not isinstance ( item , unicode ) :","if not isinstance ( item , ( list , tuple ) ) :",95.81112034,FALSE,91.68
3944,"def f ( view , s ) : if mode == modes . INTERNAL_NORMAL : view . run_command ( "" toggle_comment "" ) <MASK> pt = utils . next_non_white_space_char ( view , s . a , white_space = ""   \t "" ) else : pt = utils . next_non_white_space_char ( view , self . view . line ( s . a ) . a , white_space = ""   \t "" ) return R ( pt , pt ) return s","if utils . row_at ( self . view , s . a ) != utils . row_at ( self . view , self . view . size ( ) ) :","if view . get_option ( ""white_space"" ) :",79.90244294,FALSE,75.83
3945,"def _parse_timestamp ( value ) : if value : match = _TIMESTAMP_PATTERN . match ( value ) <MASK> if match . group ( 2 ) : format = "" % Y- % m- %d   % H: % M: % S. %f "" # use the pattern to truncate the value value = match . group ( ) else : format = "" % Y- % m- %d   % H: % M: % S "" value = datetime . datetime . strptime ( value , format ) else : raise Exception ( ' Cannot convert  "" {} ""  into a datetime ' . format ( value ) ) else : value = None return value",if match :,if match :,100,TRUE,100
3946,"def _compute_log_r ( model_trace , guide_trace ) : log_r = MultiFrameTensor ( ) stacks = get_plate_stacks ( model_trace ) for name , model_site in model_trace . nodes . items ( ) : if model_site [ "" type "" ] == "" sample "" : log_r_term = model_site [ "" log_prob "" ] <MASK> log_r_term = log_r_term - guide_trace . nodes [ name ] [ "" log_prob "" ] log_r . add ( ( stacks [ name ] , log_r_term . detach ( ) ) ) return log_r","if not model_site [ ""is_observed"" ] :","if isinstance ( log_r_term , np . ndarray ) :",86.62539944,FALSE,91.34
3947,"def get_translationproject ( self ) : """"""returns the translation project belonging to this directory."""""" if self . is_language ( ) or self . is_project ( ) : return None else : <MASK> return self . translationproject else : aux_dir = self while not aux_dir . is_translationproject ( ) and aux_dir . parent is not None : aux_dir = aux_dir . parent return aux_dir . translationproject",if self . is_translationproject ( ) :,if self . is_language ( ) :,98.22149757,FALSE,97.44
3948,"def get_hosted_content ( ) : try : scheme , rest = target . split ( "" :// "" , 1 ) prefix , host_and_port = rest . split ( "" .interactivetool. "" ) faked_host = rest <MASK> faked_host = rest . split ( "" / "" , 1 ) [ 0 ] url = "" %s :// %s "" % ( scheme , host_and_port ) response = requests . get ( url , timeout = 1 , headers = { "" Host "" : faked_host } ) return response . text except Exception as e : print ( e ) return None","if ""/"" in rest :","if prefix == ""http"" :",93.99839264,FALSE,94.73
3949,"def install ( self ) : log . info ( self . openssl_cli ) if not self . has_openssl or self . args . force : <MASK> self . _download_src ( ) else : log . debug ( "" Already has src  {} "" . format ( self . src_file ) ) self . _unpack_src ( ) self . _build_src ( ) self . _make_install ( ) else : log . info ( "" Already has installation  {} "" . format ( self . install_dir ) ) # validate installation version = self . openssl_version if self . version not in version : raise ValueError ( version )",if not self . has_src :,if self . src_file is None :,93.52036325,FALSE,94.73
3950,"def format ( self , formatstr ) : pieces = [ ] for i , piece in enumerate ( re_formatchars . split ( force_text ( formatstr ) ) ) : if i % 2 : pieces . append ( force_text ( getattr ( self , piece ) ( ) ) ) <MASK> pieces . append ( re_escaped . sub ( r "" \ 1 "" , piece ) ) return "" "" . join ( pieces )",elif piece :,elif piece :,100,TRUE,100
3951,"def get_current_events_users ( calendar ) : now = timezone . make_aware ( datetime . now ( ) , timezone . get_current_timezone ( ) ) result = [ ] day = Day ( calendar . events . all ( ) , now ) for o in day . get_occurrences ( ) : <MASK> usernames = o . event . title . split ( "" , "" ) for username in usernames : result . append ( User . objects . get ( username = username . strip ( ) ) ) return result",if o . start <= now <= o . end :,if o . event :,88.36454789,FALSE,90.83
3952,"def from_cfn_params ( self , cfn_params ) : """"""Initialize param value by parsing CFN input only if the scheduler is awsbatch."""""" cfn_converter = self . definition . get ( "" cfn_param_mapping "" , None ) if cfn_converter and cfn_params : <MASK> # we have the same CFN input parameters for both spot_price and spot_bid_percentage # so the CFN input could be a float self . value = int ( float ( get_cfn_param ( cfn_params , cfn_converter ) ) ) return self","if get_cfn_param ( cfn_params , ""Scheduler"" ) == ""awsbatch"" :","if get_cfn_param ( cfn_params , ""awsbatch"" ) :",93.62243345,FALSE,94.26
3953,"def onCompletion ( self , text ) : res = [ ] for l in text . split ( "" \n "" ) : <MASK> continue l = l . split ( "" : "" ) if len ( l ) != 2 : continue res . append ( [ l [ 0 ] . strip ( ) , l [ 1 ] . strip ( ) ] ) self . panel . setChapters ( res )",if not l :,if len ( l ) != 2 :,95.41133151,FALSE,90.31
3954,"def update_ranges ( l , i ) : for _range in l : # most common case: extend a range if i == _range [ 0 ] - 1 : _range [ 0 ] = i merge_ranges ( l ) return <MASK> _range [ 1 ] = i merge_ranges ( l ) return # somewhere outside of range proximity l . append ( [ i , i ] ) l . sort ( key = lambda x : x [ 0 ] )",elif i == _range [ 1 ] + 1 :,if i > _range [ 1 ] - 1 :,96.08551044,FALSE,92.53
3955,"def process_dollar ( token , state , command_line ) : if not state . is_range_start_line_parsed : <MASK> raise ValueError ( "" bad range:  {0} "" . format ( state . scanner . state . source ) ) command_line . line_range . start . append ( token ) else : if command_line . line_range . end : raise ValueError ( "" bad range:  {0} "" . format ( state . scanner . state . source ) ) command_line . line_range . end . append ( token ) return parse_line_ref , command_line",if command_line . line_range . start :,if command_line . line_range . start :,100,TRUE,100
3956,"def _parse_description ( self , text : str ) : result = dict ( links = [ ] , versions = [ ] ) for line in text . splitlines ( ) : clean = REX_TAG . sub ( "" "" , line . strip ( ) ) <MASK> result [ "" severity "" ] = clean . split ( ) [ 1 ] continue if clean . startswith ( "" Affects: "" ) : result [ "" name "" ] = clean . split ( ) [ 1 ] continue if ""  or higher "" in clean : result [ "" versions "" ] = self . _get_versions ( clean ) result [ "" links "" ] . extend ( REX_LINK . findall ( line ) ) return result","if clean . startswith ( ""Severity:"" ) :","if clean . startswith ( ""Severity:"" ) :",100,TRUE,100
3957,"def apply ( self , chart , grammar ) : for prod in grammar . productions ( empty = True ) : for index in compat . xrange ( chart . num_leaves ( ) + 1 ) : new_edge = TreeEdge . from_production ( prod , index ) <MASK> yield new_edge","if chart . insert ( new_edge , ( ) ) :","if chart . insert ( new_edge , ( ) ) :",100,TRUE,100
3958,"def calc ( self , arg ) : op = arg [ "" op "" ] if op == "" C "" : self . clear ( ) return str ( self . current ) num = decimal . Decimal ( arg [ "" num "" ] ) if self . op : <MASK> self . current + = num elif self . op == "" - "" : self . current - = num elif self . op == "" * "" : self . current * = num elif self . op == "" / "" : self . current / = num self . op = op else : self . op = op self . current = num res = str ( self . current ) if op == "" = "" : self . clear ( ) return res","if self . op == ""+"" :","if self . op == ""+"" :",100,TRUE,100
3959,"def cascade ( self , event = None ) : """"""Cascade all Leo windows."""""" x , y , delta = 50 , 50 , 50 for frame in g . app . windowList : w = frame and frame . top if w : r = w . geometry ( ) # a Qt.Rect # 2011/10/26: Fix bug 823601: cascade-windows fails. w . setGeometry ( QtCore . QRect ( x , y , r . width ( ) , r . height ( ) ) ) # Compute the new offsets. x + = 30 y + = 30 <MASK> x = 10 + delta y = 40 + delta delta + = 10",if x > 200 :,if delta :,98.09314372,FALSE,96.62
3960,"def redirect ( self ) : c = self . c if c . config . getBool ( "" eval-redirect "" ) : self . old_stderr = g . stdErrIsRedirected ( ) self . old_stdout = g . stdOutIsRedirected ( ) <MASK> g . redirectStderr ( ) if not self . old_stdout : g . redirectStdout ( )",if not self . old_stderr :,if not self . old_stderr :,100,TRUE,100
3961,"def on_event ( self , c , button , data ) : if self . rvGestureGrab . get_reveal_child ( ) : <MASK> self . use ( ) elif button == "" Y "" and data [ 0 ] == 0 : self . start_over ( )","if button == ""A"" and data [ 0 ] == 0 :","if button == ""X"" and data [ 0 ] == 1 :",70.46501378,FALSE,92.69
3962,"def __init__ ( self , in_feats , out_feats , norm = "" both "" , bias = True , activation = None ) : super ( DenseGraphConv , self ) . __init__ ( ) self . _in_feats = in_feats self . _out_feats = out_feats self . _norm = norm with self . name_scope ( ) : self . weight = self . params . get ( "" weight "" , shape = ( in_feats , out_feats ) , init = mx . init . Xavier ( magnitude = math . sqrt ( 2.0 ) ) , ) <MASK> self . bias = self . params . get ( "" bias "" , shape = ( out_feats , ) , init = mx . init . Zero ( ) ) else : self . bias = None self . _activation = activation",if bias :,if bias :,100,TRUE,100
3963,"def _import_top_module ( self , name ) : # scan sys.path looking for a location in the filesystem that contains # the module, or an Importer object that can import the module. for item in sys . path : if isinstance ( item , _StringType ) : module = self . fs_imp . import_from_dir ( item , name ) else : module = item . import_top ( name ) <MASK> return module return None",if module :,if module is not None :,73.02768758,FALSE,95.27
3964,"def resolver ( schemas , f ) : if not callable ( f ) : return if not hasattr ( f , "" accepts "" ) : return new_params = [ ] for p in f . accepts : <MASK> new_params . append ( p . resolve ( schemas ) ) else : raise ResolverError ( "" Invalid parameter definition  {0} "" . format ( p ) ) # FIXME: for some reason assigning params (f.accepts = new_params) does not work f . accepts . clear ( ) f . accepts . extend ( new_params )","if isinstance ( p , ( Patch , Ref , Attribute ) ) :","if isinstance ( p , ParameterResolver ) :",91.26010461,FALSE,92.99
3965,"def get_files ( d ) : res = [ ] for p in glob . glob ( os . path . join ( d , "" * "" ) ) : if not p : continue ( pth , fname ) = os . path . split ( p ) if fname == "" output "" : continue <MASK> continue if fname [ - 4 : ] == "" .pyc "" : # ehmm.. no. continue if os . path . isdir ( p ) : get_dir ( p ) else : res . append ( p ) return res","if fname == ""PureMVC_Python_1_0"" :","if fname [ - 3 : ] == "".pyc"" :",96.67187088,FALSE,91.73
3966,"def _addRightnames ( groups , kerning , leftname , rightnames , includeAll = True ) : if leftname in kerning : for rightname in kerning [ leftname ] : if rightname [ 0 ] == "" @ "" : for rightname2 in groups [ rightname ] : rightnames . add ( rightname2 ) <MASK> # TODO: in this case, pick the one rightname that has the highest # ranking in glyphorder break else : rightnames . add ( rightname )",if not includeAll :,"elif rightname [ 0 ] == ""@"" :",71.37018248,FALSE,87.54
3967,"def migrate_Stats ( self ) : for old_obj in self . session_old . query ( self . model_from [ "" Stats "" ] ) : <MASK> self . entries_count [ "" Stats "" ] - = 1 continue new_obj = self . model_to [ "" Stats "" ] ( ) for key in new_obj . __table__ . columns . _data . keys ( ) : if key not in old_obj . __table__ . columns : continue setattr ( new_obj , key , getattr ( old_obj , key ) ) self . session_new . add ( new_obj )",if not old_obj . summary :,"if ""Stats"" in self . entries_count :",84.81199338,FALSE,92.86
3968,"def _readenv ( var , msg ) : match = _ENV_VAR_PAT . match ( var ) if match and match . groups ( ) : envvar = match . groups ( ) [ 0 ] <MASK> value = os . environ [ envvar ] if six . PY2 : value = value . decode ( "" utf8 "" ) return value else : raise InvalidConfigException ( "" {}  - environment variable  ' {} '  not set "" . format ( msg , var ) ) else : raise InvalidConfigException ( "" {}  - environment variable name  ' {} '  does not match pattern  ' {} ' "" . format ( msg , var , _ENV_VAR_PAT_STR ) )",if envvar in os . environ :,if envvar in os . environ :,100,TRUE,100
3969,"def __next__ ( self ) : self . _parse_reset ( ) while True : try : line = next ( self . input_iter ) except StopIteration : # End of input OR exception <MASK> raise Error ( "" newline inside string "" ) raise self . line_num + = 1 if "" \0 "" in line : raise Error ( "" line contains NULL byte "" ) pos = 0 while pos < len ( line ) : pos = self . _parse_process_char ( line , pos ) self . _parse_eol ( ) if self . state == self . START_RECORD : break fields = self . fields self . fields = [ ] return fields",if len ( self . field ) > 0 :,if self . line_num == 0 :,96.92706153,FALSE,94.69
3970,"def createFields ( self ) : while self . current_size < self . size : pos = self . stream . searchBytes ( "" \0 \0 \1 "" , self . current_size , self . current_size + 1024 * 1024 * 8 ) # seek forward by at most 1MB <MASK> padsize = pos - self . current_size if padsize : yield PaddingBytes ( self , "" pad[] "" , padsize / / 8 ) chunk = Chunk ( self , "" chunk[] "" ) try : # force chunk to be processed, so that CustomFragments are complete chunk [ "" content/data "" ] except : pass yield chunk",if pos is not None :,if pos :,97.57298643,FALSE,96.54
3971,"def spew ( ) : seenUID = False start ( ) for part in query : if part . type == "" uid "" : seenUID = True <MASK> yield self . spew_body ( part , id , msg , write , flush ) else : f = getattr ( self , "" spew_ "" + part . type ) yield f ( id , msg , write , flush ) if part is not query [ - 1 ] : space ( ) if uid and not seenUID : space ( ) yield self . spew_uid ( id , msg , write , flush ) finish ( ) flush ( )","if part . type == ""body"" :","elif part . type == ""body"" :",78.67236035,FALSE,98
3972,"def _limit_value ( key , value , config ) : if config [ key ] . get ( "" upper_limit "" ) : limit = config [ key ] [ "" upper_limit "" ] # auto handle datetime if isinstance ( value , datetime ) and isinstance ( limit , timedelta ) : if config [ key ] [ "" inverse "" ] is True : if ( datetime . now ( ) - limit ) > value : value = datetime . now ( ) - limit else : <MASK> value = datetime . now ( ) + limit elif value > limit : value = limit return value",if ( datetime . now ( ) + limit ) < value :,if ( datetime . now ( ) + limit ) < value :,100,TRUE,100
3973,"def _fix_var_naming ( operators , names , mod = "" input "" ) : new_names = [ ] map = { } for op in operators : if mod == "" input "" : iter = op . inputs else : iter = op . outputs for i in iter : for name in names : if i . raw_name == name and name not in map : map [ i . raw_name ] = i . full_name <MASK> break for name in names : new_names . append ( map [ name ] ) return new_names",if len ( map ) == len ( names ) :,if i . name in map :,89.99322722,FALSE,90.91
3974,"def traverse ( tree ) : """"""Generator dropping comment nodes"""""" for entry in tree : # key, values = entry spaceless = [ e for e in entry if not nginxparser . spacey ( e ) ] if spaceless : key = spaceless [ 0 ] values = spaceless [ 1 ] if len ( spaceless ) > 1 else None else : key = values = "" "" if isinstance ( key , list ) : new = copy . deepcopy ( entry ) new [ 1 ] = filter_comments ( values ) yield new else : <MASK> yield spaceless","if key != ""#"" and spaceless :","if key == ""key"" :",97.34129458,FALSE,93.58
3975,"def mergeCombiners ( self , x , y ) : for item in y : <MASK> self . heap . push ( x , item ) else : self . heap . push_pop ( x , item ) return x",if len ( x ) < self . heap_limit :,"if self . heap . is_equal ( x , item ) :",85.42514255,FALSE,79.42
3976,"def test_scatter ( self , harness : primitive_harness . Harness ) : f_name = harness . params [ "" f_lax "" ] . __name__ dtype = harness . params [ "" dtype "" ] if jtu . device_under_test ( ) == "" tpu "" : <MASK> raise unittest . SkipTest ( f "" TODO: complex  { f_name }  on TPU fails in JAX "" ) self . ConvertAndCompare ( harness . dyn_fun , * harness . dyn_args_maker ( self . rng ( ) ) )","if dtype is np . complex64 and f_name in [ ""scatter_min"" , ""scatter_max"" ] :",if dtype != complex :,85.20973522,FALSE,81.54
3977,"def TryMerge ( self , decoder ) : while decoder . avail ( ) > 0 : tag = decoder . getVarInt32 ( ) if tag == TAG_BEGIN_ITEM_GROUP : ( type_id , message ) = Item . Decode ( decoder ) <MASK> self . items [ type_id ] . MergeFrom ( Item ( message ) ) else : self . items [ type_id ] = Item ( message ) continue if tag == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError decoder . skipData ( tag )",if type_id in self . items :,if type_id in self . items :,100,TRUE,100
3978,"def process_continuations ( lines ) : global continuation_pattern olines = [ ] while len ( lines ) != 0 : line = no_comments ( lines [ 0 ] ) line = line . strip ( ) lines . pop ( 0 ) if line == "" "" : continue <MASK> # combine this line with the next line if the next line exists line = continuation_pattern . sub ( "" "" , line ) if len ( lines ) > = 1 : combined_lines = [ line + lines [ 0 ] ] lines . pop ( 0 ) lines = combined_lines + lines continue olines . append ( line ) del lines return olines",if continuation_pattern . search ( line ) :,"if line . startswith ( continuation_pattern . sub ( """" , line ) ) :",94.6262675,FALSE,90.79
3979,"def _getListNextPackagesReadyToBuild ( ) : for pkg in Scheduler . listOfPackagesToBuild : if pkg in Scheduler . listOfPackagesCurrentlyBuilding : continue <MASK> Scheduler . listOfPackagesNextToBuild . put ( ( - Scheduler . _getPriority ( pkg ) , pkg ) ) Scheduler . logger . debug ( "" Adding  "" + pkg + ""  to the schedule list "" )",if constants . rpmCheck or Scheduler . _checkNextPackageIsReadyToBuild ( pkg ) :,if not Scheduler . listOfPackagesNextToBuild . get ( pkg ) :,91.62759622,FALSE,87.84
3980,"def process_signature ( app , what , name , obj , options , signature , return_annotation ) : if signature : # replace Mock function names signature = re . sub ( "" <Mock name= ' ([^ ' ]+) ' .*> "" , "" \ g<1> "" , signature ) signature = re . sub ( "" tensorflow "" , "" tf "" , signature ) # add scope name to layer signatures: if hasattr ( obj , "" use_scope "" ) : if obj . use_scope : signature = signature [ 0 ] + "" variable_scope_name,  "" + signature [ 1 : ] <MASK> signature = signature [ 0 ] + "" [variable_scope_name,]  "" + signature [ 1 : ] # signature: arg list return signature , return_annotation",elif obj . use_scope is None :,elif obj . use_scope_name :,98.03625924,FALSE,97.94
3981,"def find_distribution_modules ( name = __name__ , file = __file__ ) : current_dist_depth = len ( name . split ( "" . "" ) ) - 1 current_dist = os . path . join ( os . path . dirname ( file ) , * ( [ os . pardir ] * current_dist_depth ) ) abs = os . path . abspath ( current_dist ) dist_name = os . path . basename ( abs ) for dirpath , dirnames , filenames in os . walk ( abs ) : package = ( dist_name + dirpath [ len ( abs ) : ] ) . replace ( "" / "" , "" . "" ) <MASK> yield package for filename in filenames : if filename . endswith ( "" .py "" ) and filename != "" __init__.py "" : yield "" . "" . join ( [ package , filename ] ) [ : - 3 ]","if ""__init__.py"" in filenames :","if package not in ( ""__init__.py"" , ""__init__.py"" ) :",96.36704627,FALSE,91.75
3982,"def transform_value ( i , v , * args ) : if i not in converter_functions : # no converter defined on this field, return value as-is return v else : try : return converter_functions [ i ] ( v , * args ) except Exception as e : if failonerror == "" inline "" : return e <MASK> raise e else : return errorvalue",elif failonerror :,"elif failonerror == ""ignore"" :",98.10859522,FALSE,91.68
3983,"def _get_file ( self ) : if self . _file is None : self . _file = SpooledTemporaryFile ( max_size = self . _storage . max_memory_size , suffix = "" .S3Boto3StorageFile "" , dir = setting ( "" FILE_UPLOAD_TEMP_DIR "" ) , ) <MASK> self . _is_dirty = False self . obj . download_fileobj ( self . _file ) self . _file . seek ( 0 ) if self . _storage . gzip and self . obj . content_encoding == "" gzip "" : self . _file = GzipFile ( mode = self . _mode , fileobj = self . _file , mtime = 0.0 ) return self . _file","if ""r"" in self . _mode :",if self . _is_dirty :,74.94746965,FALSE,94.7
3984,"def connect ( self , host , port , timeout ) : fp = Telnet ( ) for i in range ( 50 ) : try : fp . sock = socket . create_connection ( ( host , int ( port ) ) , timeout = int ( timeout ) , source_address = ( "" "" , 1023 - i ) ) break except socket . error as e : <MASK> raise e self . need_handshake = True return TCP_Connection ( fp )","if ( e . errno , e . strerror ) != ( 98 , ""Address already in use"" ) :",if i == 1023 :,70.38478877,FALSE,79.09
3985,"def filtercomments ( source ) : """"""NOT USED: strips trailing comments and put them at the top."""""" trailing_comments = [ ] comment = True while comment : if re . search ( r "" ^ \ s* \ / \ * "" , source ) : comment = source [ 0 , source . index ( "" */ "" ) + 2 ] <MASK> comment = re . search ( r "" ^ \ s* \ / \ / "" , source ) . group ( 0 ) else : comment = None if comment : source = re . sub ( r "" ^ \ s+ "" , "" "" , source [ len ( comment ) : ] ) trailing_comments . append ( comment ) return "" \n "" . join ( trailing_comments ) + source","elif re . search ( r""^\s*\/\/"" , source ) :","elif re . search ( r""^\s*/"" , source ) :",99.1453293,FALSE,97.41
3986,"def yview ( self , mode = None , value = None , units = None ) : if type ( value ) == str : value = float ( value ) if mode is None : return self . vsb . get ( ) elif mode == "" moveto "" : frameHeight = self . innerframe . winfo_reqheight ( ) self . _startY = value * float ( frameHeight ) else : # mode == 'scroll' clipperHeight = self . _clipper . winfo_height ( ) <MASK> jump = int ( clipperHeight * self . _jfraction ) else : jump = clipperHeight self . _startY = self . _startY + value * jump self . reposition ( )","if units == ""units"" :",if self . _jfraction is not None :,97.58652884,FALSE,93.76
3987,"def visit ( stmt ) : """"""Collect information about VTCM buffers and their alignments."""""" if isinstance ( stmt , tvm . tir . AttrStmt ) : if stmt . attr_key == "" storage_scope "" and stmt . value == "" local.vtcm "" : vtcm_buffers . append ( stmt . node ) <MASK> if not stmt . node in alignments : alignments [ stmt . node ] = [ ] alignments [ stmt . node ] . append ( stmt . value )","elif stmt . attr_key == ""storage_alignment"" :","elif stmt . attr_key == ""alignments"" :",98.561981,FALSE,95.92
3988,"def cost ( P ) : # wda loss loss_b = 0 loss_w = 0 for i , xi in enumerate ( xc ) : xi = np . dot ( xi , P ) for j , xj in enumerate ( xc [ i : ] ) : xj = np . dot ( xj , P ) M = dist ( xi , xj ) G = sinkhorn ( wc [ i ] , wc [ j + i ] , M , reg , k ) <MASK> loss_w + = np . sum ( G * M ) else : loss_b + = np . sum ( G * M ) # loss inversed because minimization return loss_w / loss_b",if j == 0 :,if k == 1 :,73.31715475,FALSE,96.55
3989,"def __init__ ( self , comm , in_channels , out_channels , ksize , pad = 1 ) : super ( Block , self ) . __init__ ( ) with self . init_scope ( ) : <MASK> self . conv = ParallelConvolution2D ( comm , in_channels , out_channels , ksize , pad = pad , nobias = True ) else : self . conv = chainer . links . Convolution2D ( in_channels , out_channels , ksize , pad = pad , nobias = True ) self . bn = L . BatchNormalization ( out_channels )",if comm . size <= in_channels :,"if comm . get_channels ( ) . get ( ""parallel"" ) :",82.76064168,FALSE,90.28
3990,"def halfMultipartScore ( nzb_name ) : try : wrong_found = 0 for nr in [ 1 , 2 , 3 , 4 , 5 , "" i "" , "" ii "" , "" iii "" , "" iv "" , "" v "" , "" a "" , "" b "" , "" c "" , "" d "" , "" e "" ] : for wrong in [ "" cd "" , "" part "" , "" dis "" , "" disc "" , "" dvd "" ] : <MASK> wrong_found + = 1 if wrong_found == 1 : return - 30 return 0 except : log . error ( "" Failed doing halfMultipartScore:  %s "" , traceback . format_exc ( ) ) return 0","if ""%s%s"" % ( wrong , nr ) in nzb_name . lower ( ) :",if nzb_name in wrong and nzb_name in wrong :,89.36442276,FALSE,87.64
3991,"def should_include ( service ) : for f in filt : <MASK> state = filt [ f ] containers = project . containers ( [ service . name ] , stopped = True ) if not has_container_with_state ( containers , state ) : return False elif f == "" source "" : source = filt [ f ] if source == "" image "" or source == "" build "" : if source not in service . options : return False else : raise UserError ( "" Invalid value for source filter:  %s "" % source ) else : raise UserError ( "" Invalid filter:  %s "" % f ) return True","if f == ""status"" :","if f == ""state"" :",98.92491262,FALSE,98.1
3992,"def get_blob_type_declaration_sql ( self , column ) : length = column . get ( "" length "" ) if length : if length < = self . LENGTH_LIMIT_TINYBLOB : return "" TINYBLOB "" <MASK> return "" BLOB "" if length < = self . LENGTH_LIMIT_MEDIUMBLOB : return "" MEDIUMBLOB "" return "" LONGBLOB """,if length <= self . LENGTH_LIMIT_BLOB :,if length <= self . LENGTH_LIMIT_BLOB :,100,TRUE,100
3993,"def click_outside ( event ) : if event not in d : x , y , z = self . blockFaceUnderCursor [ 0 ] <MASK> y = 64 y + = 3 gotoPanel . X , gotoPanel . Y , gotoPanel . Z = x , y , z if event . num_clicks == 2 : d . dismiss ( "" Goto "" )",if y == 0 :,if y == 32 :,73.12108368,FALSE,96.51
3994,"def check_related_active_jobs ( self , obj ) : active_jobs = obj . get_active_jobs ( ) if len ( active_jobs ) > 0 : raise ActiveJobConflict ( active_jobs ) time_cutoff = now ( ) - dateutil . relativedelta . relativedelta ( minutes = 1 ) recent_jobs = obj . _get_related_jobs ( ) . filter ( finished__gte = time_cutoff ) for unified_job in recent_jobs . get_real_instances ( ) : <MASK> raise PermissionDenied ( _ ( "" Related job  {}  is still processing events. "" ) . format ( unified_job . log_format ) )",if not unified_job . event_processing_finished :,if unified_job . processing_events :,95.14128156,FALSE,94.25
3995,"def run ( self ) : self . alive = True if _log . isEnabledFor ( _DEBUG ) : _log . debug ( "" started "" ) while self . alive : task = self . queue . get ( ) <MASK> function , args , kwargs = task assert function try : function ( * args , * * kwargs ) except : _log . exception ( "" calling  %s "" , function ) if _log . isEnabledFor ( _DEBUG ) : _log . debug ( "" stopped "" )",if task :,if task :,100,TRUE,100
3996,"def update_sysconfig_file ( fn , adjustments , allow_empty = False ) : if not adjustments : return ( exists , contents ) = read_sysconfig_file ( fn ) updated_am = 0 for ( k , v ) in adjustments . items ( ) : if v is None : continue v = str ( v ) <MASK> continue contents [ k ] = v updated_am + = 1 if updated_am : lines = [ str ( contents ) , ] if not exists : lines . insert ( 0 , util . make_header ( ) ) util . write_file ( fn , "" \n "" . join ( lines ) + "" \n "" , 0o644 )",if len ( v ) == 0 and not allow_empty :,"if allow_empty and v == """" :",94.20170356,FALSE,92.76
3997,"def wrapper ( # type: ignore self : RequestHandler , * args , * * kwargs ) - > Optional [ Awaitable [ None ] ] : if self . request . path . endswith ( "" / "" ) : if self . request . method in ( "" GET "" , "" HEAD "" ) : uri = self . request . path . rstrip ( "" / "" ) <MASK> # don't try to redirect '/' to '' if self . request . query : uri + = "" ? "" + self . request . query self . redirect ( uri , permanent = True ) return None else : raise HTTPError ( 404 ) return method ( self , * args , * * kwargs )",if uri :,if uri :,75,TRUE,100
3998,def output_handles_from_execution_plan ( execution_plan ) : output_handles_for_current_run = set ( ) for step_level in execution_plan . execution_step_levels ( ) : for step in step_level : for step_input in step . step_inputs : <MASK> output_handles_for_current_run . update ( step_input . source_handles ) return output_handles_for_current_run,if step_input . source_handles :,"if isinstance ( step_input , OutputStepInput ) :",91.46281625,FALSE,92.14
3999,"def _read_value ( self , item ) : item = _normalize_path ( item ) if item in self . _store : <MASK> del self . _store [ item ] raise KeyError ( item ) return PathResult ( item , value = self . _store [ item ] ) elif item in self . _children : return PathResult ( item , dir = True ) else : raise KeyError ( item )",if item in self . _expire_time and self . _expire_time [ item ] < datetime . now ( ) :,if item in self . _children :,84.10725779,FALSE,79.12
4000,"def _line_ranges ( statements , lines ) : """"""Produce a list of ranges for `format_lines`."""""" statements = sorted ( statements ) lines = sorted ( lines ) pairs = [ ] start = None lidx = 0 for stmt in statements : if lidx > = len ( lines ) : break if stmt == lines [ lidx ] : lidx + = 1 <MASK> start = stmt end = stmt elif start : pairs . append ( ( start , end ) ) start = None if start : pairs . append ( ( start , end ) ) return pairs",if not start :,if start is None :,96.16559299,FALSE,96.42
4001,"def _update_help_obj_params ( help_obj , data_params , params_equal , attr_key_tups ) : loaded_params = [ ] for param_obj in help_obj . parameters : loaded_param = next ( ( n for n in data_params if params_equal ( param_obj , n ) ) , None ) <MASK> BaseHelpLoader . _update_obj_from_data_dict ( param_obj , loaded_param , attr_key_tups ) loaded_params . append ( param_obj ) help_obj . parameters = loaded_params",if loaded_param :,if loaded_param is not None :,95.29554876,FALSE,96.46
4002,"def __get_ratio ( self ) : """"""Return splitter ratio of the main splitter."""""" c = self . c free_layout = c . free_layout if free_layout : w = free_layout . get_main_splitter ( ) <MASK> aList = w . sizes ( ) if len ( aList ) == 2 : n1 , n2 = aList # 2017/06/07: guard against division by zero. ratio = 0.5 if n1 + n2 == 0 else float ( n1 ) / float ( n1 + n2 ) return ratio return 0.5",if w :,if w :,100,TRUE,100
4003,"def _check_required_env_variables ( vars ) : for var in vars : <MASK> self . tc . logger . error ( "" %s  is not set. Did you forget to source your build environment setup script? "" % var ) raise OEQAPreRun",if not os . environ . get ( var ) :,"if not os . environ . get ( var ) . get ( ""ENV_NAME"" )",64.45463428,FALSE,84.39
4004,"def clean_indexes ( ) : for coll_name in mongo . collection_types . keys ( ) : coll = mongo . get_collection ( coll_name ) indexes = coll_indexes [ coll_name ] try : for index in coll . list_indexes ( ) : name = index [ "" name "" ] <MASK> continue coll . drop_index ( name ) except pymongo . errors . OperationFailure : pass","if name == ""_id"" or name == ""_id_"" or name in indexes :",if name in indexes :,82.44518916,FALSE,81.51
4005,"def _compare_dirs ( self , dir1 , dir2 ) : # check that dir1 and dir2 are equivalent, # return the diff diff = [ ] for root , dirs , files in os . walk ( dir1 ) : for file_ in files : path = os . path . join ( root , file_ ) target_path = os . path . join ( dir2 , os . path . split ( path ) [ - 1 ] ) <MASK> diff . append ( file_ ) return diff",if not os . path . exists ( target_path ) :,"if self . _compare_path ( target_path , dir1 ) :",70.52757798,FALSE,90.43
4006,"def load_state_dict ( self , state_dict , strict = True ) : """"""Customized load."""""" self . language_model . load_state_dict ( state_dict [ self . _language_model_key ] , strict = strict ) if mpu . is_pipeline_last_stage ( ) : <MASK> self . multichoice_head . load_state_dict ( state_dict [ self . _multichoice_head_key ] , strict = strict ) else : print_rank_last ( "" ***WARNING*** could not find  {}  in the checkpoint,  "" "" initializing to random "" . format ( self . _multichoice_head_key ) )",if self . _multichoice_head_key in state_dict :,if self . _multichoice_head_key in state_dict :,100,TRUE,100
4007,"def _parse_timedelta ( self , value ) : try : sum = datetime . timedelta ( ) start = 0 while start < len ( value ) : m = self . _TIMEDELTA_PATTERN . match ( value , start ) <MASK> raise Exception ( ) num = float ( m . group ( 1 ) ) units = m . group ( 2 ) or "" seconds "" units = self . _TIMEDELTA_ABBREV_DICT . get ( units , units ) sum + = datetime . timedelta ( * * { units : num } ) start = m . end ( ) return sum except : raise",if not m :,if not m :,100,TRUE,100
4008,"def SetChildMenuBar ( self , pChild ) : if not pChild : # No Child, set Our menu bar back. if self . _pMyMenuBar : self . SetMenuBar ( self . _pMyMenuBar ) else : self . SetMenuBar ( self . GetMenuBar ( ) ) # Make sure we know our menu bar is in use self . _pMyMenuBar = None else : if pChild . GetMenuBar ( ) is None : return # Do we need to save the current bar? <MASK> self . _pMyMenuBar = self . GetMenuBar ( ) self . SetMenuBar ( pChild . GetMenuBar ( ) )",if self . _pMyMenuBar is None :,if self . GetMenuBar ( ) != self . GetMenuBar ( ) :,96.37569688,FALSE,90.44
4009,"def init_weights ( self ) : """"""Initialize weights of the head."""""" # retinanet_bias_init bias_cls = bias_init_with_prob ( 0.01 ) normal_init ( self . conv_reg , std = 0.01 ) normal_init ( self . conv_centerness , std = 0.01 ) normal_init ( self . conv_cls , std = 0.01 , bias = bias_cls ) for branch in [ self . cls_convs , self . reg_convs ] : for module in branch . modules ( ) : <MASK> caffe2_xavier_init ( module . conv )","if isinstance ( module , ConvModule ) and isinstance ( module . conv , nn . Conv2d ) :","if isinstance ( module , caffe2 . Conv2D ) :",68.14698681,FALSE,90.49
4010,"def handle_exception ( self , e , result ) : for k in sorted ( result . thrift_spec ) : if result . thrift_spec [ k ] [ 1 ] == "" success "" : continue _ , exc_name , exc_cls , _ = result . thrift_spec [ k ] <MASK> setattr ( result , exc_name , e ) break else : raise","if isinstance ( e , exc_cls ) :",if exc_cls is not None :,63.74285966,FALSE,90.58
4011,"def scripts ( self ) : application_root = current_app . config . get ( "" APPLICATION_ROOT "" ) subdir = application_root != "" / "" scripts = [ ] for script in get_registered_scripts ( ) : <MASK> scripts . append ( f ' <script defer src= "" { script } "" ></script> ' ) elif subdir : scripts . append ( f ' <script defer src= "" { application_root } / { script } "" ></script> ' ) else : scripts . append ( f ' <script defer src= "" { script } "" ></script> ' ) return markup ( "" \n "" . join ( scripts ) )","if script . startswith ( ""http"" ) :","if script . startswith ( ""/"" ) :",98.91120752,FALSE,98.33
4012,"def test_related_objects_local ( self ) : result_key = "" get_all_related_objects_with_model_local "" for model , expected in TEST_RESULTS [ result_key ] . items ( ) : objects = [ ( field , self . _model ( model , field ) ) for field in model . _meta . get_fields ( include_parents = False ) <MASK> ] self . assertEqual ( sorted ( self . _map_related_query_names ( objects ) , key = self . key_name ) , sorted ( expected , key = self . key_name ) , )",if field . auto_created and not field . concrete,if field . is_related and field . is_related and field . is_related,86.68741826,FALSE,89.34
4013,"def setTestOutcome ( self , event ) : """"""Update outcome, exc_info and reason based on configured mappings"""""" if event . exc_info : ec , ev , tb = event . exc_info classname = ec . __name__ if classname in self . treatAsFail : short , long_ = self . labels ( classname ) self . _setOutcome ( event , "" failed "" , short , long_ ) <MASK> short , long_ = self . labels ( classname , upper = False ) self . _setOutcome ( event , "" skipped "" , short , "" %s :  ' %s ' "" % ( long_ , ev ) , str ( ev ) )",elif classname in self . treatAsSkip :,elif classname in self . treatAsSkip :,100,TRUE,100
4014,"def small_count ( v ) : if not v : return 0 z = [ ( 1000000000 , _ ( "" b "" ) ) , ( 1000000 , _ ( "" m "" ) ) , ( 1000 , _ ( "" k "" ) ) , ] v = int ( v ) for x , y in z : o , p = divmod ( v , x ) if o : <MASK> return "" %d %s "" % ( o , y ) return "" %.1f %s "" % ( v / float ( x ) , y ) return v",if len ( str ( o ) ) > 2 or not p :,if p :,89.83492024,FALSE,89.58
4015,"def __read ( self , n ) : if self . _read_watcher is None : raise UnsupportedOperation ( "" read "" ) while 1 : try : return _read ( self . _fileno , n ) except ( IOError , OSError ) as ex : <MASK> raise wait_on_watcher ( self . _read_watcher , None , None , self . hub )",if ex . args [ 0 ] not in ignored_errors :,if ex . args [ 0 ] != errno . EINTR :,93.13599528,FALSE,92.37
4016,"def locked ( self ) : inputfiles = set ( self . all_inputfiles ( ) ) outputfiles = set ( self . all_outputfiles ( ) ) if os . path . exists ( self . _lockdir ) : for lockfile in self . _locks ( "" input "" ) : with open ( lockfile ) as lock : for f in lock : f = f . strip ( ) if f in outputfiles : return True for lockfile in self . _locks ( "" output "" ) : with open ( lockfile ) as lock : for f in lock : f = f . strip ( ) <MASK> return True return False",if f in outputfiles or f in inputfiles :,if f in inputfiles :,95.33730777,FALSE,96.65
4017,"def _flags_to_int ( flags ) : # Note, that order does not matter, libev has its own predefined order if not flags : return 0 if isinstance ( flags , integer_types ) : return flags result = 0 try : <MASK> flags = flags . split ( "" , "" ) for value in flags : value = value . strip ( ) . lower ( ) if value : result | = _flags_str2int [ value ] except KeyError as ex : raise ValueError ( "" Invalid backend or flag:  %s \n Possible values:  %s "" % ( ex , "" ,  "" . join ( sorted ( _flags_str2int . keys ( ) ) ) ) ) return result","if isinstance ( flags , basestring ) :","if "","" in flags :",72.1636901,FALSE,95.14
4018,"def setFg ( self , colour , override = False ) : if not self . ttkFlag : self . containerStack [ - 1 ] [ "" fg "" ] = colour gui . SET_WIDGET_FG ( self . _getContainerProperty ( "" container "" ) , colour , override ) for child in self . _getContainerProperty ( "" container "" ) . winfo_children ( ) : <MASK> gui . SET_WIDGET_FG ( child , colour , override ) else : gui . trace ( "" In ttk mode - trying to set FG to  %s "" , colour ) self . ttkStyle . configure ( "" TLabel "" , foreground = colour ) self . ttkStyle . configure ( "" TFrame "" , foreground = colour )",if not self . _isWidgetContainer ( child ) :,"if child . get ( ""fg"" ) == colour :",93.93787759,FALSE,92.22
4019,"def find_scintilla_constants ( f ) : lexers = [ ] states = [ ] for name in f . order : v = f . features [ name ] if v [ "" Category "" ] != "" Deprecated "" : if v [ "" FeatureType "" ] == "" val "" : if name . startswith ( "" SCE_ "" ) : states . append ( ( name , v [ "" Value "" ] ) ) <MASK> lexers . append ( ( name , v [ "" Value "" ] ) ) return ( lexers , states )","elif name . startswith ( ""SCLEX_"" ) :","elif name . startswith ( ""LN_"" ) :",98.81963958,FALSE,97.82
4020,"def extract_error_message ( response : requests . Response ) : if response . content : try : content = json . loads ( response . content ) <MASK> return content [ "" message "" ] except : logging . debug ( f "" Failed to parse the response content:  { response . content } "" ) return response . reason","if ""message"" in content :","if ""message"" in content :",100,TRUE,100
4021,"def canvas_size ( self ) : """"""Return the width and height for this sprite canvas"""""" width = height = 0 for image in self . images : x = image . x + image . absolute_width y = image . y + image . absolute_height <MASK> width = x if height < y : height = y return round_up ( width ) , round_up ( height )",if width < x :,if width < x :,100,TRUE,100
4022,"def _load_widgets ( self ) : logger . info ( "" Loading plugins preferences widgets "" ) # Collect the preferences widget for each active plugin for plugin in self . plugin_manager . get_active_plugins ( ) : plugin_name = plugin . metadata . get ( "" name "" ) try : preferences_widget = plugin . get_preferences_widget ( ) <MASK> self . _tabs . addTab ( preferences_widget , plugin_name ) except Exception as reason : logger . error ( "" Unable to add the preferences widget ( %s ):  %s "" , plugin_name , reason ) continue",if preferences_widget :,if preferences_widget :,75,TRUE,100
4023,"def clean_objects ( string , common_attributes ) : """"""Return object and attribute lists"""""" string = clean_string ( string ) words = string . split ( ) if len ( words ) > 1 : prefix_words_are_adj = True for att in words [ : - 1 ] : if att not in common_attributes : prefix_words_are_adj = False <MASK> return words [ - 1 : ] , words [ : - 1 ] else : return [ string ] , [ ] else : return [ string ] , [ ]",if prefix_words_are_adj :,if prefix_words_are_adj :,100,TRUE,100
4024,"def _reader ( ) : if shuffle : random . shuffle ( file_list ) while True : for fn in file_list : for line in open ( fn , "" r "" ) : yield self . _process_line ( line ) <MASK> break",if not cycle :,if not self . _is_file ( ) :,66.8714459,FALSE,84.41
4025,"def load ( weights , model , K , fsz , dil ) : index = 0 layers = model . layers for layer in layers . _layers : <MASK> if layer . W . shape == weights [ index ] . shape : layer . W [ : ] = weights [ index ] else : layer . W [ : ] = dilate ( weights [ index ] , K , fsz , dil ) index + = 1","if hasattr ( layer , ""W"" ) :",if layer . W . shape == weights [ index ] . shape :,90.05013844,FALSE,85.29
4026,"def upgrade ( migrate_engine ) : print ( __doc__ ) metadata . bind = migrate_engine liftoverjobs = dict ( ) jobs = context . query ( DeferredJob ) . filter_by ( plugin = "" LiftOverTransferPlugin "" ) . all ( ) for job in jobs : <MASK> liftoverjobs [ job . params [ "" parentjob "" ] ] = [ ] liftoverjobs [ job . params [ "" parentjob "" ] ] . append ( job . id ) for parent in liftoverjobs : lifts = liftoverjobs [ parent ] deferred = context . query ( DeferredJob ) . filter_by ( id = parent ) . first ( ) deferred . params [ "" liftover "" ] = lifts context . flush ( )","if job . params [ ""parentjob"" ] not in liftoverjobs :","if job . params [ ""parentjob"" ] not in liftoverjobs :",100,TRUE,100
4027,"def get_refs ( self , recursive = False ) : """""":see: AbstractExpression.get_refs()"""""" if recursive : conds_refs = self . refs + sum ( ( c . get_refs ( True ) for c in self . conds ) , [ ] ) <MASK> conds_refs . extend ( self . consequent . get_refs ( True ) ) return conds_refs else : return self . refs",if self . consequent :,if self . consequent :,100,TRUE,100
4028,"def _parse ( self , engine ) : """"""Parse the layer."""""" if isinstance ( self . args , dict ) : <MASK> self . axis = engine . evaluate ( self . args [ "" axis "" ] , recursive = True ) if not isinstance ( self . axis , int ) : raise ParsingError ( ' "" axis ""  must be an integer. ' ) if "" momentum "" in self . args : self . momentum = engine . evaluate ( self . args [ "" momentum "" ] , recursive = True ) if not isinstance ( self . momentum , ( int , float ) ) : raise ParsingError ( ' "" momentum ""  must be numeric. ' )","if ""axis"" in self . args :","if ""axis"" in self . args :",100,TRUE,100
4029,"def CountMatches ( pat , predicate ) : num_matches = 0 for i in xrange ( 256 ) : b = chr ( i ) m = pat . match ( b ) left = bool ( m ) right = predicate ( i ) if left != right : self . fail ( "" i =  %d , b =  %r , match:  %s , predicate:  %s "" % ( i , b , left , right ) ) <MASK> num_matches + = 1 return num_matches",if m :,if m :,100,TRUE,100
4030,"def __new__ ( cls , * args , * * kwargs ) : if len ( args ) == 1 : if len ( kwargs ) : raise ValueError ( "" You can either use  {}  with one positional argument or with keyword arguments, not both. "" . format ( cls . __name__ ) ) if not args [ 0 ] : return super ( ) . __new__ ( cls ) <MASK> return cls return super ( ) . __new__ ( cls , * args , * * kwargs )","if isinstance ( args [ 0 ] , cls ) :",if args [ 0 ] == kwargs [ 0 ] :,91.96967097,FALSE,92.43
4031,"def concatenateCharacterTokens ( tokens ) : pendingCharacters = [ ] for token in tokens : type = token [ "" type "" ] if type in ( "" Characters "" , "" SpaceCharacters "" ) : pendingCharacters . append ( token [ "" data "" ] ) else : <MASK> yield { "" type "" : "" Characters "" , "" data "" : "" "" . join ( pendingCharacters ) } pendingCharacters = [ ] yield token <MASK> yield { "" type "" : "" Characters "" , "" data "" : "" "" . join ( pendingCharacters ) }",if pendingCharacters :,if pendingCharacters :,100,TRUE,100
4032,"def get_ranges_from_func_set ( support_set ) : pos_start = 0 pos_end = 0 ranges = [ ] for pos , func in enumerate ( network . function ) : if func . type in support_set : pos_end = pos else : <MASK> ranges . append ( ( pos_start , pos_end ) ) pos_start = pos + 1 <MASK> ranges . append ( ( pos_start , pos_end ) ) return ranges",if pos_end >= pos_start :,"if func . type == ""function"" :",86.47502351,FALSE,83.63
4033,"def _visit ( self , func ) : fname = func [ 0 ] if fname in self . _flags : <MASK> logger . critical ( "" Fatal error! network ins not Dag. "" ) import sys sys . exit ( - 1 ) else : return else : if fname not in self . _flags : self . _flags [ fname ] = 1 for output in func [ 3 ] : for f in self . _orig : for input in f [ 2 ] : if output == input : self . _visit ( f ) self . _flags [ fname ] = 2 self . _sorted . insert ( 0 , func )",if self . _flags [ fname ] == 1 :,if fname not in self . _sorted :,92.95867832,FALSE,92.87
4034,"def graph_merge_softmax_with_crossentropy_softmax ( node ) : if node . op == softmax_with_bias : x , b = node . inputs for x_client in x . clients : <MASK> big_client = x_client [ 0 ] if big_client in [ b_client [ 0 ] for b_client in b . clients ] : xx , bb , ll = big_client . inputs mergeable_client = big_client . op ( x , b , ll ) copy_stack_trace ( node . outputs [ 0 ] , mergeable_client [ 1 ] ) return [ mergeable_client [ 1 ] ]",if x_client [ 0 ] . op == crossentropy_softmax_argmax_1hot_with_bias :,if x_client in [ x_client [ 0 ] for x_client in b .,91.51168303,FALSE,89.06
4035,"def confidence ( self ) : if self . bbox : # Units are measured in Kilometers distance = Distance ( self . northeast , self . southwest , units = "" km "" ) for score , maximum in [ ( 10 , 0.25 ) , ( 9 , 0.5 ) , ( 8 , 1 ) , ( 7 , 5 ) , ( 6 , 7.5 ) , ( 5 , 10 ) , ( 4 , 15 ) , ( 3 , 20 ) , ( 2 , 25 ) , ] : if distance < maximum : return score <MASK> return 1 # Cannot determine score return 0",if distance >= 25 :,if distance > maximum :,98.50426004,FALSE,96.99
4036,"def OnListEndLabelEdit ( self , std , extra ) : item = extra [ 0 ] text = item [ 4 ] if text is None : return item_id = self . GetItem ( item [ 0 ] ) [ 6 ] from bdb import Breakpoint for bplist in Breakpoint . bplist . itervalues ( ) : for bp in bplist : if id ( bp ) == item_id : <MASK> text = None bp . cond = text break self . RespondDebuggerData ( )","if text . strip ( ) . lower ( ) == ""none"" :",if bp . cond == text :,89.49258845,FALSE,85.99
4037,"def _handle_autocomplete_request_for_text ( text ) : if not hasattr ( text , "" autocompleter "" ) : <MASK> if isinstance ( text , CodeViewText ) : text . autocompleter = Completer ( text ) elif isinstance ( text , ShellText ) : text . autocompleter = ShellCompleter ( text ) text . bind ( "" <1> "" , text . autocompleter . on_text_click ) else : return text . autocompleter . handle_autocomplete_request ( )","if isinstance ( text , ( CodeViewText , ShellText ) ) and text . is_python_text ( ) :",if text . autocompleter is None :,86.67521728,FALSE,81.04
4038,"def visit_Macro ( self , node , frame ) : macro_frame , macro_ref = self . macro_body ( node , frame ) self . newline ( ) if frame . toplevel : <MASK> self . write ( "" context.exported_vars.add( %r ) "" % node . name ) ref = frame . symbols . ref ( node . name ) self . writeline ( "" context.vars[ %r ] =  "" % node . name ) self . write ( "" %s  =  "" % frame . symbols . ref ( node . name ) ) self . macro_def ( macro_ref , macro_frame )","if not node . name . startswith ( ""_"" ) :",if not frame . is_exported :,89.98275261,FALSE,92.07
4039,"def execute ( cls , ctx , op ) : try : pd . set_option ( "" mode.use_inf_as_na "" , op . use_inf_as_na ) <MASK> return cls . _execute_map ( ctx , op ) else : return cls . _execute_combine ( ctx , op ) finally : pd . reset_option ( "" mode.use_inf_as_na "" )",if op . stage == OperandStage . map :,"if ctx . name == ""map"" :",61.76819058,FALSE,91.19
4040,"def ranges ( self , start , end ) : try : iterators = [ i . ranges ( start , end ) for i in self . range_iterators ] starts , ends , values = zip ( * [ next ( i ) for i in iterators ] ) starts = list ( starts ) ends = list ( ends ) values = list ( values ) while start < end : min_end = min ( ends ) yield start , min_end , values start = min_end for i , iterator in enumerate ( iterators ) : <MASK> starts [ i ] , ends [ i ] , values [ i ] = next ( iterator ) except StopIteration : return",if ends [ i ] == min_end :,if start < i < start :,93.8893033,FALSE,92.57
4041,"def get_explanation ( self , spec ) : """"""Expand an explanation."""""" if spec : try : a = self . dns_txt ( spec ) <MASK> return str ( self . expand ( to_ascii ( a [ 0 ] ) , stripdot = False ) ) except PermError : # RFC4408 6.2/4 syntax errors cause exp= to be ignored if self . strict > 1 : raise # but report in harsh mode for record checking tools pass elif self . strict > 1 : raise PermError ( "" Empty domain-spec on exp= "" ) # RFC4408 6.2/4 empty domain spec is ignored # (unless you give precedence to the grammar). return None",if len ( a ) == 1 :,if len ( a ) == 1 :,100,TRUE,100
4042,"def iter_fields ( node , * , include_meta = True , exclude_unset = False ) : exclude_meta = not include_meta for field_name , field in node . _fields . items ( ) : if exclude_meta and field . meta : continue field_val = getattr ( node , field_name , _marker ) if field_val is _marker : continue <MASK> if callable ( field . default ) : default = field . default ( ) else : default = field . default if field_val == default : continue yield field_name , field_val",if exclude_unset :,if field_val is _unset :,95.42904426,FALSE,95.6
4043,"def __setattr__ ( self , name , value ) : try : field = self . _meta . get_field ( name ) <MASK> value = value [ : field . max_length ] except models . fields . FieldDoesNotExist : pass # This happens with foreign keys. super . __setattr__ ( self , name , value )","if type ( field ) in [ models . CharField , models . TextField ] and type ( value ) == str :",if field . max_length :,50.72198294,FALSE,73.46
4044,"def create_child ( self , value = None , _id = None ) : with atomic ( savepoint = False ) : child_key = self . get_next_child_key ( ) <MASK> value = child_key child = self . __class__ . objects . create ( id = _id , key = child_key , value = value ) return child",if value is None :,if value is None :,100,TRUE,100
4045,"def list_tags_for_stream ( self , stream_name , exclusive_start_tag_key = None , limit = None ) : stream = self . describe_stream ( stream_name ) tags = [ ] result = { "" HasMoreTags "" : False , "" Tags "" : tags } for key , val in sorted ( stream . tags . items ( ) , key = lambda x : x [ 0 ] ) : <MASK> result [ "" HasMoreTags "" ] = True break if exclusive_start_tag_key and key < exclusive_start_tag_key : continue tags . append ( { "" Key "" : key , "" Value "" : val } ) return result",if limit and len ( tags ) >= limit :,if limit and key >= limit :,94.35387996,FALSE,96.21
4046,"def emit ( self , record ) : try : app = get_app ( ) <MASK> msg = self . format ( record ) debug_buffer = app . layout . get_buffer_by_name ( "" debug_buffer "" ) current_document = debug_buffer . document . text if current_document : msg = "" \n "" . join ( [ current_document , msg ] ) debug_buffer . set_document ( Document ( text = msg ) , bypass_readonly = True ) else : super ( ) . emit ( record ) except : self . handleError ( record )","if app . is_running and getattr ( app , ""debug"" , False ) :",if self . debug :,72.37996621,FALSE,87.08
4047,"def worker ( ) : global error while True : ( num , q ) = pq . get ( ) <MASK> pq . task_done ( ) break try : process_one ( q ) except Exception as e : error = e finally : pq . task_done ( )",if q is None or error is not None :,if num == 0 :,78.43522816,FALSE,84.7
4048,"def transceiver ( self , data ) : out = [ ] for t in range ( 8 ) : if data [ t ] == 0 : continue value = data [ t ] for b in range ( 8 ) : <MASK> if len ( TRANSCEIVER [ t ] ) < b + 1 : out . append ( "" (unknown) "" ) else : out . append ( TRANSCEIVER [ t ] [ b ] ) value << = 1 self . annotate ( "" Transceiver compliance "" , "" ,  "" . join ( out ) )",if value & 0x80 :,if value & 1 :,98.79931461,FALSE,97.66
4049,"def skip_to_close_match ( self ) : nestedCount = 1 while 1 : tok = self . tokenizer . get_next_token ( ) ttype = tok [ "" style "" ] <MASK> return elif self . classifier . is_index_op ( tok ) : tval = tok [ "" text "" ] if self . opHash . has_key ( tval ) : if self . opHash [ tval ] [ 1 ] == 1 : nestedCount + = 1 else : nestedCount - = 1 if nestedCount < = 0 : break",if ttype == SCE_PL_UNUSED :,"if ttype == ""close"" :",96.55866545,FALSE,94.26
4050,"def GenerateVector ( self , hits , vector , level ) : """"""Generate possible hit vectors which match the rules."""""" for item in hits . get ( level , [ ] ) : if vector : <MASK> continue if item > self . max_separation + vector [ - 1 ] : break new_vector = vector + [ item ] if level + 1 == len ( hits ) : yield new_vector elif level + 1 < len ( hits ) : for result in self . GenerateVector ( hits , new_vector , level + 1 ) : yield result",if item < vector [ - 1 ] :,if item < self . min_separation + vector [ - 1 ] :,96.20636272,FALSE,94.1
4051,"def __setattr__ ( self , name , value ) : if name == "" path "" : <MASK> if value [ 0 ] != "" / "" : raise ValueError ( ' The page path should always start with a slash ( "" / "" ). ' ) elif name == "" load_time "" : if value and not isinstance ( value , int ) : raise ValueError ( "" Page load time must be specified in integer milliseconds. "" ) object . __setattr__ ( self , name , value )","if value and value != """" :",if value :,93.97833155,FALSE,93.25
4052,"def awaitTermination ( self , timeout = None ) : if self . scheduler is None : raise RuntimeError ( "" StreamimgContext not started "" ) try : deadline = time . time ( ) + timeout if timeout is not None else None while True : is_terminated = self . _runOnce ( ) <MASK> break if self . batchCallback : self . batchCallback ( ) except KeyboardInterrupt : pass finally : self . sc . stop ( ) logger . info ( "" StreamingContext stopped successfully "" )",if is_terminated or ( deadline is not None and time . time ( ) > deadline ) :,if is_terminated :,82.61641415,FALSE,84.16
4053,"def stopbutton ( self ) : if GPIOcontrol : while mediastopbutton : time . sleep ( 0.25 ) <MASK> print ( "" Stopped "" ) stop ( )",if not GPIO . input ( stoppushbutton ) :,if self . _debug :,56.53197165,FALSE,75.77
4054,"def test_create_connection_timeout ( self ) : # Issue #9792: create_connection() should not recast timeout errors # as generic socket errors. with self . mocked_socket_module ( ) : try : socket . create_connection ( ( HOST , 1234 ) ) except socket . timeout : pass except OSError as exc : <MASK> raise else : self . fail ( "" socket.timeout not raised "" )",if support . IPV6_ENABLED or exc . errno != errno . EAFNOSUPPORT :,if exc . errno != errno . ECONNRESET :,69.55682397,FALSE,89.7
4055,"def handle_exception_and_die ( e ) : if hasattr ( e , "" kind "" ) : <MASK> sys . stderr . write ( "" ABORT:  "" + e . msg + "" \n "" ) sys . exit ( e . value ) elif e . kind == "" exit "" : sys . stderr . write ( "" EXITING \n "" ) sys . exit ( e . value ) else : print ( str ( e ) ) sys . exit ( 1 )","if e . kind == ""die"" :","if e . kind == ""abort"" :",73.61870666,FALSE,97.57
4056,"def gets ( self , key ) : with self . client_pool . get_and_release ( destroy_on_fail = True ) as client : try : return client . gets ( key ) except Exception : <MASK> return ( None , None ) else : raise",if self . ignore_exc :,if self . ignore_exc :,100,TRUE,100
4057,"def _execute ( self , options , args ) : if len ( args ) < 3 : raise CommandError ( _ ( "" Not enough arguments "" ) ) tag = fsn2text ( args [ 0 ] ) value = fsn2text ( args [ 1 ] ) paths = args [ 2 : ] songs = [ ] for path in paths : song = self . load_song ( path ) <MASK> raise CommandError ( _ ( "" Can not set  %r "" ) % tag ) self . log ( "" Add  %r  to  %r "" % ( value , tag ) ) song . add ( tag , value ) songs . append ( song ) self . save_songs ( songs )",if not song . can_change ( tag ) :,"if not song . can_change ( tag , value ) :",97.66499764,FALSE,97.55
4058,"def get_place_name ( self , place_handle ) : """"""Obtain a place name"""""" text = "" "" if place_handle : place = self . dbstate . db . get_place_from_handle ( place_handle ) if place : place_title = place_displayer . display ( self . dbstate . db , place ) <MASK> if len ( place_title ) > 25 : text = place_title [ : 24 ] + "" ... "" else : text = place_title return text","if place_title != """" :",if place_title :,95.62690319,FALSE,95.21
4059,"def _Determine_Do ( self ) : self . applicable = 1 self . value = os . environ . get ( self . name , None ) if self . value is None and black . configure . items . has_key ( "" buildType "" ) : buildType = black . configure . items [ "" buildType "" ] . Get ( ) <MASK> self . value = "" warn "" else : self . value = None self . determined = 1","if buildType == ""debug"" :","if buildType == ""warn"" :",98.54515575,FALSE,97.3
4060,"def bundle_directory ( self , dirpath ) : """"""Bundle all modules/packages in the given directory."""""" dirpath = os . path . abspath ( dirpath ) for nm in os . listdir ( dirpath ) : nm = _u ( nm ) <MASK> continue itempath = os . path . join ( dirpath , nm ) if os . path . isdir ( itempath ) : if os . path . exists ( os . path . join ( itempath , "" __init__.py "" ) ) : self . bundle_package ( itempath ) elif nm . endswith ( "" .py "" ) : self . bundle_module ( itempath )","if nm . startswith ( ""."" ) :","if not nm . startswith ( ""py"" ) :",96.70381108,FALSE,96.39
4061,"def header_fields ( self , fields ) : headers = dict ( self . conn . response . getheaders ( ) ) ret = { } for field in fields : <MASK> raise ValueError ( "" %s  was not found in response header "" % ( field [ 1 ] ) ) try : ret [ field [ 0 ] ] = int ( headers [ field [ 1 ] ] ) except ValueError : ret [ field [ 0 ] ] = headers [ field [ 1 ] ] return ret",if not headers . has_key ( field [ 1 ] ) :,if field [ 0 ] not in headers :,91.60778536,FALSE,88.61
4062,"def caesar_cipher ( s , k ) : result = "" "" for char in s : n = ord ( char ) <MASK> n = ( ( n - 65 + k ) % 26 ) + 65 if 96 < n < 123 : n = ( ( n - 97 + k ) % 26 ) + 97 result = result + chr ( n ) return result",if 64 < n < 91 :,if 65 < n < 122 :,85.80823936,FALSE,93.46
4063,"def qtTypeIdent ( conn , * args ) : # We're not using the conn object at the moment, but - we will # modify the # logic to use the server version specific keywords later. res = None value = None for val in args : # DataType doesn't have len function then convert it to string if not hasattr ( val , "" __len__ "" ) : val = str ( val ) <MASK> continue value = val if Driver . needsQuoting ( val , True ) : value = value . replace ( ' "" ' , ' "" "" ' ) value = ' "" ' + value + ' "" ' res = ( ( res and res + "" . "" ) or "" "" ) + value return res",if len ( val ) == 0 :,"if val == """" :",72.56697137,FALSE,94.93
4064,"def _parse_timezone ( value : Optional [ str ] , error : Type [ Exception ] ) - > Union [ None , int , timezone ] : if value == "" Z "" : return timezone . utc elif value is not None : offset_mins = int ( value [ - 2 : ] ) if len ( value ) > 3 else 0 offset = 60 * int ( value [ 1 : 3 ] ) + offset_mins <MASK> offset = - offset try : return timezone ( timedelta ( minutes = offset ) ) except ValueError : raise error ( ) else : return None","if value [ 0 ] == ""-"" :",if offset < 0 :,81.3195061,FALSE,91.69
4065,"def indent ( elem , level = 0 ) : i = "" \n "" + level * ""    "" if len ( elem ) : if not elem . text or not elem . text . strip ( ) : elem . text = i + ""    "" <MASK> elem . tail = i for elem in elem : indent ( elem , level + 1 ) <MASK> elem . tail = i else : if level and ( not elem . tail or not elem . tail . strip ( ) ) : elem . tail = i",if not elem . tail or not elem . tail . strip ( ) :,if not elem . tail or not elem . tail . strip ( ) :,100,TRUE,100
4066,"def _make_slices ( shape : tp . Tuple [ int , . . . ] , axes : tp . Tuple [ int , . . . ] , size : int , rng : np . random . RandomState , ) - > tp . List [ slice ] : slices = [ ] for a , s in enumerate ( shape ) : if a in axes : <MASK> raise ValueError ( "" Cannot crossover on axis with size 1 "" ) start = rng . randint ( s - size ) slices . append ( slice ( start , start + size ) ) else : slices . append ( slice ( None ) ) return slices",if s <= 1 :,if s == 1 :,98.986151,FALSE,98.01
4067,"def _loadTestsFromTestCase ( self , event , testCaseClass ) : evt = events . LoadFromTestCaseEvent ( event . loader , testCaseClass ) result = self . session . hooks . loadTestsFromTestCase ( evt ) if evt . handled : loaded_suite = result or event . loader . suiteClass ( ) else : names = self . _getTestCaseNames ( event , testCaseClass ) <MASK> names = [ "" runTest "" ] # FIXME return failure test case if name not in testcase class loaded_suite = event . loader . suiteClass ( map ( testCaseClass , names ) ) if evt . extraTests : loaded_suite . addTests ( evt . extraTests ) return loaded_suite","if not names and hasattr ( testCaseClass , ""runTest"" ) :",if not names :,85.6648578,FALSE,91.91
4068,"def check_settings ( self ) : if self . settings_dict [ "" TIME_ZONE "" ] is not None : if not settings . USE_TZ : raise ImproperlyConfigured ( "" Connection  ' %s '  cannot set TIME_ZONE because USE_TZ is  "" "" False. "" % self . alias ) <MASK> raise ImproperlyConfigured ( "" Connection  ' %s '  cannot set TIME_ZONE because its engine  "" "" handles time zones conversions natively. "" % self . alias )",elif self . features . supports_timezones :,if settings . USE_TZ and settings . USE_TZ is True :,86.46521562,FALSE,86.1
4069,"def collect_conflicting_diffs ( path , decisions ) : local_conflict_diffs = [ ] remote_conflict_diffs = [ ] for d in decisions : <MASK> ld = adjust_patch_level ( path , d . common_path , d . local_diff ) rd = adjust_patch_level ( path , d . common_path , d . remote_diff ) local_conflict_diffs . extend ( ld ) remote_conflict_diffs . extend ( rd ) return local_conflict_diffs , remote_conflict_diffs",if d . conflict :,if d . common_path :,93.86294806,FALSE,96.05
4070,"def short_repr ( obj ) : if isinstance ( obj , ( type , types . ModuleType , types . BuiltinMethodType , types . BuiltinFunctionType ) , ) : return obj . __name__ if isinstance ( obj , types . MethodType ) : <MASK> return obj . im_func . __name__ + ""  (bound) "" else : return obj . im_func . __name__ if isinstance ( obj , ( tuple , list , dict , set ) ) : return "" %d  items "" % len ( obj ) if isinstance ( obj , weakref . ref ) : return "" all_weakrefs_are_one "" return repr ( obj ) [ : 40 ]",if obj . im_self is not None :,"if obj . im_func . __name__ . startswith ( ""bound_"" ) :",94.75708373,FALSE,89.45
4071,"def _massage_uri ( uri ) : if uri : <MASK> uri = uri . replace ( "" hdfs:// "" , get_defaultfs ( ) ) elif uri . startswith ( "" / "" ) : uri = get_defaultfs ( ) + uri return uri","if uri . startswith ( ""hdfs:///"" ) :","if uri . startswith ( ""hdfs://"" ) :",97.41362635,FALSE,98.07
4072,"def chsub ( self , msg , chatid ) : ( cmd , evt , params ) = self . tokenize ( msg , 3 ) if cmd == "" /sub "" : sql = "" replace into telegram_subscriptions(uid, event_type, parameters) values (?, ?, ?) "" else : <MASK> sql = "" delete from telegram_subscriptions where uid = ? and (event_type = ? or parameters = ? or 1 = 1) "" # does not look very elegant, but makes unsub'ing everythign possible else : sql = "" delete from telegram_subscriptions where uid = ? and event_type = ? and parameters = ? "" with self . bot . database as conn : conn . execute ( sql , [ chatid , evt , params ] ) conn . commit ( ) return","if evt == ""everything"" :","if evt == ""sub"" :",99.08657264,FALSE,98.47
4073,"def undefined_symbols ( self ) : result = [ ] for p in self . Productions : <MASK> continue for s in p . prod : if not s in self . Prodnames and not s in self . Terminals and s != "" error "" : result . append ( ( s , p ) ) return result",if not p :,if not p . is_symbol :,95.11270544,FALSE,91.8
4074,"def renumber ( self , x1 , y1 , x2 , y2 , dx , dy ) : out = [ ] for part in re . split ( "" ( \ w+) "" , self . formula ) : m = re . match ( "" ^([A-Z]+)([1-9][0-9]*)$ "" , part ) if m is not None : sx , sy = m . groups ( ) x = colname2num ( sx ) y = int ( sy ) <MASK> part = cellname ( x + dx , y + dy ) out . append ( part ) return FormulaCell ( "" "" . join ( out ) , self . fmt , self . alignment )",if x1 <= x <= x2 and y1 <= y <= y2 :,if x1 == x2 and y1 == y :,88.74353367,FALSE,92.26
4075,"def modify_column ( self , column : List [ Optional [ "" Cell "" ] ] ) : for i in range ( len ( column ) ) : gate = column [ i ] if gate is self : continue <MASK> # The first parity control to modify the column must merge all # of the other parity controls into itself. column [ i ] = None self . _basis_change + = gate . _basis_change self . qubits + = gate . qubits elif gate is not None : column [ i ] = gate . controlled_by ( self . qubits [ 0 ] )","elif isinstance ( gate , ParityControlCell ) :",if gate . controlled_by ( self . qubits [ 0 ] ) :,90.07030693,FALSE,89.16
4076,"def update_neighbor ( neigh_ip_address , changes ) : rets = [ ] for k , v in changes . items ( ) : <MASK> rets . append ( _update_med ( neigh_ip_address , v ) ) if k == neighbors . ENABLED : rets . append ( update_neighbor_enabled ( neigh_ip_address , v ) ) if k == neighbors . CONNECT_MODE : rets . append ( _update_connect_mode ( neigh_ip_address , v ) ) return all ( rets )",if k == neighbors . MULTI_EXIT_DISC :,if k == neighbors . Medium :,98.45096215,FALSE,94.53
4077,"def writexml ( self , stream , indent = "" "" , addindent = "" "" , newl = "" "" , strip = 0 , nsprefixes = { } , namespace = "" "" , ) : w = _streamWriteWrapper ( stream ) if self . raw : val = self . nodeValue if not isinstance ( val , str ) : val = str ( self . nodeValue ) else : v = self . nodeValue <MASK> v = str ( v ) if strip : v = ""   "" . join ( v . split ( ) ) val = escape ( v ) w ( val )","if not isinstance ( v , str ) :","if not isinstance ( v , str ) :",100,TRUE,100
4078,"def _condition ( ct ) : for qobj in args : <MASK> # normal kwargs are an AND anyway, so just use those for now for child in qobj . children : kwargs . update ( dict ( [ child ] ) ) else : raise NotImplementedError ( "" Unsupported Q object "" ) for attr , val in kwargs . items ( ) : if getattr ( ct , attr ) != val : return False return True","if qobj . connector == ""AND"" and not qobj . negated :","if isinstance ( qobj , QGroup ) :",56.08771421,FALSE,84.74
4079,"def results_iter ( self ) : <MASK> from django . db . models . fields import DateTimeField fields = [ DateTimeField ( ) ] else : needs_string_cast = self . connection . features . needs_datetime_string_cast offset = len ( self . query . extra_select ) for rows in self . execute_sql ( MULTI ) : for row in rows : date = row [ offset ] <MASK> date = self . resolve_columns ( row , fields ) [ offset ] elif needs_string_cast : date = typecast_timestamp ( str ( date ) ) yield date",if self . connection . ops . oracle :,if self . connection . features . supports_datetime_string_cast :,85.18218964,FALSE,85.77
4080,"def get_job_type ( self ) : if int ( self . job_runtime_conf . get ( "" dsl_version "" , 1 ) ) == 2 : job_type = ( self . job_runtime_conf [ "" job_parameters "" ] . get ( "" common "" , { } ) . get ( "" job_type "" ) ) <MASK> job_type = self . job_runtime_conf [ "" job_parameters "" ] . get ( "" job_type "" , "" train "" ) else : job_type = self . job_runtime_conf [ "" job_parameters "" ] . get ( "" job_type "" , "" train "" ) return job_type",if not job_type :,"if job_type == ""test"" :",79.41696067,FALSE,94.7
4081,"def validate_assessment_criteria ( self ) : if self . assessment_criteria : total_weightage = 0 for criteria in self . assessment_criteria : total_weightage + = criteria . weightage or 0 <MASK> frappe . throw ( _ ( "" Total Weightage of all Assessment Criteria must be 100 % "" ) )",if total_weightage != 100 :,if total_weightage > 100 :,97.61662152,FALSE,94.79
4082,"def get_list_of_strings_to_mongo_objects ( self , notifications_list = None ) : result = [ ] if len ( notifications_list ) > 0 : for x in notifications_list : split_provider_id = x . split ( "" : "" ) # email:id <MASK> _id = split_provider_id [ 1 ] cursor = self . get_by_id ( _id ) if cursor : # Append if exists result . append ( cursor ) return result",if len ( split_provider_id ) == 2 :,"if split_provider_id [ 0 ] == ""email:id"" :",95.36447525,FALSE,89.51
4083,"def dump_predictions_to_database ( relation , predictions ) : judge = "" iepy-run on  {} "" . format ( datetime . now ( ) . strftime ( "" % Y- % m- %d   % H: % M "" ) ) for evidence , relation_is_present in predictions . items ( ) : label = ( EvidenceLabel . YESRELATION <MASK> else EvidenceLabel . NORELATION ) evidence . set_label ( relation , label , judge , labeled_by_machine = True )",if relation_is_present,if relation_is_present,100,TRUE,100
4084,"def __init__ ( self , * * kwargs ) : # We hard-code the `to` argument for ForeignKey.__init__ dfl = get_model_label ( self . default_model_class ) if "" to "" in kwargs . keys ( ) : # pragma: no cover old_to = get_model_label ( kwargs . pop ( "" to "" ) ) <MASK> msg = "" %s  can only be a ForeignKey to  %s ;  %s  passed "" % ( self . __class__ . __name__ , dfl , old_to , ) warnings . warn ( msg , SyntaxWarning ) kwargs [ "" to "" ] = dfl super ( ) . __init__ ( * * kwargs )",if old_to . lower ( ) != dfl . lower ( ) :,if dfl != old_to :,70.08680218,FALSE,92.09
4085,"def reverse ( self ) : """"""Reverse *IN PLACE*."""""" li = self . leftindex lb = self . leftblock ri = self . rightindex rb = self . rightblock for i in range ( self . len >> 1 ) : lb . data [ li ] , rb . data [ ri ] = rb . data [ ri ] , lb . data [ li ] li + = 1 if li > = BLOCKLEN : lb = lb . rightlink li = 0 ri - = 1 <MASK> rb = rb . leftlink ri = BLOCKLEN - 1",if ri < 0 :,if ri >= BLOCKLEN :,93.57439666,FALSE,96.05
4086,"def get_api ( user , url ) : global API_CACHE if API_CACHE is None or API_CACHE . get ( url ) is None : API_CACHE_LOCK . acquire ( ) try : if API_CACHE is None : API_CACHE = { } <MASK> API_CACHE [ url ] = ImpalaDaemonApi ( url ) finally : API_CACHE_LOCK . release ( ) api = API_CACHE [ url ] api . set_user ( user ) return api",if API_CACHE . get ( url ) is None :,if not API_CACHE . get ( url ) :,89.83105942,FALSE,95.27
4087,"def invert_index ( cls , index , length ) : if np . isscalar ( index ) : return length - index elif isinstance ( index , slice ) : start , stop = index . start , index . stop new_start , new_stop = None , None if start is not None : new_stop = length - start <MASK> new_start = length - stop return slice ( new_start - 1 , new_stop - 1 ) elif isinstance ( index , Iterable ) : new_index = [ ] for ind in index : new_index . append ( length - ind ) return new_index",if stop is not None :,if stop is not None :,100,TRUE,100
4088,"def infer_returned_object ( pyfunction , args ) : """"""Infer the `PyObject` this `PyFunction` returns after calling"""""" object_info = pyfunction . pycore . object_info result = object_info . get_exact_returned ( pyfunction , args ) if result is not None : return result result = _infer_returned ( pyfunction , args ) if result is not None : <MASK> params = args . get_arguments ( pyfunction . get_param_names ( special_args = False ) ) object_info . function_called ( pyfunction , params , result ) return result return object_info . get_returned ( pyfunction , args )",if args and pyfunction . get_module ( ) . get_resource ( ) is not None :,"if object_info . is_function_called ( pyfunction , args ) :",87.16983141,FALSE,88.45
4089,"def _check_imports ( lib ) : # Make sure no conflicting libraries have been imported. libs = [ "" PyQt4 "" , "" PyQt5 "" , "" PySide "" ] libs . remove ( lib ) for lib2 in libs : lib2 + = "" .QtCore "" <MASK> raise RuntimeError ( "" Refusing to import  %s  because  %s  is already  "" "" imported. "" % ( lib , lib2 ) )",if lib2 in sys . modules :,if lib2 in libs :,72.07569191,FALSE,94.6
4090,"def _poll ( fds , timeout ) : if timeout is not None : timeout = int ( timeout * 1000 ) # timeout is in milliseconds fd_map = { } pollster = select . poll ( ) for fd in fds : pollster . register ( fd , select . POLLIN ) <MASK> fd_map [ fd . fileno ( ) ] = fd else : fd_map [ fd ] = fd ls = [ ] for fd , event in pollster . poll ( timeout ) : if event & select . POLLNVAL : raise ValueError ( "" invalid file descriptor  %i "" % fd ) ls . append ( fd_map [ fd ] ) return ls","if hasattr ( fd , ""fileno"" ) :",if fd . fileno ( ) :,97.60875129,FALSE,94.25
4091,"def default ( cls , connection = None ) : """"""show the default connection, or make CONNECTION the default"""""" if connection is not None : target = cls . _get_config_filename ( connection ) <MASK> if os . path . exists ( cls . _default_symlink ) : os . remove ( cls . _default_symlink ) os . symlink ( target , cls . _default_symlink ) else : cls . _no_config_file_error ( target ) if os . path . exists ( cls . _default_symlink ) : print ( "" Default connection is  "" + cls . _default_connection ( ) ) else : print ( "" There is no default connection set "" )",if os . path . exists ( target ) :,if target :,92.48687837,FALSE,93.86
4092,"def process ( self , fuzzresult ) : base_url = urljoin ( fuzzresult . url , "" .. "" ) for line in fuzzresult . history . content . splitlines ( ) : record = line . split ( "" / "" ) <MASK> self . queue_url ( urljoin ( base_url , record [ 1 ] ) ) # Directory if record [ 0 ] == "" D "" : self . queue_url ( urljoin ( base_url , record [ 1 ] ) ) self . queue_url ( urljoin ( base_url , "" %s /CVS/Entries "" % ( record [ 1 ] ) ) )",if len ( record ) == 6 and record [ 1 ] :,"if record [ 0 ] == ""A"" :",88.58899658,FALSE,91.34
4093,"def _GetCSVRow ( self , value ) : row = [ ] for type_info in value . __class__ . type_infos : <MASK> row . extend ( self . _GetCSVRow ( value . Get ( type_info . name ) ) ) elif isinstance ( type_info , rdf_structs . ProtoBinary ) : row . append ( text . Asciify ( value . Get ( type_info . name ) ) ) else : row . append ( str ( value . Get ( type_info . name ) ) ) return row","if isinstance ( type_info , rdf_structs . ProtoEmbedded ) :","if isinstance ( type_info , rdf_structs . ProtoStruct ) :",98.69454294,FALSE,97.89
4094,"def get_history ( self , state , dict_ , passive = PASSIVE_OFF ) : if self . key in dict_ : return History . from_scalar_attribute ( self , state , dict_ [ self . key ] ) else : <MASK> passive ^ = INIT_OK current = self . get ( state , dict_ , passive = passive ) if current is PASSIVE_NO_RESULT : return HISTORY_BLANK else : return History . from_scalar_attribute ( self , state , current )",if passive & INIT_OK :,if passive & INIT_OK :,100,TRUE,100
4095,"def _iterate_self_and_parents ( self , upto = None ) : current = self result = ( ) while current : result + = ( current , ) <MASK> break elif current . _parent is None : raise sa_exc . InvalidRequestError ( "" Transaction  %s  is not on the active transaction list "" % ( upto ) ) else : current = current . _parent return result",if current . _parent is upto :,if current . _parent is None :,91.13243971,FALSE,96.98
4096,"def get_by_uri ( self , uri : str ) - > bytes : userId , bucket , key = self . _parse_uri ( uri ) try : with db . session_scope ( ) as dbsession : result = db_archivedocument . get ( userId , bucket , key , session = dbsession ) <MASK> return utils . ensure_bytes ( self . _decode ( result ) ) else : raise ObjectKeyNotFoundError ( userId , bucket , key , caused_by = None ) except Exception as err : logger . debug ( "" cannot get data: exception -  "" + str ( err ) ) raise err",if result :,if result :,100,TRUE,100
4097,"def app ( scope , receive , send ) : while True : message = await receive ( ) if message [ "" type "" ] == "" websocket.connect "" : await send ( { "" type "" : "" websocket.accept "" } ) elif message [ "" type "" ] == "" websocket.receive "" : pass <MASK> break","elif message [ ""type"" ] == ""websocket.disconnect"" :","elif message [ ""type"" ] == ""websocket.disconnect"" :",100,TRUE,100
4098,"def recv_some ( p , t = 0.1 , e = 1 , tr = 5 , stderr = 0 ) : if tr < 1 : tr = 1 x = time . time ( ) + t y = [ ] r = "" "" if stderr : pr = p . recv_err else : pr = p . recv while time . time ( ) < x or r : r = pr ( ) if r is None : break <MASK> y . append ( r ) else : time . sleep ( max ( ( x - time . time ( ) ) / tr , 0 ) ) return "" "" . join ( y )",elif r :,if e :,67.87357064,FALSE,97.22
4099,"def mouse_down ( self , event ) : if event . button == 1 : <MASK> p = event . local if self . scroll_up_rect ( ) . collidepoint ( p ) : self . scroll_up ( ) return elif self . scroll_down_rect ( ) . collidepoint ( p ) : self . scroll_down ( ) return if event . button == 4 : self . scroll_up ( ) if event . button == 5 : self . scroll_down ( ) GridView . mouse_down ( self , event )",if self . scrolling :,if self . scroll_down_rect ( ) . collidepoint ( event ) :,94.51613369,FALSE,89.23
4100,"def copy_from ( self , other ) : if self is other : return # Myself! self . strictness = other . strictness # sets behaviors in bulk for name in self . all_behaviors : self . set_behavior ( name , other . get_behavior ( name ) ) for name in self . _plain_attrs : val = getattr ( other , name ) if isinstance ( val , set ) : val = val . copy ( ) <MASK> val = val . copy ( ) setattr ( self , name , val )","elif decimal and isinstance ( val , decimal . Decimal ) :","elif isinstance ( val , dict ) :",71.03710296,FALSE,93.02
4101,"def __array_wrap__ ( self , out_arr , context = None ) : if self . dim is None : return out_arr else : this = self [ : ] <MASK> return Quantity . __array_wrap__ ( self [ : ] , out_arr , context = context ) else : return out_arr","if isinstance ( this , Quantity ) :",if this . dim == self . dim :,78.10324899,FALSE,88.01
4102,"def _ArgumentListHasDictionaryEntry ( self , token ) : """"""Check if the function argument list has a dictionary as an arg."""""" if _IsArgumentToFunction ( token ) : while token : <MASK> length = token . matching_bracket . total_length - token . total_length return length + self . stack [ - 2 ] . indent > self . column_limit if token . ClosesScope ( ) : break if token . OpensScope ( ) : token = token . matching_bracket token = token . next_token return False","if token . value == ""{"" :",if token . OpensScope ( ) :,94.48510261,FALSE,93.32
4103,"def save_all_changed_extensions ( self ) : """"""Save configuration changes to the user config file."""""" has_changes = False for ext_name in self . extensions : options = self . extensions [ ext_name ] for opt in options : <MASK> has_changes = True if has_changes : self . ext_userCfg . Save ( )","if self . set_extension_value ( ext_name , opt ) :","if opt . name == ""changes"" :",84.45528473,FALSE,83.34
4104,"def to_dict ( self ) : out = { } for key in ACTIVITY_KEYS : attr = getattr ( self , key ) <MASK> out [ key ] = str ( attr ) else : out [ key ] = attr if self . streak : out [ "" streak "" ] = self . streak return out","if isinstance ( attr , ( datetime . timedelta , datetime . datetime ) ) :","if isinstance ( attr , ( int , float ) ) :",76.50763456,FALSE,88.99
4105,"def clean_publication_date ( cls , cleaned_input ) : for add_channel in cleaned_input . get ( "" add_channels "" , [ ] ) : is_published = add_channel . get ( "" is_published "" ) publication_date = add_channel . get ( "" publication_date "" ) <MASK> add_channel [ "" publication_date "" ] = datetime . date . today ( )",if is_published and not publication_date :,if is_published and not publication_date :,100,TRUE,100
4106,"def _random_blur ( self , batch , sigma_max ) : for i in range ( len ( batch ) ) : <MASK> # Random sigma sigma = random . uniform ( 0.0 , sigma_max ) batch [ i ] = scipy . ndimage . filters . gaussian_filter ( batch [ i ] , sigma ) return batch",if bool ( random . getrandbits ( 1 ) ) :,if random . random ( ) < sigma_max :,59.62134743,FALSE,87.45
4107,"def conninfo_parse ( dsn ) : ret = { } length = len ( dsn ) i = 0 while i < length : if dsn [ i ] . isspace ( ) : i + = 1 continue param_match = PARAMETER_RE . match ( dsn [ i : ] ) if not param_match : return param = param_match . group ( 1 ) i + = param_match . end ( ) <MASK> return value , end = read_param_value ( dsn [ i : ] ) if value is None : return i + = end ret [ param ] = value return ret",if i >= length :,"if param == ""None"" :",86.96610543,FALSE,94.2
4108,"def set_environment_vars ( env , source_env ) : """"""Copy allowed environment variables from |source_env|."""""" if not source_env : return for name , value in six . iteritems ( source_env ) : if is_forwarded_environment_variable ( name ) : # Avoid creating circular dependencies from importing environment by # using os.getenv. <MASK> value = file_host . rebase_to_worker_root ( value ) env [ name ] = value","if os . getenv ( ""TRUSTED_HOST"" ) and should_rebase_environment_value ( name ) :",if os . path . isdir ( value ) :,68.76565069,FALSE,84.04
4109,"def toterminal ( self , tw ) : # the entries might have different styles last_style = None for i , entry in enumerate ( self . reprentries ) : if entry . style == "" long "" : tw . line ( "" "" ) entry . toterminal ( tw ) <MASK> next_entry = self . reprentries [ i + 1 ] if ( entry . style == "" long "" or entry . style == "" short "" and next_entry . style == "" long "" ) : tw . sep ( self . entrysep ) if self . extraline : tw . line ( self . extraline )",if i < len ( self . reprentries ) - 1 :,if i < len ( self . reprentries ) - 1 :,75,TRUE,100
4110,"def __init__ ( self , loc , tabs = None ) : if os . path . isdir ( loc ) : for item in os . listdir ( loc ) : <MASK> continue path = os . path . join ( loc , item ) self . append ( CronTab ( user = False , tabfile = path ) ) elif os . path . isfile ( loc ) : self . append ( CronTab ( user = False , tabfile = loc ) )","if item [ 0 ] == ""."" :","if item . startswith ( ""_"" ) :",93.90245476,FALSE,91.07
4111,"def import_data ( self , fname ) : """"""Import data in current namespace"""""" if self . count ( ) : nsb = self . currentWidget ( ) nsb . refresh_table ( ) nsb . import_data ( fname ) <MASK> self . dockwidget . setVisible ( True ) self . dockwidget . raise_ ( )",if self . dockwidget and not self . ismaximized :,if self . dockwidget :,89.11237027,FALSE,91.15
4112,"def get_menu_items ( node ) : aList = [ ] for child in node . children : for tag in ( "" @menu "" , "" @item "" ) : <MASK> name = child . h [ len ( tag ) + 1 : ] . strip ( ) if tag == "" @menu "" : aList . append ( ( "" %s   %s "" % ( tag , name ) , get_menu_items ( child ) , None ) ) else : b = g . splitLines ( "" "" . join ( child . b ) ) aList . append ( ( tag , name , b [ 0 ] if b else "" "" ) ) break return aList",if child . h . startswith ( tag ) :,if child . h . startswith ( tag ) :,100,TRUE,100
4113,"def __init__ ( self , * args , * * kw ) : if len ( args ) > 1 : raise TypeError ( "" MultiDict can only be called with one positional  "" "" argument "" ) if args : if hasattr ( args [ 0 ] , "" iteritems "" ) : items = list ( args [ 0 ] . iteritems ( ) ) <MASK> items = list ( args [ 0 ] . items ( ) ) else : items = list ( args [ 0 ] ) self . _items = items else : self . _items = [ ] if kw : self . _items . extend ( kw . items ( ) )","elif hasattr ( args [ 0 ] , ""items"" ) :","elif hasattr ( args [ 0 ] , ""items"" ) :",100,TRUE,100
4114,"def open ( self ) - > "" KeyValueDb "" : """"""Create a new data base or open existing one"""""" if os . path . exists ( self . _name ) : if not os . path . isfile ( self . _name ) : raise IOError ( "" %s  exists and is not a file "" % self . _name ) <MASK> # ignore empty files return self with open ( self . _name , "" rb "" ) as _in : # binary mode self . set_records ( pickle . load ( _in ) ) else : # make sure path exists mkpath ( os . path . dirname ( self . _name ) ) self . commit ( ) return self",if os . path . getsize ( self . _name ) == 0 :,if not os . path . getsize ( self . _name ) :,95.74978461,FALSE,96.07
4115,"def sortModules ( self ) : super ( NeuronDecomposableNetwork , self ) . sortModules ( ) self . _constructParameterInfo ( ) # contains a list of lists of indices self . decompositionIndices = { } for neuron in self . _neuronIterator ( ) : self . decompositionIndices [ neuron ] = [ ] for w in range ( self . paramdim ) : inneuron , outneuron = self . paramInfo [ w ] <MASK> self . decompositionIndices [ inneuron ] . append ( w ) else : self . decompositionIndices [ outneuron ] . append ( w )",if self . espStyleDecomposition and outneuron [ 0 ] in self . outmodules :,if inneuron in self . decompositionIndices :,94.42431852,FALSE,89.09
4116,"def visit_Options ( self , node : qlast . Options ) - > None : for i , opt in enumerate ( node . options . values ( ) ) : <MASK> self . write ( ""   "" ) self . write ( opt . name ) if not isinstance ( opt , qlast . Flag ) : self . write ( f ""   { opt . val } "" )",if i > 0 :,if i > 0 :,100,TRUE,100
4117,"def is_child_of ( self , item_hash , possible_child_hash ) : if self . get_last ( item_hash ) != self . get_last ( possible_child_hash ) : return None while True : if possible_child_hash == item_hash : return True <MASK> return False possible_child_hash = self . items [ possible_child_hash ] . previous_hash",if possible_child_hash not in self . items :,if self . items [ possible_child_hash ] . previous_hash != self . items,87.88205825,FALSE,85.21
4118,"def __call__ ( self , text , * * kargs ) : words = jieba . tokenize ( text , mode = "" search "" ) token = Token ( ) for ( w , start_pos , stop_pos ) in words : <MASK> continue token . original = token . text = w token . pos = start_pos token . startchar = start_pos token . endchar = stop_pos yield token",if not accepted_chars . match ( w ) and len ( w ) <= 1 :,if w == 0 :,67.84591048,FALSE,81.43
4119,"def test_analysis_jobs_cypher_syntax ( neo4j_session ) : parameters = { "" AWS_ID "" : None , "" UPDATE_TAG "" : None , "" OKTA_ORG_ID "" : None , } for job_name in contents ( "" cartography.data.jobs.analysis "" ) : <MASK> continue try : cartography . util . run_analysis_job ( job_name , neo4j_session , parameters ) except Exception as e : pytest . fail ( f "" run_analysis_job failed for analysis job  ' { job_name } '  with exception:  { e } "" )","if not job_name . endswith ( "".json"" ) :","if not job_name . endswith ( "".py"" ) :",98.60114292,FALSE,98.13
4120,"def _interleave_dataset_results_and_tensors ( dataset_results , flat_run_tensors ) : flattened_results = [ ] for idx in range ( len ( dataset_results ) + len ( flat_run_tensors ) ) : <MASK> flattened_results . append ( dataset_results [ idx ] ) else : flattened_results . append ( flat_run_tensors . pop ( 0 ) ) return flattened_results",if dataset_results . get ( idx ) :,if len ( flat_run_tensors ) == 1 :,90.0875731,FALSE,88.01
4121,"def test_k_is_stochastic_parameter ( self ) : # k as stochastic parameter aug = iaa . MedianBlur ( k = iap . Choice ( [ 3 , 5 ] ) ) seen = [ False , False ] for i in sm . xrange ( 100 ) : observed = aug . augment_image ( self . base_img ) <MASK> seen [ 0 ] + = True elif np . array_equal ( observed , self . blur5x5 ) : seen [ 1 ] + = True else : raise Exception ( "" Unexpected result in MedianBlur@2 "" ) if all ( seen ) : break assert np . all ( seen )","if np . array_equal ( observed , self . blur3x3 ) :","if np . array_equal ( observed , self . blur3x3 ) :",75,TRUE,100
4122,"def pickPath ( self , color ) : self . path [ color ] = ( ) currentPos = self . starts [ color ] while True : minDist = None minGuide = None for guide in self . guides [ color ] : guideDist = dist ( currentPos , guide ) if minDist == None or guideDist < minDist : minDist = guideDist minGuide = guide if dist ( currentPos , self . ends [ color ] ) == 1 : return <MASK> return self . path [ color ] = self . path [ color ] + ( minGuide , ) currentPos = minGuide self . guides [ color ] . remove ( minGuide )",if minGuide == None :,"if dist ( currentPos , self . ends [ color ] ) == 0 :",94.8587803,FALSE,88.84
4123,"def UpdateRepository ( self ) : if hasattr ( self , "" commit_update "" ) : <MASK> if not path . isdir ( "" .git/ "" ) : self . gitZipRepo ( ) call ( [ "" git "" , "" reset "" , "" --hard "" , "" origin/ {} "" . format ( self . getBranch ) ] ) self . ProcessCall_ ( [ "" git "" , "" pull "" , "" origin "" , self . getBranch ] ) self . ProcessCall_ ( [ "" pip "" , "" install "" , "" -r "" , "" requirements.txt "" ] )","if self . commit_update [ ""Updates"" ] != [ ] :",if self . commit_update :,68.70111841,FALSE,91.65
4124,"def callback ( result = Cr . NS_OK , message = None , success = None ) : if success is None : <MASK> success = Ci . koIAsyncCallback . RESULT_SUCCESSFUL else : success = Ci . koIAsyncCallback . RESULT_ERROR data = Namespace ( result = result , message = message , _com_interfaces_ = [ Ci . koIErrorInfo ] ) self . _invoke_activate_callbacks ( success , data )",if Cr . NS_SUCCEEDED ( result ) :,if result == Cr . NS_OK :,91.2986397,FALSE,92.46
4125,"def get_location ( device ) : location = [ ] node = device while node : position = node . get_position ( ) or "" "" <MASK> position = ""  [ %s ] "" % position location . append ( node . name + position ) node = node . parent return ""  /  "" . join ( reversed ( location ) )",if position :,if position :,100,TRUE,100
4126,"def load_checkpoint ( path , model , optimizer , reset_optimizer ) : global global_step global global_epoch print ( "" Load checkpoint from:  {} "" . format ( path ) ) checkpoint = _load ( path ) model . load_state_dict ( checkpoint [ "" state_dict "" ] ) if not reset_optimizer : optimizer_state = checkpoint [ "" optimizer "" ] <MASK> print ( "" Load optimizer state from  {} "" . format ( path ) ) optimizer . load_state_dict ( checkpoint [ "" optimizer "" ] ) global_step = checkpoint [ "" global_step "" ] global_epoch = checkpoint [ "" global_epoch "" ] return model",if optimizer_state is not None :,if optimizer_state is not None :,100,TRUE,100
4127,"def run_command ( self , command : str , data : Dict [ str , object ] ) - > Dict [ str , object ] : """"""Run a specific command from the registry."""""" key = "" cmd_ "" + command method = getattr ( self . __class__ , key , None ) if method is None : return { "" error "" : "" Unrecognized command  ' %s ' "" % command } else : <MASK> # Only the above commands use some error formatting. del data [ "" is_tty "" ] del data [ "" terminal_width "" ] return method ( self , * * data )","if command not in { ""check"" , ""recheck"" , ""run"" } :","if ""is_tty"" in data :",90.81344768,FALSE,88.26
4128,"def call_init ( self , node , instance ) : # Call __init__ on each binding. for b in instance . bindings : <MASK> continue self . _initialized_instances . add ( b . data ) node = self . _call_init_on_binding ( node , b ) return node",if b . data in self . _initialized_instances :,if b . data in self . _initialized_instances :,75,TRUE,100
4129,"def get_request_headers ( ) - > Dict : url = urlparse ( uri ) candidates = [ "" %s :// %s "" % ( url . scheme , url . netloc ) , "" %s :// %s / "" % ( url . scheme , url . netloc ) , uri , "" * "" , ] for u in candidates : <MASK> headers = dict ( DEFAULT_REQUEST_HEADERS ) headers . update ( self . config . linkcheck_request_headers [ u ] ) return headers return { }",if u in self . config . linkcheck_request_headers :,if u in self . config . linkcheck_request_headers :,100,TRUE,100
4130,"def get_next_video_frame ( self , skip_empty_frame = True ) : if not self . video_format : return while True : # We skip video packets which are not video frames # This happens in mkv files for the first few frames. video_packet = self . _get_video_packet ( ) if video_packet . image == 0 : self . _decode_video_packet ( video_packet ) <MASK> break if _debug : print ( "" Returning "" , video_packet ) return video_packet . image",if video_packet . image is not None or not skip_empty_frame :,if skip_empty_frame :,93.94967751,FALSE,90.75
4131,"def convert_path ( ctx , tpath ) : for points , code in tpath . iter_segments ( ) : if code == Path . MOVETO : ctx . move_to ( * points ) elif code == Path . LINETO : ctx . line_to ( * points ) <MASK> ctx . curve_to ( points [ 0 ] , points [ 1 ] , points [ 0 ] , points [ 1 ] , points [ 2 ] , points [ 3 ] ) elif code == Path . CURVE4 : ctx . curve_to ( * points ) elif code == Path . CLOSEPOLY : ctx . close_path ( )",elif code == Path . CURVE3 :,elif code == Path . CURVE3 :,75,TRUE,100
4132,"def __init__ ( self , layout , value = None , string = None , * , dtype : np . dtype = np . float64 ) - > None : """"""Constructor."""""" self . layout = layout if value is None : if string is None : self . value = np . zeros ( ( self . layout . gaDims , ) , dtype = dtype ) else : self . value = layout . parse_multivector ( string ) . value else : self . value = np . array ( value ) <MASK> raise ValueError ( "" value must be a sequence of length  %s "" % self . layout . gaDims )","if self . value . shape != ( self . layout . gaDims , ) :",if len ( self . value ) != self . layout . gaDims :,94.54409882,FALSE,93.12
4133,"def to_dict ( self ) : contexts_ = { } for k , data in self . contexts . items ( ) : data_ = data . copy ( ) if "" context "" in data_ : del data_ [ "" context "" ] <MASK> del data_ [ "" loaded "" ] contexts_ [ k ] = data_ return dict ( contexts = contexts_ )","if ""loaded"" in data_ :","if ""loaded"" in data_ :",100,TRUE,100
4134,"def include_module ( module ) : if not include_these : return True result = False for check in include_these : if "" /* "" in check : <MASK> result = True else : if ( os . getcwd ( ) + "" / "" + check + "" .py "" ) == module : result = True if result : print_status ( "" Including module:  "" + module ) return result",if check [ : - 1 ] in module :,"if ( os . path . join ( check , ""py"" ) ) == module :",82.77041919,FALSE,82.63
4135,"def extract_from ( msg_body , content_type = "" text/plain "" ) : try : if content_type == "" text/plain "" : return extract_from_plain ( msg_body ) <MASK> return extract_from_html ( msg_body ) except Exception : log . exception ( "" ERROR extracting message "" ) return msg_body","elif content_type == ""text/html"" :","elif content_type == ""text/html"" :",100,TRUE,100
4136,"def test_list ( self ) : self . _create_locations ( ) response = self . client . get ( self . geojson_boxedlocation_list_url ) self . assertEqual ( response . status_code , 200 ) self . assertEqual ( len ( response . data [ "" features "" ] ) , 2 ) for feature in response . data [ "" features "" ] : self . assertIn ( "" bbox "" , feature ) fid = feature [ "" id "" ] <MASK> self . assertEqual ( feature [ "" bbox "" ] , self . bl1 . bbox_geometry . extent ) elif fid == 2 : self . assertEqual ( feature [ "" bbox "" ] , self . bl2 . bbox_geometry . extent ) else : self . fail ( "" Unexpected id:  {0} "" . format ( fid ) ) BoxedLocation . objects . all ( ) . delete ( )",if fid == 1 :,if fid == 1 :,100,TRUE,100
4137,"def overrideCommand ( self , commandName , func ) : # Override entries in c.k.masterBindingsDict k = self d = k . masterBindingsDict for key in d : d2 = d . get ( key ) for key2 in d2 : bi = d2 . get ( key2 ) <MASK> bi . func = func d2 [ key2 ] = bi",if bi . commandName == commandName :,if bi :,70.87218539,FALSE,90.64
4138,"def _lookup ( components , specs , provided , name , i , l ) : if i < l : for spec in specs [ i ] . __sro__ : comps = components . get ( spec ) <MASK> r = _lookup ( comps , specs , provided , name , i + 1 , l ) if r is not None : return r else : for iface in provided : comps = components . get ( iface ) <MASK> r = comps . get ( name ) if r is not None : return r return None",if comps :,if comps is not None :,83.147177,FALSE,92.1
4139,"def to_representation ( self , value ) : old_social_string_fields = [ "" twitter "" , "" github "" , "" linkedIn "" ] request = self . context . get ( "" request "" ) show_old_format = ( request and is_deprecated ( request . version , self . min_version ) and request . method == "" GET "" ) if show_old_format : social = value . copy ( ) for key in old_social_string_fields : if social . get ( key ) : social [ key ] = value [ key ] [ 0 ] <MASK> social [ key ] = "" "" value = social return super ( SocialField , self ) . to_representation ( value )",elif social . get ( key ) == [ ] :,elif not value [ key ] :,89.73089208,FALSE,93.53
4140,"def process_ref_attribute ( self , node , array_type = None ) : ref = qname_attr ( node , "" ref "" ) if ref : ref = self . _create_qname ( ref ) # Some wsdl's reference to xs:schema, we ignore that for now. It # might be better in the future to process the actual schema file # so that it is handled correctly <MASK> return return xsd_elements . RefAttribute ( node . tag , ref , self . schema , array_type = array_type )","if ref . namespace == ""http://www.w3.org/2001/XMLSchema"" :",if not self . schema :,96.60556455,FALSE,82.55
4141,"def unescape ( text ) : """"""Removes '\\' escaping from 'text'."""""" rv = "" "" i = 0 while i < len ( text ) : <MASK> rv + = text [ i + 1 ] i + = 1 else : rv + = text [ i ] i + = 1 return rv","if i + 1 < len ( text ) and text [ i ] == ""\\"" :","if text [ i ] == ""\\"" :",71.8481504,FALSE,87.01
4142,"def wait_child_process ( signum , frame ) : try : while True : child_pid , status = os . waitpid ( - 1 , os . WNOHANG ) if child_pid == 0 : stat_logger . info ( "" no child process was immediately available "" ) break exitcode = status >> 8 stat_logger . info ( "" child process  %s  exit with exitcode  %s "" , child_pid , exitcode ) except OSError as e : <MASK> stat_logger . warning ( "" current process has no existing unwaited-for child processes. "" ) else : raise",if e . errno == errno . ECHILD :,if e . errno == errno . ESRCH :,98.76582279,FALSE,97.95
4143,"def translate_from_sortname ( name , sortname ) : """"""'Translate' the artist name by reversing the sortname."""""" for c in name : ctg = unicodedata . category ( c ) if ctg [ 0 ] == "" L "" and unicodedata . name ( c ) . find ( "" LATIN "" ) == - 1 : for separator in ( ""  &  "" , "" ;  "" , ""  and  "" , ""  vs.  "" , ""  with  "" , ""  y  "" ) : <MASK> parts = sortname . split ( separator ) break else : parts = [ sortname ] separator = "" "" return separator . join ( map ( _reverse_sortname , parts ) ) return name",if separator in sortname :,if sortname . startswith ( separator ) :,94.71793779,FALSE,95.07
4144,"def python_value ( self , value ) : if value : <MASK> pp = lambda x : x . time ( ) return format_date_time ( value , self . formats , pp ) elif isinstance ( value , datetime . datetime ) : return value . time ( ) if value is not None and isinstance ( value , datetime . timedelta ) : return ( datetime . datetime . min + value ) . time ( ) return value","if isinstance ( value , basestring ) :","if isinstance ( value , datetime . datetime ) :",89.78912109,FALSE,95.16
4145,"def __init__ ( self , fileobj , info ) : pages = [ ] complete = False while not complete : page = OggPage ( fileobj ) <MASK> pages . append ( page ) complete = page . complete or ( len ( page . packets ) > 1 ) data = OggPage . to_packets ( pages ) [ 0 ] [ 7 : ] super ( OggTheoraCommentDict , self ) . __init__ ( data , framing = False ) self . _padding = len ( data ) - self . _size",if page . serial == info . serial :,if page . info == info :,92.79690104,FALSE,94.58
4146,"def configure ( self ) : # hack to configure 'from_' and 'to' and avoid exception if "" from_ "" in self . wmeta . properties : from_ = float ( self . wmeta . properties [ "" from_ "" ] ) to = float ( self . wmeta . properties . get ( "" to "" , 0 ) ) <MASK> to = from_ + 1 self . wmeta . properties [ "" to "" ] = str ( to ) super ( TKSpinbox , self ) . configure ( )",if from_ > to :,if from_ < to :,73.67906919,FALSE,97.52
4147,"def get_error_diagnostics ( self ) : diagnostics = [ ] if self . stdout is not None : with open ( self . stdout . name ) as fds : contents = fds . read ( ) . strip ( ) <MASK> diagnostics . append ( "" ab STDOUT: \n "" + contents ) if self . stderr is not None : with open ( self . stderr . name ) as fds : contents = fds . read ( ) . strip ( ) <MASK> diagnostics . append ( "" ab STDERR: \n "" + contents ) return diagnostics",if contents . strip ( ) :,if contents :,91.64604886,FALSE,90.54
4148,"def set_environment_vars ( env , source_env ) : """"""Copy allowed environment variables from |source_env|."""""" if not source_env : return for name , value in six . iteritems ( source_env ) : <MASK> # Avoid creating circular dependencies from importing environment by # using os.getenv. if os . getenv ( "" TRUSTED_HOST "" ) and should_rebase_environment_value ( name ) : value = file_host . rebase_to_worker_root ( value ) env [ name ] = value",if is_forwarded_environment_variable ( name ) :,if name not in env :,66.36340382,FALSE,90.74
4149,"def update_content ( self , more_content : StringList ) - > None : if isinstance ( self . object , TypeVar ) : attrs = [ repr ( self . object . __name__ ) ] for constraint in self . object . __constraints__ : attrs . append ( stringify_typehint ( constraint ) ) if self . object . __covariant__ : attrs . append ( "" covariant=True "" ) <MASK> attrs . append ( "" contravariant=True "" ) more_content . append ( _ ( "" alias of TypeVar( %s ) "" ) % "" ,  "" . join ( attrs ) , "" "" ) more_content . append ( "" "" , "" "" ) super ( ) . update_content ( more_content )",if self . object . __contravariant__ :,if self . object . __contravariant__ :,100,TRUE,100
4150,"def after ( self , event , state ) : group = event . group for plugin in self . get_plugins ( ) : <MASK> continue metrics . incr ( "" notifications.sent "" , instance = plugin . slug ) yield self . future ( plugin . rule_notify )","if not safe_execute ( plugin . should_notify , group = group , event = event ) :",if group . slug == plugin . slug :,81.42417304,FALSE,73.23
4151,"def distinct ( expr , * on ) : fields = frozenset ( expr . fields ) _on = [ ] append = _on . append for n in on : if isinstance ( n , Field ) : <MASK> n = n . _name else : raise ValueError ( "" {0}  is not a field of  {1} "" . format ( n , expr ) ) if not isinstance ( n , _strtypes ) : raise TypeError ( "" on must be a name or field, not:  {0} "" . format ( n ) ) elif n not in fields : raise ValueError ( "" {0}  is not a field of  {1} "" . format ( n , expr ) ) append ( n ) return Distinct ( expr , tuple ( _on ) )",if n . _child . isidentical ( expr ) :,if n . _name :,90.8557375,FALSE,95.33
4152,"def build_filter ( arg ) : filt = { } if arg is not None : <MASK> raise UserError ( "" Arguments to --filter should be in form KEY=VAL "" ) key , val = arg . split ( "" = "" , 1 ) filt [ key ] = val return filt","if ""="" not in arg :","if ""="" not in arg :",100,TRUE,100
4153,"def pickline ( file , key , casefold = 1 ) : try : f = open ( file , "" r "" ) except IOError : return None pat = re . escape ( key ) + "" : "" prog = re . compile ( pat , casefold and re . IGNORECASE ) while 1 : line = f . readline ( ) if not line : break <MASK> text = line [ len ( key ) + 1 : ] while 1 : line = f . readline ( ) if not line or not line [ 0 ] . isspace ( ) : break text = text + line return text . strip ( ) return None",if prog . match ( line ) :,if prog . search ( line ) :,99.02433197,FALSE,98.01
4154,"def delete_doc ( elastic_document_id , node , index = None , category = None ) : index = index or INDEX if not category : <MASK> category = "" preprint "" elif node . is_registration : category = "" registration "" else : category = node . project_or_component client ( ) . delete ( index = index , doc_type = category , id = elastic_document_id , refresh = True , ignore = [ 404 ] , )","if isinstance ( node , Preprint ) :",if node . is_preprint :,89.86582314,FALSE,92.77
4155,"def update ( self , preds , labels ) : if not _is_numpy_ ( labels ) : raise ValueError ( "" The  ' labels '  must be a numpy ndarray. "" ) if not _is_numpy_ ( preds ) : raise ValueError ( "" The  ' predictions '  must be a numpy ndarray. "" ) for i , lbl in enumerate ( labels ) : value = preds [ i , 1 ] bin_idx = int ( value * self . _num_thresholds ) assert bin_idx < = self . _num_thresholds <MASK> self . _stat_pos [ bin_idx ] + = 1.0 else : self . _stat_neg [ bin_idx ] + = 1.0",if lbl :,if lbl == 1 :,97.48613662,FALSE,96.85
4156,"def checkStatusClient ( self ) : if str ( self . comboxBoxIPAddress . currentText ( ) ) != "" "" : <MASK> self . btnEnable . setEnabled ( False ) self . btncancel . setEnabled ( True ) return None self . btnEnable . setEnabled ( True ) self . btncancel . setEnabled ( False )","if self . ClientsLogged [ str ( self . comboxBoxIPAddress . currentText ( ) ) ] [ ""Status"" ] :","if self . comboxBoxIPAddress . currentText ( ) == """" :",88.51675575,FALSE,80.84
4157,"def colorizeDiffs ( sheet , col , row , cellval ) : if not row or not col : return None vcolidx = sheet . visibleCols . index ( col ) rowidx = sheet . rows . index ( row ) if vcolidx < len ( othersheet . visibleCols ) and rowidx < len ( othersheet . rows ) : otherval = othersheet . visibleCols [ vcolidx ] . getDisplayValue ( othersheet . rows [ rowidx ] ) <MASK> return "" color_diff "" else : return "" color_diff_add """,if cellval . display != otherval :,if cellval == otherval :,83.87108059,FALSE,95.51
4158,"def identwaf ( self , findall = False ) : detected = list ( ) try : self . attackres = self . performCheck ( self . centralAttack ) except RequestBlocked : return detected for wafvendor in self . checklist : self . log . info ( "" Checking for  %s "" % wafvendor ) <MASK> detected . append ( wafvendor ) if not findall : break self . knowledge [ "" wafname "" ] = detected return detected",if self . wafdetections [ wafvendor ] ( self ) :,if self . attackres [ wafvendor ] :,92.54652275,FALSE,92.02
4159,"def get_repository_metadata_by_repository_id_changeset_revision ( app , id , changeset_revision , metadata_only = False ) : """"""Get a specified metadata record for a specified repository in the tool shed."""""" if metadata_only : repository_metadata = get_repository_metadata_by_changeset_revision ( app , id , changeset_revision ) <MASK> return repository_metadata . metadata return None return get_repository_metadata_by_changeset_revision ( app , id , changeset_revision )",if repository_metadata and repository_metadata . metadata :,if repository_metadata :,85.34570125,FALSE,94.28
4160,"def getmultiline ( self ) : line = self . getline ( ) if line [ 3 : 4 ] == "" - "" : code = line [ : 3 ] while 1 : nextline = self . getline ( ) line = line + ( "" \n "" + nextline ) <MASK> break return line","if nextline [ : 3 ] == code and nextline [ 3 : 4 ] != ""-"" :","if code in ( "" "" , "" "" ) :",82.26424875,FALSE,72.84
4161,"def _validate_reports ( value , * args , * * kwargs ) : from osf . models import OSFUser for key , val in value . items ( ) : if not OSFUser . load ( key ) : raise ValidationValueError ( "" Keys must be user IDs "" ) <MASK> raise ValidationTypeError ( "" Values must be dictionaries "" ) if ( "" category "" not in val or "" text "" not in val or "" date "" not in val or "" retracted "" not in val ) : raise ValidationValueError ( ( "" Values must include `date`, `category`,  "" , "" `text`, `retracted` keys "" ) )","if not isinstance ( val , dict ) :","if not isinstance ( val , dict ) :",100,TRUE,100
4162,"def deselectItem ( self , item ) : if self . isSelected ( item ) : <MASK> listItem = self . _getListItem ( item ) selections = self . getSelectedItems ( ) selections . remove ( self . loadHandler . getSelection ( listItem ) ) self . setSelections ( selections ) else : self . deselectAll ( )",if self . multiSelect :,if self . loadHandler . getSelected ( item ) :,90.79856789,FALSE,88.67
4163,"def __init__ ( self , * * kwargs ) : if self . name is None : raise RuntimeError ( "" RenderPrimitive cannot be used directly "" ) self . option_values = { } for key , val in kwargs . items ( ) : if not key in self . options : raise ValueError ( "" primitive ` {0} '  has no option ` {1} ' "" . format ( self . name , key ) ) self . option_values [ key ] = val # set up defaults for name , ( description , default ) in self . options . items ( ) : <MASK> self . option_values [ name ] = default",if not name in self . option_values :,"if description == ""default"" :",96.74414757,FALSE,93.08
4164,"def setup_smart_indent ( self , view , lang ) : # Configure a ""per-view"" instance if type ( view ) == gedit . View : <MASK> setattr ( view , "" smart_indent_instance "" , SmartIndent ( ) ) handler_id = view . connect ( "" key-press-event "" , view . smart_indent_instance . key_press_handler ) self . handler_ids . append ( ( handler_id , view ) ) view . smart_indent_instance . set_language ( lang , view )","if getattr ( view , ""smart_indent_instance"" , False ) == False :","if getattr ( view , ""smart_indent_instance"" , None ) is None :",71.48317211,FALSE,94.93
4165,"def get_strings_of_set ( word , char_set , threshold = 20 ) : count = 0 letters = "" "" strings = [ ] for char in word : <MASK> letters + = char count + = 1 else : if count > threshold : strings . append ( letters ) letters = "" "" count = 0 if count > threshold : strings . append ( letters ) return strings",if char in char_set :,if char in char_set :,100,TRUE,100
4166,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 10 : self . set_logout_url ( d . getPrefixedString ( ) ) continue <MASK> raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 0 :,if tt == 0 :,100,TRUE,100
4167,def __create_table ( self ) : for i in range ( 256 ) : crcreg = i for j in range ( 8 ) : <MASK> crcreg = self . __CRCPOLYNOMIAL ^ ( crcreg >> 1 ) else : crcreg >> = 1 self . __crctable [ i ] = crcreg,if ( crcreg & 1 ) != 0 :,if j == 0 :,84.19006732,FALSE,88.97
4168,"def destroy ( self ) : """"""Flush all entries and empty cache"""""" # Note: this method is currently also used for dropping the cache for i in range ( len ( self . cached_rows ) ) : id_ = self . cached_rows [ i ] self . cached_rows [ i ] = None <MASK> try : inode = self . attrs [ id_ ] except KeyError : # We may have deleted that inode pass else : del self . attrs [ id_ ] self . setattr ( inode ) assert len ( self . attrs ) == 0",if id_ is not None :,if id_ :,72.42798331,FALSE,96.25
4169,"def set_config ( self ) : """"""Set configuration options for QTextEdit."""""" c = self . c w = self . widget w . setWordWrapMode ( QtGui . QTextOption . NoWrap ) if 0 : # This only works when there is no style sheet. n = c . config . getInt ( "" qt-rich-text-zoom-in "" ) <MASK> w . zoomIn ( n ) w . updateMicroFocus ( ) # tab stop in pixels - no config for this (yet) w . setTabStopWidth ( 24 )","if n not in ( None , 0 ) :",if n :,94.92833698,FALSE,91.72
4170,"def mouseDragEvent ( self , ev ) : if self . movable and ev . button ( ) == QtCore . Qt . LeftButton : if ev . isStart ( ) : self . moving = True self . cursorOffset = self . pos ( ) - self . mapToParent ( ev . buttonDownPos ( ) ) self . startPosition = self . pos ( ) ev . accept ( ) if not self . moving : return self . setPos ( self . cursorOffset + self . mapToParent ( ev . pos ( ) ) ) self . sigDragged . emit ( self ) <MASK> self . moving = False self . sigPositionChangeFinished . emit ( self )",if ev . isFinish ( ) :,elif ev . isEnd ( ) :,96.58661466,FALSE,96.16
4171,"def reparentChildren ( self , newParent ) : if newParent . childNodes : newParent . childNodes [ - 1 ] . _element . tail + = self . _element . text else : if not newParent . _element . text : newParent . _element . text = "" "" <MASK> newParent . _element . text + = self . _element . text self . _element . text = "" "" base . Node . reparentChildren ( self , newParent )",if self . _element . text is not None :,if self . _element . text :,94.4284277,FALSE,95.27
4172,"def _no_sp_or_bp ( self , bl ) : for s in bl . vex . statements : for e in chain ( [ s ] , s . expressions ) : <MASK> reg = self . get_reg_name ( self . project . arch , e . offset ) if reg == "" ebp "" or reg == "" esp "" : return False elif e . tag == "" Ist_Put "" : reg = self . get_reg_name ( self . project . arch , e . offset ) if reg == "" ebp "" or reg == "" esp "" : return False return True","if e . tag == ""Iex_Get"" :","if e . tag == ""Ist_Get"" :",98.87138362,FALSE,98.16
4173,"def _get_import_chain ( self , * , until = None ) : stack = inspect . stack ( ) [ 2 : ] try : for frameinfo in stack : try : <MASK> continue data = dedent ( "" "" . join ( frameinfo . code_context ) ) if data . strip ( ) == until : raise StopIteration yield frameinfo . filename , frameinfo . lineno , data . strip ( ) del data finally : del frameinfo finally : del stack",if not frameinfo . code_context :,if not frameinfo . code_context :,100,TRUE,100
4174,"def stream_docker_log ( log_stream ) : async for line in log_stream : if "" stream "" in line and line [ "" stream "" ] . strip ( ) : logger . debug ( line [ "" stream "" ] . strip ( ) ) elif "" status "" in line : logger . debug ( line [ "" status "" ] . strip ( ) ) <MASK> logger . error ( line [ "" error "" ] . strip ( ) ) raise DockerBuildError","elif ""error"" in line :","elif ""error"" in line :",100,TRUE,100
4175,"def get_cycle_path ( self , curr_node , goal_node_index ) : for dep in curr_node [ "" deps "" ] : if dep == goal_node_index : return [ curr_node [ "" address "" ] ] for dep in curr_node [ "" deps "" ] : path = self . get_cycle_path ( self . get_by_address ( dep ) , goal_node_index ) # self.nodelist[dep], goal_node_index) <MASK> path . insert ( 0 , curr_node [ "" address "" ] ) return path return [ ]",if len ( path ) > 0 :,if len ( path ) > 0 :,100,TRUE,100
4176,"def prompt ( default = None ) : editor = "" nano "" with tempfile . NamedTemporaryFile ( mode = "" r+ "" ) as tmpfile : <MASK> tmpfile . write ( default ) tmpfile . flush ( ) child_pid = os . fork ( ) is_child = child_pid == 0 if is_child : os . execvp ( editor , [ editor , tmpfile . name ] ) else : os . waitpid ( child_pid , 0 ) tmpfile . seek ( 0 ) return tmpfile . read ( ) . strip ( )",if default :,if default is not None :,96.48548374,FALSE,95.87
4177,"def _get_annotated_template ( self , template ) : changed = False if template . get ( "" version "" , "" 0.12.0 "" ) > = "" 0.13.0 "" : using_js = self . spider . _filter_js_urls ( template [ "" url "" ] ) body = "" rendered_body "" if using_js else "" original_body "" <MASK> template [ "" body "" ] = body changed = True if changed or not template . get ( "" annotated "" ) : _build_sample ( template ) return template","if template . get ( ""body"" ) != body :","if not self . spider . _apply_template ( template , body ) :",90.63870012,FALSE,88.21
4178,"def collect ( self , paths ) : for path in paths or ( ) : relpath = os . path . relpath ( path , self . _artifact_root ) dst = os . path . join ( self . _directory , relpath ) safe_mkdir ( os . path . dirname ( dst ) ) <MASK> shutil . copytree ( path , dst ) else : shutil . copy ( path , dst ) self . _relpaths . add ( relpath )",if os . path . isdir ( path ) :,if os . path . isdir ( path ) :,100,TRUE,100
4179,"def dependencies ( context = None ) : """"""Return all dependencies detected by knowit."""""" deps = OrderedDict ( [ ] ) try : initialize ( context ) for name , provider_cls in _provider_map . items ( ) : <MASK> deps [ name ] = available_providers [ name ] . version else : deps [ name ] = { } except Exception : pass return deps",if name in available_providers :,if provider_cls is None :,88.64017985,FALSE,92.47
4180,"def _getaddrinfo ( self , host_bytes , port , family , socktype , proto , flags ) : while True : ares = self . cares try : return self . __getaddrinfo ( host_bytes , port , family , socktype , proto , flags ) except gaierror : <MASK> raise",if ares is self . cares :,if ares is None :,89.4138994,FALSE,93.08
4181,"def write_entries ( cmd , basename , filename ) : ep = cmd . distribution . entry_points if isinstance ( ep , basestring ) or ep is None : data = ep elif ep is not None : data = [ ] for section , contents in ep . items ( ) : <MASK> contents = EntryPoint . parse_group ( section , contents ) contents = "" \n "" . join ( map ( str , contents . values ( ) ) ) data . append ( "" [ %s ] \n %s \n \n "" % ( section , contents ) ) data = "" "" . join ( data ) cmd . write_or_delete_file ( "" entry points "" , filename , data , True )","if not isinstance ( contents , basestring ) :","if isinstance ( contents , dict ) :",88.39663156,FALSE,96.68
4182,"def _highlight_do ( self ) : new_hl_text = self . highlight_text . text ( ) if new_hl_text != self . hl_text : self . hl_text = new_hl_text if self . hl is not None : self . hl . setDocument ( None ) self . hl = None <MASK> self . hl = Highlighter ( self . hl_text , parent = self . doc ) self . clear_highlight_button . setEnabled ( bool ( self . hl ) )",if self . hl_text :,if self . highlight_text . text ( ) != self . hl_text :,90.56090969,FALSE,90.74
4183,"def traverse ( node , functions = [ ] ) : if hasattr ( node , "" grad_fn "" ) : node = node . grad_fn if hasattr ( node , "" variable "" ) : node = graph . nodes_by_id . get ( id ( node . variable ) ) <MASK> node . functions = list ( functions ) del functions [ : ] if hasattr ( node , "" next_functions "" ) : functions . append ( type ( node ) . __name__ ) for f in node . next_functions : if f [ 0 ] : functions . append ( type ( f [ 0 ] ) . __name__ ) traverse ( f [ 0 ] , functions ) if hasattr ( node , "" saved_tensors "" ) : for t in node . saved_tensors : traverse ( t )",if node :,if functions :,97.88483068,FALSE,98.5
4184,"def compress ( self , data_list ) : if data_list : page_id = data_list [ 1 ] if page_id in EMPTY_VALUES : <MASK> return None raise forms . ValidationError ( self . error_messages [ "" invalid_page "" ] ) return Page . objects . get ( pk = page_id ) return None",if not self . required :,if Page . objects . filter ( pk = page_id ) . exists ( ) :,84.55709264,FALSE,79.77
4185,"def test_field_attr_existence ( self ) : for name , item in ast . __dict__ . items ( ) : if self . _is_ast_node ( name , item ) : if name == "" Index "" : # Index(value) just returns value now. # The argument is required. continue x = item ( ) <MASK> self . assertEqual ( type ( x . _fields ) , tuple )","if isinstance ( x , ast . AST ) :","if isinstance ( x , ast . Field ) :",73.32809015,FALSE,97.39
4186,"def handle_starttag ( self , tag , attrs ) : if tag == "" base "" : self . base_url = dict ( attrs ) . get ( "" href "" ) if self . scan_tag ( tag ) : for attr , value in attrs : if self . scan_attr ( attr ) : <MASK> value = strip_html5_whitespace ( value ) url = self . process_attr ( value ) link = Link ( url = url ) self . links . append ( link ) self . current_link = link",if self . strip :,if self . scan_html5_whitespace ( attr ) :,96.32294654,FALSE,91.9
4187,"def _initialize_asset_map ( cls ) : # Generating a list of acceptable asset files reduces the possibility of # path attacks. cls . _asset_name_to_path = { } assets = os . listdir ( ASSETS_PATH ) for asset in assets : path = os . path . join ( ASSETS_PATH , asset ) <MASK> cls . _asset_name_to_path [ os . path . basename ( path ) ] = path",if os . path . isfile ( path ) :,if os . path . exists ( path ) :,73.34064158,FALSE,97.47
4188,"def dataReceived ( self , data ) : self . buf + = data if self . _paused : log . startLogging ( sys . stderr ) log . msg ( "" dataReceived while transport paused! "" ) self . transport . loseConnection ( ) else : self . transport . write ( data ) <MASK> self . transport . loseConnection ( ) else : self . pause ( )","if self . buf . endswith ( b""\n0\n"" ) :",if self . _paused :,89.41684835,FALSE,83.72
4189,"def test_case_sensitive ( self ) : with support . EnvironmentVarGuard ( ) as env : env . unset ( "" PYTHONCASEOK "" ) <MASK> self . skipTest ( "" os.environ changes not reflected in  "" "" _os.environ "" ) loader = self . find_module ( ) self . assertIsNone ( loader )","if b""PYTHONCASEOK"" in _bootstrap_external . _os . environ :","if ""PYTHONCASEOK"" not in _os . environ :",91.25884376,FALSE,87.37
4190,"def manifest ( self ) : """"""The current manifest dictionary."""""" if self . reload : if not self . exists ( self . manifest_path ) : return { } mtime = self . getmtime ( self . manifest_path ) <MASK> self . _manifest = self . get_manifest ( ) self . _mtime = mtime return self . _manifest",if self . _mtime is None or mtime > self . _mtime :,if mtime is not None :,80.46663998,FALSE,83.97
4191,"def test_named_parameters_and_constraints ( self ) : likelihood = gpytorch . likelihoods . GaussianLikelihood ( ) model = ExactGPModel ( None , None , likelihood ) for name , _param , constraint in model . named_parameters_and_constraints ( ) : <MASK> self . assertIsInstance ( constraint , gpytorch . constraints . GreaterThan ) elif name == "" mean_module.constant "" : self . assertIsNone ( constraint ) elif name == "" covar_module.raw_outputscale "" : self . assertIsInstance ( constraint , gpytorch . constraints . Positive ) elif name == "" covar_module.base_kernel.raw_lengthscale "" : self . assertIsInstance ( constraint , gpytorch . constraints . Positive )","if name == ""likelihood.noise_covar.raw_noise"" :","if name == ""gpytorch.gaussian_bias.gaussian_bias""",94.89963439,FALSE,92.64
4192,"def process_plugin_result ( name , result ) : if result : try : jsonify ( test = result ) except Exception : logger . exception ( "" Error while jsonifying settings from plugin  {} , please contact the plugin author about this "" . format ( name ) ) raise else : <MASK> del result [ "" __enabled "" ] data [ name ] = result","if ""__enabled"" in result :","if ""__enabled"" in result :",100,TRUE,100
4193,"def benchmarking ( net , ctx , num_iteration , datashape = 300 , batch_size = 64 ) : input_shape = ( batch_size , 3 ) + ( datashape , datashape ) data = mx . random . uniform ( - 1.0 , 1.0 , shape = input_shape , ctx = ctx , dtype = "" float32 "" ) dryrun = 5 for i in range ( dryrun + num_iteration ) : <MASK> tic = time . time ( ) ids , scores , bboxes = net ( data ) ids . asnumpy ( ) scores . asnumpy ( ) bboxes . asnumpy ( ) toc = time . time ( ) - tic return toc",if i == dryrun :,if i % 2 == 0 :,92.68516722,FALSE,95.64
4194,"def merge_weekdays ( base_wd , icu_wd ) : result = [ ] for left , right in zip ( base_wd , icu_wd ) : <MASK> result . append ( left ) continue left = set ( left . split ( "" | "" ) ) right = set ( right . split ( "" | "" ) ) result . append ( "" | "" . join ( left | right ) ) return result",if left == right :,"if left . startswith ( ""w"" ) or right . startswith ( ""w"" ) :",87.25000653,FALSE,82.97
4195,"def create_key ( self , request ) : if self . _ignored_parameters : url , body = self . _remove_ignored_parameters ( request ) else : url , body = request . url , request . body key = hashlib . sha256 ( ) key . update ( _to_bytes ( request . method . upper ( ) ) ) key . update ( _to_bytes ( url ) ) if request . body : key . update ( _to_bytes ( body ) ) else : <MASK> for name , value in sorted ( request . headers . items ( ) ) : key . update ( _to_bytes ( name ) ) key . update ( _to_bytes ( value ) ) return key . hexdigest ( )",if self . _include_get_headers and request . headers != _DEFAULT_HEADERS :,if request . headers :,94.24358917,FALSE,88.85
4196,"def test_invalid_mountinfo ( self ) : line = ( "" 20 1 252:1 / / rw,relatime - ext4 /dev/mapper/vg0-root "" "" rw,errors=remount-ro,data=ordered "" ) elements = line . split ( ) for i in range ( len ( elements ) + 1 ) : lines = [ ""   "" . join ( elements [ 0 : i ] ) ] <MASK> expected = None else : expected = ( "" /dev/mapper/vg0-root "" , "" ext4 "" , "" / "" ) self . assertEqual ( expected , util . parse_mount_info ( "" / "" , lines ) )",if i < 10 :,if i == 0 :,89.65111918,FALSE,96.67
4197,"def nested_filter ( self , items , mask ) : keep_current = self . current_mask ( mask ) keep_nested_lookup = self . nested_masks ( mask ) for k , v in items : keep_nested = keep_nested_lookup . get ( k ) if k in keep_current : if keep_nested is not None : <MASK> yield k , dict ( self . nested_filter ( v . items ( ) , keep_nested ) ) else : yield k , v","if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",100,TRUE,100
4198,"def traverse_trees ( node_pos , sample , trees : List [ HeteroDecisionTreeGuest ] ) : if node_pos [ "" reach_leaf_node "" ] . all ( ) : return node_pos for t_idx , tree in enumerate ( trees ) : cur_node_idx = node_pos [ "" node_pos "" ] [ t_idx ] # reach leaf if cur_node_idx == - 1 : continue rs , reach_leaf = HeteroSecureBoostingTreeGuest . traverse_a_tree ( tree , sample , cur_node_idx ) <MASK> node_pos [ "" reach_leaf_node "" ] [ t_idx ] = True node_pos [ "" node_pos "" ] [ t_idx ] = rs return node_pos",if reach_leaf :,if not reach_leaf :,98.99152839,FALSE,98.4
4199,"def _pop_waiting_trial_id ( self ) - > Optional [ int ] : # TODO(c-bata): Reduce database query counts for extracting waiting trials. for trial in self . _storage . get_all_trials ( self . _study_id , deepcopy = False ) : <MASK> continue if not self . _storage . set_trial_state ( trial . _trial_id , TrialState . RUNNING ) : continue _logger . debug ( "" Trial  {}  popped from the trial queue. "" . format ( trial . number ) ) return trial . _trial_id return None",if trial . state != TrialState . WAITING :,if not trial . _trial_id :,70.94765773,FALSE,93.17
4200,"def get_step_best ( self , step_models ) : best_score = None best_model = "" "" for model in step_models : model_info = self . models_trained [ model ] score = model_info . get_score ( ) <MASK> continue if best_score is None or score < best_score : best_score = score best_model = model LOGGER . info ( f "" step  { self . n_step } , best model  { best_model } "" ) return best_model",if score is None :,if score is None :,100,TRUE,100
4201,"def iter_filters ( filters , block_end = False ) : queue = deque ( filters ) while queue : f = queue . popleft ( ) if f is not None and f . type in ( "" or "" , "" and "" , "" not "" ) : <MASK> queue . appendleft ( None ) for gf in f . filters : queue . appendleft ( gf ) yield f",if block_end :,if block_end :,100,TRUE,100
4202,"def _buffer_decode ( self , input , errors , final ) : if self . decoder is None : ( output , consumed , byteorder ) = codecs . utf_16_ex_decode ( input , errors , 0 , final ) if byteorder == - 1 : self . decoder = codecs . utf_16_le_decode <MASK> self . decoder = codecs . utf_16_be_decode elif consumed > = 2 : raise UnicodeError ( "" UTF-16 stream does not start with BOM "" ) return ( output , consumed ) return self . decoder ( input , self . errors , final )",elif byteorder == 1 :,elif byteorder == 1 :,100,TRUE,100
4203,"def _load_db ( self ) : try : with open ( self . db ) as db : content = db . read ( 8 ) db . seek ( 0 ) <MASK> data = StringIO ( ) if self . encryptor : self . encryptor . decrypt ( db , data ) else : raise EncryptionError ( "" Encrpyted credential storage:  {} "" . format ( self . db ) ) return json . loads ( data . getvalue ( ) ) else : return json . load ( db ) except : return { "" creds "" : [ ] }","if content == ( ""Salted__"" ) :","if content == b"""" :",94.78596863,FALSE,93.15
4204,"def _getbytes ( self , start , l = 1 ) : out = [ ] for ad in range ( l ) : offset = ad + start + self . base_address <MASK> raise IOError ( "" not enough bytes "" ) out . append ( int_to_byte ( Byte ( offset ) ) ) return b "" "" . join ( out )",if not is_mapped ( offset ) :,if not ( 0 <= offset < self . size ) :,92.76131935,FALSE,87.66
4205,"def cache_sqs_queues_across_accounts ( ) - > bool : function : str = f "" { __name__ } . { sys . _getframe ( ) . f_code . co_name } "" # First, get list of accounts accounts_d : list = async_to_sync ( get_account_id_to_name_mapping ) ( ) # Second, call tasks to enumerate all the roles across all accounts for account_id in accounts_d . keys ( ) : if config . get ( "" environment "" ) == "" prod "" : cache_sqs_queues_for_account . delay ( account_id ) else : <MASK> cache_sqs_queues_for_account . delay ( account_id ) stats . count ( f "" { function } .success "" ) return True","if account_id in config . get ( ""celery.test_account_ids"" , [ ] ) :","if config . get ( ""environment"" ) == ""prod"" :",95.84517007,FALSE,90.26
4206,"def insertLine ( self , refnum , linenum , line ) : i = - 1 for i , row in enumerate ( self . rows ) : if row [ 0 ] == linenum : <MASK> row [ refnum + 1 ] = line return # else keep looking elif row [ 0 ] > linenum : break self . rows . insert ( i , self . newRow ( linenum , refnum , line ) )",if row [ refnum + 1 ] is None :,if i == 0 :,85.19276889,FALSE,88.96
4207,"def __setattr__ ( self , name , val ) : if self . __dict__ . get ( name , "" hamster_graphics_no_value_really "" ) == val : return Sprite . __setattr__ ( self , name , val ) if name == "" image_data "" : self . _surface = None <MASK> self . __dict__ [ "" width "" ] = self . image_data . get_width ( ) self . __dict__ [ "" height "" ] = self . image_data . get_height ( )",if self . image_data :,"if self . __dict__ . get ( name , None ) is not None :",92.77104986,FALSE,88.33
4208,"def process_signature ( app , what , name , obj , options , signature , return_annotation ) : if signature : # replace Mock function names signature = re . sub ( "" <Mock name= ' ([^ ' ]+) ' .*> "" , "" \ g<1> "" , signature ) signature = re . sub ( "" tensorflow "" , "" tf "" , signature ) # add scope name to layer signatures: if hasattr ( obj , "" use_scope "" ) : <MASK> signature = signature [ 0 ] + "" variable_scope_name,  "" + signature [ 1 : ] elif obj . use_scope is None : signature = signature [ 0 ] + "" [variable_scope_name,]  "" + signature [ 1 : ] # signature: arg list return signature , return_annotation",if obj . use_scope :,if obj . use_scope is True :,99.06799824,FALSE,97.96
4209,"def L_op ( self , inputs , outputs , gout ) : ( x , ) = inputs ( gz , ) = gout if x . type in complex_types : raise NotImplementedError ( ) if outputs [ 0 ] . type in discrete_types : <MASK> return [ x . zeros_like ( dtype = theano . config . floatX ) ] else : return [ x . zeros_like ( ) ] return ( gz * ( 1 - sqr ( tanh ( x ) ) ) , )",if x . type in discrete_types :,if x . type in discrete_types :,100,TRUE,100
4210,"def confirm_on_console ( topic , msg ) : done = False print ( topic ) while not done : output = raw_input ( msg + "" :[y/n] "" ) <MASK> return True if output . lower ( ) == "" n "" : return False","if output . lower ( ) == ""y"" :","if output . lower ( ) == ""y"" :",100,TRUE,100
4211,"def replace_documentation_for_matching_shape ( self , event_name , section , * * kwargs ) : if self . _shape_name == section . context . get ( "" shape "" ) : self . _replace_documentation ( event_name , section ) for section_name in section . available_sections : sub_section = section . get_section ( section_name ) <MASK> self . _replace_documentation ( event_name , sub_section ) else : self . replace_documentation_for_matching_shape ( event_name , sub_section )","if self . _shape_name == sub_section . context . get ( ""shape"" ) :","if self . _shape_name == sub_section . context . get ( ""shape""",82.84346129,FALSE,97.49
4212,"def confirm_on_console ( topic , msg ) : done = False print ( topic ) while not done : output = raw_input ( msg + "" :[y/n] "" ) if output . lower ( ) == "" y "" : return True <MASK> return False","if output . lower ( ) == ""n"" :","if output . lower ( ) == ""n"" :",100,TRUE,100
4213,"def __getitem__ ( self , index ) : if self . _check ( ) : if isinstance ( index , int ) : <MASK> raise IndexError ( index ) if self . features [ index ] is None : feature = self . device . feature_request ( FEATURE . FEATURE_SET , 0x10 , index ) if feature : ( feature , ) = _unpack ( "" !H "" , feature [ : 2 ] ) self . features [ index ] = FEATURE [ feature ] return self . features [ index ] elif isinstance ( index , slice ) : indices = index . indices ( len ( self . features ) ) return [ self . __getitem__ ( i ) for i in range ( * indices ) ]",if index < 0 or index >= len ( self . features ) :,if index < 0 :,89.30365714,FALSE,92.6
4214,"def _parse_locator ( self , locator ) : prefix = None criteria = locator if not locator . startswith ( "" // "" ) : locator_parts = locator . partition ( "" = "" ) <MASK> prefix = locator_parts [ 0 ] criteria = locator_parts [ 2 ] . strip ( ) return ( prefix , criteria )",if len ( locator_parts [ 1 ] ) > 0 :,if len ( locator_parts ) > 2 :,68.75750856,FALSE,91.33
4215,"def trakt_episode_data_generate ( self , data ) : # Find how many unique season we have uniqueSeasons = [ ] for season , episode in data : <MASK> uniqueSeasons . append ( season ) # build the query seasonsList = [ ] for searchedSeason in uniqueSeasons : episodesList = [ ] for season , episode in data : if season == searchedSeason : episodesList . append ( { "" number "" : episode } ) seasonsList . append ( { "" number "" : searchedSeason , "" episodes "" : episodesList } ) post_data = { "" seasons "" : seasonsList } return post_data",if season not in uniqueSeasons :,if len ( data [ season ] ) > self . max_season :,70.82769624,FALSE,88.8
4216,"def __init__ ( self , data , n_bins ) : bin_width = span / n_bins bins = [ 0 ] * n_bins for x in data : b = int ( mpfloor ( ( x - minimum ) / bin_width ) ) <MASK> b = 0 elif b > = n_bins : b = n_bins - 1 bins [ b ] + = 1 self . bins = bins self . bin_width = bin_width",if b < 0 :,if b == n_bins - 1 :,93.41566197,FALSE,91.77
4217,"def infer_context ( typ , context = "" http://schema.org "" ) : parsed_context = urlparse ( typ ) if parsed_context . netloc : base = "" "" . join ( [ parsed_context . scheme , "" :// "" , parsed_context . netloc ] ) <MASK> context = urljoin ( base , parsed_context . path ) typ = parsed_context . fragment . strip ( "" / "" ) elif parsed_context . path : context = base typ = parsed_context . path . strip ( "" / "" ) return context , typ",if parsed_context . path and parsed_context . fragment :,if parsed_context . fragment :,77.17744269,FALSE,95.24
4218,"def parse ( self , items ) : for index , item in enumerate ( items ) : keys = self . build_key ( item ) if keys is None : continue # Update `items` self . items [ tuple ( keys ) ] = ( index , item ) # Update `table` <MASK> log . info ( "" Unable to update table (keys:  %r ) "" , keys )","if not self . path_set ( self . table , keys , ( index , item ) ) :",if not self . table . update ( keys ) :,92.54865021,FALSE,84.19
4219,"def dict_to_XML ( tag , dictionary , * * kwargs ) : """"""Return XML element converting dicts recursively."""""" elem = Element ( tag , * * kwargs ) for key , val in dictionary . items ( ) : if tag == "" layers "" : child = dict_to_XML ( "" layer "" , val , name = key ) elif isinstance ( val , MutableMapping ) : child = dict_to_XML ( key , val ) else : <MASK> child = Element ( "" variable "" , name = key ) else : child = Element ( key ) child . text = str ( val ) elem . append ( child ) return elem","if tag == ""config"" :","if tag == ""variables"" :",98.94116209,FALSE,98.21
4220,"def _get_config_value ( self , section , key ) : if section : <MASK> self . log . error ( "" Error: Config section  ' %s '  not found "" , section ) return None return self . config [ section ] . get ( key , self . config [ key ] ) else : return self . config [ key ]",if section not in self . config :,if not self . config [ section ] :,88.81364312,FALSE,91.36
4221,"def h_line_down ( self , input ) : end_this_line = self . value . find ( "" \n "" , self . cursor_position ) if end_this_line == - 1 : if self . scroll_exit : self . h_exit_down ( None ) else : self . cursor_position = len ( self . value ) else : self . cursor_position = end_this_line + 1 for x in range ( self . cursorx ) : <MASK> break elif self . value [ self . cursor_position ] == "" \n "" : break else : self . cursor_position + = 1",if self . cursor_position > len ( self . value ) - 1 :,if self . value [ self . cursor_position ] == input [ x ] :,92.61908785,FALSE,92.39
4222,"def printsumfp ( fp , filename , out = sys . stdout ) : m = md5 ( ) try : while 1 : data = fp . read ( bufsize ) <MASK> break if isinstance ( data , str ) : data = data . encode ( fp . encoding ) m . update ( data ) except IOError as msg : sys . stderr . write ( "" %s : I/O error:  %s \n "" % ( filename , msg ) ) return 1 out . write ( "" %s   %s \n "" % ( m . hexdigest ( ) , filename ) ) return 0",if not data :,if not data :,100,TRUE,100
4223,"def main ( input ) : logging . info ( "" Running Azure Cloud Custodian Policy  %s "" , input ) context = { "" config_file "" : join ( function_directory , "" config.json "" ) , "" auth_file "" : join ( function_directory , "" auth.json "" ) , } event = None subscription_id = None if isinstance ( input , QueueMessage ) : <MASK> return event = input . get_json ( ) subscription_id = ResourceIdParser . get_subscription_id ( event [ "" subject "" ] ) handler . run ( event , context , subscription_id )",if input . dequeue_count > max_dequeue_count :,"if input . get_json ( ) [ ""type"" ] != ""queue_message""",92.77071845,FALSE,87.07
4224,"def maybeExtractTarball ( self ) : if self . tarball : tar = self . computeTarballOptions ( ) + [ "" -xvf "" , self . tarball ] res = yield self . _Cmd ( tar , abandonOnFailure = False ) <MASK> # error with tarball.. erase repo dir and tarball yield self . _Cmd ( [ "" rm "" , "" -f "" , self . tarball ] , abandonOnFailure = False ) yield self . runRmdir ( self . repoDir ( ) , abandonOnFailure = False )",if res :,if res != 0 :,96.23177796,FALSE,95.32
4225,"def execute ( self , arbiter , props ) : watcher = self . _get_watcher ( arbiter , props . pop ( "" name "" ) ) action = 0 for key , val in props . get ( "" options "" , { } ) . items ( ) : <MASK> new_action = 0 for name , _val in val . items ( ) : action = watcher . set_opt ( "" hooks. %s "" % name , _val ) if action == 1 : new_action = 1 else : new_action = watcher . set_opt ( key , val ) if new_action == 1 : action = 1 # trigger needed action return watcher . do_action ( action )","if key == ""hooks"" :","if isinstance ( val , dict ) :",85.72730252,FALSE,94.94
4226,"def _import_playlists ( self , fns , library ) : added = 0 for filename in fns : name = _name_for ( filename ) with open ( filename , "" rb "" ) as f : <MASK> playlist = parse_m3u ( f , name , library = library ) elif filename . endswith ( "" .pls "" ) : playlist = parse_pls ( f , name , library = library ) else : print_w ( "" Unsupported playlist type for  ' %s ' "" % filename ) continue self . changed ( playlist ) library . add ( playlist ) added + = 1 return added","if filename . endswith ( "".m3u"" ) or filename . endswith ( "".m3u8"" ) :","if filename . endswith ( "".m3u"" ) :",90.47083836,FALSE,92.41
4227,"def unwrap_term_buckets ( self , timestamp , term_buckets ) : for term_data in term_buckets : <MASK> self . unwrap_interval_buckets ( timestamp , term_data [ "" key "" ] , term_data [ "" interval_aggs "" ] [ "" buckets "" ] ) else : self . check_matches ( timestamp , term_data [ "" key "" ] , term_data )","if ""interval_aggs"" in term_data :","if ""buckets"" in term_data :",72.93376225,FALSE,95.12
4228,"def _get_exception ( flags , timeout_ms , payload_size ) : if flags & FLAG_ERROR : if flags & FLAG_TIMEOUT : return SpicommTimeoutError ( timeout_ms / 1000.0 ) <MASK> return SpicommOverflowError ( payload_size ) return SpicommError ( ) return None",if flags & FLAG_OVERFLOW :,if flags & FLAG_OVERFLOW :,100,TRUE,100
4229,"def _get_pattern ( self , pattern_id ) : """"""Get pattern item by id."""""" for key in ( Tag . PATTERNS1 , Tag . PATTERNS2 , Tag . PATTERNS3 ) : if key in self . tagged_blocks : data = self . tagged_blocks . get_data ( key ) for pattern in data : <MASK> return pattern return None",if pattern . pattern_id == pattern_id :,if pattern . id == pattern_id :,97.98057299,FALSE,95.84
4230,"def print_quiet ( self , context , * args , * * kwargs ) : for index , ( key , value ) in enumerate ( itertools . chain ( enumerate ( args ) , kwargs . items ( ) ) ) : <MASK> print ( self . format_quiet ( index , key , value , fields = context . get_input_fields ( ) ) )","if self . filter ( index , key , value ) :","if key not in ( ""help"" , ""help_text"" ) :",64.73892651,FALSE,84.02
4231,"def complete ( self , block ) : with self . _condition : if not self . _final : return False if self . _complete ( ) : self . _calculate_state_root_if_not_already_done ( ) return True <MASK> self . _condition . wait_for ( self . _complete ) self . _calculate_state_root_if_not_already_done ( ) return True return False",if block :,if block :,100,TRUE,100
4232,"def compression_rotator ( source , dest ) : with open ( source , "" rb "" ) as sf : with gzip . open ( dest , "" wb "" ) as wf : while True : data = sf . read ( CHUNK_SIZE ) <MASK> break wf . write ( data ) os . remove ( source )",if not data :,if not data :,100,TRUE,100
4233,"def mockup ( self , records ) : provider = TransipProvider ( "" "" , "" "" , "" "" ) _dns_entries = [ ] for record in records : <MASK> entries_for = getattr ( provider , "" _entries_for_ {} "" . format ( record . _type ) ) # Root records have '@' as name name = record . name if name == "" "" : name = provider . ROOT_RECORD _dns_entries . extend ( entries_for ( name , record ) ) # NS is not supported as a DNS Entry, # so it should cover the if statement _dns_entries . append ( DnsEntry ( "" @ "" , "" 3600 "" , "" NS "" , "" ns01.transip.nl. "" ) ) self . mockupEntries = _dns_entries",if record . _type in provider . SUPPORTS :,if record . _type :,90.47078928,FALSE,96.72
4234,"def parse_known_args ( self , args = None , namespace = None ) : entrypoint = self . prog . split ( ""   "" ) [ 0 ] try : defs = get_defaults_for_argparse ( entrypoint ) ignore = defs . pop ( "" Ignore "" , None ) self . set_defaults ( * * defs ) <MASK> set_notebook_diff_ignores ( ignore ) except ValueError : pass return super ( ConfigBackedParser , self ) . parse_known_args ( args = args , namespace = namespace )",if ignore :,if ignore is not None :,96.53200714,FALSE,95.84
4235,"def _maybeRebuildAtlas ( self , threshold = 4 , minlen = 1000 ) : n = len ( self . fragmentAtlas ) if ( n > minlen ) and ( n > threshold * len ( self . data ) ) : self . fragmentAtlas . rebuild ( list ( zip ( * self . _style ( [ "" symbol "" , "" size "" , "" pen "" , "" brush "" ] ) ) ) ) self . data [ "" sourceRect "" ] = 0 <MASK> self . _sourceQRect . clear ( ) self . updateSpots ( )",if _USE_QRECT :,if self . _sourceQRect :,96.70011645,FALSE,95.09
4236,"def dispatch_return ( self , frame , arg ) : if self . stop_here ( frame ) or frame == self . returnframe : # Ignore return events in generator except when stepping. <MASK> return self . trace_dispatch try : self . frame_returning = frame self . user_return ( frame , arg ) finally : self . frame_returning = None if self . quitting : raise BdbQuit # The user issued a 'next' or 'until' command. if self . stopframe is frame and self . stoplineno != - 1 : self . _set_stopinfo ( None , None ) return self . trace_dispatch",if self . stopframe and frame . f_code . co_flags & CO_GENERATOR :,if self . trace_dispatch :,95.36755012,FALSE,88.06
4237,"def tearDown ( self ) : if not self . is_playback ( ) : try : if self . hosted_service_name is not None : self . sms . delete_hosted_service ( self . hosted_service_name ) except : pass try : <MASK> self . sms . delete_storage_account ( self . storage_account_name ) except : pass try : self . sms . delete_affinity_group ( self . affinity_group_name ) except : pass return super ( LegacyMgmtAffinityGroupTest , self ) . tearDown ( )",if self . storage_account_name is not None :,if self . storage_account_name is not None :,100,TRUE,100
4238,"def make_log_msg ( self , msg , * other_messages ) : MAX_MESSAGE_LENGTH = 1000 if not other_messages : # assume that msg is a single string return msg [ - MAX_MESSAGE_LENGTH : ] else : if len ( msg ) : msg + = "" \n ... \n "" NEXT_MESSAGE_OFFSET = MAX_MESSAGE_LENGTH - len ( msg ) else : NEXT_MESSAGE_OFFSET = MAX_MESSAGE_LENGTH <MASK> msg + = other_messages [ 0 ] [ - NEXT_MESSAGE_OFFSET : ] return self . make_log_msg ( msg , * other_messages [ 1 : ] ) else : return self . make_log_msg ( msg )",if NEXT_MESSAGE_OFFSET > 0 :,if NEXT_MESSAGE_OFFSET < len ( other_messages ) :,97.58803097,FALSE,94.73
4239,"def wrapper ( # type: ignore self : RequestHandler , * args , * * kwargs ) - > Optional [ Awaitable [ None ] ] : if self . request . path . endswith ( "" / "" ) : if self . request . method in ( "" GET "" , "" HEAD "" ) : uri = self . request . path . rstrip ( "" / "" ) if uri : # don't try to redirect '/' to '' <MASK> uri + = "" ? "" + self . request . query self . redirect ( uri , permanent = True ) return None else : raise HTTPError ( 404 ) return method ( self , * args , * * kwargs )",if self . request . query :,if self . request . query :,75,TRUE,100
4240,"def process_lib ( vars_ , coreval ) : for d in vars_ : var = d . upper ( ) if var == "" QTCORE "" : continue value = env [ "" LIBPATH_ "" + var ] <MASK> core = env [ coreval ] accu = [ ] for lib in value : if lib in core : continue accu . append ( lib ) env [ "" LIBPATH_ "" + var ] = accu",if value :,if value :,100,TRUE,100
4241,"def _attach_children ( self , other , exclude_worldbody , dry_run = False ) : for other_child in other . all_children ( ) : <MASK> self_child = self . get_children ( other_child . spec . name ) self_child . _attach ( other_child , exclude_worldbody , dry_run ) # pylint: disable=protected-access",if not other_child . spec . repeated :,if not self . _has_child ( other_child . spec . name ) :,88.7699929,FALSE,87.1
4242,"def getDictFromTree ( tree ) : ret_dict = { } for child in tree . getchildren ( ) : if child . getchildren ( ) : ## Complex-type child. Recurse content = getDictFromTree ( child ) else : content = child . text <MASK> if not type ( ret_dict [ child . tag ] ) == list : ret_dict [ child . tag ] = [ ret_dict [ child . tag ] ] ret_dict [ child . tag ] . append ( content or "" "" ) else : ret_dict [ child . tag ] = content or "" "" return ret_dict",if ret_dict . has_key ( child . tag ) :,if child . tag in ret_dict :,96.49833387,FALSE,91.87
4243,"def nsUriMatch ( self , value , wanted , strict = 0 , tt = type ( ( ) ) ) : """"""Return a true value if two namespace uri values match."""""" if value == wanted or ( type ( wanted ) is tt ) and value in wanted : return 1 if not strict and value is not None : wanted = type ( wanted ) is tt and wanted or ( wanted , ) value = value [ - 1 : ] != "" / "" and value or value [ : - 1 ] for item in wanted : <MASK> return 1 return 0",if item == value or item [ : - 1 ] == value :,if item in value and item in value :,93.28096483,FALSE,89.86
4244,"def update_repository ( self , ignore_issues = False , force = False ) : """"""Update."""""" if not await self . common_update ( ignore_issues , force ) : return # Get appdaemon objects. if self . repository_manifest : <MASK> self . content . path . remote = "" "" if self . content . path . remote == "" apps "" : self . data . domain = get_first_directory_in_directory ( self . tree , self . content . path . remote ) self . content . path . remote = f "" apps/ { self . data . name } "" # Set local path self . content . path . local = self . localpath",if self . data . content_in_root :,"if self . content . path . remote == """" :",97.45607096,FALSE,93.63
4245,"def addOutput ( self , data , isAsync = None , * * kwargs ) : isAsync = _get_async_param ( isAsync , * * kwargs ) if isAsync : self . terminal . eraseLine ( ) self . terminal . cursorBackward ( len ( self . lineBuffer ) + len ( self . ps [ self . pn ] ) ) self . terminal . write ( data ) if isAsync : <MASK> self . terminal . nextLine ( ) self . terminal . write ( self . ps [ self . pn ] ) if self . lineBuffer : oldBuffer = self . lineBuffer self . lineBuffer = [ ] self . lineBufferIndex = 0 self . _deliverBuffer ( oldBuffer )",if self . _needsNewline ( ) :,if self . lineBufferIndex == 0 :,96.51379445,FALSE,95.87
4246,"def is_installed ( self , dlc_title = "" "" ) - > bool : installed = False if dlc_title : dlc_version = self . get_dlc_info ( "" version "" , dlc_title ) installed = True if dlc_version else False # Start: Code for compatibility with minigalaxy 1.0 <MASK> status = self . legacy_get_dlc_status ( dlc_title ) installed = True if status in [ "" installed "" , "" updatable "" ] else False # End: Code for compatibility with minigalaxy 1.0 else : if self . install_dir and os . path . exists ( self . install_dir ) : installed = True return installed",if not installed :,if self . legacy_get_dlc_status and os . path . exists ( self,96.12738964,FALSE,87.62
4247,"def close ( self ) : self . selector . close ( ) if self . sock : sockname = None try : sockname = self . sock . getsockname ( ) except ( socket . error , OSError ) : pass self . sock . close ( ) <MASK> # it was a Unix domain socket, remove it from the filesystem if os . path . exists ( sockname ) : os . remove ( sockname ) self . sock = None",if type ( sockname ) is str :,"if isinstance ( sockname , str ) :",93.06371666,FALSE,92.93
4248,"def post_file ( self , file_path , graph_type = "" edges "" , file_type = "" csv "" ) : dataset_id = self . dataset_id tok = self . token base_path = self . server_base_path with open ( file_path , "" rb "" ) as file : out = requests . post ( f "" { base_path } /api/v2/upload/datasets/ { dataset_id } / { graph_type } / { file_type } "" , verify = self . certificate_validation , headers = { "" Authorization "" : f "" Bearer  { tok } "" } , data = file . read ( ) , ) . json ( ) <MASK> raise Exception ( out ) return out","if not out [ ""success"" ] :","if ""error"" in out :",94.93421998,FALSE,95.12
4249,"def _get_vqa_v2_image_raw_dataset ( directory , image_root_url , image_urls ) : """"""Extract the VQA V2 image data set to directory unless it's there."""""" for url in image_urls : filename = os . path . basename ( url ) download_url = os . path . join ( image_root_url , url ) path = generator_utils . maybe_download ( directory , filename , download_url ) unzip_dir = os . path . join ( directory , filename . strip ( "" .zip "" ) ) <MASK> zipfile . ZipFile ( path , "" r "" ) . extractall ( directory )",if not tf . gfile . Exists ( unzip_dir ) :,if os . path . exists ( unzip_dir ) :,92.74379476,FALSE,95.24
4250,"def __call__ ( self , environ , start_response ) : for key in "" REQUEST_URL "" , "" REQUEST_URI "" , "" UNENCODED_URL "" : <MASK> continue request_uri = unquote ( environ [ key ] ) script_name = unquote ( environ . get ( "" SCRIPT_NAME "" , "" "" ) ) if request_uri . startswith ( script_name ) : environ [ "" PATH_INFO "" ] = request_uri [ len ( script_name ) : ] . split ( "" ? "" , 1 ) [ 0 ] break return self . app ( environ , start_response )",if key not in environ :,if key not in environ :,100,TRUE,100
4251,"def _instrument_model ( self , model ) : for key , value in list ( model . __dict__ . items ( ) ) : # avoid ""dictionary keys changed during iteration"" if isinstance ( value , tf . keras . layers . Layer ) : new_layer = self . _instrument ( value ) if new_layer is not value : setattr ( model , key , new_layer ) <MASK> for i , item in enumerate ( value ) : if isinstance ( item , tf . keras . layers . Layer ) : value [ i ] = self . _instrument ( item ) return model","elif isinstance ( value , list ) :","elif isinstance ( value , ( list , tuple ) ) :",98.11501702,FALSE,95.23
4252,"def __init__ ( self , parent , dir , mask , with_dirs = True ) : filelist = [ ] dirlist = [ "" .. "" ] self . dir = dir self . file = "" "" mask = mask . upper ( ) pattern = self . MakeRegex ( mask ) for i in os . listdir ( dir ) : if i == "" . "" or i == "" .. "" : continue path = os . path . join ( dir , i ) if os . path . isdir ( path ) : dirlist . append ( i ) continue path = path . upper ( ) value = i . upper ( ) <MASK> filelist . append ( i ) self . files = filelist if with_dirs : self . dirs = dirlist",if pattern . match ( value ) is not None :,"if pattern . match ( path , value ) :",96.77577416,FALSE,96.41
4253,"def get_text ( self , nodelist ) : """"""Return a string representation of the motif's properties listed on nodelist ."""""" retlist = [ ] for node in nodelist : if node . nodeType == Node . TEXT_NODE : retlist . append ( node . wholeText ) <MASK> retlist . append ( self . get_text ( node . childNodes ) ) return re . sub ( r "" \ s+ "" , ""   "" , "" "" . join ( retlist ) )",elif node . hasChildNodes :,elif node . nodeType == Node . ELEMENT_NODE :,94.36485242,FALSE,90.98
4254,"def _persist_metadata ( self , dirname , filename ) : metadata_path = "" {0} / {1} .json "" . format ( dirname , filename ) if self . media_metadata or self . comments or self . include_location : if self . posts : if self . latest : self . merge_json ( { "" GraphImages "" : self . posts } , metadata_path ) else : self . save_json ( { "" GraphImages "" : self . posts } , metadata_path ) <MASK> if self . latest : self . merge_json ( { "" GraphStories "" : self . stories } , metadata_path ) else : self . save_json ( { "" GraphStories "" : self . stories } , metadata_path )",if self . stories :,if self . stories :,100,TRUE,100
4255,"def _get_python_wrapper_content ( self , job_class , args ) : job = job_class ( [ "" -r "" , "" hadoop "" ] + list ( args ) ) job . sandbox ( ) with job . make_runner ( ) as runner : runner . _create_setup_wrapper_scripts ( ) <MASK> with open ( runner . _spark_python_wrapper_path ) as f : return f . read ( ) else : return None",if runner . _spark_python_wrapper_path :,if runner . _spark_python_wrapper_path :,100,TRUE,100
4256,"def computeLeadingWhitespaceWidth ( s , tab_width ) : w = 0 for ch in s : <MASK> w + = 1 elif ch == "" \t "" : w + = abs ( tab_width ) - ( w % abs ( tab_width ) ) else : break return w","if ch == "" "" :","if ch == "" "" :",100,TRUE,100
4257,def run ( self ) : # if the i3status process dies we want to restart it. # We give up restarting if we have died too often for _ in range ( 10 ) : <MASK> break self . spawn_i3status ( ) # check if we never worked properly and if so quit now if not self . ready : break # limit restart rate self . lock . wait ( 5 ),if not self . py3_wrapper . running :,if self . process . poll ( ) is not None :,69.97068823,FALSE,88.05
4258,"def translate_len ( builder : IRBuilder , expr : CallExpr , callee : RefExpr ) - > Optional [ Value ] : # Special case builtins.len if len ( expr . args ) == 1 and expr . arg_kinds == [ ARG_POS ] : expr_rtype = builder . node_type ( expr . args [ 0 ] ) <MASK> # len() of fixed-length tuple can be trivially determined statically, # though we still need to evaluate it. builder . accept ( expr . args [ 0 ] ) return Integer ( len ( expr_rtype . types ) ) else : obj = builder . accept ( expr . args [ 0 ] ) return builder . builtin_len ( obj , - 1 ) return None","if isinstance ( expr_rtype , RTuple ) :","if isinstance ( expr_rtype , FixedLengthType ) :",74.0391309,FALSE,98.36
4259,"def parse_auth ( val ) : if val is not None : authtype , params = val . split ( ""   "" , 1 ) if authtype in known_auth_schemes : <MASK> # this is the ""Authentication: Basic XXXXX=="" case pass else : params = parse_auth_params ( params ) return authtype , params return val","if authtype == ""Basic"" and '""' not in params :","if params == ""basic"" :",84.18956963,FALSE,84.33
4260,"def toxml ( self ) : text = self . value self . parent . setBidi ( getBidiType ( text ) ) if not text . startswith ( HTML_PLACEHOLDER_PREFIX ) : if self . parent . nodeName == "" p "" : text = text . replace ( "" \n "" , "" \n     "" ) <MASK> text = "" \n       "" + text . replace ( "" \n "" , "" \n       "" ) text = self . doc . normalizeEntities ( text ) return text","elif self . parent . nodeName == ""li"" and self . parent . childNodes [ 0 ] == self :","elif self . parent . nodeName == ""a"" :",87.3834856,FALSE,86.73
4261,"def get_all_related_many_to_many_objects ( self ) : try : # Try the cache first. return self . _all_related_many_to_many_objects except AttributeError : rel_objs = [ ] for klass in get_models ( ) : for f in klass . _meta . many_to_many : <MASK> rel_objs . append ( RelatedObject ( f . rel . to , klass , f ) ) self . _all_related_many_to_many_objects = rel_objs return rel_objs",if f . rel and self == f . rel . to . _meta :,if f . rel and f . rel . to not in self . _all_related_,70.16987881,FALSE,91.57
4262,"def state_highstate ( self , state , dirpath ) : opts = copy . copy ( self . config ) opts [ "" file_roots "" ] = dict ( base = [ dirpath ] ) HIGHSTATE = HighState ( opts ) HIGHSTATE . push_active ( ) try : high , errors = HIGHSTATE . render_highstate ( state ) <MASK> import pprint pprint . pprint ( "" \n "" . join ( errors ) ) pprint . pprint ( high ) out = HIGHSTATE . state . call_high ( high ) # pprint.pprint(out) finally : HIGHSTATE . pop_active ( )",if errors :,if errors :,100,TRUE,100
4263,"def _update_target_host ( self , target , target_host ) : """"""Update target host."""""" target_host = None if target_host == "" "" else target_host if not target_host : for device_type , tgt in target . items ( ) : <MASK> target_host = tgt break if not target_host : target_host = "" llvm "" if tvm . runtime . enabled ( "" llvm "" ) else "" stackvm "" if isinstance ( target_host , str ) : target_host = tvm . target . Target ( target_host ) return target_host",if device_type . value == tvm . nd . cpu ( 0 ) . device_type :,"if device_type == ""host"" :",81.73732091,FALSE,88.18
4264,"def __console_writer ( self ) : while True : self . __writer_event . wait ( ) self . __writer_event . clear ( ) if self . __console_view : <MASK> self . log . debug ( "" Writing console view to STDOUT "" ) sys . stdout . write ( self . console_markup . clear ) sys . stdout . write ( self . __console_view ) sys . stdout . write ( self . console_markup . TOTAL_RESET )",if not self . short_only :,if self . console_markup . TOTAL_RESET in self . console_view_list,63.72201877,FALSE,85.19
4265,"def goToPrevMarkedHeadline ( self , event = None ) : """"""Select the next marked node."""""" c = self p = c . p if not p : return p . moveToThreadBack ( ) wrapped = False while 1 : if p and p . isMarked ( ) : break elif p : p . moveToThreadBack ( ) <MASK> break else : wrapped = True p = c . rootPosition ( ) if not p : g . blue ( "" done "" ) c . treeSelectHelper ( p ) # Sets focus.",elif wrapped :,elif wrapped :,100,TRUE,100
4266,"def delete_map ( self , query = None ) : query_map = self . interpolated_map ( query = query ) for alias , drivers in six . iteritems ( query_map . copy ( ) ) : for driver , vms in six . iteritems ( drivers . copy ( ) ) : for vm_name , vm_details in six . iteritems ( vms . copy ( ) ) : if vm_details == "" Absent "" : query_map [ alias ] [ driver ] . pop ( vm_name ) if not query_map [ alias ] [ driver ] : query_map [ alias ] . pop ( driver ) <MASK> query_map . pop ( alias ) return query_map",if not query_map [ alias ] :,if not query_map [ alias ] [ vm_details ] :,96.81772275,FALSE,96.25
4267,"def get_shadows_zip ( filename ) : import zipfile shadow_pkgs = set ( ) with zipfile . ZipFile ( filename ) as lib_zip : already_test = [ ] for fname in lib_zip . namelist ( ) : pname , fname = os . path . split ( fname ) <MASK> continue if pname not in already_test and "" / "" not in pname : already_test . append ( pname ) if is_shadowing ( pname ) : shadow_pkgs . add ( pname ) return shadow_pkgs",if fname or ( pname and fname ) :,"if pname . startswith ( ""test_"" ) :",91.48193808,FALSE,92.04
4268,"def make_chains ( chains_info ) : chains = [ [ ] for _ in chains_info [ 0 ] [ 1 ] ] for i , num_ids in enumerate ( chains_info [ : - 1 ] ) : num , ids = num_ids for j , ident in enumerate ( ids ) : <MASK> next_chain_info = chains_info [ i + 1 ] previous = next_chain_info [ 1 ] [ j ] block = SimpleBlock ( num , ident , previous ) chains [ j ] . append ( block ) chains = { i : make_generator ( chain ) for i , chain in enumerate ( chains ) } return chains","if ident != """" :",if i + 1 < len ( chains_info ) :,88.59435748,FALSE,91.98
4269,"def filter_input ( mindate , maxdate , files ) : mindate = parse ( mindate ) if mindate is not None else datetime . datetime . min maxdate = parse ( maxdate ) if maxdate is not None else datetime . datetime . max for line in fileinput . input ( files ) : tweet = json . loads ( line ) created_at = parse ( tweet [ "" created_at "" ] ) created_at = created_at . replace ( tzinfo = None ) <MASK> print ( json . dumps ( tweet ) )",if mindate < created_at and maxdate > created_at :,if mindate < created_at . year < maxdate :,93.41089007,FALSE,93.6
4270,"def get ( self ) : """"""If a value/an exception is stored, return/raise it. Otherwise until switch() or throw() is called."""""" if self . _exception is not _NONE : if self . _exception is None : return self . value getcurrent ( ) . throw ( * self . _exception ) # pylint:disable=undefined-variable else : <MASK> raise ConcurrentObjectUseError ( "" This Waiter is already used by  %r "" % ( self . greenlet , ) ) self . greenlet = getcurrent ( ) # pylint:disable=undefined-variable try : return self . hub . switch ( ) finally : self . greenlet = None",if self . greenlet is not None :,if self . greenlet is not None :,100,TRUE,100
4271,"def default_loader ( href , parse , encoding = None ) : with open ( href ) as file : <MASK> data = ElementTree . parse ( file ) . getroot ( ) else : data = file . read ( ) if encoding : data = data . decode ( encoding ) return data","if parse == ""xml"" :",if parse :,91.33176011,FALSE,89.42
4272,def is_all_qud ( world ) : m = True for obj in world : <MASK> if obj . nice : m = m and True else : m = m and False else : m = m and True return m,if obj . blond :,"if isinstance ( obj , QuantizableObject ) :",90.34891807,FALSE,85.51
4273,"def run ( self , edit ) : if not self . has_selection ( ) : region = sublime . Region ( 0 , self . view . size ( ) ) originalBuffer = self . view . substr ( region ) prefixed = self . prefix ( originalBuffer ) if prefixed : self . view . replace ( edit , region , prefixed ) return for region in self . view . sel ( ) : <MASK> continue originalBuffer = self . view . substr ( region ) prefixed = self . prefix ( originalBuffer ) if prefixed : self . view . replace ( edit , region , prefixed )",if region . empty ( ) :,if region == 0 :,96.43655052,FALSE,95.35
4274,"def add_fields ( self , params ) : for ( key , val ) in params . iteritems ( ) : <MASK> new_params = { } for k in val : new_params [ "" %s __ %s "" % ( key , k ) ] = val [ k ] self . add_fields ( new_params ) else : self . add_field ( key , val )","if isinstance ( val , dict ) :","if isinstance ( val , dict ) :",100,TRUE,100
4275,"def find_magic ( self , f , pos , magic ) : f . seek ( pos ) block = f . read ( 32 * 1024 ) if len ( block ) < len ( magic ) : return - 1 p = block . find ( magic ) while p < 0 : pos + = len ( block ) - len ( magic ) + 1 block = block [ 1 - len ( magic ) : ] + f . read ( 32 << 10 ) <MASK> return - 1 p = block . find ( magic ) return pos + p",if len ( block ) == len ( magic ) - 1 :,if len ( block ) < len ( magic ) :,94.39608453,FALSE,94.15
4276,"def check_strings ( self ) : """"""Check that all strings have been consumed."""""" for i , aList in enumerate ( self . string_tokens ) : <MASK> g . trace ( "" warning: line  %s . unused strings "" % i ) for z in aList : print ( self . dump_token ( z ) )",if aList :,if i not in self . unused_strings :,91.98611545,FALSE,87.7
4277,"def get_tokens_unprocessed ( self , text ) : from pygments . lexers . _cocoa_builtins import ( COCOA_INTERFACES , COCOA_PROTOCOLS , COCOA_PRIMITIVES , ) for index , token , value in RegexLexer . get_tokens_unprocessed ( self , text ) : <MASK> if ( value in COCOA_INTERFACES or value in COCOA_PROTOCOLS or value in COCOA_PRIMITIVES ) : token = Name . Builtin . Pseudo yield index , token , value",if token is Name or token is Name . Class :,if token is Name . Builtin :,87.49730632,FALSE,93.43
4278,"def key_from_key_value_dict ( key_info ) : res = [ ] if not "" key_value "" in key_info : return res for value in key_info [ "" key_value "" ] : <MASK> e = base64_to_long ( value [ "" rsa_key_value "" ] [ "" exponent "" ] ) m = base64_to_long ( value [ "" rsa_key_value "" ] [ "" modulus "" ] ) key = RSA . construct ( ( m , e ) ) res . append ( key ) return res","if ""rsa_key_value"" in value :","if ""rsa_key_value"" in value :",100,TRUE,100
4279,"def run ( self , edit ) : if not self . has_selection ( ) : region = sublime . Region ( 0 , self . view . size ( ) ) originalBuffer = self . view . substr ( region ) prefixed = self . prefix ( originalBuffer ) <MASK> self . view . replace ( edit , region , prefixed ) return for region in self . view . sel ( ) : if region . empty ( ) : continue originalBuffer = self . view . substr ( region ) prefixed = self . prefix ( originalBuffer ) <MASK> self . view . replace ( edit , region , prefixed )",if prefixed :,if prefixed :,100,TRUE,100
4280,def finalize ( self ) : if self . ct < 1 : return elif self . ct == 1 : return 0 total = ct = 0 dtp = None while self . heap : <MASK> if dtp is None : dtp = heapq . heappop ( self . heap ) continue dt = heapq . heappop ( self . heap ) diff = dt - dtp ct + = 1 total + = total_seconds ( diff ) dtp = dt return float ( total ) / ct,if total == 0 :,if self . heap [ 0 ] < self . min_heap :,93.6161192,FALSE,87.17
4281,"def _test_configuration ( self ) : config_path = self . _write_config ( ) try : self . _log . debug ( "" testing configuration "" ) verboseflag = "" -Q "" <MASK> verboseflag = "" -v "" p = subprocess . Popen ( [ self . PATH_SLAPTEST , verboseflag , "" -f "" , config_path ] ) if p . wait ( ) != 0 : raise RuntimeError ( "" configuration test failed "" ) self . _log . debug ( "" configuration seems ok "" ) finally : os . remove ( config_path )",if self . _log . isEnabledFor ( logging . DEBUG ) :,"if sys . version_info < ( 3 , 0 ) :",87.64154731,FALSE,91.19
4282,"def exe ( self , ret ) : if not ret : self . assertEqual ( ret , "" "" ) else : assert os . path . isabs ( ret ) , ret # Note: os.stat() may return False even if the file is there # hence we skip the test, see: # http://stackoverflow.com/questions/3112546/os-path-exists-lies <MASK> assert os . path . isfile ( ret ) , ret if hasattr ( os , "" access "" ) and hasattr ( os , "" X_OK "" ) : # XXX may fail on OSX self . assertTrue ( os . access ( ret , os . X_OK ) )",if POSIX :,"if hasattr ( os , ""stat"" ) :",97.76181248,FALSE,93.08
4283,"def _do_cleanup ( sg_name , device_id ) : masking_view_list = self . rest . get_masking_views_from_storage_group ( array , sg_name ) for masking_view in masking_view_list : <MASK> self . rest . delete_masking_view ( array , masking_view ) self . rest . remove_vol_from_sg ( array , sg_name , device_id , extra_specs ) self . rest . delete_volume ( array , device_id ) self . rest . delete_storage_group ( array , sg_name )","if ""STG-"" in masking_view :","if masking_view . get ( ""vol_id"" ) == device_id :",91.13086447,FALSE,88.46
4284,"def hide_tooltip_if_necessary ( self , key ) : """"""Hide calltip when necessary"""""" try : calltip_char = self . get_character ( self . calltip_position ) before = self . is_cursor_before ( self . calltip_position , char_offset = 1 ) other = key in ( Qt . Key_ParenRight , Qt . Key_Period , Qt . Key_Tab ) <MASK> QToolTip . hideText ( ) except ( IndexError , TypeError ) : QToolTip . hideText ( )","if calltip_char not in ( ""?"" , ""("" ) or before or other :",if before and other :,88.30316808,FALSE,84.49
4285,"def list_tags_for_stream ( self , stream_name , exclusive_start_tag_key = None , limit = None ) : stream = self . describe_stream ( stream_name ) tags = [ ] result = { "" HasMoreTags "" : False , "" Tags "" : tags } for key , val in sorted ( stream . tags . items ( ) , key = lambda x : x [ 0 ] ) : if limit and len ( tags ) > = limit : result [ "" HasMoreTags "" ] = True break <MASK> continue tags . append ( { "" Key "" : key , "" Value "" : val } ) return result",if exclusive_start_tag_key and key < exclusive_start_tag_key :,if exclusive_start_tag_key and key == exclusive_start_tag_key :,98.92830758,FALSE,97.6
4286,"def parametrize_function_name ( request , function_name ) : suffixes = [ ] if "" parametrize "" in request . keywords : argnames = request . keywords [ "" parametrize "" ] . args [ : : 2 ] argnames = [ x . strip ( ) for names in argnames for x in names . split ( "" , "" ) ] for name in argnames : value = request . getfuncargvalue ( name ) <MASK> value = value . __name__ suffixes . append ( "" {} = {} "" . format ( name , value ) ) return "" + "" . join ( [ function_name ] + suffixes )",if inspect . isclass ( value ) :,"if hasattr ( value , ""__name__"" ) :",91.8280071,FALSE,91.37
4287,"def add_entities ( self , positions ) : e1 = EntityFactory ( ) for p in positions : <MASK> start , length = p else : start , length = p , 1 EntityOccurrenceFactory ( document = self . doc , entity = e1 , offset = start , offset_end = start + length , alias = "" AB "" , )","if isinstance ( p , tuple ) :","if isinstance ( p , ( tuple , list ) ) :",91.82255269,FALSE,91.67
4288,"def transform_value ( value ) : if isinstance ( value , collections . MutableMapping ) : <MASK> return DBRef ( value [ "" _ns "" ] , transform_value ( value [ "" _id "" ] ) ) else : return transform_dict ( SON ( value ) ) elif isinstance ( value , list ) : return [ transform_value ( v ) for v in value ] return value","if ""_id"" in value and ""_ns"" in value :","if value [ ""_ns"" ] :",90.56098138,FALSE,86.83
4289,"def remove ( self , items ) : """"""Remove messages from lease management."""""" with self . _add_remove_lock : # Remove the ack ID from lease management, and decrement the # byte counter. for item in items : if self . _leased_messages . pop ( item . ack_id , None ) is not None : self . _bytes - = item . byte_size else : _LOGGER . debug ( "" Item  %s  was not managed. "" , item . ack_id ) <MASK> _LOGGER . debug ( "" Bytes was unexpectedly negative:  %d "" , self . _bytes ) self . _bytes = 0",if self . _bytes < 0 :,if self . _bytes < 0 :,100,TRUE,100
4290,"def parse_hgsub ( lines ) : """"""Fills OrderedDict with hgsub file content passed as list of lines"""""" rv = OrderedDict ( ) for l in lines : ls = l . strip ( ) <MASK> continue name , value = l . split ( "" = "" , 1 ) rv [ name . strip ( ) ] = value . strip ( ) return rv","if not ls or ls [ 0 ] == ""#"" :","if ls [ 0 ] != ""#"" :",88.37059628,FALSE,91.74
4291,"def del_ ( self , key ) : initial_hash = hash_ = self . hash ( key ) while True : if self . _keys [ hash_ ] is self . _empty : # That key was never assigned return None elif self . _keys [ hash_ ] == key : # key found, assign with deleted sentinel self . _keys [ hash_ ] = self . _deleted self . _values [ hash_ ] = self . _deleted self . _len - = 1 return hash_ = self . _rehash ( hash_ ) <MASK> # table is full and wrapped around return None",if initial_hash == hash_ :,if hash_ == initial_hash :,98.19971167,FALSE,95.71
4292,"def atom ( token , no_symbol = False ) : try : return int ( token ) except ValueError : try : return float ( token ) except ValueError : <MASK> return token [ 1 : - 1 ] elif no_symbol : return token else : return Symbol ( token )","if token . startswith ( ""'"" ) or token . startswith ( '""' ) :","if token . startswith ( ""a"" ) and token [ - 1 ] == ""a""",85.02755995,FALSE,80.19
4293,"def __Suffix_Noun_Step1b ( self , token ) : for suffix in self . __suffix_noun_step1b : <MASK> token = token [ : - 1 ] self . suffixe_noun_step1b_success = True break return token",if token . endswith ( suffix ) and len ( token ) > 5 :,if token . endswith ( suffix ) :,60.11021394,FALSE,86
4294,"def _guardAgainstUnicode ( self , data ) : # Only accept byte strings or ascii unicode values, otherwise # there is no way to correctly decode the data into bytes. if _pythonMajorVersion < 3 : <MASK> data = data . encode ( "" utf8 "" ) else : if isinstance ( data , str ) : # Only accept ascii unicode values. try : return data . encode ( "" ascii "" ) except UnicodeEncodeError : pass raise ValueError ( "" pyDes can only work with encoded strings, not Unicode. "" ) return data","if isinstance ( data , unicode ) :","if isinstance ( data , bytes ) :",73.75651053,FALSE,97.64
4295,"def populate_resource_parameters ( self , tool_source ) : root = getattr ( tool_source , "" root "" , None ) if ( root is not None and hasattr ( self . app , "" job_config "" ) and hasattr ( self . app . job_config , "" get_tool_resource_xml "" ) ) : resource_xml = self . app . job_config . get_tool_resource_xml ( root . get ( "" id "" ) , self . tool_type ) if resource_xml is not None : inputs = root . find ( "" inputs "" ) <MASK> inputs = parse_xml_string ( "" <inputs/> "" ) root . append ( inputs ) inputs . append ( resource_xml )",if inputs is None :,if not inputs :,80.71441105,FALSE,97.26
4296,"def test_arguments_regex ( self ) : argument_matches = ( ( "" pip=1.1 "" , ( "" pip "" , "" 1.1 "" ) ) , ( "" pip==1.1 "" , None ) , ( "" pip=1.2=1 "" , ( "" pip "" , "" 1.2=1 "" ) ) , ) for argument , match in argument_matches : <MASK> self . assertIsNone ( salt . utils . args . KWARG_REGEX . match ( argument ) ) else : self . assertEqual ( salt . utils . args . KWARG_REGEX . match ( argument ) . groups ( ) , match )",if match is None :,if match is None :,100,TRUE,100
4297,"def _get_sidebar_selected ( self ) : sidebar_selected = None if self . businessline_id : sidebar_selected = "" bl_ %s "" % self . businessline_id if self . service_id : sidebar_selected + = "" _s_ %s "" % self . service_id <MASK> sidebar_selected + = "" _env_ %s "" % self . environment_id return sidebar_selected",if self . environment_id :,if self . environment_id :,100,TRUE,100
4298,"def get_ip_info ( ipaddress ) : """"""Returns device information by IP address"""""" result = { } try : ip = IPAddress . objects . select_related ( ) . get ( address = ipaddress ) except IPAddress . DoesNotExist : pass else : if ip . venture is not None : result [ "" venture_id "" ] = ip . venture . id <MASK> result [ "" device_id "" ] = ip . device . id if ip . device . venture is not None : result [ "" venture_id "" ] = ip . device . venture . id return result",if ip . device is not None :,if ip . device is not None :,100,TRUE,100
4299,"def apply ( self , db , person ) : for family_handle in person . get_family_handle_list ( ) : family = db . get_family_from_handle ( family_handle ) if family : for event_ref in family . get_event_ref_list ( ) : if event_ref : event = db . get_event_from_handle ( event_ref . ref ) if not event . get_place_handle ( ) : return True <MASK> return True return False",if not event . get_date_object ( ) :,if not event . get_date_handle ( ) :,98.29642918,FALSE,97.8
4300,"def killIfDead ( ) : if not self . _isalive : self . log . debug ( "" WampLongPoll: killing inactive WAMP session with transport  ' {0} ' "" . format ( self . _transport_id ) ) self . onClose ( False , 5000 , "" session inactive "" ) self . _receive . _kill ( ) <MASK> del self . _parent . _transports [ self . _transport_id ] else : self . log . debug ( "" WampLongPoll: transport  ' {0} '  is still alive "" . format ( self . _transport_id ) ) self . _isalive = False self . reactor . callLater ( killAfter , killIfDead )",if self . _transport_id in self . _parent . _transports :,if self . _parent :,92.68853779,FALSE,91.86
4301,"def offsets ( self ) : offsets = { } offset_so_far = 0 for name , ty in self . fields . items ( ) : <MASK> l . warning ( "" Found a bottom field in struct  %s . Ignore and increment the offset using the default  "" "" element size. "" , self . name , ) continue if not self . _pack : align = ty . alignment if offset_so_far % align != 0 : offset_so_far + = align - offset_so_far % align offsets [ name ] = offset_so_far offset_so_far + = ty . size / / self . _arch . byte_width return offsets","if isinstance ( ty , SimTypeBottom ) :",if ty . size == 0 :,92.46815187,FALSE,94.93
4302,"def get_override_css ( self ) : """"""handls allow_css_overrides setting."""""" if self . settings . get ( "" allow_css_overrides "" ) : filename = self . view . file_name ( ) filetypes = self . settings . get ( "" markdown_filetypes "" ) <MASK> for filetype in filetypes : if filename . endswith ( filetype ) : css_filename = filename . rpartition ( filetype ) [ 0 ] + "" .css "" if os . path . isfile ( css_filename ) : return u "" <style> %s </style> "" % load_utf8 ( css_filename ) return "" """,if filename and filetypes :,if filetypes :,94.76192406,FALSE,97.43
4303,"def setFullCSSSource ( self , fullsrc , inline = False ) : self . fullsrc = fullsrc if type ( self . fullsrc ) == six . binary_type : self . fullsrc = six . text_type ( self . fullsrc , "" utf-8 "" ) if inline : self . inline = inline if self . fullsrc : self . srcFullIdx = self . fullsrc . find ( self . src ) if self . srcFullIdx < 0 : del self . srcFullIdx self . ctxsrcFullIdx = self . fullsrc . find ( self . ctxsrc ) <MASK> del self . ctxsrcFullIdx",if self . ctxsrcFullIdx < 0 :,if self . ctxsrcFullIdx < 0 :,100,TRUE,100
4304,"def title ( self ) : ret = theme [ "" title "" ] if isinstance ( self . name , six . string_types ) : width = self . statwidth ( ) return ( ret + self . name [ 0 : width ] . center ( width ) . replace ( ""   "" , "" - "" ) + theme [ "" default "" ] ) for i , name in enumerate ( self . name ) : width = self . colwidth ( ) ret = ret + name [ 0 : width ] . center ( width ) . replace ( ""   "" , "" - "" ) if i + 1 != len ( self . vars ) : <MASK> ret = ret + theme [ "" frame "" ] + char [ "" dash "" ] + theme [ "" title "" ] else : ret = ret + char [ "" space "" ] return ret",if op . color :,if i == 0 :,97.28297357,FALSE,96.75
4305,"def _get_requested_databases ( self ) : """"""Returns a list of databases requested, not including ignored dbs"""""" requested_databases = [ ] if ( self . _requested_namespaces is not None ) and ( self . _requested_namespaces != [ ] ) : for requested_namespace in self . _requested_namespaces : <MASK> return [ ] elif requested_namespace [ 0 ] not in IGNORE_DBS : requested_databases . append ( requested_namespace [ 0 ] ) return requested_databases","if requested_namespace [ 0 ] is ""*"" :",if requested_namespace [ 0 ] in NO_DBS :,94.43727117,FALSE,95.22
4306,"def add_channels ( cls , voucher , add_channels ) : for add_channel in add_channels : channel = add_channel [ "" channel "" ] defaults = { "" currency "" : channel . currency_code } <MASK> defaults [ "" discount_value "" ] = add_channel . get ( "" discount_value "" ) if "" min_amount_spent "" in add_channel . keys ( ) : defaults [ "" min_spent_amount "" ] = add_channel . get ( "" min_amount_spent "" , None ) models . VoucherChannelListing . objects . update_or_create ( voucher = voucher , channel = channel , defaults = defaults , )","if ""discount_value"" in add_channel . keys ( ) :","if ""discount_value"" in add_channel . keys ( ) :",100,TRUE,100
4307,"def read_xml ( path ) : with tf . gfile . GFile ( path ) as f : root = etree . fromstring ( f . read ( ) ) annotations = { } for node in root . getchildren ( ) : key , val = node2dict ( node ) # If `key` is object, it's actually a list. <MASK> annotations . setdefault ( key , [ ] ) . append ( val ) else : annotations [ key ] = val return annotations","if key == ""object"" :","if isinstance ( val , list ) :",96.68853555,FALSE,92.21
4308,"def get_ip_info ( ipaddress ) : """"""Returns device information by IP address"""""" result = { } try : ip = IPAddress . objects . select_related ( ) . get ( address = ipaddress ) except IPAddress . DoesNotExist : pass else : <MASK> result [ "" venture_id "" ] = ip . venture . id if ip . device is not None : result [ "" device_id "" ] = ip . device . id if ip . device . venture is not None : result [ "" venture_id "" ] = ip . device . venture . id return result",if ip . venture is not None :,if ip . venture is not None :,100,TRUE,100
4309,"def test_large_headers ( self ) : with ExpectLog ( gen_log , "" Unsatisfiable read "" , required = False ) : try : self . fetch ( "" / "" , headers = { "" X-Filler "" : "" a "" * 1000 } , raise_error = True ) self . fail ( "" did not raise expected exception "" ) except HTTPError as e : # 431 is ""Request Header Fields Too Large"", defined in RFC # 6585. However, many implementations just close the # connection in this case, resulting in a missing response. <MASK> self . assertIn ( e . response . code , ( 431 , 599 ) )",if e . response is not None :,if e . response :,72.73143043,FALSE,96.59
4310,"def validate_reserved_serial_no_consumption ( self ) : for item in self . items : if item . s_warehouse and not item . t_warehouse and item . serial_no : for sr in get_serial_nos ( item . serial_no ) : sales_order = frappe . db . get_value ( "" Serial No "" , sr , "" sales_order "" ) <MASK> msg = _ ( "" (Serial No:  {0} ) cannot be consumed as it ' s reserverd to fullfill Sales Order  {1} . "" ) . format ( sr , sales_order ) frappe . throw ( _ ( "" Item  {0}   {1} "" ) . format ( item . item_code , msg ) )",if sales_order :,if sales_order :,100,TRUE,100
4311,"def force_decode ( string , encoding ) : if isinstance ( string , str ) : <MASK> string = string . decode ( encoding ) else : try : # try decoding with utf-8, should only work for real UTF-8 string = string . decode ( "" utf-8 "" ) except UnicodeError : # last resort -- can't fail string = string . decode ( "" latin1 "" ) return string",if encoding :,if encoding :,100,TRUE,100
4312,"def _add_cs ( master_cs , sub_cs , prefix , delimiter = "" . "" , parent_hp = None ) : new_parameters = [ ] for hp in sub_cs . get_hyperparameters ( ) : new_parameter = copy . deepcopy ( hp ) # Allow for an empty top-level parameter if new_parameter . name == "" "" : new_parameter . name = prefix <MASK> new_parameter . name = "" {} {} {} "" . format ( prefix , SPLITTER , new_parameter . name ) new_parameters . append ( new_parameter ) for hp in new_parameters : _add_hp ( master_cs , hp )","elif not prefix == """" :",elif new_parameter . name . startswith ( delimiter ) :,96.31297588,FALSE,92.25
4313,"def __call__ ( self , * args , * * kwargs ) : if self . log_file is not None : kwargs [ "" file "" ] = self . log_file print ( * args , * * kwargs ) <MASK> # get immediate feedback self . log_file . flush ( ) elif self . log_func is not None : self . log_func ( * args , * * kwargs )","if hasattr ( self . log_file , ""flush"" ) :",if self . log_file . flush_size :,90.98199898,FALSE,89.82
4314,"def df_index_expr ( self , length_expr = None , as_range = False ) : """"""Generate expression to get or create index of DF"""""" if isinstance ( self . index , types . NoneType ) : <MASK> length_expr = df_length_expr ( self ) if as_range : return f "" range( { length_expr } ) "" else : return f "" numpy.arange( { length_expr } ) "" return "" self._index """,if length_expr is None :,if length_expr is None :,100,TRUE,100
4315,"def _setWeight ( self , value ) : if value is None : self . _fontWeight = None else : <MASK> raise TextFormatException ( f "" Not a supported fontWeight:  { value } "" ) self . _fontWeight = value . lower ( )","if value . lower ( ) not in ( ""normal"" , ""bold"" ) :","if value . lower ( ) not in ( ""1"" , ""2"" , ""3",62.19638322,FALSE,86.26
4316,"def _test_configuration ( self ) : config_path = self . _write_config ( ) try : self . _log . debug ( "" testing configuration "" ) verboseflag = "" -Q "" if self . _log . isEnabledFor ( logging . DEBUG ) : verboseflag = "" -v "" p = subprocess . Popen ( [ self . PATH_SLAPTEST , verboseflag , "" -f "" , config_path ] ) <MASK> raise RuntimeError ( "" configuration test failed "" ) self . _log . debug ( "" configuration seems ok "" ) finally : os . remove ( config_path )",if p . wait ( ) != 0 :,if not p . returncode :,94.5064994,FALSE,92.82
4317,"def filter_queryset ( self , request , queryset , view ) : kwargs = { } for field in view . filterset_fields : value = request . GET . get ( field ) if not value : continue if field == "" node_id "" : value = get_object_or_none ( Node , pk = value ) kwargs [ "" node "" ] = value continue <MASK> field = "" asset "" kwargs [ field ] = value if kwargs : queryset = queryset . filter ( * * kwargs ) logger . debug ( "" Filter  {} "" . format ( kwargs ) ) return queryset","elif field == ""asset_id"" :","if field == ""asset_id"" :",97.01600426,FALSE,98.01
4318,"def _find_closing_brace ( string , start_pos ) : """"""Finds the corresponding closing brace after start_pos."""""" bracks_open = 1 for idx , char in enumerate ( string [ start_pos : ] ) : if char == "" ( "" : <MASK> bracks_open + = 1 elif char == "" ) "" : <MASK> bracks_open - = 1 if not bracks_open : return start_pos + idx + 1","if string [ idx + start_pos - 1 ] != ""\\"" :",if _is_brace ( string [ start_pos + idx ] ) :,79.0630444,FALSE,77.09
4319,"def _set_hostport ( self , host , port ) : if port is None : i = host . rfind ( "" : "" ) j = host . rfind ( "" ] "" ) # ipv6 addresses have [...] if i > j : try : port = int ( host [ i + 1 : ] ) except ValueError : raise InvalidURL ( "" nonnumeric port:  ' %s ' "" % host [ i + 1 : ] ) host = host [ : i ] else : port = self . default_port <MASK> host = host [ 1 : - 1 ] self . host = host self . port = port","if host and host [ 0 ] == ""["" and host [ - 1 ] == ""]"" :","if host [ - 1 ] == "":"" :",95.10464808,FALSE,89.4
4320,"def __getstate__ ( self ) : state = { } for cls in type ( self ) . mro ( ) : cls_slots = getattr ( cls , "" __slots__ "" , ( ) ) for slot in cls_slots : if slot != "" __weakref__ "" : <MASK> state [ slot ] = getattr ( self , slot ) state [ "" _cookiejar_cookies "" ] = list ( self . cookiejar ) del state [ "" cookiejar "" ] return state","if hasattr ( self , slot ) :","if hasattr ( self , slot ) :",100,TRUE,100
4321,"def _evp_pkey_from_der_traditional_key ( self , bio_data , password ) : key = self . _lib . d2i_PrivateKey_bio ( bio_data . bio , self . _ffi . NULL ) if key != self . _ffi . NULL : key = self . _ffi . gc ( key , self . _lib . EVP_PKEY_free ) <MASK> raise TypeError ( "" Password was given but private key is not encrypted. "" ) return key else : self . _consume_errors ( ) return None",if password is not None :,if not key :,75.310588,FALSE,95.48
4322,"def is_special ( s , i , directive ) : """"""Return True if the body text contains the @ directive."""""" # j = skip_line(s,i) ; trace(s[i:j],':',directive) assert directive and directive [ 0 ] == "" @ "" # 10/23/02: all directives except @others must start the line. skip_flag = directive in ( "" @others "" , "" @all "" ) while i < len ( s ) : <MASK> return True , i else : i = skip_line ( s , i ) if skip_flag : i = skip_ws ( s , i ) return False , - 1","if match_word ( s , i , directive ) :",if s [ i ] == directive [ 0 ] :,70.7534123,FALSE,93.11
4323,"def _decorator ( coro_func ) : fut = asyncio . ensure_future ( coro_func ( ) ) self . _tests . append ( ( coro_func . __name__ , fut ) ) if timeout_sec is not None : timeout_at = self . _loop . time ( ) + timeout_sec handle = self . MASTER_LOOP . call_at ( timeout_at , self . _set_exception_if_not_done , fut , asyncio . TimeoutError ( ) ) fut . add_done_callback ( lambda * args : handle . cancel ( ) ) <MASK> self . _global_timeout_at = timeout_at return coro_func",if timeout_at > self . _global_timeout_at :,if self . _global_timeout_at is None :,94.11826035,FALSE,95.36
4324,"def _load ( self , db , owner ) : self . __init ( owner ) db_result = db ( "" SELECT ship_id, state_id FROM ai_combat_ship WHERE owner_id = ? "" , self . owner . worldid , ) for ( ship_id , state_id , ) in db_result : ship = WorldObject . get_object_by_id ( ship_id ) state = self . shipStates [ state_id ] # add move callbacks corresponding to given state <MASK> ship . add_move_callback ( Callback ( BehaviorMoveCallback . _arrived , ship ) ) self . add_new_unit ( ship , state )",if state == self . shipStates . moving :,"if state . get ( ""move"" ) :",96.45596087,FALSE,94.24
4325,"def addError ( self , test , err ) : if err [ 0 ] is SkipTest : <MASK> self . stream . writeln ( str ( err [ 1 ] ) ) elif self . dots : self . stream . write ( "" s "" ) self . stream . flush ( ) return _org_AddError ( self , test , err )",if self . showAll :,if self . verbose :,98.12108368,FALSE,96.36
4326,"def _construct ( self , node ) : self . flatten_mapping ( node ) ret = self . construct_pairs ( node ) keys = [ d [ 0 ] for d in ret ] keys_sorted = sorted ( keys , key = _natsort_key ) for key in keys : expected = keys_sorted . pop ( 0 ) <MASK> raise ConstructorError ( None , None , "" keys out of order:  "" "" expected  {}  got  {}  at  {} "" . format ( expected , key , node . start_mark ) , ) return dict ( ret )",if key != expected :,if key not in expected :,98.50861616,FALSE,97.03
4327,"def sample_pos_items_for_u ( u , num ) : # sample num pos items for u-th user pos_items = self . train_items [ u ] n_pos_items = len ( pos_items ) pos_batch = [ ] while True : if len ( pos_batch ) == num : break pos_id = np . random . randint ( low = 0 , high = n_pos_items , size = 1 ) [ 0 ] pos_i_id = pos_items [ pos_id ] <MASK> pos_batch . append ( pos_i_id ) return pos_batch",if pos_i_id not in pos_batch :,if pos_i_id not in pos_batch :,75,TRUE,100
4328,"def _get_id ( self , type , id ) : fields = id . split ( "" : "" ) if len ( fields ) > = 3 : <MASK> logger . warning ( "" Expected id of type  %s  but found type  %s   %s "" , type , fields [ - 2 ] , id ) return fields [ - 1 ] fields = id . split ( "" / "" ) if len ( fields ) > = 3 : itype = fields [ - 2 ] if type != itype : logger . warning ( "" Expected id of type  %s  but found type  %s   %s "" , type , itype , id ) return fields [ - 1 ] . split ( "" ? "" ) [ 0 ] return id",if type != fields [ - 2 ] :,if type != fields [ - 2 ] :,100,TRUE,100
4329,"def uninstall_environments ( self , environments ) : environments = [ env if not env . startswith ( self . conda_context . envs_path ) else os . path . basename ( env ) for env in environments ] return_codes = [ self . conda_context . exec_remove ( [ env ] ) for env in environments ] final_return_code = 0 for env , return_code in zip ( environments , return_codes ) : <MASK> log . debug ( "" Conda environment  ' %s '  successfully removed. "" % env ) else : log . debug ( "" Conda environment  ' %s '  could not be removed. "" % env ) final_return_code = return_code return final_return_code",if return_code == 0 :,if self . conda_context . exec_remove ( [ env ] ) :,94.22262567,FALSE,90.49
4330,"def _add_hit_offset ( self , context_list , string_name , original_offset , value ) : for context in context_list : hits_by_context_dict = self . hits_by_context . setdefault ( context , { } ) <MASK> hits_by_context_dict [ string_name ] = ( original_offset , value . encode ( "" base64 "" ) , )",if string_name not in hits_by_context_dict :,if string_name not in hits_by_context_dict :,100,TRUE,100
4331,"def detab ( self , text , length = None ) : """"""Remove a tab from the front of each line of the given text."""""" if length is None : length = self . tab_length newtext = [ ] lines = text . split ( "" \n "" ) for line in lines : if line . startswith ( ""   "" * length ) : newtext . append ( line [ length : ] ) <MASK> newtext . append ( "" "" ) else : break return "" \n "" . join ( newtext ) , "" \n "" . join ( lines [ len ( newtext ) : ] )",elif not line . strip ( ) :,"elif line . startswith ( "" "" * length ) :",94.0980152,FALSE,93.5
4332,"def dump ( self ) : print ( self . package_name ) for package , value in self . entries : print ( str ( package . version ) ) <MASK> print ( ""     [FILTERED] "" ) elif isinstance ( value , list ) : variants = value for variant in variants : print ( ""      %s "" % str ( variant ) ) else : print ( ""      %s "" % str ( package ) )",if value is None :,"if isinstance ( value , ( dict , list ) ) :",90.76178941,FALSE,87.8
4333,"def __lexical_scope ( * args , * * kwargs ) : try : scope = Scope ( quasi ) <MASK> binding_name_set_stack [ - 1 ] . add_child ( scope ) binding_name_set_stack . append ( scope ) return func ( * args , * * kwargs ) finally : if binding_name_set_stack [ - 1 ] is scope : binding_name_set_stack . pop ( )",if quasi :,if not binding_name_set_stack [ - 1 ] . is_scope :,86.09999469,FALSE,83.7
4334,"def getnotes ( self , origin = None ) : if origin is None : result = self . translator_comments <MASK> if result : result + = "" \n "" + self . developer_comments else : result = self . developer_comments return result elif origin == "" translator "" : return self . translator_comments elif origin in ( "" programmer "" , "" developer "" , "" source code "" ) : return self . developer_comments else : raise ValueError ( "" Comment type not valid "" )",if self . developer_comments :,if self . developer_comments :,100,TRUE,100
4335,"def fix_datetime_fields ( data : TableData , table : TableName ) - > None : for item in data [ table ] : for field_name in DATE_FIELDS [ table ] : <MASK> item [ field_name ] = datetime . datetime . fromtimestamp ( item [ field_name ] , tz = datetime . timezone . utc )",if item [ field_name ] is not None :,if field_name in item :,87.63750775,FALSE,88.41
4336,"def _check_for_cart_error ( cart ) : if cart . _safe_get_element ( "" Cart.Request.Errors "" ) is not None : error = cart . _safe_get_element ( "" Cart.Request.Errors.Error.Code "" ) . text <MASK> raise CartInfoMismatchException ( "" CartGet failed: AWS.ECommerceService.CartInfoMismatch  "" "" make sure AssociateTag, CartId and HMAC are correct  "" "" (dont use URLEncodedHMAC!!!) "" ) raise CartException ( "" CartGet failed:  "" + error )","if error == ""AWS.ECommerceService.CartInfoMismatch"" :","if error . startswith ( ""InvalidRequestError"" ) :",93.03905038,FALSE,91.6
4337,"def check_bounds ( geometry ) : if isinstance ( geometry [ 0 ] , ( list , tuple ) ) : return list ( map ( check_bounds , geometry ) ) else : <MASK> raise ValueError ( "" Longitude is out of bounds, check your JSON format or data "" ) if geometry [ 1 ] > 90 or geometry [ 1 ] < - 90 : raise ValueError ( "" Latitude is out of bounds, check your JSON format or data "" )",if geometry [ 0 ] > 180 or geometry [ 0 ] < - 180 :,if geometry [ 0 ] > 180 or geometry [ 0 ] < - 180 :,100,TRUE,100
4338,"def _mapper_output_protocol ( self , step_num , step_map ) : map_key = self . _step_key ( step_num , "" mapper "" ) if map_key in step_map : <MASK> return self . output_protocol ( ) else : return self . internal_protocol ( ) else : # mapper is not a script substep, so protocols don't apply at all return RawValueProtocol ( )",if step_map [ map_key ] >= ( len ( step_map ) - 1 ) :,"if step_map [ map_key ] == ""script"" :",82.46260134,FALSE,87.29
4339,"def asset ( * paths ) : for path in paths : fspath = www_root + "" /assets/ "" + path etag = "" "" try : if env . cache_static : etag = asset_etag ( fspath ) else : os . stat ( fspath ) except FileNotFoundError as e : <MASK> if not os . path . exists ( fspath + "" .spt "" ) : tell_sentry ( e , { } ) else : continue except Exception as e : tell_sentry ( e , { } ) return asset_url + path + ( etag and "" ?etag= "" + etag )",if path == paths [ - 1 ] :,if e . errno == errno . ENOENT :,88.31436978,FALSE,93.11
4340,"def ping ( self , payload : Union [ str , bytes ] = "" "" ) - > None : if self . trace_enabled and self . ping_pong_trace_enabled : <MASK> payload = payload . decode ( "" utf-8 "" ) self . logger . debug ( "" Sending a ping data frame  "" f "" (session id:  { self . session_id } , payload:  { payload } ) "" ) data = _build_data_frame_for_sending ( payload , FrameHeader . OPCODE_PING ) with self . sock_send_lock : self . sock . send ( data )","if isinstance ( payload , bytes ) :","if isinstance ( payload , bytes ) :",100,TRUE,100
4341,"def is_ac_power_connected ( ) : for power_source_path in Path ( "" /sys/class/power_supply/ "" ) . iterdir ( ) : try : with open ( power_source_path / "" type "" , "" r "" ) as f : if f . read ( ) . strip ( ) != "" Mains "" : continue with open ( power_source_path / "" online "" , "" r "" ) as f : <MASK> return True except IOError : continue return False","if f . read ( 1 ) == ""1"" :","if f . read ( ) . strip ( ) == ""true"" :",65.75201062,FALSE,93.21
4342,"def handle_noargs ( self , * * options ) : self . style = color_style ( ) print ( "" Running Django ' s own validation: "" ) self . validate ( display_num_errors = True ) for model in loading . get_models ( ) : <MASK> self . validate_base_model ( model ) if hasattr ( model , "" _feincms_content_models "" ) : self . validate_content_type ( model )","if hasattr ( model , ""_create_content_base"" ) :","if hasattr ( model , ""_feincms_base_models"" ) :",98.30068623,FALSE,94.69
4343,"def _init_weights ( self , module ) : if isinstance ( module , nn . Linear ) : module . weight . data . normal_ ( mean = 0.0 , std = self . config . init_std ) <MASK> module . bias . data . zero_ ( ) elif isinstance ( module , nn . Embedding ) : module . weight . data . normal_ ( mean = 0.0 , std = self . config . init_std ) if module . padding_idx is not None : module . weight . data [ module . padding_idx ] . zero_ ( )",if module . bias is not None :,if module . bias is not None :,100,TRUE,100
4344,"def walk ( msg , callback , data ) : partnum = 0 for part in msg . walk ( ) : # multipart/* are just containers if part . get_content_maintype ( ) == "" multipart "" : continue ctype = part . get_content_type ( ) <MASK> ctype = OCTET_TYPE filename = part . get_filename ( ) if not filename : filename = PART_FN_TPL % ( partnum ) headers = dict ( part ) LOG . debug ( headers ) headers [ "" Content-Type "" ] = ctype payload = util . fully_decoded_payload ( part ) callback ( data , filename , payload , headers ) partnum = partnum + 1",if ctype is None :,"if ctype == ""application/octet-stream"" :",98.15465739,FALSE,93.98
4345,"def _mark_lcs ( mask , dirs , m , n ) : while m != 0 and n != 0 : if dirs [ m , n ] == "" | "" : m - = 1 n - = 1 mask [ m ] = 1 <MASK> m - = 1 elif dirs [ m , n ] == "" < "" : n - = 1 else : raise UnboundLocalError ( "" Illegal move "" ) return mask","elif dirs [ m , n ] == ""^"" :","elif dirs [ m , n ] == "">"" :",98.57842009,FALSE,97.42
4346,"def valid_localparts ( strip_delimiters = False ) : for line in ABRIDGED_LOCALPART_VALID_TESTS . split ( "" \n "" ) : # strip line, skip over empty lines line = line . strip ( ) <MASK> continue # skip over comments or empty lines match = COMMENT . match ( line ) if match : continue # skip over localparts with delimiters if strip_delimiters : if "" , "" in line or "" ; "" in line : continue yield line","if line == """" :",if not line :,97.47075078,FALSE,93.62
4347,"def fetch ( self , * tileables , * * kw ) : ret_list = False if len ( tileables ) == 1 and isinstance ( tileables [ 0 ] , ( tuple , list ) ) : ret_list = True tileables = tileables [ 0 ] elif len ( tileables ) > 1 : ret_list = True result = self . _sess . fetch ( * tileables , * * kw ) ret = [ ] for r , t in zip ( result , tileables ) : <MASK> ret . append ( r . item ( ) ) else : ret . append ( r ) if ret_list : return ret return ret [ 0 ]","if hasattr ( t , ""isscalar"" ) and t . isscalar ( ) and getattr ( r , ""size"" , None ) == 1 :","if isinstance ( t , ( Tuple , list ) ) :",84.35996668,FALSE,82.05
4348,"def _convert ( container ) : if _value_marker in container : force_list = False values = container . pop ( _value_marker ) if container . pop ( _list_marker , False ) : force_list = True values . extend ( _convert ( x [ 1 ] ) for x in sorted ( container . items ( ) ) ) <MASK> values = values [ 0 ] if not container : return values return _convert ( container ) elif container . pop ( _list_marker , False ) : return [ _convert ( x [ 1 ] ) for x in sorted ( container . items ( ) ) ] return dict_cls ( ( k , _convert ( v ) ) for k , v in iteritems ( container ) )",if not force_list and len ( values ) == 1 :,if force_list :,93.71303818,FALSE,92.49
4349,"def _transform_init_kwargs ( cls , kwargs ) : transformed = [ ] for field in list ( kwargs . keys ( ) ) : prop = getattr ( cls , field , None ) <MASK> value = kwargs . pop ( field ) _transform_single_init_kwarg ( prop , field , value , kwargs ) transformed . append ( ( field , value ) ) return transformed","if isinstance ( prop , MoneyProperty ) :",if prop is not None :,81.74804265,FALSE,91.18
4350,"def haslayer ( self , cls ) : """"""true if self has a layer that is an instance of cls. Superseded by ""cls in self"" syntax."""""" if self . __class__ == cls or self . __class__ . __name__ == cls : return 1 for f in self . packetfields : fvalue_gen = self . getfieldval ( f . name ) if fvalue_gen is None : continue if not f . islist : fvalue_gen = SetGen ( fvalue_gen , _iterpacket = 0 ) for fvalue in fvalue_gen : if isinstance ( fvalue , Packet ) : ret = fvalue . haslayer ( cls ) <MASK> return ret return self . payload . haslayer ( cls )",if ret :,if ret is not None :,96.74512436,FALSE,97.06
4351,def insert_broken_add_sometimes ( node ) : if node . op == theano . tensor . add : last_time_replaced [ 0 ] = not last_time_replaced [ 0 ] <MASK> return [ off_by_half ( * node . inputs ) ] return False,if last_time_replaced [ 0 ] :,if last_time_replaced [ 0 ] :,100,TRUE,100
4352,"def testReadChunk10 ( self ) : # ""Test BZ2File.read() in chunks of 10 bytes"" self . createTempFile ( ) with BZ2File ( self . filename ) as bz2f : text = "" "" while 1 : str = bz2f . read ( 10 ) <MASK> break text + = str self . assertEqual ( text , self . TEXT )",if not str :,if not str :,75,TRUE,100
4353,"def generate_sv_faces ( dcel_mesh , point_index , only_select = False , del_flag = None ) : # This part of function creates faces in SV format # It ignores  boundless super face sv_faces = [ ] for i , face in enumerate ( dcel_mesh . faces ) : if face . inners and face . outer : "" Face ( {} ) has inner components! Sverchok cant show polygons with holes. "" . format ( i ) if not face . outer or del_flag in face . flags : continue <MASK> continue sv_faces . append ( [ point_index [ hedge . origin ] for hedge in face . outer . loop_hedges ] ) return sv_faces",if only_select and not face . select :,if only_select and not face . outer . select :,99.2789048,FALSE,98.2
4354,"def __check_dict_contains ( dct , dict_name , keys , comment = "" "" , result = True ) : for key in keys : <MASK> result = False comment = __append_comment ( "" Missing  {0}  in  {1} "" . format ( key , dict_name ) , comment ) return result , comment",if key not in six . iterkeys ( dct ) :,if key not in dct :,85.28114113,FALSE,90.64
4355,"def _dump_arg_defaults ( kwargs ) : """"""Inject default arguments for dump functions."""""" if current_app : kwargs . setdefault ( "" cls "" , current_app . json_encoder ) <MASK> kwargs . setdefault ( "" ensure_ascii "" , False ) kwargs . setdefault ( "" sort_keys "" , current_app . config [ "" JSON_SORT_KEYS "" ] ) else : kwargs . setdefault ( "" sort_keys "" , True ) kwargs . setdefault ( "" cls "" , JSONEncoder )","if not current_app . config [ ""JSON_AS_ASCII"" ] :","if current_app . config [ ""JSON_SORT_KEYS"" ] :",93.32977949,FALSE,94.41
4356,"def _on_change ( self ) : changed = False self . save ( ) for key , value in self . data . items ( ) : if isinstance ( value , bool ) : if value : changed = True break if isinstance ( value , int ) : if value != 1 : changed = True break elif value is None : continue <MASK> changed = True break self . _reset_button . disabled = not changed",elif len ( value ) != 0 :,"if self . _set_button . disabled and key == ""value"" :",80.33936478,FALSE,82.87
4357,"def parse_win_proxy ( val ) : proxies = [ ] for p in val . split ( "" ; "" ) : <MASK> tab = p . split ( "" = "" , 1 ) if tab [ 0 ] == "" socks "" : tab [ 0 ] = "" SOCKS4 "" proxies . append ( ( tab [ 0 ] . upper ( ) , tab [ 1 ] , None , None ) ) # type, addr:port, username, password else : proxies . append ( ( "" HTTP "" , p , None , None ) ) return proxies","if ""="" in p :","if ""="" in p :",100,TRUE,100
4358,"def predict ( collect_dir , keys ) : run_all = len ( keys ) == 0 validate_keys ( keys ) for exp_cfg in cfg : <MASK> key = exp_cfg [ "" key "" ] _predict ( key , exp_cfg [ "" sample_img "" ] , collect_dir )","if run_all or exp_cfg [ ""key"" ] in keys :",if run_all :,82.37626626,FALSE,83.33
4359,"def convert_port_bindings ( port_bindings ) : result = { } for k , v in six . iteritems ( port_bindings ) : key = str ( k ) <MASK> key + = "" /tcp "" if isinstance ( v , list ) : result [ key ] = [ _convert_port_binding ( binding ) for binding in v ] else : result [ key ] = [ _convert_port_binding ( v ) ] return result","if ""/"" not in key :","if k . startswith ( ""tcp"" ) :",91.74828609,FALSE,90.91
4360,"def assert_conll_writer_output ( dataset : InternalBioNerDataset , expected_output : List [ str ] , sentence_splitter : SentenceSplitter = None , ) : outfile_path = tempfile . mkstemp ( ) [ 1 ] try : sentence_splitter = ( sentence_splitter <MASK> else NoSentenceSplitter ( tokenizer = SpaceTokenizer ( ) ) ) writer = CoNLLWriter ( sentence_splitter = sentence_splitter ) writer . write_to_conll ( dataset , Path ( outfile_path ) ) contents = [ l . strip ( ) for l in open ( outfile_path ) . readlines ( ) if l . strip ( ) ] finally : os . remove ( outfile_path ) assert contents == expected_output",if sentence_splitter,"if isinstance ( sentence_splitter , SentenceSplitter )",84.18322407,FALSE,94.48
4361,"def post ( self , request , * args , * * kwargs ) : self . comment_obj = get_object_or_404 ( Comment , id = request . POST . get ( "" commentid "" ) ) if request . user == self . comment_obj . commented_by : form = LeadCommentForm ( request . POST , instance = self . comment_obj ) <MASK> return self . form_valid ( form ) return self . form_invalid ( form ) data = { "" error "" : "" You don ' t have permission to edit this comment. "" } return JsonResponse ( data )",if form . is_valid ( ) :,if form . is_valid ( ) :,100,TRUE,100
4362,"def trivia_list ( self , ctx : commands . Context ) : """"""List available trivia categories."""""" lists = set ( p . stem for p in self . _all_lists ( ) ) if await ctx . embed_requested ( ) : await ctx . send ( embed = discord . Embed ( title = _ ( "" Available trivia lists "" ) , colour = await ctx . embed_colour ( ) , description = "" ,  "" . join ( sorted ( lists ) ) , ) ) else : msg = box ( bold ( _ ( "" Available trivia lists "" ) ) + "" \n \n "" + "" ,  "" . join ( sorted ( lists ) ) ) <MASK> await ctx . author . send ( msg ) else : await ctx . send ( msg )",if len ( msg ) > 1000 :,if ctx . author :,78.79463576,FALSE,95.43
4363,"def validate ( self ) : result = validators . SUCCESS msgs = [ ] for validator in self . _validators : res , err = validator . validate ( ) if res == validators . ERROR : result = res elif res == validators . WARNING and result != validators . ERROR : result = res <MASK> msgs . append ( err ) return result , "" \n "" . join ( msgs )",if len ( err ) > 0 :,if err :,93.28512609,FALSE,91.41
4364,"def get_code ( self , fullname = None ) : fullname = self . _fix_name ( fullname ) if self . code is None : mod_type = self . etc [ 2 ] if mod_type == imp . PY_SOURCE : source = self . get_source ( fullname ) self . code = compile ( source , self . filename , "" exec "" ) <MASK> self . _reopen ( ) try : self . code = read_code ( self . file ) finally : self . file . close ( ) elif mod_type == imp . PKG_DIRECTORY : self . code = self . _get_delegate ( ) . get_code ( ) return self . code",elif mod_type == imp . PY_COMPILED :,if self . file is None :,71.08252669,FALSE,91.88
4365,"def flush_file ( self , key , f ) : f . flush ( ) if self . compress : f . compress = zlib . compressobj ( 9 , zlib . DEFLATED , - zlib . MAX_WBITS , zlib . DEF_MEM_LEVEL , 0 ) if len ( self . files ) > self . MAX_OPEN_FILES : if self . compress : open_files = sum ( 1 for f in self . files . values ( ) if f . fileobj is not None ) <MASK> f . fileobj . close ( ) f . fileobj = None else : f . close ( ) self . files . pop ( key )",if open_files > self . MAX_OPEN_FILES :,if open_files == 0 :,91.45459852,FALSE,93.05
4366,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) <MASK> self . add_version ( d . getPrefixedString ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 10 :,if tt == 10 :,100,TRUE,100
4367,"def init_author_file ( self ) : self . author_map = { } if self . ui . config ( "" git "" , "" authors "" ) : f = open ( self . repo . wjoin ( self . ui . config ( "" git "" , "" authors "" ) ) ) try : for line in f : line = line . strip ( ) <MASK> continue from_ , to = RE_AUTHOR_FILE . split ( line , 2 ) self . author_map [ from_ ] = to finally : f . close ( )","if not line or line . startswith ( ""#"" ) :","if not line or line . startswith ( ""#"" ) :",100,TRUE,100
4368,"def decode_imsi ( self , imsi ) : new_imsi = "" "" for a in imsi : c = hex ( a ) <MASK> new_imsi + = str ( c [ 3 ] ) + str ( c [ 2 ] ) else : new_imsi + = str ( c [ 2 ] ) + "" 0 "" mcc = new_imsi [ 1 : 4 ] mnc = new_imsi [ 4 : 6 ] return new_imsi , mcc , mnc",if len ( c ) == 4 :,if c [ 3 ] :,72.76727106,FALSE,91.76
4369,"def _get_infoset ( self , prefname ) : """"""Return methods with the name starting with prefname."""""" infoset = [ ] excludes = ( "" %s infoset "" % prefname , ) preflen = len ( prefname ) for name in dir ( self . __class__ ) : if name . startswith ( prefname ) and name not in excludes : member = getattr ( self . __class__ , name ) <MASK> infoset . append ( name [ preflen : ] . replace ( "" _ "" , ""   "" ) ) return infoset","if isinstance ( member , MethodType ) :","if isinstance ( member , types . MethodType ) and member . __module__ == self .",81.45751267,FALSE,87.14
4370,"def skip_to_close_match ( self ) : nestedCount = 1 while 1 : tok = self . tokenizer . get_next_token ( ) ttype = tok [ "" style "" ] if ttype == SCE_PL_UNUSED : return elif self . classifier . is_index_op ( tok ) : tval = tok [ "" text "" ] if self . opHash . has_key ( tval ) : if self . opHash [ tval ] [ 1 ] == 1 : nestedCount + = 1 else : nestedCount - = 1 <MASK> break",if nestedCount <= 0 :,if nestedCount == 0 :,98.62700002,FALSE,97.78
4371,"def findMarkForUnitTestNodes ( self ) : """"""return the position of *all* non-ignored @mark-for-unit-test nodes."""""" c = self . c p , result , seen = c . rootPosition ( ) , [ ] , [ ] while p : <MASK> p . moveToNodeAfterTree ( ) else : seen . append ( p . v ) if g . match_word ( p . h , 0 , "" @ignore "" ) : p . moveToNodeAfterTree ( ) elif p . h . startswith ( "" @mark-for-unit-tests "" ) : result . append ( p . copy ( ) ) p . moveToNodeAfterTree ( ) else : p . moveToThreadNext ( ) return result",if p . v in seen :,if p . h in seen :,98.9320861,FALSE,98.07
4372,"def assert_parts_cleaned ( self , earlier_parts , current_parts , expected_parts , hint ) : cleaned_parts = [ ] for earlier in earlier_parts : earlier_part = earlier [ "" part "" ] earlier_step = earlier [ "" step "" ] found = False for current in current_parts : if earlier_part == current [ "" part "" ] and earlier_step == current [ "" step "" ] : found = True break <MASK> cleaned_parts . append ( dict ( part = earlier_part , step = earlier_step ) ) self . assertThat ( cleaned_parts , HasLength ( len ( expected_parts ) ) , hint ) for expected in expected_parts : self . assertThat ( cleaned_parts , Contains ( expected ) , hint )",if not found :,if found :,96.1404694,FALSE,98.45
4373,"def unmark_first_parents ( event = None ) : """"""Mark the node and all its parents."""""" c = event . get ( "" c "" ) if not c : return changed = [ ] for parent in c . p . self_and_parents ( ) : <MASK> parent . v . clearMarked ( ) parent . setAllAncestorAtFileNodesDirty ( ) changed . append ( parent . copy ( ) ) if changed : # g.es(""unmarked: "" + ', '.join([z.h for z in changed])) c . setChanged ( ) c . redraw ( ) return changed",if parent . isMarked ( ) :,if parent . v . markAsMarked ( ) :,96.26574759,FALSE,96.57
4374,"def stop ( self ) : self . _log ( "" Monitor stop "" ) self . _stop_requested = True try : <MASK> fd = os . open ( self . fifo_path , os . O_WRONLY ) os . write ( fd , b "" X "" ) os . close ( fd ) except Exception as e : self . _log ( "" err while closing:  {0} "" . format ( str ( e ) ) ) if self . _thread : self . _thread . join ( ) self . _thread = None",if os . path . exists ( self . fifo_path ) :,if self . fifo_path :,89.83982986,FALSE,91.64
4375,"def DeleteEmptyCols ( self ) : cols2delete = [ ] for c in range ( 0 , self . GetCols ( ) ) : f = True for r in range ( 0 , self . GetRows ( ) ) : <MASK> f = False if f : cols2delete . append ( c ) for i in range ( 0 , len ( cols2delete ) ) : self . ShiftColsLeft ( cols2delete [ i ] + 1 ) cols2delete = [ x - 1 for x in cols2delete ]","if self . FindItemAtPosition ( ( r , c ) ) is not None :",if cols2delete [ r ] == cols2delete [ r ] :,87.94349771,FALSE,85.93
4376,"def _load_objects ( self , obj_id_zset , limit , chunk_size = 1000 ) : ct = i = 0 while True : id_chunk = obj_id_zset [ i : i + chunk_size ] <MASK> return i + = chunk_size for raw_data in self . _data [ id_chunk ] : if not raw_data : continue if self . _use_json : yield json . loads ( decode ( raw_data ) ) else : yield raw_data ct + = 1 if limit and ct == limit : return",if not id_chunk :,if id_chunk == 0 :,95.24063363,FALSE,95.11
4377,"def _convert_example ( example , use_bfloat16 ) : """"""Cast int64 into int32 and float32 to bfloat16 if use_bfloat16."""""" for key in list ( example . keys ( ) ) : val = example [ key ] if tf . keras . backend . is_sparse ( val ) : val = tf . sparse . to_dense ( val ) <MASK> val = tf . cast ( val , tf . int32 ) if use_bfloat16 and val . dtype == tf . float32 : val = tf . cast ( val , tf . bfloat16 ) example [ key ] = val",if val . dtype == tf . int64 :,if use_bfloat16 and val . dtype == tf . int64 :,95.49966671,FALSE,95.84
4378,"def print_callees ( self , * amount ) : width , list = self . get_print_list ( amount ) if list : self . calc_callees ( ) self . print_call_heading ( width , "" called... "" ) for func in list : <MASK> self . print_call_line ( width , func , self . all_callees [ func ] ) else : self . print_call_line ( width , func , { } ) print >> self . stream print >> self . stream return self",if func in self . all_callees :,if func in self . all_callees :,100,TRUE,100
4379,"def on_task_input ( self , task , config ) : if config is False : return for entry in task . entries : <MASK> log_once ( "" Corrected ` %s ` url (replaced &amp; with &) "" % entry [ "" title "" ] , logger = log , ) entry [ "" url "" ] = entry [ "" url "" ] . replace ( "" &amp; "" , "" & "" )","if ""&amp;"" in entry [ ""url"" ] :","if entry [ ""url"" ] :",91.38283597,FALSE,94.01
4380,"def function ( self , inputs , outputs , ignore_empty = False ) : f = function ( inputs , outputs , mode = self . mode ) if self . mode is not None or theano . config . mode != "" FAST_COMPILE "" : topo = f . maker . fgraph . toposort ( ) topo_ = [ node for node in topo if not isinstance ( node . op , self . ignore_topo ) ] if ignore_empty : assert len ( topo_ ) < = 1 , topo_ else : assert len ( topo_ ) == 1 , topo_ <MASK> assert type ( topo_ [ 0 ] . op ) is self . op return f",if len ( topo_ ) > 0 :,if self . mode is not None :,92.82960435,FALSE,93.99
4381,"def _get_env_command ( self ) - > Sequence [ str ] : """"""Get command sequence for `env` with configured flags."""""" env_list = [ "" env "" ] # Pass through configurable environment variables. for key in [ "" http_proxy "" , "" https_proxy "" ] : value = self . build_provider_flags . get ( key ) <MASK> continue # Ensure item is treated as string and append it. value = str ( value ) env_list . append ( f "" { key } = { value } "" ) return env_list",if not value :,if value is None :,97.81752479,FALSE,96.54
4382,"def _compare_single_run ( self , compares_done ) : try : compare_id , redo = self . in_queue . get ( timeout = float ( self . config [ "" ExpertSettings "" ] [ "" block_delay "" ] ) ) except Empty : pass else : if self . _decide_whether_to_process ( compare_id , redo , compares_done ) : if redo : self . db_interface . delete_old_compare_result ( compare_id ) compares_done . add ( compare_id ) self . _process_compare ( compare_id ) <MASK> self . callback ( )",if self . callback :,if self . callback :,100,TRUE,100
4383,"def clean ( self ) : # TODO: check for clashes if the random code is already taken if not self . code : self . code = u "" static- %s "" % uuid . uuid4 ( ) if not self . site : placeholders = StaticPlaceholder . objects . filter ( code = self . code , site__isnull = True ) if self . pk : placeholders = placeholders . exclude ( pk = self . pk ) <MASK> raise ValidationError ( _ ( "" A static placeholder with the same site and code already exists "" ) )",if placeholders . exists ( ) :,if placeholders . exists ( ) :,75,TRUE,100
4384,"def load_parser ( self ) : result = OrderedDict ( ) for name , flags in self . filenames : filename = self . get_filename ( name ) for match in sorted ( glob ( filename ) , key = self . file_key ) : # Needed to allow overlapping globs, more specific first <MASK> continue result [ match ] = TextParser ( match , os . path . relpath ( match , self . base ) , flags ) return result",if match in result :,if match in self . _overlapped_globs :,97.96706222,FALSE,92.21
4385,"def __init__ ( self , selectable , name = None ) : baseselectable = selectable while isinstance ( baseselectable , Alias ) : baseselectable = baseselectable . element self . original = baseselectable self . supports_execution = baseselectable . supports_execution if self . supports_execution : self . _execution_options = baseselectable . _execution_options self . element = selectable if name is None : <MASK> name = getattr ( self . original , "" name "" , None ) name = _anonymous_label ( "" %% ( %d   %s )s "" % ( id ( self ) , name or "" anon "" ) ) self . name = name",if self . original . named_with_column :,"if hasattr ( self . original , ""name"" ) :",95.57667823,FALSE,92.8
4386,"def load_tour ( self , tour_id ) : for tour_dir in self . tour_directories : tour_path = os . path . join ( tour_dir , tour_id + "" .yaml "" ) if not os . path . exists ( tour_path ) : tour_path = os . path . join ( tour_dir , tour_id + "" .yml "" ) <MASK> return self . _load_tour_from_path ( tour_path )",if os . path . exists ( tour_path ) :,if os . path . exists ( tour_path ) :,100,TRUE,100
4387,"def _get_md_bg_color_down ( self ) : t = self . theme_cls c = self . md_bg_color # Default to no change on touch # Material design specifies using darker hue when on Dark theme if t . theme_style == "" Dark "" : <MASK> c = t . primary_dark elif self . md_bg_color == t . accent_color : c = t . accent_dark return c",if self . md_bg_color == t . primary_color :,if self . md_bg_color == t . primary_color :,100,TRUE,100
4388,"def get_data ( self , state = None , request = None ) : if self . load_in_memory : data , shapes = self . _in_memory_get_data ( state , request ) else : data , shapes = self . _out_of_memory_get_data ( state , request ) for i in range ( len ( data ) ) : if shapes [ i ] is not None : <MASK> data [ i ] = data [ i ] . reshape ( shapes [ i ] ) else : for j in range ( len ( data [ i ] ) ) : data [ i ] [ j ] = data [ i ] [ j ] . reshape ( shapes [ i ] [ j ] ) return tuple ( data )","if isinstance ( request , numbers . Integral ) :","if isinstance ( data [ i ] , np . ndarray ) :",94.792459,FALSE,94.45
4389,"def onClicked ( event ) : if not self . path : <MASK> os . makedirs ( mh . getPath ( "" render "" ) ) self . path = mh . getPath ( "" render "" ) filename , ftype = mh . getSaveFileName ( os . path . splitext ( self . path ) [ 0 ] , "" PNG Image (*.png);;JPEG Image (*.jpg);;Thumbnail (*.thumb);;All files (*.*) "" , ) if filename : if "" Thumbnail "" in ftype : self . image . save ( filename , iformat = "" PNG "" ) else : self . image . save ( filename ) self . path = os . path . dirname ( filename )","if not os . path . exists ( mh . getPath ( ""render"" ) ) :","if not os . path . exists ( mh . getPath ( ""render"" ) ) :",100,TRUE,100
4390,"def _build_dom ( cls , content , mode ) : assert mode in ( "" html "" , "" xml "" ) if mode == "" html "" : <MASK> THREAD_STORAGE . html_parser = HTMLParser ( ) dom = defusedxml . lxml . parse ( StringIO ( content ) , parser = THREAD_STORAGE . html_parser ) return dom . getroot ( ) else : if not hasattr ( THREAD_STORAGE , "" xml_parser "" ) : THREAD_STORAGE . xml_parser = XMLParser ( ) dom = defusedxml . lxml . parse ( BytesIO ( content ) , parser = THREAD_STORAGE . xml_parser ) return dom . getroot ( )","if not hasattr ( THREAD_STORAGE , ""html_parser"" ) :","if not hasattr ( THREAD_STORAGE , ""html_parser"" ) :",100,TRUE,100
4391,"def convert_path ( ctx , tpath ) : for points , code in tpath . iter_segments ( ) : <MASK> ctx . move_to ( * points ) elif code == Path . LINETO : ctx . line_to ( * points ) elif code == Path . CURVE3 : ctx . curve_to ( points [ 0 ] , points [ 1 ] , points [ 0 ] , points [ 1 ] , points [ 2 ] , points [ 3 ] ) elif code == Path . CURVE4 : ctx . curve_to ( * points ) elif code == Path . CLOSEPOLY : ctx . close_path ( )",if code == Path . MOVETO :,if code == Path . MOVETO :,75,TRUE,100
4392,"def _targets ( self , sigmaparser ) : # build list of matching target mappings targets = set ( ) for condfield in self . conditions : if condfield in sigmaparser . values : rulefieldvalues = sigmaparser . values [ condfield ] for condvalue in self . conditions [ condfield ] : <MASK> targets . update ( self . conditions [ condfield ] [ condvalue ] ) return targets",if condvalue in rulefieldvalues :,if condvalue in rulefieldvalues :,75,TRUE,100
4393,"def create_image_upload ( ) : if request . method == "" POST "" : image = request . form [ "" image "" ] <MASK> image_file = uploaded_file ( file_content = image ) image_url = upload_local ( image_file , UPLOAD_PATHS [ "" temp "" ] [ "" image "" ] . format ( uuid = uuid4 ( ) ) ) return jsonify ( { "" status "" : "" ok "" , "" image_url "" : image_url } ) else : return jsonify ( { "" status "" : "" no_image "" } )",if image :,if image :,100,TRUE,100
4394,"def lookup_actions ( self , resp ) : actions = { } for action , conditions in self . actions . items ( ) : for condition , opts in conditions : for key , val in condition : if key [ - 1 ] == "" ! "" : <MASK> break else : if not resp . match ( key , val ) : break else : actions [ action ] = opts return actions","if resp . match ( key [ : - 1 ] , val ) :","if not resp . match ( key , val ) :",89.54171998,FALSE,91.17
4395,"def accept_quality ( accept , default = 1 ) : """"""Separates out the quality score from the accepted content_type"""""" quality = default if accept and "" ; "" in accept : accept , rest = accept . split ( "" ; "" , 1 ) accept_quality = RE_ACCEPT_QUALITY . search ( rest ) <MASK> quality = float ( accept_quality . groupdict ( ) . get ( "" quality "" , quality ) . strip ( ) ) return ( quality , accept . strip ( ) )",if accept_quality :,if accept_quality :,100,TRUE,100
4396,"def save ( self , session = None , to = None , pickler = None ) : if to and pickler : self . _save_to = ( pickler , to ) if self . _save_to and len ( self ) > 0 : with self . _lock : pickler , fn = self . _save_to <MASK> session . ui . mark ( _ ( "" Saving  %s  state to  %s "" ) % ( self , fn ) ) pickler ( self , fn )",if session :,if pickler :,97.67066659,FALSE,97.44
4397,"def get_safe_settings ( ) : "" Returns a dictionary of the settings module, with sensitive settings blurred out. "" settings_dict = { } for k in dir ( settings ) : if k . isupper ( ) : <MASK> settings_dict [ k ] = "" ******************** "" else : settings_dict [ k ] = getattr ( settings , k ) return settings_dict",if HIDDEN_SETTINGS . search ( k ) :,"if k . startswith ( ""*"" ) :",92.89407867,FALSE,92.39
4398,def _init_table_h ( ) : _table_h = [ ] for i in range ( 256 ) : part_l = i part_h = 0 for j in range ( 8 ) : rflag = part_l & 1 part_l >> = 1 if part_h & 1 : part_l | = 1 << 31 part_h >> = 1 <MASK> part_h ^ = 0xD8000000 _table_h . append ( part_h ) return _table_h,if rflag :,if rflag :,100,TRUE,100
4399,"def dns_query ( server , timeout , protocol , qname , qtype , qclass ) : request = dns . message . make_query ( qname , qtype , qclass ) if protocol == "" tcp "" : response = dns . query . tcp ( request , server , timeout = timeout , one_rr_per_rrset = True ) else : response = dns . query . udp ( request , server , timeout = timeout , one_rr_per_rrset = True ) <MASK> response = dns . query . tcp ( request , server , timeout = timeout , one_rr_per_rrset = True ) return response",if response . flags & dns . flags . TC :,"elif protocol == ""tcp"" :",91.10422242,FALSE,91.37
4400,"def sum_and_divide ( self , losses ) : if self . total_divisor != 0 : output = torch . sum ( losses ) / self . total_divisor <MASK> # remove from autograd graph if necessary self . total_divisor = self . total_divisor . item ( ) return output return torch . sum ( losses * 0 )",if torch . is_tensor ( self . total_divisor ) :,if self . autograd_graph :,80.91819117,FALSE,84.38
4401,"def __iter__ ( self ) : for chunk in self . source : if chunk is not None : self . wait_counter = 0 yield chunk <MASK> self . wait_counter + = 1 else : logger . warning ( "" Data poller has been receiving no data for  {}  seconds. \n "" "" Closing data poller "" . format ( self . wait_cntr_max * self . poll_period ) ) break time . sleep ( self . poll_period )",elif self . wait_counter < self . wait_cntr_max :,elif self . wait_counter == self . wait_cntr_max :,98.5324207,FALSE,96.82
4402,"def test_find_directive_from_block ( self ) : blocks = self . config . parser_root . find_blocks ( "" virtualhost "" ) found = False for vh in blocks : <MASK> servername = vh . find_directives ( "" servername "" ) self . assertEqual ( servername [ 0 ] . parameters [ 0 ] , "" certbot.demo "" ) found = True self . assertTrue ( found )","if vh . filepath . endswith ( ""sites-enabled/certbot.conf"" ) :","if vh . name == ""virtualhost"" :",89.10696487,FALSE,86.25
4403,"def assign_products ( request , discount_id ) : """"""Assign products to given property group with given property_group_id."""""" discount = lfs_get_object_or_404 ( Discount , pk = discount_id ) for temp_id in request . POST . keys ( ) : <MASK> temp_id = temp_id . split ( "" - "" ) [ 1 ] product = Product . objects . get ( pk = temp_id ) discount . products . add ( product ) html = [ [ "" #products-inline "" , products_inline ( request , discount_id , as_string = True ) ] ] result = json . dumps ( { "" html "" : html , "" message "" : _ ( u "" Products have been assigned. "" ) } , cls = LazyEncoder ) return HttpResponse ( result , content_type = "" application/json "" )","if temp_id . startswith ( ""product"" ) :","if ""-"" in temp_id :",64.41988638,FALSE,94.99
4404,"def ChangeStyle ( self , combos ) : style = 0 for combo in combos : if combo . GetValue ( ) == 1 : <MASK> style = style | HTL . TR_VIRTUAL else : try : style = style | eval ( "" wx. "" + combo . GetLabel ( ) ) except : style = style | eval ( "" HTL. "" + combo . GetLabel ( ) ) if self . GetAGWWindowStyleFlag ( ) != style : self . SetAGWWindowStyleFlag ( style )","if combo . GetLabel ( ) == ""TR_VIRTUAL"" :","if combo . GetLabel ( ) == ""Trans"" :",98.65526439,FALSE,95.64
4405,"def _set_autocomplete ( self , notebook ) : if notebook : try : <MASK> notebook = NotebookInfo ( notebook ) obj , x = build_notebook ( notebook ) self . form . widgets [ "" namespace "" ] . notebook = obj self . form . widgets [ "" page "" ] . notebook = obj logger . debug ( "" Notebook for autocomplete:  %s  ( %s ) "" , obj , notebook ) except : logger . exception ( "" Could not set notebook:  %s "" , notebook ) else : self . form . widgets [ "" namespace "" ] . notebook = None self . form . widgets [ "" page "" ] . notebook = None logger . debug ( "" Notebook for autocomplete unset "" )","if isinstance ( notebook , str ) :","if not isinstance ( notebook , str ) :",97.05972788,FALSE,98.3
4406,"def emitSubDomainData ( self , subDomainData , event ) : self . emitRawRirData ( subDomainData , event ) for subDomainElem in subDomainData : <MASK> return None subDomain = subDomainElem . get ( "" subdomain "" , "" "" ) . strip ( ) if subDomain : self . emitHostname ( subDomain , event )",if self . checkForStop ( ) :,"if not subDomainElem . get ( ""domain"" ) :",87.61211569,FALSE,85.48
4407,"def get_all_subnets ( self , subnet_ids = None , filters = None ) : # Extract a list of all subnets matches = itertools . chain ( * [ x . values ( ) for x in self . subnets . values ( ) ] ) if subnet_ids : matches = [ sn for sn in matches if sn . id in subnet_ids ] <MASK> unknown_ids = set ( subnet_ids ) - set ( matches ) raise InvalidSubnetIdError ( unknown_ids ) if filters : matches = generic_filter ( filters , matches ) return matches",if len ( subnet_ids ) > len ( matches ) :,if len ( matches ) != len ( subnet_ids ) :,98.28745373,FALSE,96.32
4408,"def _compat_map ( self , avs ) : apps = { } for av in avs : av . version = self app_id = av . application <MASK> apps [ amo . APP_IDS [ app_id ] ] = av return apps",if app_id in amo . APP_IDS :,if app_id in amo . APP_IDS :,100,TRUE,100
4409,"def generator ( self , data ) : if self . _config . SILENT : silent_vars = self . _get_silent_vars ( ) for task in data : for var , val in task . environment_variables ( ) : if self . _config . SILENT : <MASK> continue yield ( 0 , [ int ( task . UniqueProcessId ) , str ( task . ImageFileName ) , Address ( task . Peb . ProcessParameters . Environment ) , str ( var ) , str ( val ) , ] , )",if var in silent_vars :,if var in silent_vars :,100,TRUE,100
4410,"def warn_if_repeatable_read ( self ) : if "" mysql "" in self . current_engine ( ) . lower ( ) : cursor = self . connection_for_read ( ) . cursor ( ) if cursor . execute ( "" SELECT @@tx_isolation "" ) : isolation = cursor . fetchone ( ) [ 0 ] <MASK> warnings . warn ( TxIsolationWarning ( "" Polling results with transaction isolation level  "" "" repeatable-read within the same transaction  "" "" may give outdated results. Be sure to commit the  "" "" transaction for each poll iteration. "" ) )","if isolation == ""REPEATABLE-READ"" :",if isolation > 1 :,93.8102281,FALSE,94.5
4411,"def filter_by_level ( record , level_per_module ) : name = record [ "" name "" ] level = 0 if name in level_per_module : level = level_per_module [ name ] elif name is not None : lookup = "" "" <MASK> level = level_per_module [ "" "" ] for n in name . split ( "" . "" ) : lookup + = n if lookup in level_per_module : level = level_per_module [ lookup ] lookup + = "" . "" if level is False : return False return record [ "" level "" ] . no > = level","if """" in level_per_module :","if ""."" in level_per_module :",98.79614303,FALSE,98.16
4412,"def _readStream ( self , handle : str , path : str ) - > None : eof = False file = Path ( path ) with file . open ( "" w "" ) as f : while not eof : response = await self . _client . send ( "" IO.read "" , { "" handle "" : handle } ) eof = response . get ( "" eof "" , False ) <MASK> f . write ( response . get ( "" data "" , "" "" ) ) await self . _client . send ( "" IO.close "" , { "" handle "" : handle } )",if path :,if response :,98.02429603,FALSE,97.88
4413,"def sendall ( self , data , flags = 0 ) : if self . _sslobj : <MASK> raise ValueError ( "" non-zero flags not allowed in calls to sendall() on  %s "" % self . __class__ ) amount = len ( data ) count = 0 while count < amount : v = self . send ( data [ count : ] ) count + = v return amount else : return socket . sendall ( self , data , flags )",if flags != 0 :,if flags != 0 :,100,TRUE,100
4414,"def run ( self ) : utils . assert_main_thread ( ) # As a convenience, we'll set up the connection # if there isn't one. So F5 (etc) can be hit # to get started. if not channel : <MASK> SwiDebugStartChromeCommand . run ( self ) else : self . window . run_command ( "" swi_debug_start "" ) elif paused : logger . info ( "" Resuming... "" ) channel . send ( webkit . Debugger . resume ( ) ) else : logger . info ( "" Pausing... "" ) channel . send ( webkit . Debugger . setSkipAllPauses ( False ) ) channel . send ( webkit . Debugger . pause ( ) )",if not chrome_launched ( ) :,if self . window . is_running ( ) :,72.80545304,FALSE,94.32
4415,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 10 : length = d . getVarInt32 ( ) tmp = ProtocolBuffer . Decoder ( d . buffer ( ) , d . pos ( ) , d . pos ( ) + length ) d . skip ( length ) self . add_presence_response ( ) . TryMerge ( tmp ) continue <MASK> raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 0 :,if tt == 0 :,100,TRUE,100
4416,"def _replace_home ( x ) : if xp . ON_WINDOWS : home = ( builtins . __xonsh__ . env [ "" HOMEDRIVE "" ] + builtins . __xonsh__ . env [ "" HOMEPATH "" ] [ 0 ] ) if x . startswith ( home ) : x = x . replace ( home , "" ~ "" , 1 ) <MASK> x = x . replace ( os . sep , os . altsep ) return x else : home = builtins . __xonsh__ . env [ "" HOME "" ] if x . startswith ( home ) : x = x . replace ( home , "" ~ "" , 1 ) return x","if builtins . __xonsh__ . env . get ( ""FORCE_POSIX_PATHS"" ) :",if x . startswith ( os . sep ) :,71.57729631,FALSE,86.8
4417,"def semanticTags ( self , semanticTags ) : if semanticTags is None : self . __semanticTags = OrderedDict ( ) # check for key , value in list ( semanticTags . items ( ) ) : <MASK> raise TypeError ( "" At least one key is not a valid int position "" ) if not isinstance ( value , list ) : raise TypeError ( "" At least one value of the provided dict is not a list of string "" ) for x in value : if not isinstance ( x , str ) : raise TypeError ( "" At least one value of the provided dict is not a list of string "" ) self . __semanticTags = semanticTags","if not isinstance ( key , int ) :","if not isinstance ( key , int ) :",100,TRUE,100
4418,"def _recv ( ) : try : return sock . recv ( bufsize ) except SSLWantReadError : pass except socket . error as exc : error_code = extract_error_code ( exc ) if error_code is None : raise <MASK> raise r , w , e = select . select ( ( sock , ) , ( ) , ( ) , sock . gettimeout ( ) ) if r : return sock . recv ( bufsize )",if error_code != errno . EAGAIN or error_code != errno . EWOULDBLOCK :,if error_code != errno . ECONNRESET :,88.39594483,FALSE,88.39
4419,"def _authenticate ( self ) : oauth_token = self . options . get ( "" oauth_token "" ) if oauth_token and not self . api . oauth_token : self . logger . info ( "" Attempting to authenticate using OAuth token "" ) self . api . oauth_token = oauth_token user = self . api . user ( schema = _user_schema ) <MASK> self . logger . info ( "" Successfully logged in as  {0} "" , user ) else : self . logger . error ( "" Failed to authenticate, the access token  "" "" is not valid "" ) else : return JustinTVPluginBase . _authenticate ( self )",if user :,if user . is_authenticated :,69.54810243,FALSE,95.9
4420,"def reverse ( self , * args ) : assert self . _path is not None , "" Cannot reverse url regex  "" + self . regex . pattern assert len ( args ) == self . _group_count , "" required number of arguments  "" "" not found "" if not len ( args ) : return self . _path converted_args = [ ] for a in args : <MASK> a = str ( a ) converted_args . append ( escape . url_escape ( utf8 ( a ) , plus = False ) ) return self . _path % tuple ( converted_args )","if not isinstance ( a , ( unicode_type , bytes ) ) :","if not isinstance ( a , ( unicode , bytes ) ) :",98.9102713,FALSE,97.31
4421,"def determine_block_hints ( self , text ) : hints = "" "" if text : <MASK> hints + = str ( self . best_indent ) if text [ - 1 ] not in "" \n \x85 \u2028 \u2029 "" : hints + = "" - "" elif len ( text ) == 1 or text [ - 2 ] in "" \n \x85 \u2028 \u2029 "" : hints + = "" + "" return hints","if text [ 0 ] in "" \n\x85\u2028\u2029"" :",if len ( text ) > 1 :,91.20283557,FALSE,84.19
4422,"def find_package_modules ( package , mask ) : import fnmatch if hasattr ( package , "" __loader__ "" ) and hasattr ( package . __loader__ , "" _files "" ) : path = package . __name__ . replace ( "" . "" , os . path . sep ) mask = os . path . join ( path , mask ) for fnm in package . __loader__ . _files . iterkeys ( ) : <MASK> yield os . path . splitext ( fnm ) [ 0 ] . replace ( os . path . sep , "" . "" ) else : path = package . __path__ [ 0 ] for fnm in os . listdir ( path ) : <MASK> yield "" %s . %s "" % ( package . __name__ , os . path . splitext ( fnm ) [ 0 ] )","if fnmatch . fnmatchcase ( fnm , mask ) :","if fnmatch . fnmatch ( fnm , mask ) :",98.46021501,FALSE,97.4
4423,"def _condition ( ct ) : for qobj in args : if qobj . connector == "" AND "" and not qobj . negated : # normal kwargs are an AND anyway, so just use those for now for child in qobj . children : kwargs . update ( dict ( [ child ] ) ) else : raise NotImplementedError ( "" Unsupported Q object "" ) for attr , val in kwargs . items ( ) : <MASK> return False return True","if getattr ( ct , attr ) != val :","if not isinstance ( val , ct ) :",95.47542413,FALSE,90.12
4424,"def process ( self , resources ) : session = local_session ( self . manager . session_factory ) client = session . client ( "" logs "" ) state = self . data . get ( "" state "" , True ) key = self . resolve_key ( self . data . get ( "" kms-key "" ) ) for r in resources : try : <MASK> client . associate_kms_key ( logGroupName = r [ "" logGroupName "" ] , kmsKeyId = key ) else : client . disassociate_kms_key ( logGroupName = r [ "" logGroupName "" ] ) except client . exceptions . ResourceNotFoundException : continue",if state :,if state :,100,TRUE,100
4425,"def get_xmm ( env , ii ) : if is_gather ( ii ) : <MASK> return gen_reg_simd_unified ( env , "" xmm_evex "" , True ) return gen_reg_simd_unified ( env , "" xmm "" , False ) <MASK> return gen_reg ( env , "" xmm_evex "" ) return gen_reg ( env , "" xmm "" )","if ii . space == ""evex"" :","if ii . get ( ""evex"" ) :",87.44337807,FALSE,87.38
4426,"def parent ( self ) : """"""Return the parent device."""""" if self . _has_parent is None : _parent = self . _ctx . backend . get_parent ( self . _ctx . dev ) self . _has_parent = _parent is not None <MASK> self . _parent = Device ( _parent , self . _ctx . backend ) else : self . _parent = None return self . _parent",if self . _has_parent :,if _parent is not None :,80.59828291,FALSE,92.97
4427,"def cascade ( self , event = None ) : """"""Cascade all Leo windows."""""" x , y , delta = 50 , 50 , 50 for frame in g . app . windowList : w = frame and frame . top <MASK> r = w . geometry ( ) # a Qt.Rect # 2011/10/26: Fix bug 823601: cascade-windows fails. w . setGeometry ( QtCore . QRect ( x , y , r . width ( ) , r . height ( ) ) ) # Compute the new offsets. x + = 30 y + = 30 if x > 200 : x = 10 + delta y = 40 + delta delta + = 10",if w :,if w :,100,TRUE,100
4428,"def _GetGoodDispatchAndUserName ( IDispatch , userName , clsctx ) : # Get a dispatch object, and a 'user name' (ie, the name as # displayed to the user in repr() etc. if userName is None : if isinstance ( IDispatch , str ) : userName = IDispatch <MASK> # We always want the displayed name to be a real string userName = IDispatch . encode ( "" ascii "" , "" replace "" ) elif type ( userName ) == unicode : # As above - always a string... userName = userName . encode ( "" ascii "" , "" replace "" ) else : userName = str ( userName ) return ( _GetGoodDispatch ( IDispatch , clsctx ) , userName )","elif isinstance ( IDispatch , unicode ) :",elif type ( IDispatch ) == str :,72.47247249,FALSE,94.63
4429,"def _infer_return_type ( * args ) : """"""Look at the type of all args and divine their implied return type."""""" return_type = None for arg in args : if arg is None : continue if isinstance ( arg , bytes ) : if return_type is str : raise TypeError ( "" Can ' t mix bytes and non-bytes in  "" "" path components. "" ) return_type = bytes else : <MASK> raise TypeError ( "" Can ' t mix bytes and non-bytes in  "" "" path components. "" ) return_type = str if return_type is None : return str # tempfile APIs return a str by default. return return_type",if return_type is bytes :,if return_type is str :,98.89819868,FALSE,98.21
4430,"def test_ESPnetDataset_h5file_1 ( h5file_1 ) : dataset = IterableESPnetDataset ( path_name_type_list = [ ( h5file_1 , "" data4 "" , "" hdf5 "" ) ] , preprocess = preprocess , ) for key , data in dataset : if key == "" a "" : assert data [ "" data4 "" ] . shape == ( 100 , 80 , ) <MASK> assert data [ "" data4 "" ] . shape == ( 150 , 80 , )","if key == ""b"" :","if key == ""b"" :",100,TRUE,100
4431,"def iter_fields ( node , * , include_meta = True , exclude_unset = False ) : exclude_meta = not include_meta for field_name , field in node . _fields . items ( ) : if exclude_meta and field . meta : continue field_val = getattr ( node , field_name , _marker ) <MASK> continue if exclude_unset : if callable ( field . default ) : default = field . default ( ) else : default = field . default if field_val == default : continue yield field_name , field_val",if field_val is _marker :,if field_val is None :,93.17234267,FALSE,97.14
4432,"def then ( self , matches , when_response , context ) : if is_iterable ( when_response ) : ret = [ ] when_response = list ( when_response ) for match in when_response : <MASK> if self . match_name : match . name = self . match_name matches . append ( match ) ret . append ( match ) return ret if self . match_name : when_response . name = self . match_name if when_response not in matches : matches . append ( when_response ) return when_response",if match not in matches :,"if isinstance ( match , Match ) :",93.497755,FALSE,93.91
4433,"def _set_chat_ids ( self , chat_id : SLT [ int ] ) - > None : with self . __lock : <MASK> raise RuntimeError ( f "" Can ' t set  { self . chat_id_name }  in conjunction with (already set)  "" f "" { self . username_name } s. "" ) self . _chat_ids = self . _parse_chat_id ( chat_id )",if chat_id and self . _usernames :,if self . _chat_ids :,88.44482395,FALSE,92.48
4434,"def discover ( self , * objlist ) : ret = [ ] for l in self . splitlines ( ) : <MASK> continue if l [ 0 ] == "" Filename "" : continue try : int ( l [ 2 ] ) int ( l [ 3 ] ) except : continue #           ret.append(improve(l[0])) ret . append ( l [ 0 ] ) ret . sort ( ) for item in objlist : ret . append ( item ) return ret",if len ( l ) < 5 :,if len ( l ) < 4 :,98.59996198,FALSE,97.49
4435,"def get_changed_module ( self ) : source = self . resource . read ( ) change_collector = codeanalyze . ChangeCollector ( source ) if self . replacement is not None : change_collector . add_change ( self . skip_start , self . skip_end , self . replacement ) for occurrence in self . occurrence_finder . find_occurrences ( self . resource ) : start , end = occurrence . get_primary_range ( ) <MASK> self . handle . occurred_inside_skip ( change_collector , occurrence ) else : self . handle . occurred_outside_skip ( change_collector , occurrence ) result = change_collector . get_changed ( ) if result is not None and result != source : return result",if self . skip_start <= start < self . skip_end :,if start < self . skip_start and end < self . skip_end :,93.99743957,FALSE,96.5
4436,"def hpat_pandas_series_var_impl ( self , axis = None , skipna = None , level = None , ddof = 1 , numeric_only = None ) : if skipna is None : skipna = True if skipna : valuable_length = len ( self . _data ) - numpy . sum ( numpy . isnan ( self . _data ) ) <MASK> return numpy . nan return ( numpy_like . nanvar ( self . _data ) * valuable_length / ( valuable_length - ddof ) ) if len ( self . _data ) < = ddof : return numpy . nan return self . _data . var ( ) * len ( self . _data ) / ( len ( self . _data ) - ddof )",if valuable_length <= ddof :,if valuable_length <= ddof :,100,TRUE,100
4437,"def to_dict ( self , validate = True , ignore = ( ) , context = None ) : context = context or { } condition = getattr ( self , "" condition "" , Undefined ) copy = self # don't copy unless we need to if condition is not Undefined : <MASK> pass elif "" field "" in condition and "" type "" not in condition : kwds = parse_shorthand ( condition [ "" field "" ] , context . get ( "" data "" , None ) ) copy = self . copy ( deep = [ "" condition "" ] ) copy . condition . update ( kwds ) return super ( ValueChannelMixin , copy ) . to_dict ( validate = validate , ignore = ignore , context = context )","if isinstance ( condition , core . SchemaBase ) :","if isinstance ( condition , ( list , tuple ) ) :",98.12062869,FALSE,95.67
4438,"def get_field_result ( self , result , field_name ) : if isinstance ( result . field , models . ImageField ) : <MASK> img = getattr ( result . obj , field_name ) result . text = mark_safe ( ' <a href= "" %s ""  target= "" _blank ""  title= "" %s ""  data-gallery= "" gallery "" ><img src= "" %s ""  class= "" field_img "" /></a> ' % ( img . url , result . label , img . url ) ) self . include_image = True return result",if result . value :,if not self . include_image :,92.45345236,FALSE,94.3
4439,"def run ( self ) : try : while True : dp = self . queue_get_stoppable ( self . inq ) <MASK> return # cannot ignore None here. will lead to unsynced send/recv obj = self . func ( dp ) self . queue_put_stoppable ( self . outq , obj ) except Exception : <MASK> pass # skip duplicated error messages else : raise finally : self . stop ( )",if self . stopped ( ) :,if dp is None :,59.63465248,FALSE,85.86
4440,"def _evaluate_local_single ( self , iterator ) : for batch in iterator : in_arrays = convert . _call_converter ( self . converter , batch , self . device ) with function . no_backprop_mode ( ) : if isinstance ( in_arrays , tuple ) : results = self . calc_local ( * in_arrays ) elif isinstance ( in_arrays , dict ) : results = self . calc_local ( * * in_arrays ) else : results = self . calc_local ( in_arrays ) <MASK> self . _progress_hook ( batch ) yield results",if self . _progress_hook :,if self . _progress_hook :,100,TRUE,100
4441,"def merge ( self , other ) : d = self . _name2ft for name , ( f , t ) in other . _name2ft . items ( ) : <MASK> # Don't print here by default, since doing #     so breaks some of the buildbots # print ""*** DocTestRunner.merge: '"" + name + ""' in both"" \ #    "" testers; summing outcomes."" f2 , t2 = d [ name ] f = f + f2 t = t + t2 d [ name ] = f , t",if name in d :,if name in d :,100,TRUE,100
4442,"def _addSettingsToPanels ( self , category , left , right ) : count = len ( profile . getSubCategoriesFor ( category ) ) + len ( profile . getSettingsForCategory ( category ) ) p = left n = 0 for title in profile . getSubCategoriesFor ( category ) : n + = 1 + len ( profile . getSettingsForCategory ( category , title ) ) <MASK> p = right configBase . TitleRow ( p , _ ( title ) ) for s in profile . getSettingsForCategory ( category , title ) : configBase . SettingRow ( p , s . getName ( ) )",if n > count / 2 :,if n > count :,92.43729603,FALSE,96.8
4443,"def __init__ ( self , parent , dir , mask , with_dirs = True ) : filelist = [ ] dirlist = [ "" .. "" ] self . dir = dir self . file = "" "" mask = mask . upper ( ) pattern = self . MakeRegex ( mask ) for i in os . listdir ( dir ) : <MASK> continue path = os . path . join ( dir , i ) if os . path . isdir ( path ) : dirlist . append ( i ) continue path = path . upper ( ) value = i . upper ( ) if pattern . match ( value ) is not None : filelist . append ( i ) self . files = filelist if with_dirs : self . dirs = dirlist","if i == ""."" or i == "".."" :","if i . startswith ( ""."" ) :",95.24438839,FALSE,92
4444,def check_network_private ( test_network ) : test_net = ipaddress . IPNetwork ( test_network ) test_start = test_net . network test_end = test_net . broadcast for network in settings . vpn . safe_priv_subnets : network = ipaddress . IPNetwork ( network ) net_start = network . network net_end = network . broadcast <MASK> return True return False,if test_start >= net_start and test_end <= net_end :,if net_start <= test_start and net_end <= test_end :,94.29515758,FALSE,91.1
4445,"def _end_description ( self ) : if self . _summaryKey == "" content "" : self . _end_content ( ) else : value = self . popContent ( "" description "" ) context = self . _getContext ( ) <MASK> context [ "" textinput "" ] [ "" description "" ] = value elif self . inimage : context [ "" image "" ] [ "" description "" ] = value self . _summaryKey = None",if self . intextinput :,if self . intextinput :,100,TRUE,100
4446,def compute_nullable_nonterminals ( self ) : nullable = { } num_nullable = 0 while 1 : for p in self . grammar . Productions [ 1 : ] : <MASK> nullable [ p . name ] = 1 continue for t in p . prod : if not t in nullable : break else : nullable [ p . name ] = 1 if len ( nullable ) == num_nullable : break num_nullable = len ( nullable ) return nullable,if p . len == 0 :,"if p . type in ( ""Union"" , ""Union"" ) :",87.08648535,FALSE,87.9
4447,"def process_bind_param ( self , value , dialect ) : if value is not None : if MAX_METADATA_VALUE_SIZE is not None : for k , v in list ( value . items ( ) ) : sz = total_size ( v ) <MASK> del value [ k ] log . warning ( "" Refusing to bind metadata key  {}  due to size ( {} ) "" . format ( k , sz ) ) value = json_encoder . encode ( value ) . encode ( ) return value",if sz > MAX_METADATA_VALUE_SIZE :,if sz > MAX_METADATA_VALUE_SIZE :,100,TRUE,100
4448,"def process_input_line ( self , line , store_history = True ) : """"""process the input, capturing stdout"""""" stdout = sys . stdout splitter = self . IP . input_splitter try : sys . stdout = self . cout splitter . push ( line ) more = splitter . push_accepts_more ( ) <MASK> try : source_raw = splitter . source_raw_reset ( ) [ 1 ] except : # recent ipython #4504 source_raw = splitter . raw_reset ( ) self . IP . run_cell ( source_raw , store_history = store_history ) finally : sys . stdout = stdout",if not more :,if not more :,100,TRUE,100
4449,"def _dump_section ( self , name , values , f ) : doc = "" __doc__ "" <MASK> print ( "" #  %s "" % values [ doc ] , file = f ) print ( "" %s ( "" % name , file = f ) for k , v in values . items ( ) : if k . endswith ( "" __doc__ "" ) : continue doc = k + "" __doc__ "" <MASK> print ( ""     #  %s "" % values [ doc ] , file = f ) print ( ""      %s  =  %s , "" % ( k , pprint . pformat ( v , indent = 8 ) ) , file = f ) print ( "" ) \n "" , file = f )",if doc in values :,if doc in values :,100,TRUE,100
4450,"def open_session ( self , app , request ) : sid = request . cookies . get ( app . session_cookie_name ) if sid : stored_session = self . cls . objects ( sid = sid ) . first ( ) if stored_session : expiration = stored_session . expiration <MASK> expiration = expiration . replace ( tzinfo = utc ) if expiration > datetime . datetime . utcnow ( ) . replace ( tzinfo = utc ) : return MongoEngineSession ( initial = stored_session . data , sid = stored_session . sid ) return MongoEngineSession ( sid = str ( uuid . uuid4 ( ) ) )",if not expiration . tzinfo :,if expiration . tzinfo :,97.36301038,FALSE,98.02
4451,"def table_entry ( mode1 , bind_type1 , mode2 , bind_type2 ) : with sock ( mode1 ) as sock1 : bind ( sock1 , bind_type1 ) try : with sock ( mode2 ) as sock2 : bind ( sock2 , bind_type2 ) except OSError as exc : <MASK> return "" INUSE "" elif exc . winerror == errno . WSAEACCES : return "" ACCESS "" raise else : return "" Success """,if exc . winerror == errno . WSAEADDRINUSE :,if exc . winerror == errno . EINVAL :,98.42433826,FALSE,97.21
4452,"def __init__ ( self , ruleset ) : # Organize rules by path self . ruleset = ruleset self . rules = { } for filename in self . ruleset . rules : for rule in self . ruleset . rules [ filename ] : <MASK> continue manage_dictionary ( self . rules , rule . path , [ ] ) self . rules [ rule . path ] . append ( rule )",if not rule . enabled :,if not rule . path :,73.37188563,FALSE,96.94
4453,"def talk ( self , words ) : if self . writeSentence ( words ) == 0 : return r = [ ] while 1 : i = self . readSentence ( ) if len ( i ) == 0 : continue reply = i [ 0 ] attrs = { } for w in i [ 1 : ] : j = w . find ( "" = "" , 1 ) <MASK> attrs [ w ] = "" "" else : attrs [ w [ : j ] ] = w [ j + 1 : ] r . append ( ( reply , attrs ) ) if reply == "" !done "" : return r",if j == - 1 :,if j == - 1 :,100,TRUE,100
4454,"def _check_decorator_overload ( name : str , old : str , new : str ) - > int : """"""Conditions for a decorator to overload an existing one."""""" properties = _property_decorators ( name ) if old == new : return _MERGE elif old in properties and new in properties : p_old , p_new = properties [ old ] . precedence , properties [ new ] . precedence <MASK> return _DISCARD elif p_old == p_new : return _MERGE else : return _REPLACE raise OverloadedDecoratorError ( name , "" "" )",if p_old > p_new :,if p_old == p_new :,98.64802025,FALSE,97.2
4455,"def validate_pk ( self ) : try : self . _key = serialization . load_pem_private_key ( self . key , password = None , backend = default_backend ( ) ) if self . _key . key_size > 2048 : AWSValidationException ( "" The private key length is not supported. Only 1024-bit and 2048-bit are allowed. "" ) except Exception as err : <MASK> raise raise AWSValidationException ( "" The private key is not PEM-encoded or is not valid. "" )","if isinstance ( err , AWSValidationException ) :","if err . errno != ""PEM-encoded"" :",89.15687188,FALSE,91.45
4456,"def _add_custom_statement ( self , custom_statements ) : if custom_statements is None : return self . resource_policy [ "" Version "" ] = "" 2012-10-17 "" if self . resource_policy . get ( "" Statement "" ) is None : self . resource_policy [ "" Statement "" ] = custom_statements else : <MASK> custom_statements = [ custom_statements ] statement = self . resource_policy [ "" Statement "" ] if not isinstance ( statement , list ) : statement = [ statement ] for s in custom_statements : if s not in statement : statement . append ( s ) self . resource_policy [ "" Statement "" ] = statement","if not isinstance ( custom_statements , list ) :","if not isinstance ( custom_statements , list ) :",100,TRUE,100
4457,"def load ( self , repn ) : for key in repn : tmp = self . _convert ( key ) <MASK> self . declare ( tmp ) item = dict . __getitem__ ( self , tmp ) item . _active = True item . load ( repn [ key ] )",if tmp not in self :,if tmp not in self . _active :,92.17310564,FALSE,92.97
4458,"def on_press_release ( x ) : """"""Keyboard callback function."""""" global is_recording , enable_trigger_record press = keyboard . KeyboardEvent ( "" down "" , 28 , "" space "" ) release = keyboard . KeyboardEvent ( "" up "" , 28 , "" space "" ) if x . event_type == "" down "" and x . name == press . name : if ( not is_recording ) and enable_trigger_record : sys . stdout . write ( "" Start Recording ...  "" ) sys . stdout . flush ( ) is_recording = True if x . event_type == "" up "" and x . name == release . name : <MASK> is_recording = False",if is_recording == True :,if ( not is_recording ) and enable_trigger_record :,96.01130149,FALSE,92.55
4459,"def apply_mask ( self , mask , data_t , data_f ) : ind_t , ind_f = 0 , 0 out = [ ] for m in cycle ( mask ) : if m : if ind_t == len ( data_t ) : return out out . append ( data_t [ ind_t ] ) ind_t + = 1 else : <MASK> return out out . append ( data_f [ ind_f ] ) ind_f + = 1 return out",if ind_f == len ( data_f ) :,if ind_f == len ( data_f ) :,100,TRUE,100
4460,"def oo_contains_rule ( source , apiGroups , resources , verbs ) : """"""Return true if the specified rule is contained within the provided source"""""" rules = source [ "" rules "" ] if rules : for rule in rules : if set ( rule [ "" apiGroups "" ] ) == set ( apiGroups ) : if set ( rule [ "" resources "" ] ) == set ( resources ) : <MASK> return True return False","if set ( rule [ ""verbs"" ] ) == set ( verbs ) :","if verbs in rule [ ""verbs"" ] :",90.16987555,FALSE,88.46
4461,"def _maybe_commit_artifact ( self , artifact_id ) : artifact_status = self . _artifacts [ artifact_id ] if artifact_status [ "" pending_count "" ] == 0 and artifact_status [ "" commit_requested "" ] : for callback in artifact_status [ "" pre_commit_callbacks "" ] : callback ( ) <MASK> self . _api . commit_artifact ( artifact_id ) for callback in artifact_status [ "" post_commit_callbacks "" ] : callback ( )","if artifact_status [ ""finalize"" ] :","if artifact_status [ ""commit_requested"" ] :",98.25777289,FALSE,96.09
4462,"def shuffler ( iterator , pool_size = 10 * * 5 , refill_threshold = 0.9 ) : yields_between_refills = round ( pool_size * ( 1 - refill_threshold ) ) # initialize pool; this step may or may not exhaust the iterator. pool = take_n ( pool_size , iterator ) while True : random . shuffle ( pool ) for i in range ( yields_between_refills ) : yield pool . pop ( ) next_batch = take_n ( yields_between_refills , iterator ) <MASK> break pool . extend ( next_batch ) # finish consuming whatever's left - no need for further randomization. yield from pool",if not next_batch :,if len ( next_batch ) == 0 :,97.38319545,FALSE,93.7
4463,"def __getitem__ ( self , key , _get_mode = False ) : if not _get_mode : if isinstance ( key , ( int , long ) ) : return self . _list [ key ] <MASK> return self . __class__ ( self . _list [ key ] ) ikey = key . lower ( ) for k , v in self . _list : if k . lower ( ) == ikey : return v # micro optimization: if we are in get mode we will catch that # exception one stack level down so we can raise a standard # key error instead of our special one. if _get_mode : raise KeyError ( ) raise BadRequestKeyError ( key )","elif isinstance ( key , slice ) :","elif isinstance ( key , str ) :",99.01539752,FALSE,98.31
4464,"def find ( self , path ) : if os . path . isfile ( path ) or os . path . islink ( path ) : self . num_files = self . num_files + 1 if self . match_function ( path ) : self . files . append ( path ) elif os . path . isdir ( path ) : for content in os . listdir ( path ) : file = os . path . join ( path , content ) if os . path . isfile ( file ) or os . path . islink ( file ) : self . num_files = self . num_files + 1 <MASK> self . files . append ( file ) else : self . find ( file )",if self . match_function ( file ) :,if self . match_function ( file ) :,100,TRUE,100
4465,"def validate_nb ( self , nb ) : super ( MetadataValidatorV3 , self ) . validate_nb ( nb ) ids = set ( [ ] ) for cell in nb . cells : <MASK> continue grade = cell . metadata [ "" nbgrader "" ] [ "" grade "" ] solution = cell . metadata [ "" nbgrader "" ] [ "" solution "" ] locked = cell . metadata [ "" nbgrader "" ] [ "" locked "" ] if not grade and not solution and not locked : continue grade_id = cell . metadata [ "" nbgrader "" ] [ "" grade_id "" ] if grade_id in ids : raise ValidationError ( "" Duplicate grade id:  {} "" . format ( grade_id ) ) ids . add ( grade_id )","if ""nbgrader"" not in cell . metadata :","if cell . metadata [ ""nbgrader"" ] [ ""type"" ] != ""notebook""",92.36002019,FALSE,89.93
4466,"def _skip_start ( self ) : start , stop = self . start , self . stop for chunk in self . app_iter : self . _pos + = len ( chunk ) if self . _pos < start : continue elif self . _pos == start : return b "" "" else : chunk = chunk [ start - self . _pos : ] <MASK> chunk = chunk [ : stop - self . _pos ] assert len ( chunk ) == stop - start return chunk else : raise StopIteration ( )",if stop is not None and self . _pos > stop :,if stop :,78.71825303,FALSE,90.5
4467,"def _SetUser ( self , users ) : for user in users . items ( ) : username = user [ 0 ] settings = user [ 1 ] room = settings [ "" room "" ] [ "" name "" ] if "" room "" in settings else None file_ = settings [ "" file "" ] if "" file "" in settings else None if "" event "" in settings : if "" joined "" in settings [ "" event "" ] : self . _client . userlist . addUser ( username , room , file_ ) <MASK> self . _client . removeUser ( username ) else : self . _client . userlist . modUser ( username , room , file_ )","elif ""left"" in settings [ ""event"" ] :","elif ""removed"" in settings [ ""event"" ] :",99.04567827,FALSE,98.21
4468,"def run_tests ( ) : # type: () -> None x = 5 with switch ( x ) as case : if case ( 0 ) : print ( "" zero "" ) print ( "" zero "" ) <MASK> print ( "" one or two "" ) elif case ( 3 , 4 ) : print ( "" three or four "" ) else : print ( "" default "" ) print ( "" another "" )","elif case ( 1 , 2 ) :","elif case ( 1 , 2 ) :",75,TRUE,100
4469,"def _populate ( ) : for fname in glob . glob ( os . path . join ( os . path . dirname ( __file__ ) , "" data "" , "" *.json "" ) ) : with open ( fname ) as inf : data = json . load ( inf ) data = data [ list ( data . keys ( ) ) [ 0 ] ] data = data [ list ( data . keys ( ) ) [ 0 ] ] for item in data : <MASK> LOGGER . warning ( "" Repeated emoji  {} "" . format ( item [ "" key "" ] ) ) else : TABLE [ item [ "" key "" ] ] = item [ "" value "" ]","if item [ ""key"" ] in TABLE :","if item [ ""key"" ] in TABLE :",100,TRUE,100
4470,"def slot_to_material ( bobject : bpy . types . Object , slot : bpy . types . MaterialSlot ) : mat = slot . material # Pick up backed material if present if mat is not None : baked_mat = mat . name + "" _ "" + bobject . name + "" _baked "" <MASK> mat = bpy . data . materials [ baked_mat ] return mat",if baked_mat in bpy . data . materials :,if baked_mat in bpy . data . materials :,100,TRUE,100
4471,"def __keyPress ( self , widget , event ) : if event . key == "" G "" and event . modifiers & event . Modifiers . Control : if not all ( hasattr ( p , "" isGanged "" ) for p in self . getPlugs ( ) ) : return False <MASK> self . __ungang ( ) else : self . __gang ( ) return True return False",if all ( p . isGanged ( ) for p in self . getPlugs ( ) ) :,elif event . modifiers & event . Modifiers . Control :,70.38557541,FALSE,80.34
4472,"def check_expected ( result , expected , contains = False ) : if sys . version_info [ 0 ] > = 3 : if isinstance ( result , str ) : result = result . encode ( "" ascii "" ) <MASK> expected = expected . encode ( "" ascii "" ) resultlines = result . splitlines ( ) expectedlines = expected . splitlines ( ) if len ( resultlines ) != len ( expectedlines ) : return False for rline , eline in zip ( resultlines , expectedlines ) : if contains : if eline not in rline : return False else : if not rline . endswith ( eline ) : return False return True","if isinstance ( expected , str ) :","if isinstance ( expected , str ) :",100,TRUE,100
4473,"def hosts_to_domains ( self , hosts , exclusions = [ ] ) : domains = [ ] for host in hosts : elements = host . split ( "" . "" ) # recursively walk through the elements # extracting all possible (sub)domains while len ( elements ) > = 2 : # account for domains stored as hosts if len ( elements ) == 2 : domain = "" . "" . join ( elements ) else : # drop the host element domain = "" . "" . join ( elements [ 1 : ] ) <MASK> domains . append ( domain ) del elements [ 0 ] return domains",if domain not in domains + exclusions :,if domain not in exclusions :,98.53678312,FALSE,97.21
4474,"def hsconn_sender ( self ) : while not self . stop_event . is_set ( ) : try : # Block, but timeout, so that we can exit the loop gracefully request = self . send_queue . get ( True , 6.0 ) <MASK> # Socket got closed and set to None in another thread... self . socket . sendall ( request ) if self . send_queue is not None : self . send_queue . task_done ( ) except queue . Empty : pass except OSError : self . stop_event . set ( )",if self . socket is not None :,if request is not None :,72.7871261,FALSE,96.23
4475,"def get_url_args ( self , item ) : if self . url_args : <MASK> url_args = self . url_args ( item ) else : url_args = dict ( self . url_args ) url_args [ "" id "" ] = item . id return url_args else : return dict ( operation = self . label , id = item . id )","if hasattr ( self . url_args , ""__call__"" ) :",if callable ( self . url_args ) :,86.32719108,FALSE,87.02
4476,"def list_projects ( self ) : projects = [ ] page = 1 while True : repos = self . _client . get ( "" /user/repos "" , { "" sort "" : "" full_name "" , "" page "" : page , "" per_page "" : 100 } ) page + = 1 for repo in repos : projects . append ( { "" id "" : repo [ "" full_name "" ] , "" name "" : repo [ "" full_name "" ] , "" description "" : repo [ "" description "" ] , "" is_private "" : repo [ "" private "" ] , } ) <MASK> break return projects",if len ( repos ) < 100 :,"if not repo [ ""is_private"" ] :",92.98475878,FALSE,92.41
4477,"def scripts ( self ) : application_root = current_app . config . get ( "" APPLICATION_ROOT "" ) subdir = application_root != "" / "" scripts = [ ] for script in get_registered_scripts ( ) : if script . startswith ( "" http "" ) : scripts . append ( f ' <script defer src= "" { script } "" ></script> ' ) <MASK> scripts . append ( f ' <script defer src= "" { application_root } / { script } "" ></script> ' ) else : scripts . append ( f ' <script defer src= "" { script } "" ></script> ' ) return markup ( "" \n "" . join ( scripts ) )",elif subdir :,elif subdir :,100,TRUE,100
4478,"def print_map ( node , l ) : if node . title not in l : l [ node . title ] = [ ] for n in node . children : <MASK> w = { n . title : [ ] } l [ node . title ] . append ( w ) print_map ( n , w ) else : l [ node . title ] . append ( n . title )",if len ( n . children ) > 0 :,"if isinstance ( n , Mapping ) :",89.66241095,FALSE,90.11
4479,"def _validate_distinct_on_different_types_and_field_orders ( self , collection , query , expected_results , get_mock_result ) : self . count = 0 self . get_mock_result = get_mock_result query_iterable = collection . query_items ( query , enable_cross_partition_query = True ) results = list ( query_iterable ) for i in range ( len ( expected_results ) ) : if isinstance ( results [ i ] , dict ) : self . assertDictEqual ( results [ i ] , expected_results [ i ] ) <MASK> self . assertListEqual ( results [ i ] , expected_results [ i ] ) else : self . assertEqual ( results [ i ] , expected_results [ i ] ) self . count = 0","elif isinstance ( results [ i ] , list ) :","elif isinstance ( results [ i ] , list ) :",100,TRUE,100
4480,"def run ( self ) : for k , v in iteritems ( self . objs ) : <MASK> continue if ( v [ "" _class "" ] == "" Question "" or v [ "" _class "" ] == "" Message "" or v [ "" _class "" ] == "" Announcement "" ) : v [ "" admin "" ] = None return self . objs","if k . startswith ( ""_"" ) :","if k . startswith ( ""_"" ) :",100,TRUE,100
4481,"def qvec ( self ) : #        if self.polrep != 'stokes': #            raise Exception(""qvec is not defined unless self.polrep=='stokes'"") qvec = np . array ( [ ] ) if self . polrep == "" stokes "" : qvec = self . _imdict [ "" Q "" ] elif self . polrep == "" circ "" : <MASK> qvec = np . real ( 0.5 * ( self . lrvec + self . rlvec ) ) return qvec",if len ( self . rlvec ) != 0 and len ( self . lrvec ) != 0 :,if self . lrvec is not None :,65.83867975,FALSE,82.57
4482,"def display_value ( self , key , w ) : if key == "" vdevices "" : # Very special case nids = [ n [ "" deviceID "" ] for n in self . get_value ( "" devices "" ) ] for device in self . app . devices . values ( ) : <MASK> b = Gtk . CheckButton ( device . get_title ( ) , False ) b . set_tooltip_text ( device [ "" id "" ] ) self [ "" vdevices "" ] . pack_start ( b , False , False , 0 ) b . set_active ( device [ "" id "" ] in nids ) self [ "" vdevices "" ] . show_all ( ) else : EditorDialog . display_value ( self , key , w )","if device [ ""id"" ] != self . app . daemon . get_my_id ( ) :","if device [ ""id"" ] in nids :",96.14896678,FALSE,90.21
4483,"def _set_xflux_setting ( self , * * kwargs ) : for key , value in kwargs . items ( ) : if key in self . _settings_map : <MASK> self . _set_xflux_screen_color ( value ) self . _current_color = str ( value ) # hackish - changing the current color unpauses xflux, # must reflect that with state change if self . state == self . states [ "" PAUSED "" ] : self . state = self . states [ "" RUNNING "" ] else : self . _xflux . sendline ( self . _settings_map [ key ] + str ( value ) ) self . _c ( )","if key == ""color"" :",if self . _settings_map [ key ] == value :,92.90811651,FALSE,91.44
4484,"def apply_acceleration ( self , veh_ids , acc ) : """"""See parent class."""""" # to hand the case of a single vehicle if type ( veh_ids ) == str : veh_ids = [ veh_ids ] acc = [ acc ] for i , vid in enumerate ( veh_ids ) : <MASK> this_vel = self . get_speed ( vid ) next_vel = max ( [ this_vel + acc [ i ] * self . sim_step , 0 ] ) self . kernel_api . vehicle . slowDown ( vid , next_vel , 1e-3 )",if acc [ i ] is not None and vid in self . get_ids ( ) :,if self . kernel_api . vehicle . is_slowDown ( vid ) :,67.57818975,FALSE,88.89
4485,"def largest_factor_relatively_prime ( a , b ) : """"""Return the largest factor of a relatively prime to b."""""" while 1 : d = gcd ( a , b ) <MASK> break b = d while 1 : q , r = divmod ( a , d ) if r > 0 : break a = q return a",if d <= 1 :,if d < 0 :,97.14569573,FALSE,95.28
4486,"def check_status ( self ) : try : du = psutil . disk_usage ( "" / "" ) <MASK> raise ServiceWarning ( "" {host}   {percent} % d isk usage exceeds  {disk_usage} % "" . format ( host = host , percent = du . percent , disk_usage = DISK_USAGE_MAX ) ) except ValueError as e : self . add_error ( ServiceReturnedUnexpectedResult ( "" ValueError "" ) , e )",if DISK_USAGE_MAX and du . percent >= DISK_USAGE_MAX :,if du . disk_usage > DISK_USAGE_MAX :,88.95870746,FALSE,89.13
4487,"def build_reply ( self , msg , text = None , private = False , threaded = False ) : response = self . build_message ( text ) if msg . is_group : <MASK> response . frm = self . bot_identifier response . to = IRCPerson ( str ( msg . frm ) ) else : response . frm = IRCRoomOccupant ( str ( self . bot_identifier ) , msg . frm . room ) response . to = msg . frm . room else : response . frm = self . bot_identifier response . to = msg . frm return response",if private :,if private :,100,TRUE,100
4488,"def _dict_refs ( obj , named ) : """"""Return key and value objects of a dict/proxy."""""" try : <MASK> for k , v in _items ( obj ) : s = str ( k ) yield _NamedRef ( "" [K]  "" + s , k ) yield _NamedRef ( "" [V]  "" + s + "" :  "" + _repr ( v ) , v ) else : for k , v in _items ( obj ) : yield k yield v except ( KeyError , ReferenceError , TypeError ) as x : warnings . warn ( "" Iterating  ' %s ' :  %r "" % ( _classof ( obj ) , x ) )",if named :,if named :,100,TRUE,100
4489,"def fetch_images ( ) : images = [ ] marker = None while True : batch = image_service . detail ( context , filters = filters , marker = marker , sort_key = "" created_at "" , sort_dir = "" desc "" , ) <MASK> break images + = batch marker = batch [ - 1 ] [ "" id "" ] return images",if not batch :,if not batch :,100,TRUE,100
4490,"def compress ( self , data_list ) : warn_untested ( ) if data_list : if data_list [ 1 ] in forms . fields . EMPTY_VALUES : error = self . error_messages [ "" invalid_year "" ] raise forms . ValidationError ( error ) <MASK> error = self . error_messages [ "" invalid_month "" ] raise forms . ValidationError ( error ) year = int ( data_list [ 1 ] ) month = int ( data_list [ 0 ] ) # find last day of the month day = monthrange ( year , month ) [ 1 ] return date ( year , month , day ) return None",if data_list [ 0 ] in forms . fields . EMPTY_VALUES :,if data_list [ 0 ] in forms . fields . EMPTY_VALUES :,100,TRUE,100
4491,"def _diff_dict ( self , old , new ) : diff = { } removed = [ ] added = [ ] for key , value in old . items ( ) : if key not in new : removed . append ( key ) <MASK> # modified is indicated by a remove and add removed . append ( key ) added . append ( key ) for key , value in new . items ( ) : if key not in old : added . append ( key ) if removed : diff [ "" removed "" ] = sorted ( removed ) if added : diff [ "" added "" ] = sorted ( added ) return diff",elif old [ key ] != new [ key ] :,if value is not None :,70.87883088,FALSE,90.51
4492,"def add_filters ( self , function ) : try : subscription = self . exists ( function ) <MASK> response = self . _sns . call ( "" set_subscription_attributes "" , SubscriptionArn = subscription [ "" SubscriptionArn "" ] , AttributeName = "" FilterPolicy "" , AttributeValue = json . dumps ( self . filters ) , ) kappa . event_source . sns . LOG . debug ( response ) except Exception : kappa . event_source . sns . LOG . exception ( "" Unable to add filters for SNS topic  %s "" , self . arn )",if subscription :,if subscription :,100,TRUE,100
4493,"def init_weights ( self , pretrained = None ) : if isinstance ( pretrained , str ) : logger = logging . getLogger ( ) load_checkpoint ( self , pretrained , strict = False , logger = logger ) elif pretrained is None : for m in self . modules ( ) : if isinstance ( m , nn . Conv2d ) : kaiming_init ( m ) <MASK> constant_init ( m , 1 ) else : raise TypeError ( "" pretrained must be a str or None "" )","elif isinstance ( m , ( _BatchNorm , nn . GroupNorm ) ) :","elif isinstance ( m , nn . BatchNorm2d ) :",93.51050607,FALSE,92.39
4494,def test_is_native_login ( self ) : for campaign in self . campaign_lists : native = campaigns . is_native_login ( campaign ) <MASK> assert_true ( native ) else : assert_false ( native ) native = campaigns . is_proxy_login ( self . invalid_campaign ) assert_true ( native is None ),"if campaign == ""prereg"" or campaign == ""erpc"" :","if campaign == ""test"" :",67.82978866,FALSE,88.54
4495,"def _process_filter ( self , query , host_state ) : """"""Recursively parse the query structure."""""" if not query : return True cmd = query [ 0 ] method = self . commands [ cmd ] cooked_args = [ ] for arg in query [ 1 : ] : if isinstance ( arg , list ) : arg = self . _process_filter ( arg , host_state ) <MASK> arg = self . _parse_string ( arg , host_state ) if arg is not None : cooked_args . append ( arg ) result = method ( self , cooked_args ) return result","elif isinstance ( arg , basestring ) :","elif isinstance ( arg , six . string_types ) :",96.82611447,FALSE,95.23
4496,"def find_go_files_mtime ( app_files ) : files , mtime = [ ] , 0 for f , mt in app_files . items ( ) : <MASK> continue if APP_CONFIG . nobuild_files . match ( f ) : continue files . append ( f ) mtime = max ( mtime , mt ) return files , mtime","if not f . endswith ( "".go"" ) :","if f . startswith ( ""_"" ) :",94.59494274,FALSE,89.83
4497,"def ExcludePath ( self , path ) : """"""Check to see if this is a service url and matches inbound_services."""""" skip = False for reserved_path in self . reserved_paths . keys ( ) : <MASK> if ( not self . inbound_services or self . reserved_paths [ reserved_path ] not in self . inbound_services ) : return ( True , self . reserved_paths [ reserved_path ] ) return ( False , None )",if path . startswith ( reserved_path ) :,if path . startswith ( reserved_path ) :,100,TRUE,100
4498,"def param_cov ( self ) - > DataFrame : """"""Parameter covariance"""""" if self . _param_cov is not None : param_cov = self . _param_cov else : params = np . asarray ( self . params ) <MASK> param_cov = self . model . compute_param_cov ( params ) else : param_cov = self . model . compute_param_cov ( params , robust = False ) return DataFrame ( param_cov , columns = self . _names , index = self . _names )","if self . cov_type == ""robust"" :",if self . _use_bias :,81.2868666,FALSE,92.35
4499,"def test_calculate_all_attentions ( module , atype ) : m = importlib . import_module ( module ) args = make_arg ( atype = atype ) <MASK> batch = prepare_inputs ( "" pytorch "" ) else : raise NotImplementedError model = m . E2E ( 6 , 5 , args ) with chainer . no_backprop_mode ( ) : <MASK> att_ws = model . calculate_all_attentions ( * batch ) [ 0 ] else : raise NotImplementedError print ( att_ws . shape )","if ""pytorch"" in module :","if isinstance ( atype , ( list , tuple ) ) :",60.49716493,FALSE,81.33
4500,"def __eq__ ( self , other ) : try : if self . type != other . type : return False <MASK> return self . askAnswer == other . askAnswer elif self . type == "" SELECT "" : return self . vars == other . vars and self . bindings == other . bindings else : return self . graph == other . graph except : return False","if self . type == ""ASK"" :","if self . type == ""Ask"" :",73.31710501,FALSE,97.08
4501,"def validate_memory ( self , value ) : for k , v in value . viewitems ( ) : if v is None : # use NoneType to unset a value continue if not re . match ( PROCTYPE_MATCH , k ) : raise serializers . ValidationError ( "" Process types can only contain [a-z] "" ) <MASK> raise serializers . ValidationError ( "" Limit format: <number><unit>, where unit = B, K, M or G "" ) return value","if not re . match ( MEMLIMIT_MATCH , str ( v ) ) :","if re . match ( LIMIT_MATCH , k ) :",69.56121865,FALSE,90.13
4502,"def get_connections ( data_about ) : data = data_about . find ( "" h3 "" , text = "" Connections "" ) . findNext ( ) connections = { } for row in data . find_all ( "" tr "" ) : key = row . find_all ( "" td "" ) [ 0 ] . text value = row . find_all ( "" td "" ) [ 1 ] <MASK> connections [ key ] = get_all_links ( value ) else : connections [ key ] = value . text return connections","if ""Teams"" in key :","if isinstance ( value , dict ) :",87.81333584,FALSE,93.38
4503,"def _compute_map ( self , first_byte , second_byte = None ) : if first_byte != 0x0F : return "" XED_ILD_MAP0 "" else : if second_byte == None : return "" XED_ILD_MAP1 "" if second_byte == 0x38 : return "" XED_ILD_MAP2 "" if second_byte == 0x3A : return "" XED_ILD_MAP3 "" <MASK> return "" XED_ILD_MAPAMD "" die ( "" Unhandled escape  {}  / map  {}  bytes "" . format ( first_byte , second_byte ) )",if second_byte == 0x0F and self . amd_enabled :,if second_byte == 0x4A :,74.53749054,FALSE,93.42
4504,"def compress ( self , data_list ) : if data_list : page_id = data_list [ 1 ] <MASK> if not self . required : return None raise forms . ValidationError ( self . error_messages [ "" invalid_page "" ] ) return Page . objects . get ( pk = page_id ) return None",if page_id in EMPTY_VALUES :,if page_id == 0 :,93.41720644,FALSE,92.5
4505,"def find_module ( self , fullname , path = None ) : path = path or self . path_entry # print('looking for ""%s"" in %s ...' % (fullname, path)) for _ext in [ "" js "" , "" pyj "" , "" py "" ] : _filepath = os . path . join ( self . path_entry , "" %s . %s "" % ( fullname , _ext ) ) <MASK> print ( "" module found at  %s : %s "" % ( _filepath , fullname ) ) return VFSModuleLoader ( _filepath , fullname ) print ( "" module  %s  not found "" % fullname ) raise ImportError ( ) return None",if _filepath in VFS :,if os . path . exists ( _filepath ) :,97.01739588,FALSE,93.46
4506,"def __decToBin ( self , myDec ) : n = 0 binOfDec = "" "" while myDec > 2 * * n : n = n + 1 if ( myDec < 2 * * n ) & ( myDec != 0 ) : n = n - 1 while n > = 0 : <MASK> myDec = myDec - 2 * * n binOfDec = binOfDec + "" 1 "" else : binOfDec = binOfDec + "" 0 "" n = n - 1 return binOfDec",if myDec >= 2 ** n :,if ( myDec > 2 ** n ) & ( myDec != 0 ) :,85.16702426,FALSE,87.46
4507,"def __str__ ( self ) : try : <MASK> NVMLError . _errcode_to_string [ self . value ] = str ( nvmlErrorString ( self . value ) ) return NVMLError . _errcode_to_string [ self . value ] except NVMLError_Uninitialized : return "" NVML Error with code  %d "" % self . value",if self . value not in NVMLError . _errcode_to_string :,if self . value not in NVMLError . _errcode_to_string :,100,TRUE,100
4508,"def abspath ( pathdir : str ) - > str : if Path is not None and isinstance ( pathdir , Path ) : return pathdir . abspath ( ) else : pathdir = path . abspath ( pathdir ) <MASK> try : pathdir = pathdir . decode ( fs_encoding ) except UnicodeDecodeError as exc : raise UnicodeDecodeError ( "" multibyte filename not supported on  "" "" this filesystem encoding  "" "" ( %r ) "" % fs_encoding ) from exc return pathdir","if isinstance ( pathdir , bytes ) :","if isinstance ( pathdir , bytes ) :",100,TRUE,100
4509,"def _get_vtkjs ( self ) : if self . _vtkjs is None and self . object is not None : <MASK> if isfile ( self . object ) : with open ( self . object , "" rb "" ) as f : vtkjs = f . read ( ) else : data_url = urlopen ( self . object ) vtkjs = data_url . read ( ) elif hasattr ( self . object , "" read "" ) : vtkjs = self . object . read ( ) self . _vtkjs = vtkjs return self . _vtkjs","if isinstance ( self . object , string_types ) and self . object . endswith ( "".vtkjs"" ) :","if hasattr ( self . object , ""read"" ) :",90.8707928,FALSE,85.83
4510,"def _set_uid ( self , val ) : if val is not None : if pwd is None : self . bus . log ( "" pwd module not available; ignoring uid. "" , level = 30 ) val = None <MASK> val = pwd . getpwnam ( val ) [ 2 ] self . _uid = val","elif isinstance ( val , text_or_bytes ) :","elif hasattr ( pwd , ""getpwnam"" ) :",79.06490456,FALSE,86.33
4511,"def get_attached_nodes ( self , external_account ) : for node in self . get_nodes_with_oauth_grants ( external_account ) : <MASK> continue node_settings = node . get_addon ( self . oauth_provider . short_name ) if node_settings is None : continue if node_settings . external_account == external_account : yield node",if node is None :,if not node . get_attached ( ) :,83.38210204,FALSE,89.39
4512,"def from_obj ( cls , py_obj ) : if not isinstance ( py_obj , Image ) : raise TypeError ( "" py_obj must be a wandb.Image "" ) else : if hasattr ( py_obj , "" _boxes "" ) and py_obj . _boxes : box_keys = list ( py_obj . _boxes . keys ( ) ) else : box_keys = [ ] <MASK> mask_keys = list ( py_obj . masks . keys ( ) ) else : mask_keys = [ ] return cls ( box_keys , mask_keys )","if hasattr ( py_obj , ""masks"" ) and py_obj . masks :","if hasattr ( py_obj , ""masks"" ) and py_obj . masks :",100,TRUE,100
4513,"def write ( self , * bits ) : for bit in bits : if not self . bytestream : self . bytestream . append ( 0 ) byte = self . bytestream [ self . bytenum ] if self . bitnum == 8 : <MASK> byte = 0 self . bytestream + = bytes ( [ byte ] ) self . bytenum + = 1 self . bitnum = 0 mask = 2 * * self . bitnum if bit : byte | = mask else : byte & = ~ mask self . bytestream [ self . bytenum ] = byte self . bitnum + = 1",if self . bytenum == len ( self . bytestream ) - 1 :,if byte == 0 :,89.00810374,FALSE,88.57
4514,"def destroy ( self , wipe = False ) : if self . state == self . UP : image = self . image ( ) <MASK> return self . confirm_destroy ( image , self . full_name , abort = False ) else : self . warn ( "" tried to destroy  {0}  which didn ' t exist "" . format ( self . full_name ) ) return True",if image :,if image :,100,TRUE,100
4515,"def get_host_metadata ( self ) : meta = { } if self . agent_url : try : resp = requests . get ( self . agent_url + ECS_AGENT_METADATA_PATH , timeout = 1 ) . json ( ) <MASK> match = AGENT_VERSION_EXP . search ( resp . get ( "" Version "" ) ) if match is not None and len ( match . groups ( ) ) == 1 : meta [ "" ecs_version "" ] = match . group ( 1 ) except Exception as e : self . log . debug ( "" Error getting ECS version:  %s "" % str ( e ) ) return meta","if ""Version"" in resp :","if resp . get ( ""Code"" ) == ""ECS_VERSION"" :",68.4741804,FALSE,89.22
4516,"def _path_type ( st , lst ) : parts = [ ] if st : <MASK> parts . append ( "" file "" ) elif stat . S_ISDIR ( st . st_mode ) : parts . append ( "" dir "" ) else : parts . append ( "" other "" ) if lst : if stat . S_ISLNK ( lst . st_mode ) : parts . append ( "" link "" ) return ""   "" . join ( parts )",if stat . S_ISREG ( st . st_mode ) :,if stat . S_ISREG ( st . st_mode ) :,100,TRUE,100
4517,"def changed ( self , action ) : # Something was changed in the 'files' list if len ( action . key ) > = 1 and action . key [ 0 ] . lower ( ) == "" files "" : # Refresh project files model <MASK> # Don't clear the existing items if only inserting new things self . update_model ( clear = False ) else : # Clear existing items self . update_model ( clear = True )","if action . type == ""insert"" :",if self . _model . get_model ( clear = False ) . get_model (,69.10969466,FALSE,81.42
4518,"def process ( self , resources , event = None ) : client = local_session ( self . manager . session_factory ) . client ( "" es "" ) for r in resources : <MASK> result = self . manager . retry ( client . describe_elasticsearch_domain_config , DomainName = r [ "" DomainName "" ] , ignore_err_codes = ( "" ResourceNotFoundException "" , ) , ) if result : r [ self . policy_attribute ] = json . loads ( result . get ( "" DomainConfig "" ) . get ( "" AccessPolicies "" ) . get ( "" Options "" ) ) return super ( ) . process ( resources )",if self . policy_attribute not in r :,if self . policy_attribute not in r :,100,TRUE,100
4519,"def line_items ( self ) : line_items = [ ] for line in self . lines_str : line = line . split ( "" | "" ) line = line [ 1 : - 1 ] # del first and last empty item (consequence of split) items = [ ] for item in line : i = re . search ( r "" ( \ S+([  \ t]+ \ S+)*)+ "" , item ) <MASK> items . append ( i . group ( ) ) else : items . append ( ""   "" ) line_items . append ( items ) return line_items",if i :,if i :,100,TRUE,100
4520,"def on_data ( res ) : if terminate . is_set ( ) : return if args . strings and not args . no_content : if type ( res ) == tuple : f , v = res if type ( f ) == unicode : f = f . encode ( "" utf-8 "" ) if type ( v ) == unicode : v = v . encode ( "" utf-8 "" ) self . success ( "" {} :  {} "" . format ( f , v ) ) <MASK> self . success ( res ) else : self . success ( res )",elif not args . content_only :,elif args . strings :,92.4481496,FALSE,94.42
4521,"def get_servers ( self , detail = True , search_opts = None ) : rel_url = "" /servers/detail "" if detail else "" /servers "" if search_opts is not None : qparams = { } for opt , val in search_opts . iteritems ( ) : qparams [ opt ] = val <MASK> query_string = "" ? %s "" % urllib . urlencode ( qparams ) rel_url + = query_string return self . api_get ( rel_url ) [ "" servers "" ]",if qparams :,if qparams :,100,TRUE,100
4522,"def run ( self ) : while not self . __exit__ : <MASK> sleep ( 10 ) continue o = self . playlist [ 0 ] self . playlist . remove ( o ) obj = json . loads ( o ) if not "" args "" in obj : obj [ "" args "" ] = { "" ua "" : "" "" , "" header "" : "" "" , "" title "" : "" "" , "" referer "" : "" "" } obj [ "" play "" ] = False self . handle = launch_player ( obj [ "" urls "" ] , obj [ "" ext "" ] , * * obj [ "" args "" ] ) self . handle . wait ( )",if len ( self . playlist ) == 0 :,if self . playlist [ 0 ] . is_playing :,95.03960821,FALSE,93.27
4523,"def get_to_download_runs_ids ( session , headers ) : last_date = 0 result = [ ] while 1 : r = session . get ( RUN_DATA_API . format ( last_date = last_date ) , headers = headers ) <MASK> run_logs = r . json ( ) [ "" data "" ] [ "" records "" ] result . extend ( [ i [ "" logs "" ] [ 0 ] [ "" stats "" ] [ "" id "" ] for i in run_logs ] ) last_date = r . json ( ) [ "" data "" ] [ "" lastTimestamp "" ] since_time = datetime . utcfromtimestamp ( last_date / 1000 ) print ( f "" pares keep ids data since  { since_time } "" ) time . sleep ( 1 ) # spider rule if not last_date : break return result",if r . ok :,if r . status_code == 200 :,83.79530627,FALSE,95.86
4524,"def __saveWork ( self , work , results ) : """"""Stores the resulting last log line to the cache with the proxy key"""""" del work # pylint: disable=broad-except try : <MASK> __cached = self . __cache [ results [ 0 ] ] __cached [ self . __TIME ] = time . time ( ) __cached [ self . __LINE ] = results [ 1 ] __cached [ self . __LLU ] = results [ 2 ] except KeyError as e : # Could happen while switching jobs with work in the queue pass except Exception as e : list ( map ( logger . warning , cuegui . Utils . exceptionOutput ( e ) ) )",if results :,if results :,100,TRUE,100
4525,"def read_notes ( rec ) : found = [ ] for tag in range ( 500 , 595 ) : if tag in ( 505 , 520 ) : continue fields = rec . get_fields ( str ( tag ) ) <MASK> continue for f in fields : x = f . get_lower_subfields ( ) if x : found . append ( ""   "" . join ( x ) . strip ( ""   "" ) ) if found : return "" \n \n "" . join ( found )",if not fields :,if not fields :,100,TRUE,100
4526,"def serialize_to ( self , stream , alternate_script = None ) : stream . write ( self . txo_ref . tx_ref . hash ) stream . write_uint32 ( self . txo_ref . position ) if alternate_script is not None : stream . write_string ( alternate_script ) else : <MASK> stream . write_string ( self . coinbase ) else : stream . write_string ( self . script . source ) stream . write_uint32 ( self . sequence )",if self . is_coinbase :,if self . coinbase is not None :,94.99078668,FALSE,95.27
4527,"def func_named ( self , arg ) : result = None target = "" do_ "" + arg if target in dir ( self ) : result = target else : <MASK> # accept shortened versions of commands funcs = [ fname for fname in self . keywords if fname . startswith ( arg ) ] if len ( funcs ) == 1 : result = "" do_ "" + funcs [ 0 ] return result",if self . abbrev :,"if arg . startswith ( ""cmd_"" ) :",72.24632044,FALSE,88.51
4528,"def static_login ( self , token , * , bot ) : # Necessary to get aiohttp to stop complaining about session creation self . __session = aiohttp . ClientSession ( connector = self . connector , ws_response_class = DiscordClientWebSocketResponse ) old_token , old_bot = self . token , self . bot_token self . _token ( token , bot = bot ) try : data = await self . request ( Route ( "" GET "" , "" /users/@me "" ) ) except HTTPException as exc : self . _token ( old_token , bot = old_bot ) <MASK> raise LoginFailure ( "" Improper token has been passed. "" ) from exc raise return data",if exc . response . status == 401 :,"if ""Invalid token"" in str ( exc ) :",71.64043628,FALSE,93.04
4529,"def render_buttons ( self ) : for x , button in enumerate ( self . button_list ) : gcolor = Gdk . color_parse ( self . color_list [ x ] ) <MASK> fgcolor = Gdk . color_parse ( "" #FFFFFF "" ) else : fgcolor = Gdk . color_parse ( "" #000000 "" ) button . set_label ( self . color_list [ x ] ) button . set_sensitive ( True ) button . modify_bg ( Gtk . StateType . NORMAL , gcolor ) button . modify_fg ( Gtk . StateType . NORMAL , fgcolor )","if util . get_hls_val ( self . color_list [ x ] , ""light"" ) < 99 :",if x == 0 :,72.95659706,FALSE,82.1
4530,"def _set_text ( self , data ) : lines = [ ] for key , value in data . items ( ) : lines . append ( "" "" ) txt = yaml . dump ( { key : value } , default_flow_style = False ) title = self . titles . get ( key ) <MASK> lines . append ( "" #  %s "" % title ) lines . append ( txt . rstrip ( ) ) txt = "" \n "" . join ( lines ) + "" \n "" txt = txt . lstrip ( ) self . edit . setPlainText ( txt )",if title :,if title :,100,TRUE,100
4531,"def build_path ( self ) : for variable in re_path_template . findall ( self . path ) : name = variable . strip ( "" {} "" ) <MASK> # No 'user' parameter provided, fetch it from Auth instead. value = self . api . auth . get_username ( ) else : try : value = quote ( self . session . params [ name ] ) except KeyError : raise TweepError ( "" No parameter value found for path variable:  %s "" % name ) del self . session . params [ name ] self . path = self . path . replace ( variable , value )","if name == ""user"" and ""user"" not in self . session . params and self . api . auth :",if not name :,78.22556547,FALSE,82.46
4532,"def _calculate_writes_for_built_in_indices ( self , entity ) : writes = 0 for prop_name in entity . keys ( ) : <MASK> prop_vals = entity [ prop_name ] if isinstance ( prop_vals , ( list ) ) : num_prop_vals = len ( prop_vals ) else : num_prop_vals = 1 writes + = 2 * num_prop_vals return writes",if not prop_name in entity . unindexed_properties ( ) :,if self . _built_in_indices . get ( prop_name ) :,84.49822278,FALSE,87.32
4533,"def create_connection ( self , address , protocol_factory = None , * * kw ) : """"""Helper method for creating a connection to an ``address``."""""" protocol_factory = protocol_factory or self . create_protocol if isinstance ( address , tuple ) : host , port = address <MASK> self . logger . debug ( "" Create connection  %s : %s "" , host , port ) _ , protocol = await self . _loop . create_connection ( protocol_factory , host , port , * * kw ) await protocol . event ( "" connection_made "" ) else : raise NotImplementedError ( "" Could not connect to  %s "" % str ( address ) ) return protocol",if self . debug :,if self . _loop . get_debug ( ) :,73.82225764,FALSE,94
4534,def _increment_bracket_num ( self ) : self . _current_bracket - = 1 if self . _current_bracket < 0 : self . _current_bracket = self . _get_num_brackets ( ) - 1 self . _current_iteration + = 1 <MASK> self . _current_bracket = 0,if self . _current_iteration > self . hyperband_iterations :,if self . _current_iteration >= self . _get_num_brackets ( ),79.27875028,FALSE,86.14
4535,"def get_cycle_path ( self , curr_node , goal_node_index ) : for dep in curr_node [ "" deps "" ] : <MASK> return [ curr_node [ "" address "" ] ] for dep in curr_node [ "" deps "" ] : path = self . get_cycle_path ( self . get_by_address ( dep ) , goal_node_index ) # self.nodelist[dep], goal_node_index) if len ( path ) > 0 : path . insert ( 0 , curr_node [ "" address "" ] ) return path return [ ]",if dep == goal_node_index :,if dep == self . get_by_address ( dep ) :,93.76482017,FALSE,91.98
4536,"def as_dict ( path = "" "" , version = "" latest "" , section = "" meta-data "" ) : result = { } dirs = dir ( path , version , section ) if not dirs : return None for item in dirs : if item . endswith ( "" / "" ) : records = as_dict ( path + item , version , section ) <MASK> result [ item [ : - 1 ] ] = records elif is_dict . match ( item ) : idx , name = is_dict . match ( item ) . groups ( ) records = as_dict ( path + idx + "" / "" , version , section ) <MASK> result [ name ] = records else : result [ item ] = valueconv ( get ( path + item , version , section ) ) return result",if records :,if records :,100,TRUE,100
4537,"def preprocess_raw_enwik9 ( input_filename , output_filename ) : with open ( input_filename , "" r "" ) as f1 : with open ( output_filename , "" w "" ) as f2 : while True : line = f1 . readline ( ) if not line : break line = list ( enwik9_norm_transform ( [ line ] ) ) [ 0 ] if line != ""   "" and line != "" "" : <MASK> line = line [ 1 : ] f2 . writelines ( line + "" \n "" )","if line [ 0 ] == "" "" :","if line [ 0 ] == ""#"" :",98.22182715,FALSE,97.86
4538,"def _handle_unsubscribe ( self , web_sock ) : index = None with await self . _subscriber_lock : for i , ( subscriber_web_sock , _ ) in enumerate ( self . _subscribers ) : <MASK> index = i break if index is not None : del self . _subscribers [ index ] if not self . _subscribers : asyncio . ensure_future ( self . _unregister_subscriptions ( ) )",if subscriber_web_sock == web_sock :,if web_sock == subscriber_web_sock :,97.24815923,FALSE,96.84
4539,"def formatmonthname ( self , theyear , themonth , withyear = True ) : with TimeEncoding ( self . locale ) as encoding : s = month_name [ themonth ] if encoding is not None : s = s . decode ( encoding ) <MASK> s = "" %s   %s "" % ( s , theyear ) return ' <tr><th colspan= "" 7 ""  class= "" month "" > %s </th></tr> ' % s",if withyear :,if withyear :,100,TRUE,100
4540,"def generate_sitemaps ( filename ) : rows = ( line . strip ( ) . split ( "" \t "" ) for line in open ( filename ) ) for sortkey , chunk in itertools . groupby ( rows , lambda row : row [ 0 ] ) : things = [ ] _chunk = list ( chunk ) for segment in _chunk : sortkey = segment . pop ( 0 ) last_modified = segment . pop ( - 1 ) path = "" "" . join ( segment ) things . append ( web . storage ( path = path , last_modified = last_modified ) ) <MASK> write ( "" sitemaps/sitemap_ %s .xml.gz "" % sortkey , sitemap ( things ) )",if things :,if things :,100,TRUE,100
4541,"def use_index ( self , term : Union [ str , Index ] , * terms : Union [ str , Index ] ) - > "" QueryBuilder "" : for t in ( term , * terms ) : <MASK> self . _use_indexes . append ( t ) elif isinstance ( t , str ) : self . _use_indexes . append ( Index ( t ) )","if isinstance ( t , Index ) :","if isinstance ( t , Index ) :",100,TRUE,100
4542,"def get_changed ( self ) : if self . _is_expression ( ) : result = self . _get_node_text ( self . ast ) <MASK> return None return result else : collector = codeanalyze . ChangeCollector ( self . source ) last_end = - 1 for match in self . matches : start , end = match . get_region ( ) if start < last_end : if not self . _is_expression ( ) : continue last_end = end replacement = self . _get_matched_text ( match ) collector . add_change ( start , end , replacement ) return collector . get_changed ( )",if result == self . source :,if result is None :,83.70406737,FALSE,95.23
4543,"def quiet_f ( * args ) : vars = { arg_name : Real ( arg ) for arg_name , arg in zip ( arg_names , args ) } value = dynamic_scoping ( quiet_expr . evaluate , vars , evaluation ) if expect_list : if value . has_form ( "" List "" , None ) : value = [ extract_pyreal ( item ) for item in value . leaves ] <MASK> return None return value else : return None else : value = extract_pyreal ( value ) if value is None or isinf ( value ) or isnan ( value ) : return None return value",if any ( item is None for item in value ) :,if value is None or isinf ( value ) or isinf ( value ) :,89.86749737,FALSE,91.61
4544,"def _reemit_nested_event ( self , event : Event ) : source_index = self . index ( event . source ) for attr in ( "" index "" , "" new_index "" ) : <MASK> src_index = ensure_tuple_index ( event . index ) setattr ( event , attr , ( source_index , ) + src_index ) if not hasattr ( event , "" index "" ) : setattr ( event , "" index "" , source_index ) # reemit with this object's EventEmitter of the same type if present # otherwise just emit with the EmitterGroup itself getattr ( self . events , event . type , self . events ) ( event )","if hasattr ( event , attr ) :","if hasattr ( event , attr ) :",100,TRUE,100
4545,"def check ( self ) : """"""Perform required checks to conclude if it's safe to operate"""""" if self . interpreter . manual is None : <MASK> self . error = self . process . error self . tip = self . process . tip return False start = time . time ( ) while not self . _status ( ) : if time . time ( ) - start > = 2 : # 2s self . error = "" can ' t connect to the minserver on  {} : {} "" . format ( self . interpreter . host , self . interpreter . port ) self . tip = "" check your vagrant machine is running "" return False time . sleep ( 0.1 ) return True",if not self . process . healthy :,if self . process . error :,95.00751806,FALSE,96.5
4546,"def apply ( self ) : new_block = self . block . copy ( ) new_block . clear ( ) for inst in self . block . body : <MASK> const_assign = self . _assign_const ( inst ) new_block . append ( const_assign ) inst = self . _assign_getitem ( inst , index = const_assign . target ) new_block . append ( inst ) return new_block","if isinstance ( inst , Assign ) and inst . value in self . getattrs :","if isinstance ( inst , types . Constant ) :",83.10981186,FALSE,89.04
4547,"def _get_orientation ( self ) : if self . state : rotation = [ 0 ] * 9 inclination = [ 0 ] * 9 gravity = [ ] geomagnetic = [ ] gravity = self . listener_a . values geomagnetic = self . listener_m . values <MASK> ff_state = SensorManager . getRotationMatrix ( rotation , inclination , gravity , geomagnetic ) if ff_state : values = [ 0 , 0 , 0 ] values = SensorManager . getOrientation ( rotation , values ) return values",if gravity [ 0 ] is not None and geomagnetic [ 0 ] is not None :,if self . listener_a . is_enabled ( ) :,74.34385746,FALSE,85.39
4548,def getFirstSubGraph ( graph ) : if len ( graph ) == 0 : return None subg = { } todo = [ graph . keys ( ) [ 0 ] ] while len ( todo ) > 0 : <MASK> subg [ todo [ 0 ] ] = graph [ todo [ 0 ] ] todo . extend ( graph [ todo [ 0 ] ] ) del graph [ todo [ 0 ] ] del todo [ 0 ] return subg,if todo [ 0 ] in graph . keys ( ) :,if graph [ todo [ 0 ] ] is None :,91.29777711,FALSE,90.57
4549,"def decorated_function ( * args , * * kwargs ) : rv = f ( * args , * * kwargs ) if "" Last-Modified "" not in rv . headers : try : result = date if callable ( result ) : result = result ( rv ) <MASK> from werkzeug . http import http_date result = http_date ( result ) if result : rv . headers [ "" Last-Modified "" ] = result except Exception : logging . getLogger ( __name__ ) . exception ( "" Error while calculating the lastmodified value for response  {!r} "" . format ( rv ) ) return rv","if not isinstance ( result , basestring ) :","if isinstance ( result , datetime ) :",95.98851383,FALSE,95.99
4550,"def set_invoice_details ( self , row ) : invoice_details = self . invoice_details . get ( row . voucher_no , { } ) if row . due_date : invoice_details . pop ( "" due_date "" , None ) row . update ( invoice_details ) if row . voucher_type == "" Sales Invoice "" : <MASK> self . set_delivery_notes ( row ) if self . filters . show_sales_person and row . sales_team : row . sales_person = "" ,  "" . join ( row . sales_team ) del row [ "" sales_team "" ]",if self . filters . show_delivery_notes :,if self . filters . delivery_notes :,98.72281368,FALSE,97.47
4551,"def process ( output ) : modules = { } for line in output : name , size , instances , depends , state , _ = line . split ( ""   "" , 5 ) instances = int ( instances ) module = { "" size "" : size , "" instances "" : instances , "" state "" : state , } <MASK> module [ "" depends "" ] = [ value for value in depends . split ( "" , "" ) if value ] modules [ name ] = module return modules","if depends != ""-"" :",if depends :,82.86667259,FALSE,93.53
4552,"def _get_host_from_zc_service_info ( service_info : zeroconf . ServiceInfo ) : """"""Get hostname or IP + port from zeroconf service_info."""""" host = None port = None if ( service_info and service_info . port and ( service_info . server or len ( service_info . addresses ) > 0 ) ) : <MASK> host = socket . inet_ntoa ( service_info . addresses [ 0 ] ) else : host = service_info . server . lower ( ) port = service_info . port return ( host , port )",if len ( service_info . addresses ) > 0 :,"if service_info . server . lower ( ) == ""inet"" :",91.49965693,FALSE,90.8
4553,"def _init_weights ( self , module ) : if isinstance ( module , nn . Linear ) : module . weight . data . normal_ ( mean = 0.0 , std = self . config . init_std ) if module . bias is not None : module . bias . data . zero_ ( ) elif isinstance ( module , nn . Embedding ) : module . weight . data . normal_ ( mean = 0.0 , std = self . config . init_std ) <MASK> module . weight . data [ module . padding_idx ] . zero_ ( )",if module . padding_idx is not None :,if module . padding_idx is not None :,100,TRUE,100
4554,"def visitFromImport ( self , import_stmt , import_info ) : new_pairs = [ ] if not import_info . is_star_import ( ) : for name , alias in import_info . names_and_aliases : try : pyname = self . pymodule [ alias or name ] <MASK> continue except exceptions . AttributeNotFoundError : pass new_pairs . append ( ( name , alias ) ) return importinfo . FromImport ( import_info . module_name , import_info . level , new_pairs )","if occurrences . same_pyname ( self . pyname , pyname ) :","if pyname . startswith ( ""py"" ) :",88.10994525,FALSE,89.77
4555,"def _apply_patches ( self ) : try : s = Subprocess ( log = self . logfile , cwd = self . build_dir , verbose = self . options . verbose ) for patch in self . patches : <MASK> for ed , source in patch . items ( ) : s . shell ( "" ed -  %s  <  %s "" % ( source , ed ) ) else : s . shell ( "" patch -p0 <  %s "" % patch ) except : logger . error ( "" Failed to patch ` %s `. \n %s "" % ( self . build_dir , sys . exc_info ( ) [ 1 ] ) ) sys . exit ( 1 )",if type ( patch ) is dict :,"if isinstance ( patch , dict ) :",94.3243092,FALSE,95.61
4556,"def __init__ ( self , parent , dir , mask , with_dirs = True ) : filelist = [ ] dirlist = [ "" .. "" ] self . dir = dir self . file = "" "" mask = mask . upper ( ) pattern = self . MakeRegex ( mask ) for i in os . listdir ( dir ) : if i == "" . "" or i == "" .. "" : continue path = os . path . join ( dir , i ) <MASK> dirlist . append ( i ) continue path = path . upper ( ) value = i . upper ( ) if pattern . match ( value ) is not None : filelist . append ( i ) self . files = filelist if with_dirs : self . dirs = dirlist",if os . path . isdir ( path ) :,if not os . path . isdir ( path ) :,98.69218407,FALSE,98.44
4557,"def remove_invalid_dirs ( paths , bp_dir , module_name ) : ret = [ ] for path in paths : <MASK> ret . append ( path ) else : logging . warning ( ' Dir  "" %s ""  of module  "" %s ""  does not exist ' , path , module_name ) return ret","if os . path . isdir ( os . path . join ( bp_dir , path ) ) :",if path . startswith ( bp_dir ) :,80.76443354,FALSE,79.8
4558,"def update_sockets ( self ) : inputs = self . inputs inputs_n = "" ABabcd "" penta_sockets = pentagon_dict [ self . grid_type ] . input_sockets for socket in inputs_n : if socket in penta_sockets : <MASK> inputs [ socket ] . hide_safe = False else : inputs [ socket ] . hide_safe = True",if inputs [ socket ] . hide_safe :,if not inputs [ socket ] . hide_safe :,90.49635663,FALSE,96.98
4559,"def __cut ( sentence ) : global emit_P prob , pos_list = viterbi ( sentence , "" BMES "" , start_P , trans_P , emit_P ) begin , nexti = 0 , 0 # print pos_list, sentence for i , char in enumerate ( sentence ) : pos = pos_list [ i ] if pos == "" B "" : begin = i elif pos == "" E "" : yield sentence [ begin : i + 1 ] nexti = i + 1 <MASK> yield char nexti = i + 1 if nexti < len ( sentence ) : yield sentence [ nexti : ]","elif pos == ""S"" :","elif pos == ""D"" :",98.85563479,FALSE,98.07
4560,"def validate ( self ) : if self . data . get ( "" encrypted "" , True ) : key = self . data . get ( "" target_key "" ) <MASK> raise PolicyValidationError ( "" Encrypted snapshot copy requires kms key on  %s "" % ( self . manager . data , ) ) return self",if not key :,"if not key . startswith ( ""kms"" ) :",92.82319761,FALSE,87.72
4561,"def __init__ ( self , patch_files , patch_directories ) : files = [ ] files_data = { } for filename_data in patch_files : if isinstance ( filename_data , list ) : filename , data = filename_data else : filename = filename_data data = None <MASK> filename = "" {0} {1} "" . format ( FakeState . deploy_dir , filename ) files . append ( filename ) if data : files_data [ filename ] = data self . files = files self . files_data = files_data self . directories = patch_directories",if not filename . startswith ( os . sep ) :,if FakeState . deploy_dir :,77.64341382,FALSE,92.24
4562,"def validate_name_and_description ( body , check_length = True ) : for attribute in [ "" name "" , "" description "" , "" display_name "" , "" display_description "" ] : value = body . get ( attribute ) if value is not None : if isinstance ( value , six . string_types ) : body [ attribute ] = value . strip ( ) <MASK> try : utils . check_string_length ( body [ attribute ] , attribute , min_length = 0 , max_length = 255 ) except exception . InvalidInput as error : raise webob . exc . HTTPBadRequest ( explanation = error . msg )",if check_length :,if check_length :,100,TRUE,100
4563,"def pick ( items , sel ) : for x , s in zip ( items , sel ) : if match ( s ) : yield x <MASK> yield x . restructure ( x . head , pick ( x . leaves , s . leaves ) , evaluation )",elif not x . is_atom ( ) and not s . is_atom ( ) :,"elif isinstance ( s , Tree ) :",54.03510085,FALSE,73.83
4564,"def wait_or_kill ( self ) : """"""Wait for the program to terminate, or kill it after 5s."""""" if self . instance . poll ( ) is None : # We try one more time to kill gracefully using Ctrl-C. logger . info ( "" Interrupting  %s  and waiting... "" , self . coord ) self . instance . send_signal ( signal . SIGINT ) # FIXME on py3 this becomes self.instance.wait(timeout=5) t = monotonic_time ( ) while monotonic_time ( ) - t < 5 : <MASK> logger . info ( "" Terminated  %s . "" , self . coord ) break time . sleep ( 0.1 ) else : self . kill ( )",if self . instance . poll ( ) is not None :,if self . instance . poll ( ) is None :,98.86482809,FALSE,98.4
4565,"def sort_collection ( self , models , many ) : ordering = self . ordering if not many or not ordering : return models for key in reversed ( ordering ) : reverse = key [ 0 ] == "" - "" <MASK> key = key [ 1 : ] models = sorted ( models , key = partial ( deep_getattr , key = key ) , reverse = reverse ) return models",if reverse :,if reverse :,100,TRUE,100
4566,"def get_palette_for_custom_classes ( self , class_names , palette = None ) : if self . label_map is not None : # return subset of palette palette = [ ] for old_id , new_id in sorted ( self . label_map . items ( ) , key = lambda x : x [ 1 ] ) : <MASK> palette . append ( self . PALETTE [ old_id ] ) palette = type ( self . PALETTE ) ( palette ) elif palette is None : if self . PALETTE is None : palette = np . random . randint ( 0 , 255 , size = ( len ( class_names ) , 3 ) ) else : palette = self . PALETTE return palette",if new_id != - 1 :,if old_id in class_names :,97.87631403,FALSE,94.83
4567,"def _find_tcl_dir ( ) : lib_dirs = [ os . path . dirname ( _x ) for _x in sys . path if _x . lower ( ) . endswith ( "" lib "" ) ] for lib_dir in lib_dirs : base_dir = os . path . join ( lib_dir , TclLibrary . FOLDER ) <MASK> for root , _ , files in os . walk ( base_dir ) : if TclLibrary . INIT_TCL in files : return root",if os . path . exists ( base_dir ) :,if os . path . isdir ( base_dir ) :,98.56633619,FALSE,97.72
4568,"def __next__ ( self ) : """"""Special paging functionality"""""" if self . iter is None : self . iter = iter ( self . objs ) try : return next ( self . iter ) except StopIteration : self . iter = None self . objs = [ ] <MASK> self . page + = 1 self . _connection . get_response ( self . action , self . params , self . page , self ) return next ( self ) else : raise",if int ( self . page ) < int ( self . total_pages ) :,if self . page < self . _limit :,85.20209899,FALSE,87.46
4569,"def parse ( cls , api , json ) : lst = List ( api ) setattr ( lst , "" _json "" , json ) for k , v in json . items ( ) : if k == "" user "" : setattr ( lst , k , User . parse ( api , v ) ) <MASK> setattr ( lst , k , parse_datetime ( v ) ) else : setattr ( lst , k , v ) return lst","elif k == ""created_at"" :","elif k == ""date"" :",98.54169345,FALSE,95.23
4570,"def real_type ( self ) : # Find the real type representation by updating it as required real_type = self . type if self . flag_indicator : real_type = "" # "" if self . is_vector : <MASK> real_type = "" Vector< {} > "" . format ( real_type ) else : real_type = "" vector< {} > "" . format ( real_type ) if self . is_generic : real_type = "" ! {} "" . format ( real_type ) if self . is_flag : real_type = "" flags. {} ? {} "" . format ( self . flag_index , real_type ) return real_type",if self . use_vector_id :,if self . flag_index == 0 :,73.38195679,FALSE,95.26
4571,"def check_fs ( path ) : with open ( path , "" rb "" ) as f : code = python_bytes_to_unicode ( f . read ( ) , errors = "" replace "" ) <MASK> module = _load_module ( evaluator , path , code ) module_name = sys_path . dotted_path_in_sys_path ( evaluator . project . sys_path , path ) if module_name is not None : add_module ( evaluator , module_name , module ) return module",if name in code :,if code is not None :,83.56150126,FALSE,95.18
4572,"def infoCalendar ( users ) : calendarId = normalizeCalendarId ( sys . argv [ 5 ] , checkPrimary = True ) i = 0 count = len ( users ) for user in users : i + = 1 user , cal = buildCalendarGAPIObject ( user ) if not cal : continue result = gapi . call ( cal . calendarList ( ) , "" get "" , soft_errors = True , calendarId = calendarId ) <MASK> print ( f "" User:  { user } , Calendar: { display . current_count ( i , count ) } "" ) _showCalendar ( result , 1 , 1 )",if result :,if result :,100,TRUE,100
4573,"def set_hidestate_input_sockets_to_cope_with_switchnum ( self ) : tndict = get_indices_that_should_be_visible ( self . node_state ) for key , value in tndict . items ( ) : socket = self . inputs [ key ] desired_hide_state = not ( value ) <MASK> socket . hide_safe = desired_hide_state",if not socket . hide == desired_hide_state :,if socket . is_switchnum :,83.76460476,FALSE,87.26
4574,"def get_class_name ( item ) : class_name , module_name = None , None for parent in reversed ( item . listchain ( ) ) : if isinstance ( parent , pytest . Class ) : class_name = parent . name <MASK> module_name = parent . module . __name__ break # heuristic: # - better to group gpu and task tests, since tests from those modules #   are likely to share caching more # - split up the rest by class name because slow tests tend to be in #   the same module if class_name and "" .tasks. "" not in module_name : return "" {} . {} "" . format ( module_name , class_name ) else : return module_name","elif isinstance ( parent , pytest . Module ) :","elif isinstance ( parent , pytest . Module ) :",100,TRUE,100
4575,"def run ( self ) : versions = versioneer . get_versions ( ) tempdir = tempfile . mkdtemp ( ) generated = os . path . join ( tempdir , "" rundemo "" ) with open ( generated , "" wb "" ) as f : for line in open ( "" src/rundemo-template "" , "" rb "" ) : <MASK> f . write ( ( "" versions =  %r \n "" % ( versions , ) ) . encode ( "" ascii "" ) ) else : f . write ( line ) self . scripts = [ generated ] rc = build_scripts . run ( self ) os . unlink ( generated ) os . rmdir ( tempdir ) return rc","if line . strip ( ) . decode ( ""ascii"" ) == ""#versions"" :","if line . startswith ( ""version="" ) :",94.09375953,FALSE,89.4
4576,"def get_user_context ( request , escape = False ) : if isinstance ( request , HttpRequest ) : user = getattr ( request , "" user "" , None ) result = { "" ip_address "" : request . META [ "" REMOTE_ADDR "" ] } <MASK> result . update ( { "" email "" : user . email , "" id "" : user . id , } ) if user . name : result [ "" name "" ] = user . name else : result = { } return mark_safe ( json . dumps ( result ) )",if user and user . is_authenticated ( ) :,if user :,92.47121302,FALSE,91.9
4577,"def tokens_to_spans ( ) - > Iterable [ Tuple [ str , Optional [ Style ] ] ] : """"""Convert tokens to spans."""""" tokens = iter ( line_tokenize ( ) ) line_no = 0 _line_start = line_start - 1 # Skip over tokens until line start while line_no < _line_start : _token_type , token = next ( tokens ) yield ( token , None ) <MASK> line_no + = 1 # Generate spans until line end for token_type , token in tokens : yield ( token , _get_theme_style ( token_type ) ) <MASK> line_no + = 1 if line_no > = line_end : break","if token . endswith ( ""\n"" ) :","if _token_type == ""span"" :",93.69085974,FALSE,88.3
4578,"def encode ( self , encodeFun , value , defMode , maxChunkSize ) : substrate , isConstructed = self . encodeValue ( encodeFun , value , defMode , maxChunkSize ) tagSet = value . getTagSet ( ) if tagSet : <MASK> # primitive form implies definite mode defMode = 1 return ( self . encodeTag ( tagSet [ - 1 ] , isConstructed ) + self . encodeLength ( len ( substrate ) , defMode ) + substrate + self . _encodeEndOfOctets ( encodeFun , defMode ) ) else : return substrate # untagged value",if not isConstructed :,if tagSet [ - 2 ] == 1 :,91.40207956,FALSE,90.8
4579,def _run ( self ) : while True : request = self . _requests . get ( ) <MASK> self . shutdown ( ) break self . process ( request ) self . _requests . task_done ( ),if request is None :,if request is None :,100,TRUE,100
4580,"def _decode_payload ( self , payload ) : # we need to decrypt it if payload [ "" enc "" ] == "" aes "" : try : payload [ "" load "" ] = self . crypticle . loads ( payload [ "" load "" ] ) except salt . crypt . AuthenticationError : <MASK> raise payload [ "" load "" ] = self . crypticle . loads ( payload [ "" load "" ] ) return payload",if not self . _update_aes ( ) :,"if ""enc"" not in payload :",70.78695822,FALSE,88.76
4581,"def test_row ( self , row ) : for idx , test in self . patterns . items ( ) : try : value = row [ idx ] except IndexError : value = "" "" result = test ( value ) if self . any_match : <MASK> return not self . inverse # True else : if not result : return self . inverse # False if self . any_match : return self . inverse # False else : return not self . inverse # True",if result :,if result :,100,TRUE,100
4582,"def setup_parameter_node ( self , param_node ) : if param_node . bl_idname == "" SvNumberNode "" : if self . use_prop or self . get_prop_name ( ) : value = self . sv_get ( ) [ 0 ] [ 0 ] print ( "" V "" , value ) if isinstance ( value , int ) : param_node . selected_mode = "" int "" param_node . int_ = value <MASK> param_node . selected_mode = "" float "" param_node . float_ = value","elif isinstance ( value , float ) :","elif isinstance ( value , float ) :",100,TRUE,100
4583,"def iter_modules ( self , by_clients = False , clients_filter = None ) : """"""iterate over all modules"""""" clients = None if by_clients : clients = self . get_clients ( clients_filter ) if not clients : return self . _refresh_modules ( ) for module_name in self . modules : try : module = self . get_module ( module_name ) except PupyModuleDisabled : continue <MASK> for client in clients : if module . is_compatible_with ( client ) : yield module break else : yield module",if clients is not None :,if clients :,94.26223336,FALSE,96.19
4584,"def filter_pricing_rule_based_on_condition ( pricing_rules , doc = None ) : filtered_pricing_rules = [ ] if doc : for pricing_rule in pricing_rules : if pricing_rule . condition : try : <MASK> filtered_pricing_rules . append ( pricing_rule ) except : pass else : filtered_pricing_rules . append ( pricing_rule ) else : filtered_pricing_rules = pricing_rules return filtered_pricing_rules","if frappe . safe_eval ( pricing_rule . condition , None , doc . as_dict ( ) ) :",if doc . get ( pricing_rule . name ) :,79.00967973,FALSE,84.3
4585,"def build_query_string ( kv_data , ignore_none = True ) : # {""a"": 1, ""b"": ""test""} -> ""?a=1&b=test"" query_string = "" "" for k , v in kv_data . iteritems ( ) : <MASK> continue if query_string != "" "" : query_string + = "" & "" else : query_string = "" ? "" query_string + = k + "" = "" + str ( v ) return query_string",if ignore_none is True and kv_data [ k ] is None :,"if ignore_none and k . startswith ( ""a"" ) :",93.01015586,FALSE,90.42
4586,"def sample ( self , * * config ) : """"""Sample a configuration from this search space."""""" ret = { } ret . update ( self . data ) kwspaces = self . kwspaces kwspaces . update ( config ) striped_keys = [ k . split ( SPLITTER ) [ 0 ] for k in config . keys ( ) ] for k , v in kwspaces . items ( ) : if k in striped_keys : <MASK> sub_config = _strip_config_space ( config , prefix = k ) ret [ k ] = v . sample ( * * sub_config ) else : ret [ k ] = v return ret","if isinstance ( v , NestedSpace ) :","if hasattr ( v , ""sample"" ) :",95.93949158,FALSE,94.92
4587,"def task_failed ( self , task_id , hostname , reason ) : logger . debug ( "" task  %d  failed with message  %s "" , task_id , str ( reason ) ) if hostname in self . host_dict : host_status = self . host_dict [ hostname ] host_status . task_failed ( task_id ) <MASK> self . task_host_failed_dict [ task_id ] = set ( ) self . task_host_failed_dict [ task_id ] . add ( hostname )",if task_id not in self . task_host_failed_dict :,if task_id not in self . task_host_failed_dict :,100,TRUE,100
4588,"def match ( path ) : for pat , _type , _property , default_title in patterns : m = web . re_compile ( "" ^ "" + pat ) . match ( path ) <MASK> prefix = m . group ( ) extra = web . lstrips ( path , prefix ) tokens = extra . split ( "" / "" , 2 ) # `extra` starts with ""/"". So first token is always empty. middle = web . listget ( tokens , 1 , "" "" ) suffix = web . listget ( tokens , 2 , "" "" ) if suffix : suffix = "" / "" + suffix return _type , _property , default_title , prefix , middle , suffix return None , None , None , None , None , None",if m :,if m :,100,TRUE,100
4589,"def _get_cached_resources ( self , ids ) : key = self . get_cache_key ( None ) if self . _cache . load ( ) : resources = self . _cache . get ( key ) <MASK> self . log . debug ( "" Using cached results for get_resources "" ) m = self . get_model ( ) id_set = set ( ids ) return [ r for r in resources if r [ m . id ] in id_set ] return None",if resources is not None :,if resources :,94.7862834,FALSE,95.68
4590,"def has_api_behaviour ( self , protocol ) : config = get_config ( ) try : r = self . session . get ( f "" { protocol } :// { self . event . host } : { self . event . port } "" , timeout = config . network_timeout , ) <MASK> return True except requests . exceptions . SSLError : logger . debug ( f "" { [ protocol ] }  protocol not accepted on  { self . event . host } : { self . event . port } "" ) except Exception : logger . debug ( f "" Failed probing  { self . event . host } : { self . event . port } "" , exc_info = True )","if ( ""k8s"" in r . text ) or ( '""code""' in r . text and r . status_code != 200 ) :",if r . status_code == 200 :,81.40680985,FALSE,83.07
4591,"def get_file_type ( self , context , parent_context = None ) : file_type = context . get ( self . file_type_name , None ) if file_type == "" "" : <MASK> file_type = parent_context . get ( self . file_type_name , self . default_file_type ) else : file_type = self . default_file_type return file_type",if parent_context :,if parent_context :,100,TRUE,100
4592,"def selectionToChunks ( self , remove = False , add = False ) : box = self . selectionBox ( ) if box : if box == self . level . bounds : self . selectedChunks = set ( self . level . allChunks ) return selectedChunks = self . selectedChunks boxedChunks = set ( box . chunkPositions ) <MASK> remove = True if remove and not add : selectedChunks . difference_update ( boxedChunks ) else : selectedChunks . update ( boxedChunks ) self . selectionTool . selectNone ( )",if boxedChunks . issubset ( selectedChunks ) :,if remove and boxedChunks == set ( self . level . allChunks ) :,91.48372174,FALSE,87.86
4593,"def _run_split_on_punc ( self , text , never_split = None ) : """"""Splits punctuation on a piece of text."""""" if never_split is not None and text in never_split : return [ text ] chars = list ( text ) i = 0 start_new_word = True output = [ ] while i < len ( chars ) : char = chars [ i ] <MASK> output . append ( [ char ] ) start_new_word = True else : if start_new_word : output . append ( [ ] ) start_new_word = False output [ - 1 ] . append ( char ) i + = 1 return [ "" "" . join ( x ) for x in output ]",if _is_punctuation ( char ) :,if _is_punctuation ( char ) :,100,TRUE,100
4594,"def _save_images ( notebook ) : if os . getenv ( "" NB_NO_IMAGES "" ) == "" 1 "" : return logged = False for filename , img_bytes in _iter_notebook_images ( notebook ) : <MASK> log . info ( "" Saving images "" ) logged = True with open ( filename , "" wb "" ) as f : f . write ( img_bytes )",if not logged :,if not logged :,100,TRUE,100
4595,"def pickPath ( self , color ) : self . path [ color ] = ( ) currentPos = self . starts [ color ] while True : minDist = None minGuide = None for guide in self . guides [ color ] : guideDist = dist ( currentPos , guide ) <MASK> minDist = guideDist minGuide = guide if dist ( currentPos , self . ends [ color ] ) == 1 : return if minGuide == None : return self . path [ color ] = self . path [ color ] + ( minGuide , ) currentPos = minGuide self . guides [ color ] . remove ( minGuide )",if minDist == None or guideDist < minDist :,if guideDist < minDist :,95.67514935,FALSE,94.65
4596,"def _terminal_messenger ( tp = "" write "" , msg = "" "" , out = sys . stdout ) : try : <MASK> out . write ( msg ) elif tp == "" flush "" : out . flush ( ) elif tp == "" write_flush "" : out . write ( msg ) out . flush ( ) elif tp == "" print "" : print ( msg , file = out ) else : raise ValueError ( "" Unsupported type:  "" + tp ) except IOError as e : logger . critical ( "" {} :  {} "" . format ( type ( e ) . __name__ , ucd ( e ) ) ) pass","if tp == ""write"" :","if tp == ""write"" :",100,TRUE,100
4597,"def __new__ ( mcs , name , bases , attrs ) : include_profile = include_trace = include_garbage = True bases = list ( bases ) if name == "" SaltLoggingClass "" : for base in bases : <MASK> include_trace = False if hasattr ( base , "" garbage "" ) : include_garbage = False if include_profile : bases . append ( LoggingProfileMixin ) if include_trace : bases . append ( LoggingTraceMixin ) if include_garbage : bases . append ( LoggingGarbageMixin ) return super ( LoggingMixinMeta , mcs ) . __new__ ( mcs , name , tuple ( bases ) , attrs )","if hasattr ( base , ""trace"" ) :","if hasattr ( base , ""trace"" ) :",100,TRUE,100
4598,"def generatePidEncryptionTable ( ) : table = [ ] for counter1 in range ( 0 , 0x100 ) : value = counter1 for counter2 in range ( 0 , 8 ) : <MASK> value = value >> 1 else : value = value >> 1 value = value ^ 0xEDB88320 table . append ( value ) return table",if value & 1 == 0 :,if counter2 == 0 :,90.8189926,FALSE,93.1
4599,"def pytest_collection_modifyitems ( items ) : for item in items : if item . nodeid . startswith ( "" tests/params "" ) : <MASK> item . add_marker ( pytest . mark . stage ( "" unit "" ) ) if "" init "" not in item . keywords : item . add_marker ( pytest . mark . init ( rng_seed = 123 ) )","if ""stage"" not in item . keywords :","if ""stage"" not in item . keywords :",100,TRUE,100
4600,"def python_value ( self , value ) : if value : if isinstance ( value , basestring ) : pp = lambda x : x . time ( ) return format_date_time ( value , self . formats , pp ) <MASK> return value . time ( ) if value is not None and isinstance ( value , datetime . timedelta ) : return ( datetime . datetime . min + value ) . time ( ) return value","elif isinstance ( value , datetime . datetime ) :","elif isinstance ( value , datetime . datetime ) :",100,TRUE,100
4601,"def list_interesting_hosts ( self ) : hosts = [ ] targets = self . target [ "" other "" ] for target in targets : <MASK> hosts . append ( { "" ip "" : target . ip , "" description "" : target . domain + ""  /  "" + target . name } ) return hosts",if self . is_interesting ( target ) and target . status and target . status != 400 :,"if target . domain and target . name not in [ ""localhost"" , ""127.0",78.97229901,FALSE,75.9
4602,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 10 : length = d . getVarInt32 ( ) tmp = ProtocolBuffer . Decoder ( d . buffer ( ) , d . pos ( ) , d . pos ( ) + length ) d . skip ( length ) self . mutable_cost ( ) . TryMerge ( tmp ) continue <MASK> self . add_version ( d . getVarInt64 ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 24 :,if tt == 16 :,98.85225991,FALSE,97.82
4603,"def _wait_for_finish ( self ) - > PollExitResponse : while True : if self . _backend : poll_exit_resp = self . _backend . interface . communicate_poll_exit ( ) logger . info ( "" got exit ret:  %s "" , poll_exit_resp ) if poll_exit_resp : done = poll_exit_resp . done pusher_stats = poll_exit_resp . pusher_stats <MASK> self . _on_finish_progress ( pusher_stats , done ) if done : return poll_exit_resp time . sleep ( 2 )",if pusher_stats :,if pusher_stats :,100,TRUE,100
4604,"def listing_items ( method ) : marker = None once = True items = [ ] while once or items : for i in items : yield i if once or marker : <MASK> items = method ( parms = { "" marker "" : marker } ) else : items = method ( ) if len ( items ) == 10000 : marker = items [ - 1 ] else : marker = None once = False else : items = [ ]",if marker :,if marker :,100,TRUE,100
4605,"def call ( monad , * args ) : for arg , name in izip ( args , ( "" hour "" , "" minute "" , "" second "" , "" microsecond "" ) ) : if not isinstance ( arg , NumericMixin ) or arg . type is not int : throw ( TypeError , "" ' %s '  argument of time(...) function must be of  ' int '  type. Got:  %r "" % ( name , type2str ( arg . type ) ) , ) <MASK> throw ( NotImplementedError ) return ConstMonad . new ( time ( * tuple ( arg . value for arg in args ) ) )","if not isinstance ( arg , ConstMonad ) :",if monad . __name__ != name :,85.43019092,FALSE,91.24
4606,"def group_by_sign ( seq , slop = sin ( pi / 18 ) , key = lambda x : x ) : sign = None subseq = [ ] for i in seq : ki = key ( i ) if sign is None : subseq . append ( i ) if ki != 0 : sign = ki / abs ( ki ) else : subseq . append ( i ) <MASK> sign = ki / abs ( ki ) yield subseq subseq = [ i ] if subseq : yield subseq",if sign * ki < - slop :,if ki % slop == 0 :,93.74568862,FALSE,93.2
4607,"def walk_links ( self ) : link_info_list = [ ] for item in self . content : <MASK> link_info = LinkInfo ( link = item , name = item . name , sections = ( ) ) link_info_list . append ( link_info ) else : link_info_list . extend ( item . walk_links ( ) ) return link_info_list","if isinstance ( item , Link ) :","if isinstance ( item , Link ) :",100,TRUE,100
4608,"def get_subkeys ( self , key ) : # TODO: once we revamp the registry emulation, # make this better parent_path = key . get_path ( ) subkeys = [ ] for k in self . keys : test_path = k . get_path ( ) if test_path . lower ( ) . startswith ( parent_path . lower ( ) ) : sub = test_path [ len ( parent_path ) : ] if sub . startswith ( "" \\ "" ) : sub = sub [ 1 : ] end_slash = sub . find ( "" \\ "" ) <MASK> sub = sub [ : end_slash ] if not sub : continue subkeys . append ( sub ) return subkeys",if end_slash >= 0 :,if end_slash > - 1 :,73.37939401,FALSE,97.67
4609,"def load_dict ( dict_path , reverse = False ) : word_dict = { } with open ( dict_path , "" rb "" ) as fdict : for idx , line in enumerate ( fdict ) : line = cpt . to_text ( line ) <MASK> word_dict [ idx ] = line . strip ( "" \n "" ) else : word_dict [ line . strip ( "" \n "" ) ] = idx return word_dict",if reverse :,if reverse :,100,TRUE,100
4610,"def test_network ( coords , feats , model , batch_sizes , forward_only = True ) : for batch_size in batch_sizes : bcoords = batched_coordinates ( [ coords for i in range ( batch_size ) ] ) bfeats = torch . cat ( [ feats for i in range ( batch_size ) ] , 0 ) <MASK> with torch . no_grad ( ) : time , length = forward ( bcoords , bfeats , model ) else : time , length = train ( bcoords , bfeats , model ) print ( f "" { net . __name__ } \t { voxel_size } \t { batch_size } \t { length } \t { time } "" ) torch . cuda . empty_cache ( )",if forward_only :,if forward_only :,100,TRUE,100
4611,"def markUVs ( self , indices = None ) : if isinstance ( indices , tuple ) : indices = indices [ 0 ] ntexco = len ( self . texco ) if indices is None : self . utexc = True else : <MASK> self . utexc = np . zeros ( ntexco , dtype = bool ) if self . utexc is not True : self . utexc [ indices ] = True",if self . utexc is False :,if indices not in self . utexc :,92.38453054,FALSE,92.46
4612,"def has_module ( self , module , version ) : has_module = False for directory in self . directories : module_directory = join ( directory , module ) has_module_directory = isdir ( module_directory ) <MASK> has_module = has_module_directory or exists ( module_directory ) # could be a bare modulefile else : modulefile = join ( module_directory , version ) has_modulefile = exists ( modulefile ) has_module = has_module_directory and has_modulefile if has_module : break return has_module",if not version :,"if version == ""0.0.0.0"" :",86.98287731,FALSE,93.81
4613,"def get_editops ( self ) : if not self . _editops : <MASK> self . _editops = editops ( self . _opcodes , self . _str1 , self . _str2 ) else : self . _editops = editops ( self . _str1 , self . _str2 ) return self . _editops",if self . _opcodes :,if self . _opcodes :,100,TRUE,100
4614,"def to_representation ( self , data ) : value = super ( CredentialTypeSerializer , self ) . to_representation ( data ) # translate labels and help_text for credential fields ""managed by Tower"" if value . get ( "" managed_by_tower "" ) : value [ "" name "" ] = _ ( value [ "" name "" ] ) for field in value . get ( "" inputs "" , { } ) . get ( "" fields "" , [ ] ) : field [ "" label "" ] = _ ( field [ "" label "" ] ) <MASK> field [ "" help_text "" ] = _ ( field [ "" help_text "" ] ) return value","if ""help_text"" in field :","if field . get ( ""help_text"" ) :",97.59573441,FALSE,94.57
4615,"def sort_nested_dictionary_lists ( d ) : for k , v in d . items ( ) : if isinstance ( v , list ) : for i in range ( 0 , len ( v ) ) : <MASK> v [ i ] = await sort_nested_dictionary_lists ( v [ i ] ) d [ k ] = sorted ( v ) if isinstance ( v , dict ) : d [ k ] = await sort_nested_dictionary_lists ( v ) return d","if isinstance ( v [ i ] , dict ) :","if isinstance ( v [ i ] , dict ) :",100,TRUE,100
4616,"def messageSourceStamps ( self , source_stamps ) : text = "" "" for ss in source_stamps : source = "" "" if ss [ "" branch "" ] : source + = "" [branch  %s ]  "" % ss [ "" branch "" ] if ss [ "" revision "" ] : source + = str ( ss [ "" revision "" ] ) else : source + = "" HEAD "" <MASK> source + = ""  (plus patch) "" discriminator = "" "" if ss [ "" codebase "" ] : discriminator = ""   ' %s ' "" % ss [ "" codebase "" ] text + = "" Build Source Stamp %s :  %s \n "" % ( discriminator , source ) return text","if ss [ ""patch"" ] is not None :","if ss [ ""patch"" ] :",96.57745894,FALSE,96.92
4617,"def fit_one ( self , x ) : for i , xi in x . items ( ) : <MASK> self . median [ i ] . update ( xi ) if self . with_scaling : self . iqr [ i ] . update ( xi ) return self",if self . with_centering :,if self . median [ i ] :,93.58253668,FALSE,90.53
4618,"def start_response ( self , status , headers , exc_info = None ) : if exc_info : try : if self . started : six . reraise ( exc_info [ 0 ] , exc_info [ 1 ] , exc_info [ 2 ] ) finally : exc_info = None self . request . status = int ( status [ : 3 ] ) for key , val in headers : <MASK> self . request . set_content_length ( int ( val ) ) elif key . lower ( ) == "" content-type "" : self . request . content_type = val else : self . request . headers_out . add ( key , val ) return self . write","if key . lower ( ) == ""content-length"" :","if key . lower ( ) == ""content-length"" :",100,TRUE,100
4619,"def _osp2ec ( self , bytes ) : compressed = self . _from_bytes ( bytes ) y = compressed >> self . _bits x = compressed & ( 1 << self . _bits ) - 1 if x == 0 : y = self . _curve . b else : result = self . sqrtp ( x * * 3 + self . _curve . a * x + self . _curve . b , self . _curve . field . p ) if len ( result ) == 1 : y = result [ 0 ] <MASK> y1 , y2 = result y = y1 if ( y1 & 1 == y ) else y2 else : return None return ec . Point ( self . _curve , x , y )",elif len ( result ) == 2 :,elif len ( result ) == 2 :,100,TRUE,100
4620,"def trace ( self , ee , rname ) : print ( type ( self ) ) self . traceIndent ( ) guess = "" "" if self . inputState . guessing > 0 : guess = ""  [guessing] "" print ( ( ee + rname + guess ) ) for i in xrange ( 1 , self . k + 1 ) : if i != 1 : print ( "" ,  "" ) <MASK> v = self . LT ( i ) . getText ( ) else : v = "" null "" print ( "" LA( %s ) ==  %s "" % ( i , v ) ) print ( "" \n "" )",if self . LT ( i ) :,if self . inputState . guessing == 0 :,94.85187728,FALSE,94.33
4621,"def _table_schema ( self , table ) : rows = self . db . execute_sql ( "" PRAGMA table_info( ' %s ' ) "" % table ) . fetchall ( ) # Build list of fields from table information result = { } for _ , name , data_type , not_null , _ , primary_key in rows : parts = [ data_type ] if primary_key : parts . append ( "" PRIMARY KEY "" ) <MASK> parts . append ( "" NOT NULL "" ) result [ name ] = ""   "" . join ( parts ) return result",if not_null :,if not_null :,100,TRUE,100
4622,"def _parse_csrf ( self , response ) : for d in response : if d . startswith ( "" Set-Cookie: "" ) : for c in d . split ( "" : "" , 1 ) [ 1 ] . split ( "" ; "" ) : if c . strip ( ) . startswith ( "" CSRF-Token- "" ) : self . _CSRFtoken = c . strip ( ""   \r \n "" ) log . verbose ( "" Got new cookie:  %s "" , self . _CSRFtoken ) break <MASK> break",if self . _CSRFtoken != None :,if self . _CSRFtoken :,70.67601409,FALSE,96.07
4623,"def _update_from_item ( self , row , download_item ) : progress_stats = download_item . progress_stats for key in self . columns : column = self . columns [ key ] [ 0 ] <MASK> # Not the best place but we build the playlist status here status = "" {0}   {1} / {2} "" . format ( progress_stats [ "" status "" ] , progress_stats [ "" playlist_index "" ] , progress_stats [ "" playlist_size "" ] , ) self . SetStringItem ( row , column , status ) else : self . SetStringItem ( row , column , progress_stats [ key ] )","if key == ""status"" and progress_stats [ ""playlist_index"" ] :","if progress_stats [ ""status"" ] > 0 :",89.86419564,FALSE,91.15
4624,"def unmarshal_package_repositories ( cls , data : Any ) - > List [ "" PackageRepository "" ] : repositories = list ( ) if data is not None : <MASK> raise RuntimeError ( f "" invalid package-repositories:  { data !r} "" ) for repository in data : package_repo = cls . unmarshal ( repository ) repositories . append ( package_repo ) return repositories","if not isinstance ( data , list ) :","if not isinstance ( data , ( list , tuple ) ) :",91.67881676,FALSE,92.56
4625,"def remove_message ( e = None ) : itop = scanbox . nearest ( 0 ) sel = scanbox . curselection ( ) if not sel : dialog ( root , "" No Message To Remove "" , "" Please select a message to remove "" , "" "" , 0 , "" OK "" , ) return todo = [ ] for i in sel : line = scanbox . get ( i ) <MASK> todo . append ( string . atoi ( scanparser . group ( 1 ) ) ) mhf . removemessages ( todo ) rescan ( ) fixfocus ( min ( todo ) , itop )",if scanparser . match ( line ) >= 0 :,"if line . startswith ( ""message"" ) :",91.86168331,FALSE,92.07
4626,"def test_patches ( ) : print ( "" Botocore version:  {}  aiohttp version:  {} "" . format ( botocore . __version__ , aiohttp . __version__ ) ) success = True for obj , digests in chain ( _AIOHTTP_DIGESTS . items ( ) , _API_DIGESTS . items ( ) ) : digest = hashlib . sha1 ( getsource ( obj ) . encode ( "" utf-8 "" ) ) . hexdigest ( ) <MASK> print ( "" Digest of  {} : {}  not found in:  {} "" . format ( obj . __qualname__ , digest , digests ) ) success = False assert success",if digest not in digests :,if digest not in digests :,100,TRUE,100
4627,"def sample_admin_user ( ) : """"""List of iris messages"""""" with iris_ctl . db_from_config ( sample_db_config ) as ( conn , cursor ) : cursor . execute ( "" SELECT `name` FROM `target` JOIN `user` on `target`.`id` = `user`.`target_id` WHERE `user`.`admin` = TRUE LIMIT 1 "" ) result = cursor . fetchone ( ) <MASK> return result [ 0 ]",if result :,if result :,100,TRUE,100
4628,"def _addRightnames ( groups , kerning , leftname , rightnames , includeAll = True ) : if leftname in kerning : for rightname in kerning [ leftname ] : <MASK> for rightname2 in groups [ rightname ] : rightnames . add ( rightname2 ) if not includeAll : # TODO: in this case, pick the one rightname that has the highest # ranking in glyphorder break else : rightnames . add ( rightname )","if rightname [ 0 ] == ""@"" :",if rightname in rightnames :,90.75114598,FALSE,88.74
4629,"def build ( self , input_shape ) : if isinstance ( input_shape , list ) and len ( input_shape ) == 2 : self . data_mode = "" disjoint "" self . F = input_shape [ 0 ] [ - 1 ] else : <MASK> self . data_mode = "" single "" else : self . data_mode = "" batch "" self . F = input_shape [ - 1 ]",if len ( input_shape ) == 2 :,if len ( input_shape ) == 1 :,96.95695488,FALSE,97.36
4630,"def update_ranges ( l , i ) : for _range in l : # most common case: extend a range <MASK> _range [ 0 ] = i merge_ranges ( l ) return elif i == _range [ 1 ] + 1 : _range [ 1 ] = i merge_ranges ( l ) return # somewhere outside of range proximity l . append ( [ i , i ] ) l . sort ( key = lambda x : x [ 0 ] )",if i == _range [ 0 ] - 1 :,if i == _range [ 0 ] + 1 :,98.62400469,FALSE,97.59
4631,"def transform ( a , cmds ) : buf = a . split ( "" \n "" ) for cmd in cmds : ctype , line , col , char = cmd <MASK> if char != "" \n "" : buf [ line ] = buf [ line ] [ : col ] + buf [ line ] [ col + len ( char ) : ] else : buf [ line ] = buf [ line ] + buf [ line + 1 ] del buf [ line + 1 ] elif ctype == "" I "" : buf [ line ] = buf [ line ] [ : col ] + char + buf [ line ] [ col : ] buf = "" \n "" . join ( buf ) . split ( "" \n "" ) return "" \n "" . join ( buf )","if ctype == ""D"" :","if ctype == ""A"" :",99.17841686,FALSE,98.44
4632,"def _media_files_drag_received ( widget , context , x , y , data , info , timestamp ) : uris = data . get_uris ( ) files = [ ] for uri in uris : try : uri_tuple = GLib . filename_from_uri ( uri ) except : continue uri , unused = uri_tuple if os . path . exists ( uri ) == True : <MASK> files . append ( uri ) if len ( files ) == 0 : return open_dropped_files ( files )",if utils . is_media_file ( uri ) == True :,if os . path . isfile ( uri ) :,93.7896783,FALSE,89.25
4633,"def __walk_proceed_remote_dir_act ( self , r , args ) : dirjs , filejs = args j = r . json ( ) if "" list "" not in j : self . pd ( "" Key  ' list '  not found in the response of directory listing request: \n {} "" . format ( j ) ) return const . ERequestFailed paths = j [ "" list "" ] for path in paths : <MASK> dirjs . append ( path ) else : filejs . append ( path ) return const . ENoError","if path [ ""isdir"" ] :",if os . path . isdir ( path ) :,93.60668223,FALSE,92.04
4634,"def TaskUpdatesVerbose ( task , progress ) : if isinstance ( task . info . progress , int ) : info = task . info <MASK> progress = "" %d %%  ( %s ) "" % ( info . progress , info . state ) print ( "" Task  %s  (key: %s , desc: %s ) -  %s "" % ( info . name . info . name , info . key , info . description , progress ) )","if not isinstance ( progress , str ) :",if progress :,77.56160148,FALSE,91.33
4635,"def dump_constants ( header ) : output = StringIO . StringIO ( ) output . write ( header ) for attribute in dir ( FSEvents ) : value = getattr ( FSEvents , attribute ) <MASK> output . write ( ""      %s  =  %s \n "" % ( attribute , hex ( value ) ) ) content = output . getvalue ( ) output . close ( ) return content","if attribute . startswith ( ""k"" ) and isinstance ( value , int ) :","if attribute . startswith ( ""constants_"" ) :",90.72034209,FALSE,88.68
4636,"def _ensure_data_is_loaded ( self , sql_object , input_params , stdin_file , stdin_filename = "" - "" , stop_after_analysis = False , ) : data_loads = [ ] # Get each ""table name"" which is actually the file name for filename in sql_object . qtable_names : data_load = self . _load_data ( filename , input_params , stdin_file = stdin_file , stdin_filename = stdin_filename , stop_after_analysis = stop_after_analysis , ) <MASK> data_loads . append ( data_load ) return data_loads",if data_load is not None :,if data_load is not None :,100,TRUE,100
4637,"def _get_instantiation ( self ) : if self . _data is None : f , l , c , o = c_object_p ( ) , c_uint ( ) , c_uint ( ) , c_uint ( ) SourceLocation_loc ( self , byref ( f ) , byref ( l ) , byref ( c ) , byref ( o ) ) <MASK> f = File ( f ) else : f = None self . _data = ( f , int ( l . value ) , int ( c . value ) , int ( c . value ) ) return self . _data",if f :,if o . value == 0 :,96.6588067,FALSE,94.1
4638,"def _get_all_info_lines ( data ) : infos = [ ] for row in data : splitrow = row . split ( ) <MASK> if splitrow [ 0 ] == "" INFO: "" : infos . append ( ""   "" . join ( splitrow [ 1 : ] ) ) return infos",if len ( splitrow ) > 0 :,if len ( splitrow ) > 1 :,97.78654564,FALSE,96.14
4639,"def _brush_modified_cb ( self , settings ) : """"""Updates the brush's base setting adjustments on brush changes"""""" for cname in settings : adj = self . brush_adjustment . get ( cname , None ) <MASK> continue value = self . brush . get_base_value ( cname ) adj . set_value ( value )",if adj is None :,if adj is None :,100,TRUE,100
4640,"def migrate_node_facts ( facts ) : """"""Migrate facts from various roles into node"""""" params = { "" common "" : ( "" dns_ip "" ) , } if "" node "" not in facts : facts [ "" node "" ] = { } # pylint: disable=consider-iterating-dictionary for role in params . keys ( ) : if role in facts : for param in params [ role ] : <MASK> facts [ "" node "" ] [ param ] = facts [ role ] . pop ( param ) return facts",if param in facts [ role ] :,"if param in facts [ ""node"" ] :",98.71693323,FALSE,96.02
4641,"def serialize_content_range ( value ) : if isinstance ( value , ( tuple , list ) ) : <MASK> raise ValueError ( "" When setting content_range to a list/tuple, it must  "" "" be length 2 or 3 (not  %r ) "" % value ) if len ( value ) == 2 : begin , end = value length = None else : begin , end , length = value value = ContentRange ( begin , end , length ) value = str ( value ) . strip ( ) if not value : return None return value","if len ( value ) not in ( 2 , 3 ) :",if len ( value ) != 3 :,90.88897723,FALSE,93.12
4642,"def clean ( self ) : data = super ( ) . clean ( ) if data . get ( "" expires "" ) : <MASK> data [ "" expires "" ] = make_aware ( datetime . combine ( data [ "" expires "" ] , time ( hour = 23 , minute = 59 , second = 59 ) ) , self . instance . event . timezone , ) else : data [ "" expires "" ] = data [ "" expires "" ] . replace ( hour = 23 , minute = 59 , second = 59 ) if data [ "" expires "" ] < now ( ) : raise ValidationError ( _ ( "" The new expiry date needs to be in the future. "" ) ) return data","if isinstance ( data [ ""expires"" ] , date ) :",if self . instance . event . timezone :,92.14914142,FALSE,91.51
4643,"def _build ( self , obj , stream , context ) : if self . include_name : name , obj = obj for sc in self . subcons : <MASK> sc . _build ( obj , stream , context ) return else : for sc in self . subcons : stream2 = BytesIO ( ) context2 = context . __copy__ ( ) try : sc . _build ( obj , stream2 , context2 ) except Exception : pass else : context . __update__ ( context2 ) stream . write ( stream2 . getvalue ( ) ) return raise SelectError ( "" no subconstruct matched "" , obj )",if sc . name == name :,if sc . name == name :,100,TRUE,100
4644,"def records ( account_id ) : """"""Fetch locks data"""""" s = boto3 . Session ( ) table = s . resource ( "" dynamodb "" ) . Table ( "" Sphere11.Dev.ResourceLocks "" ) results = table . scan ( ) for r in results [ "" Items "" ] : <MASK> r [ "" LockDate "" ] = datetime . fromtimestamp ( r [ "" LockDate "" ] ) if "" RevisionDate "" in r : r [ "" RevisionDate "" ] = datetime . fromtimestamp ( r [ "" RevisionDate "" ] ) print ( tabulate . tabulate ( results [ "" Items "" ] , headers = "" keys "" , tablefmt = "" fancy_grid "" ) )","if ""LockDate"" in r :","if ""LockDate"" in r :",100,TRUE,100
4645,"def visitIf ( self , node , scope ) : for test , body in node . tests : if isinstance ( test , ast . Const ) : <MASK> if not test . value : continue self . visit ( test , scope ) self . visit ( body , scope ) if node . else_ : self . visit ( node . else_ , scope )",if type ( test . value ) in self . _const_types :,"if isinstance ( test . value , ast . Name ) :",89.36413138,FALSE,85.93
4646,"def validate_max_discount ( self ) : if self . rate_or_discount == "" Discount Percentage "" and self . get ( "" items "" ) : for d in self . items : max_discount = frappe . get_cached_value ( "" Item "" , d . item_code , "" max_discount "" ) <MASK> throw ( _ ( "" Max discount allowed for item:  {0}  is  {1} % "" ) . format ( self . item_code , max_discount ) )",if max_discount and flt ( self . discount_percentage ) > flt ( max_discount ) :,if max_discount > 100 :,61.87599633,FALSE,85.51
4647,"def has_invalid_cce ( yaml_file , product_yaml = None ) : rule = yaml . open_and_macro_expand ( yaml_file , product_yaml ) if "" identifiers "" in rule and rule [ "" identifiers "" ] is not None : for i_type , i_value in rule [ "" identifiers "" ] . items ( ) : if i_type [ 0 : 3 ] == "" cce "" : <MASK> return True return False","if not checks . is_cce_value_valid ( ""CCE-"" + str ( i_value ) ) :","if i_value [ 0 ] == ""invalid"" :",86.45695645,FALSE,81.18
4648,"def parse_calendar_eras ( data , calendar ) : eras = data . setdefault ( "" eras "" , { } ) for width in calendar . findall ( "" eras/* "" ) : width_type = NAME_MAP [ width . tag ] widths = eras . setdefault ( width_type , { } ) for elem in width . getiterator ( ) : if elem . tag == "" era "" : _import_type_text ( widths , elem , type = int ( elem . attrib . get ( "" type "" ) ) ) <MASK> eras [ width_type ] = Alias ( _translate_alias ( [ "" eras "" , width_type ] , elem . attrib [ "" path "" ] ) )","elif elem . tag == ""alias"" :","elif elem . tag == ""path"" :",98.99863054,FALSE,98.35
4649,"def validate_grammar ( ) - > None : for fn in _NONTERMINAL_CONVERSIONS_SEQUENCE : fn_productions = get_productions ( fn ) if all ( p . name == fn_productions [ 0 ] . name for p in fn_productions ) : # all the production names are the same, ensure that the `convert_` function # is named correctly production_name = fn_productions [ 0 ] . name expected_name = f "" convert_ { production_name } "" <MASK> raise Exception ( f "" The conversion function for  ' { production_name } '   "" + f "" must be called  ' { expected_name } ' , not  ' { fn . __name__ } ' . "" )",if fn . __name__ != expected_name :,if not callable ( expected_name ) :,97.34401451,FALSE,92.44
4650,"def split_ratio ( row ) : if float ( row [ "" Numerator "" ] ) > 0 : <MASK> n , m = row [ "" Splitratio "" ] . split ( "" : "" ) return float ( m ) / float ( n ) else : return eval ( row [ "" Splitratio "" ] ) else : return 1","if "":"" in row [ ""Splitratio"" ] :","if "":"" in row [ ""Splitratio"" ] :",100,TRUE,100
4651,"def _handle_def_errors ( testdef ) : # If the test generation had an error, raise if testdef . error : if testdef . exception : <MASK> raise testdef . exception else : raise Exception ( testdef . exception ) else : raise Exception ( "" Test parse failure "" )","if isinstance ( testdef . exception , Exception ) :","if isinstance ( testdef . exception , Exception ) :",75,TRUE,100
4652,"def _get_quota_availability ( self ) : quotas_ok = defaultdict ( int ) qa = QuotaAvailability ( ) qa . queue ( * [ k for k , v in self . _quota_diff . items ( ) if v > 0 ] ) qa . compute ( now_dt = self . now_dt ) for quota , count in self . _quota_diff . items ( ) : if count < = 0 : quotas_ok [ quota ] = 0 break avail = qa . results [ quota ] <MASK> quotas_ok [ quota ] = min ( count , avail [ 1 ] ) else : quotas_ok [ quota ] = count return quotas_ok",if avail [ 1 ] is not None and avail [ 1 ] < count :,if avail [ 0 ] > 0 :,89.76117244,FALSE,91.04
4653,"def reverse ( self ) : """"""Reverse *IN PLACE*."""""" li = self . leftindex lb = self . leftblock ri = self . rightindex rb = self . rightblock for i in range ( self . len >> 1 ) : lb . data [ li ] , rb . data [ ri ] = rb . data [ ri ] , lb . data [ li ] li + = 1 <MASK> lb = lb . rightlink li = 0 ri - = 1 if ri < 0 : rb = rb . leftlink ri = BLOCKLEN - 1",if li >= BLOCKLEN :,if li > BLOCKLEN - 1 :,94.73310261,FALSE,96.27
4654,"def __manipulate_item ( self , item ) : if self . _Cursor__manipulate : db = self . _Cursor__collection . database son = db . _fix_outgoing ( item , self . _Cursor__collection ) else : son = item if self . __wrap is not None : <MASK> return getattr ( self . _Cursor__collection , son [ self . __wrap . type_field ] ) ( son ) return self . __wrap ( son , collection = self . _Cursor__collection ) else : return son",if self . __wrap . type_field in son :,if self . __wrap . type_field in son :,100,TRUE,100
4655,"def apply_transforms ( self ) : """"""Apply all of the stored transforms, in priority order."""""" self . document . reporter . attach_observer ( self . document . note_transform_message ) while self . transforms : <MASK> # Unsorted initially, and whenever a transform is added. self . transforms . sort ( ) self . transforms . reverse ( ) self . sorted = 1 priority , transform_class , pending , kwargs = self . transforms . pop ( ) transform = transform_class ( self . document , startnode = pending ) transform . apply ( * * kwargs ) self . applied . append ( ( priority , transform_class , pending , kwargs ) )",if not self . sorted :,if self . sorted :,98.77140841,FALSE,98.23
4656,"def format_sql ( sql , params ) : rv = [ ] if isinstance ( params , dict ) : # convert sql with named parameters to sql with unnamed parameters conv = _FormatConverter ( params ) <MASK> sql = sql_to_string ( sql ) sql = sql % conv params = conv . params else : params = ( ) for param in params or ( ) : if param is None : rv . append ( "" NULL "" ) param = safe_repr ( param ) rv . append ( param ) return sql , rv",if params :,if conv . params :,98.72590373,FALSE,96.76
4657,"def on_execution_item ( self , cpath , execution ) : if not isinstance ( execution , dict ) : return if "" executor "" in execution and execution . get ( "" executor "" ) != "" jmeter "" : return scenario = execution . get ( "" scenario "" , None ) <MASK> return if isinstance ( scenario , str ) : scenario_name = scenario scenario = self . get_named_scenario ( scenario_name ) <MASK> scenario = None scenario_path = Path ( "" scenarios "" , scenario_name ) else : scenario_path = cpath . copy ( ) scenario_path . add_component ( "" scenario "" ) if scenario is not None : self . check_jmeter_scenario ( scenario_path , scenario )",if not scenario :,if not scenario :,100,TRUE,100
4658,"def _poll_ipc_requests ( self ) - > None : try : if self . _ipc_requests . empty ( ) : return while not self . _ipc_requests . empty ( ) : args = self . _ipc_requests . get ( ) try : for filename in args : <MASK> self . get_editor_notebook ( ) . show_file ( filename ) except Exception as e : logger . exception ( "" Problem processing ipc request "" , exc_info = e ) self . become_active_window ( ) finally : self . after ( 50 , self . _poll_ipc_requests )",if os . path . isfile ( filename ) :,"if filename . endswith ( "".py"" ) :",94.54043726,FALSE,93.64
4659,"def get_scroll_distance_to_element ( driver , element ) : try : scroll_position = driver . execute_script ( "" return window.scrollY; "" ) element_location = None element_location = element . location [ "" y "" ] element_location = element_location - 130 <MASK> element_location = 0 distance = element_location - scroll_position return distance except Exception : return 0",if element_location < 0 :,if element_location < 0 :,100,TRUE,100
4660,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 10 : self . set_access_token ( d . getPrefixedString ( ) ) continue <MASK> self . set_expiration_time ( d . getVarInt64 ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 16 :,if tt == 16 :,100,TRUE,100
4661,"def _validate_and_define ( params , key , value ) : ( key , force_generic ) = _validate_key ( _unescape ( key ) ) if key in params : raise SyntaxError ( f ' duplicate key  "" { key } "" ' ) cls = _class_for_key . get ( key , GenericParam ) emptiness = cls . emptiness ( ) if value is None : if emptiness == Emptiness . NEVER : raise SyntaxError ( "" value cannot be empty "" ) value = cls . from_value ( value ) else : <MASK> value = cls . from_wire_parser ( dns . wire . Parser ( _unescape ( value ) ) ) else : value = cls . from_value ( value ) params [ key ] = value",if force_generic :,if force_generic :,100,TRUE,100
4662,"def iter_fields ( node , * , include_meta = True , exclude_unset = False ) : exclude_meta = not include_meta for field_name , field in node . _fields . items ( ) : <MASK> continue field_val = getattr ( node , field_name , _marker ) if field_val is _marker : continue if exclude_unset : if callable ( field . default ) : default = field . default ( ) else : default = field . default if field_val == default : continue yield field_name , field_val",if exclude_meta and field . meta :,"if field_name . startswith ( ""_"" ) :",92.75417801,FALSE,91.4
4663,"def tearDown ( self ) : """"""Shutdown the server."""""" try : if self . server : self . server . stop ( ) <MASK> self . root_logger . removeHandler ( self . sl_hdlr ) self . sl_hdlr . close ( ) finally : BaseTest . tearDown ( self )",if self . sl_hdlr :,if self . sl_hdlr :,100,TRUE,100
4664,"def _wait_for_async_copy ( self , share_name , file_path ) : count = 0 share_client = self . fsc . get_share_client ( share_name ) file_client = share_client . get_file_client ( file_path ) properties = file_client . get_file_properties ( ) while properties . copy . status != "" success "" : count = count + 1 <MASK> self . fail ( "" Timed out waiting for async copy to complete. "" ) self . sleep ( 6 ) properties = file_client . get_file_properties ( ) self . assertEqual ( properties . copy . status , "" success "" )",if count > 10 :,if count > 5 :,98.74390283,FALSE,98.24
4665,"def __new__ ( cls , message_type : OrderBookMessageType , content : Dict [ str , any ] , timestamp : Optional [ float ] = None , * args , * * kwargs , ) : if timestamp is None : <MASK> raise ValueError ( "" timestamp must not be None when initializing snapshot messages. "" ) timestamp = int ( time . time ( ) ) return super ( KucoinOrderBookMessage , cls ) . __new__ ( cls , message_type , content , timestamp = timestamp , * args , * * kwargs )",if message_type is OrderBookMessageType . SNAPSHOT :,if message_type is OrderBookMessageType . SNAPSHOT :,100,TRUE,100
4666,"def _drop_unique_features ( X : DataFrame , feature_metadata : FeatureMetadata , max_unique_ratio ) - > list : features_to_drop = [ ] X_len = len ( X ) max_unique_value_count = X_len * max_unique_ratio for column in X : unique_value_count = len ( X [ column ] . unique ( ) ) <MASK> features_to_drop . append ( column ) elif feature_metadata . get_feature_type_raw ( column ) in [ R_CATEGORY , R_OBJECT , ] and ( unique_value_count > max_unique_value_count ) : features_to_drop . append ( column ) return features_to_drop",if unique_value_count == 1 :,if feature_metadata . get_feature_type_raw ( column ) in [ R_,92.61441081,FALSE,88.78
4667,"def get_src_findex_by_pad ( s , S , padding_mode , align_corners ) : if padding_mode == "" zero "" : return get_src_findex_with_zero_pad ( s , S ) elif padding_mode == "" reflect "" : <MASK> return get_src_findex_with_reflect_pad ( s , S , True ) else : sf = get_src_findex_with_reflect_pad ( s , S , False ) return get_src_findex_with_repeat_pad ( sf , S ) elif padding_mode == "" repeat "" : return get_src_findex_with_repeat_pad ( s , S )",if align_corners :,if align_corners :,100,TRUE,100
4668,"def _iterate_self_and_parents ( self , upto = None ) : current = self result = ( ) while current : result + = ( current , ) if current . _parent is upto : break <MASK> raise sa_exc . InvalidRequestError ( "" Transaction  %s  is not on the active transaction list "" % ( upto ) ) else : current = current . _parent return result",elif current . _parent is None :,if current . _parent is None :,75.99646072,FALSE,96.98
4669,"def __setattr__ ( self , name : str , val : Any ) : if name . startswith ( "" COMPUTED_ "" ) : <MASK> old_val = self [ name ] if old_val == val : return raise KeyError ( "" Computed attributed  ' {} '  already exists  "" "" with a different value! old= {} , new= {} . "" . format ( name , old_val , val ) ) self [ name ] = val else : super ( ) . __setattr__ ( name , val )",if name in self :,if name in self :,100,TRUE,100
4670,"def get_fnlist ( bbhandler , pkg_pn , preferred ) : """"""Get all recipe file names"""""" <MASK> ( latest_versions , preferred_versions ) = bb . providers . findProviders ( bbhandler . config_data , bbhandler . cooker . recipecaches [ "" "" ] , pkg_pn ) fn_list = [ ] for pn in sorted ( pkg_pn ) : <MASK> fn_list . append ( preferred_versions [ pn ] [ 1 ] ) else : fn_list . extend ( pkg_pn [ pn ] ) return fn_list",if preferred :,if pn in preferred_versions :,91.64858438,FALSE,89.9
4671,"def links_extracted ( self , _ , links ) : links_deduped = { } for link in links : link_fingerprint = link . meta [ FIELD_FINGERPRINT ] <MASK> continue links_deduped [ link_fingerprint ] = link [ self . _redis_pipeline . hmset ( fingerprint , self . _create_link_extracted ( link ) ) for ( fingerprint , link ) in links_deduped . items ( ) ] self . _redis_pipeline . execute ( )",if link_fingerprint in links_deduped :,if link_fingerprint in self . _linked_links :,93.75152676,FALSE,93.33
4672,"def __call__ ( self , name , rawtext , text , lineno , inliner , options = None , content = None ) : options = options or { } content = content or [ ] issue_nos = [ each . strip ( ) for each in utils . unescape ( text ) . split ( "" , "" ) ] config = inliner . document . settings . env . app . config ret = [ ] for i , issue_no in enumerate ( issue_nos ) : node = self . make_node ( name , issue_no , config , options = options ) ret . append ( node ) <MASK> sep = nodes . raw ( text = "" ,  "" , format = "" html "" ) ret . append ( sep ) return ret , [ ]",if i != len ( issue_nos ) - 1 :,if i == 0 :,86.55801781,FALSE,93.1
4673,"def init_messengers ( messengers ) : for messenger in messengers : <MASK> module_path = messenger [ "" type "" ] messenger [ "" type "" ] = messenger [ "" type "" ] . split ( "" . "" ) [ - 1 ] else : module_path = "" oncall.messengers. "" + messenger [ "" type "" ] instance = getattr ( importlib . import_module ( module_path ) , messenger [ "" type "" ] ) ( messenger ) for transport in instance . supports : _active_messengers [ transport ] . append ( instance )","if ""."" in messenger [ ""type"" ] :","if ""."" not in messenger [ ""type"" ] :",99.00159976,FALSE,97.93
4674,"def _process_enum_definition ( self , tok ) : fields = [ ] for field in tok . fields : <MASK> expression = self . expression_parser . parse ( field . expression ) else : expression = None fields . append ( c_ast . CEnumField ( name = field . name . first , value = expression ) ) name = tok . enum_name if name : name = "" enum  %s "" % tok . enum_name . first else : name = self . _make_anonymous_type ( "" enum "" ) return c_ast . CTypeDefinition ( name = name , type_definition = c_ast . CEnum ( attributes = tok . attributes , fields = fields , name = name ) , )",if field . expression :,"if field . type == ""enum"" :",84.20105184,FALSE,95.11
4675,def result_iterator ( ) : try : # reverse to keep finishing order fs . reverse ( ) while fs : # Careful not to keep a reference to the popped future <MASK> yield fs . pop ( ) . result ( ) else : yield fs . pop ( ) . result ( end_time - time . time ( ) ) finally : for future in fs : future . cancel ( ),if timeout is None :,if end_time is None :,73.38041934,FALSE,94.52
4676,"def has_encrypted_ssh_key_data ( self ) : try : ssh_key_data = self . get_input ( "" ssh_key_data "" ) except AttributeError : return False try : pem_objects = validate_ssh_private_key ( ssh_key_data ) for pem_object in pem_objects : <MASK> return True except ValidationError : pass return False","if pem_object . get ( ""key_enc"" , False ) :",if pem_object . encrypted :,87.52468027,FALSE,87.23
4677,"def test_seq_object_transcription_method ( self ) : for nucleotide_seq in test_seqs : <MASK> self . assertEqual ( repr ( Seq . transcribe ( nucleotide_seq ) ) , repr ( nucleotide_seq . transcribe ( ) ) , )","if isinstance ( nucleotide_seq , Seq . Seq ) :","if isinstance ( nucleotide_seq , Seq . Seq ) :",75,TRUE,100
4678,"def max_elevation ( self ) : max_el = None for y in xrange ( self . height ) : for x in xrange ( self . width ) : el = self . elevation [ "" data "" ] [ y ] [ x ] <MASK> max_el = el return max_el",if max_el is None or el > max_el :,if el > max_el :,82.27229851,FALSE,88.98
4679,"def stress ( mapping , index ) : for count in range ( OPERATIONS ) : function = random . choice ( functions ) function ( mapping , index ) <MASK> print ( "" \r "" , len ( mapping ) , ""   "" * 7 , end = "" "" ) print ( )",if count % 1000 == 0 :,if count % 100 == 0 :,97.89141969,FALSE,95.88
4680,"def sync_terminology ( self ) : if self . is_source : return store = self . store missing = [ ] for source in self . component . get_all_sources ( ) : if "" terminology "" not in source . all_flags : continue try : _unit , add = store . find_unit ( source . context , source . source ) except UnitNotFound : add = True # Unit is already present <MASK> continue missing . append ( ( source . context , source . source , "" "" ) ) if missing : self . add_units ( None , missing )",if not add :,if add :,98.66935679,FALSE,97.88
4681,"def get_generators ( self ) : """"""Get a dict with all registered generators, indexed by name"""""" generators = { } for core in self . db . find ( ) : <MASK> _generators = core . get_generators ( { } ) if _generators : generators [ str ( core . name ) ] = _generators return generators","if hasattr ( core , ""get_generators"" ) :","if hasattr ( core , ""get_generators"" ) :",100,TRUE,100
4682,"def act ( self , state ) : if self . body . env . clock . frame < self . training_start_step : return policy_util . random ( state , self , self . body ) . cpu ( ) . squeeze ( ) . numpy ( ) else : action = self . action_policy ( state , self , self . body ) <MASK> action = self . scale_action ( torch . tanh ( action ) ) # continuous action bound return action . cpu ( ) . squeeze ( ) . numpy ( )",if not self . body . is_discrete :,if self . scale_mode :,91.65370548,FALSE,92.47
4683,"def try_open_completions_event ( self , event = None ) : "" (./) Open completion list after pause with no movement. "" lastchar = self . text . get ( "" insert-1c "" ) if lastchar in TRIGGERS : args = TRY_A if lastchar == "" . "" else TRY_F self . _delayed_completion_index = self . text . index ( "" insert "" ) <MASK> self . text . after_cancel ( self . _delayed_completion_id ) self . _delayed_completion_id = self . text . after ( self . popupwait , self . _delayed_open_completions , args )",if self . _delayed_completion_id is not None :,if self . _delayed_completion_id is not None :,100,TRUE,100
4684,"def token_is_available ( self ) : if self . token : try : resp = requests . get ( "" https://api.shodan.io/account/profile?key= {0} "" . format ( self . token ) ) <MASK> return True except Exception as ex : logger . error ( str ( ex ) ) return False","if resp and resp . status_code == 200 and ""member"" in resp . json ( ) :","if resp and resp . status_code == 200 and ""available"" in resp . text :",87.61532143,FALSE,92.21
4685,"def next_bar_ ( self , event ) : bars = event . bar_dict self . _current_minute = self . _minutes_since_midnight ( self . ucontext . now . hour , self . ucontext . now . minute ) for day_rule , time_rule , func in self . _registry : <MASK> with ExecutionContext ( EXECUTION_PHASE . SCHEDULED ) : with ModifyExceptionFromType ( EXC_TYPE . USER_EXC ) : func ( self . ucontext , bars ) self . _last_minute = self . _current_minute",if day_rule ( ) and time_rule ( ) :,"if day_rule ( self . ucontext , bars ) :",91.65251969,FALSE,93.77
4686,"def decoder ( s ) : r = [ ] decode = [ ] for c in s : if c == "" & "" and not decode : decode . append ( "" & "" ) <MASK> if len ( decode ) == 1 : r . append ( "" & "" ) else : r . append ( modified_unbase64 ( "" "" . join ( decode [ 1 : ] ) ) ) decode = [ ] elif decode : decode . append ( c ) else : r . append ( c ) if decode : r . append ( modified_unbase64 ( "" "" . join ( decode [ 1 : ] ) ) ) bin_str = "" "" . join ( r ) return ( bin_str , len ( s ) )","elif c == ""-"" and decode :","elif c == ""&"" :",96.15589434,FALSE,96.57
4687,"def admin_audit_get ( admin_id ) : if settings . app . demo_mode : resp = utils . demo_get_cache ( ) <MASK> return utils . jsonify ( resp ) if not flask . g . administrator . super_user : return utils . jsonify ( { "" error "" : REQUIRES_SUPER_USER , "" error_msg "" : REQUIRES_SUPER_USER_MSG , } , 400 , ) admin = auth . get_by_id ( admin_id ) resp = admin . get_audit_events ( ) if settings . app . demo_mode : utils . demo_set_cache ( resp ) return utils . jsonify ( resp )",if resp :,if resp :,100,TRUE,100
4688,"def vjp ( self , argnum , outgrad , ans , vs , gvs , args , kwargs ) : try : return self . vjps [ argnum ] ( outgrad , ans , vs , gvs , * args , * * kwargs ) except KeyError : <MASK> errstr = "" Gradient of  {0}  not yet implemented. "" else : errstr = "" Gradient of  {0}  w.r.t. arg number  {1}  not yet implemented. "" raise NotImplementedError ( errstr . format ( self . fun . __name__ , argnum ) )",if self . vjps == { } :,"if self . fun . __name__ == ""Conv2d"" :",91.51605369,FALSE,89.48
4689,"def update ( self , * args , * * kwargs ) : assert not self . readonly longest_key = 0 _dict = self . _dict reverse = self . reverse casereverse = self . casereverse for iterable in args + ( kwargs , ) : <MASK> iterable = iterable . items ( ) for key , value in iterable : longest_key = max ( longest_key , len ( key ) ) _dict [ key ] = value reverse [ value ] . append ( key ) casereverse [ value . lower ( ) ] [ value ] + = 1 self . _longest_key = max ( self . _longest_key , longest_key )","if isinstance ( iterable , ( dict , StenoDictionary ) ) :","if isinstance ( iterable , dict ) :",85.22351714,FALSE,95.5
4690,"def update_ui ( self , window ) : view = window . get_active_view ( ) self . set_status ( view ) lang = "" plain_text "" if view : buf = view . get_buffer ( ) language = buf . get_language ( ) <MASK> lang = language . get_id ( ) self . setup_smart_indent ( view , lang )",if language :,if language :,100,TRUE,100
4691,"def number_operators ( self , a , b , skip = [ ] ) : dict = { "" a "" : a , "" b "" : b } for name , expr in self . binops . items ( ) : if name not in skip : name = "" __ %s __ "" % name <MASK> res = eval ( expr , dict ) self . binop_test ( a , b , res , expr , name ) for name , expr in self . unops . items ( ) : if name not in skip : name = "" __ %s __ "" % name <MASK> res = eval ( expr , dict ) self . unop_test ( a , res , expr , name )","if hasattr ( a , name ) :","if self . binop_test ( a , b , name ) :",86.99160387,FALSE,88.84
4692,"def _getItemHeight ( self , item , ctrl = None ) : """"""Returns the full height of the item to be inserted in the form"""""" if type ( ctrl ) == psychopy . visual . TextBox2 : return ctrl . size [ 1 ] if type ( ctrl ) == psychopy . visual . Slider : # Set radio button layout <MASK> return 0.03 + ctrl . labelHeight * 3 elif item [ "" layout "" ] == "" vert "" : # for vertical take into account the nOptions return ctrl . labelHeight * len ( item [ "" options "" ] )","if item [ ""layout"" ] == ""horiz"" :","if item [ ""layout"" ] == ""horizontal"" :",98.86606557,FALSE,97.98
4693,"def test_cleanup_params ( self , body , rpc_mock ) : res = self . _get_resp_post ( body ) self . assertEqual ( http_client . ACCEPTED , res . status_code ) rpc_mock . assert_called_once_with ( self . context , mock . ANY ) cleanup_request = rpc_mock . call_args [ 0 ] [ 1 ] for key , value in body . items ( ) : if key in ( "" disabled "" , "" is_up "" ) : <MASK> value = value == "" true "" self . assertEqual ( value , getattr ( cleanup_request , key ) ) self . assertEqual ( self . _expected_services ( * SERVICES ) , res . json )",if value is not None :,"if key == ""is_up"" :",94.84565363,FALSE,94.15
4694,"def _read_json_content ( self , body_is_optional = False ) : if "" content-length "" not in self . headers : return self . send_error ( 411 ) if not body_is_optional else { } try : content_length = int ( self . headers . get ( "" content-length "" ) ) if content_length == 0 and body_is_optional : return { } request = json . loads ( self . rfile . read ( content_length ) . decode ( "" utf-8 "" ) ) <MASK> return request except Exception : logger . exception ( "" Bad request "" ) self . send_error ( 400 )","if isinstance ( request , dict ) and ( request or body_is_optional ) :","if request [ ""content-type"" ] == ""application/json"" :",87.30563866,FALSE,88.21
4695,"def env_purge_doc ( app : Sphinx , env : BuildEnvironment , docname : str ) - > None : modules = getattr ( env , "" _viewcode_modules "" , { } ) for modname , entry in list ( modules . items ( ) ) : <MASK> continue code , tags , used , refname = entry for fullname in list ( used ) : if used [ fullname ] == docname : used . pop ( fullname ) if len ( used ) == 0 : modules . pop ( modname )",if entry is False :,"if modname == ""module"" :",94.2816631,FALSE,93.01
4696,"def frames ( self ) : """"""an array of all the frames (including iframes) in the current window"""""" from thug . DOM . W3C . HTML . HTMLCollection import HTMLCollection frames = set ( ) for frame in self . _findAll ( [ "" frame "" , "" iframe "" ] ) : <MASK> from thug . DOM . W3C . Core . DOMImplementation import DOMImplementation DOMImplementation . createHTMLElement ( self . window . doc , frame ) frames . add ( frame . _node ) return HTMLCollection ( self . doc , list ( frames ) )","if not getattr ( frame , ""_node"" , None ) :",if frame . _node :,91.53124515,FALSE,89.37
4697,"def check ( self , * * kw ) : if not kw : return exists ( self . strpath ) if len ( kw ) == 1 : <MASK> return not kw [ "" dir "" ] ^ isdir ( self . strpath ) if "" file "" in kw : return not kw [ "" file "" ] ^ isfile ( self . strpath ) return super ( LocalPath , self ) . check ( * * kw )","if ""dir"" in kw :","if ""dir"" in kw :",100,TRUE,100
4698,"def __init__ ( self , folders ) : self . folders = folders self . duplicates = { } for folder , path in folders . items ( ) : duplicates = [ ] for other_folder , other_path in folders . items ( ) : <MASK> continue if other_path == path : duplicates . append ( other_folder ) if len ( duplicates ) : self . duplicates [ folder ] = duplicates",if other_folder == folder :,if other_folder in self . duplicates :,94.39483167,FALSE,93.96
4699,"def next ( self , buf , pos ) : if pos > = len ( buf ) : return EOF , "" "" , pos mo = self . tokens_re . match ( buf , pos ) if mo : text = mo . group ( ) type , regexp , test_lit = self . tokens [ mo . lastindex - 1 ] pos = mo . end ( ) <MASK> type = self . literals . get ( text , type ) return type , text , pos else : c = buf [ pos ] return self . symbols . get ( c , None ) , c , pos + 1",if test_lit :,if regexp and test_lit :,96.77305094,FALSE,97.15
4700,"def step ( self , action ) : """"""Repeat action, sum reward, and max over last observations."""""" total_reward = 0.0 done = None for i in range ( self . _skip ) : obs , reward , done , info = self . env . step ( action ) <MASK> self . _obs_buffer [ 0 ] = obs if i == self . _skip - 1 : self . _obs_buffer [ 1 ] = obs total_reward + = reward if done : break # Note that the observation on the done=True frame # doesn't matter max_frame = self . _obs_buffer . max ( axis = 0 ) return max_frame , total_reward , done , info",if i == self . _skip - 2 :,if i == self . _skip - 2 :,100,TRUE,100
4701,"def convert ( self , ctx , argument ) : arg = argument . replace ( "" 0x "" , "" "" ) . lower ( ) if arg [ 0 ] == "" # "" : arg = arg [ 1 : ] try : value = int ( arg , base = 16 ) if not ( 0 < = value < = 0xFFFFFF ) : raise BadColourArgument ( arg ) return discord . Colour ( value = value ) except ValueError : arg = arg . replace ( ""   "" , "" _ "" ) method = getattr ( discord . Colour , arg , None ) <MASK> raise BadColourArgument ( arg ) return method ( )","if arg . startswith ( ""from_"" ) or method is None or not inspect . ismethod ( method ) :",if not method :,87.28534359,FALSE,83.67
4702,"def run ( self , * * inputs ) : if self . inputs . copy_inputs : self . inputs . subjects_dir = os . getcwd ( ) <MASK> inputs [ "" subjects_dir "" ] = self . inputs . subjects_dir for originalfile in [ self . inputs . in_file , self . inputs . in_norm ] : copy2subjdir ( self , originalfile , folder = "" mri "" ) return super ( SegmentCC , self ) . run ( * * inputs )","if ""subjects_dir"" in inputs :",if self . inputs . subjects_dir :,84.96243789,FALSE,93.35
4703,"def get_queryset ( self ) : if not hasattr ( self , "" _queryset "" ) : <MASK> qs = self . queryset else : qs = self . model . _default_manager . get_queryset ( ) # If the queryset isn't already ordered we need to add an # artificial ordering here to make sure that all formsets # constructed from this queryset have the same form order. if not qs . ordered : qs = qs . order_by ( self . model . _meta . pk . name ) # Removed queryset limiting here. As per discussion re: #13023 # on django-dev, max_num should not prevent existing # related objects/inlines from being displayed. self . _queryset = qs return self . _queryset",if self . queryset is not None :,if self . queryset :,91.2965128,FALSE,97.02
4704,"def visit_simple_stmt ( self , node : Node ) - > Iterator [ Line ] : """"""Visit a statement without nested statements."""""" is_suite_like = node . parent and node . parent . type in STATEMENT if is_suite_like : <MASK> yield from self . visit_default ( node ) else : yield from self . line ( + 1 ) yield from self . visit_default ( node ) yield from self . line ( - 1 ) else : if not self . is_pyi or not node . parent or not is_stub_suite ( node . parent ) : yield from self . line ( ) yield from self . visit_default ( node )",if self . is_pyi and is_stub_body ( node ) :,if self . is_pyi :,93.88317463,FALSE,93.13
4705,"def rawDataReceived ( self , data ) : if self . timeout > 0 : self . resetTimeout ( ) self . _pendingSize - = len ( data ) if self . _pendingSize > 0 : self . _pendingBuffer . write ( data ) else : passon = b "" "" <MASK> data , passon = data [ : self . _pendingSize ] , data [ self . _pendingSize : ] self . _pendingBuffer . write ( data ) rest = self . _pendingBuffer self . _pendingBuffer = None self . _pendingSize = None rest . seek ( 0 , 0 ) self . _parts . append ( rest . read ( ) ) self . setLineMode ( passon . lstrip ( b "" \r \n "" ) )",if self . _pendingSize < 0 :,if len ( data ) > self . _pendingSize :,91.34940103,FALSE,94.62
4706,"def handle ( self , * args , * * options ) : app_name = options . get ( "" app_name "" ) job_name = options . get ( "" job_name "" ) # hack since we are using job_name nargs='?' for -l to work if app_name and not job_name : job_name = app_name app_name = None if options . get ( "" list_jobs "" ) : print_jobs ( only_scheduled = False , show_when = True , show_appname = True ) else : <MASK> print ( "" Run a single maintenance job. Please specify the name of the job. "" ) return self . runjob ( app_name , job_name , options )",if not job_name :,"if options . get ( ""maintenance_jobs"" ) :",97.5159047,FALSE,93.02
4707,"def _exportReceived ( self , content , error = False , server = None , context = { } , * * kwargs ) : if error : <MASK> self . error . emit ( content [ "" message "" ] , True ) else : self . error . emit ( "" Can ' t export the project from the server "" , True ) self . finished . emit ( ) return self . finished . emit ( )",if content :,"if content [ ""message"" ] :",96.52738982,FALSE,92.54
4708,"def __iter__ ( self ) : n = self . n k = self . k j = int ( np . ceil ( n / k ) ) for i in range ( k ) : test_index = np . zeros ( n , dtype = bool ) <MASK> test_index [ i * j : ( i + 1 ) * j ] = True else : test_index [ i * j : ] = True train_index = np . logical_not ( test_index ) yield train_index , test_index",if i < k - 1 :,if i * j == k :,77.86795282,FALSE,94.52
4709,"def addType ( self , graphene_type ) : meta = get_meta ( graphene_type ) if meta : <MASK> self . _typeMap [ meta . name ] = graphene_type else : raise Exception ( "" Type  {typeName}  already exists in the registry. "" . format ( typeName = meta . name ) ) else : raise Exception ( "" Cannot add unnamed type or a non-type to registry. "" )",if not graphene_type in self . _typeMap :,if meta . name in self . _typeMap :,63.59960998,FALSE,93.91
4710,"def test_len ( self ) : eq = self . assertEqual eq ( base64MIME . base64_len ( "" hello "" ) , len ( base64MIME . encode ( "" hello "" , eol = "" "" ) ) ) for size in range ( 15 ) : if size == 0 : bsize = 0 elif size < = 3 : bsize = 4 <MASK> bsize = 8 elif size < = 9 : bsize = 12 elif size < = 12 : bsize = 16 else : bsize = 20 eq ( base64MIME . base64_len ( "" x "" * size ) , bsize )",elif size <= 6 :,elif size <= 6 :,100,TRUE,100
4711,"def _asStringList ( self , sep = "" "" ) : out = [ ] for item in self . _toklist : <MASK> out . append ( sep ) if isinstance ( item , ParseResults ) : out + = item . _asStringList ( ) else : out . append ( str ( item ) ) return out",if out and sep :,if sep :,92.72509714,FALSE,94.46
4712,"def open_file_input ( cli_parsed ) : files = glob . glob ( os . path . join ( cli_parsed . d , "" *report.html "" ) ) if len ( files ) > 0 : print ( "" \n [*] Done! Report written in the  "" + cli_parsed . d + ""  folder! "" ) print ( "" Would you like to open the report now? [Y/n] "" ) while True : try : response = input ( ) . lower ( ) <MASK> return True else : return strtobool ( response ) except ValueError : print ( "" Please respond with y or n "" ) else : print ( "" [*] No report files found to open, perhaps no hosts were successful "" ) return False","if response == """" :","if response == ""y"" :",99.05235405,FALSE,98.43
4713,"def init_values ( self ) : config = self . _raw_config for valname , value in self . overrides . iteritems ( ) : <MASK> realvalname , key = valname . split ( "" . "" , 1 ) config . setdefault ( realvalname , { } ) [ key ] = value else : config [ valname ] = value for name in config : if name in self . values : self . __dict__ [ name ] = config [ name ] del self . _raw_config","if ""."" in valname :","if ""."" in valname :",100,TRUE,100
4714,"def get_result ( self ) : result_list = [ ] exc_info = None for f in self . children : try : result_list . append ( f . get_result ( ) ) except Exception as e : <MASK> exc_info = sys . exc_info ( ) else : if not isinstance ( e , self . quiet_exceptions ) : app_log . error ( "" Multiple exceptions in yield list "" , exc_info = True ) if exc_info is not None : raise_exc_info ( exc_info ) if self . keys is not None : return dict ( zip ( self . keys , result_list ) ) else : return list ( result_list )",if exc_info is None :,if self . quiet_exceptions :,95.36533779,FALSE,95.75
4715,"def test01e_json ( self ) : "" Testing GeoJSON input/output. "" if not GEOJSON : return for g in self . geometries . json_geoms : geom = OGRGeometry ( g . wkt ) <MASK> self . assertEqual ( g . json , geom . json ) self . assertEqual ( g . json , geom . geojson ) self . assertEqual ( OGRGeometry ( g . wkt ) , OGRGeometry ( geom . json ) )","if not hasattr ( g , ""not_equal"" ) :","if hasattr ( g , ""geojson"" ) :",93.98612106,FALSE,92.45
4716,"def __init__ ( self , hub = None ) : # pylint: disable=unused-argument if resolver . _resolver is None : _resolver = resolver . _resolver = _DualResolver ( ) <MASK> _resolver . network_resolver . nameservers [ : ] = config . resolver_nameservers if config . resolver_timeout : _resolver . network_resolver . lifetime = config . resolver_timeout # Different hubs in different threads could be sharing the same # resolver. assert isinstance ( resolver . _resolver , _DualResolver ) self . _resolver = resolver . _resolver",if config . resolver_nameservers :,if config . resolver_nameservers :,100,TRUE,100
4717,"def __iadd__ ( self , term ) : if isinstance ( term , ( int , long ) ) : <MASK> _gmp . mpz_add_ui ( self . _mpz_p , self . _mpz_p , c_ulong ( term ) ) return self if - 65535 < term < 0 : _gmp . mpz_sub_ui ( self . _mpz_p , self . _mpz_p , c_ulong ( - term ) ) return self term = Integer ( term ) _gmp . mpz_add ( self . _mpz_p , self . _mpz_p , term . _mpz_p ) return self",if 0 <= term < 65536 :,if 0 <= term < 65535 :,98.70807951,FALSE,98.2
4718,"def copy ( dst , src ) : for ( k , v ) in src . iteritems ( ) : <MASK> d = { } dst [ k ] = d copy ( d , v ) else : dst [ k ] = v","if isinstance ( v , dict ) :","if isinstance ( v , dict ) :",100,TRUE,100
4719,"def generator ( self , data ) : self . procs = OrderedDict ( ) for task in data : self . recurse_task ( task , 0 , 0 , self . procs ) for offset , name , level , pid , ppid , uid , euid , gid in self . procs . values ( ) : <MASK> yield ( 0 , [ Address ( offset ) , str ( name ) , str ( level ) , int ( pid ) , int ( ppid ) , int ( uid ) , int ( gid ) , int ( euid ) , ] , )",if offset :,if offset :,100,TRUE,100
4720,"def apply ( self , db , person ) : families = person . get_parent_family_handle_list ( ) if families == [ ] : return True for family_handle in person . get_parent_family_handle_list ( ) : family = db . get_family_from_handle ( family_handle ) <MASK> father_handle = family . get_father_handle ( ) mother_handle = family . get_mother_handle ( ) if not father_handle : return True if not mother_handle : return True return False",if family :,if family . get_parent_family_handle ( ) in families :,94.25971018,FALSE,89.31
4721,"def _arctic_task_exec ( request ) : request . start_time = time . time ( ) logging . debug ( "" Executing asynchronous request for  {} / {} "" . format ( request . library , request . symbol ) ) result = None try : request . is_running = True <MASK> result = mongo_retry ( request . fun ) ( * request . args , * * request . kwargs ) else : result = request . fun ( * request . args , * * request . kwargs ) except Exception as e : request . exception = e finally : request . data = result request . end_time = time . time ( ) request . is_running = False return result",if request . mongo_retry :,if request . retry :,86.51115114,FALSE,97.57
4722,"def _setup_styles ( self ) : for ttype , ndef in self . style : escape = EscapeSequence ( ) <MASK> escape . fg = self . _color_index ( ndef [ "" color "" ] ) if ndef [ "" bgcolor "" ] : escape . bg = self . _color_index ( ndef [ "" bgcolor "" ] ) if self . usebold and ndef [ "" bold "" ] : escape . bold = True if self . useunderline and ndef [ "" underline "" ] : escape . underline = True self . style_string [ str ( ttype ) ] = ( escape . color_string ( ) , escape . reset_string ( ) )","if ndef [ ""color"" ] :","if ndef [ ""color"" ] :",100,TRUE,100
4723,"def process_string ( self , remove_repetitions , sequence ) : string = "" "" for i , char in enumerate ( sequence ) : if char != self . int_to_char [ self . blank_index ] : # if this char is a repetition and remove_repetitions=true, # skip. if remove_repetitions and i != 0 and char == sequence [ i - 1 ] : pass <MASK> string + = ""   "" else : string = string + char return string",elif char == self . labels [ self . space_index ] :,elif remove_repetitions and i == 0 :,93.83763583,FALSE,87.92
4724,"def arith_expr ( self , nodelist ) : node = self . com_node ( nodelist [ 0 ] ) for i in range ( 2 , len ( nodelist ) , 2 ) : right = self . com_node ( nodelist [ i ] ) <MASK> node = Add ( node , right , lineno = nodelist [ 1 ] . context ) elif nodelist [ i - 1 ] . type == token . MINUS : node = Sub ( node , right , lineno = nodelist [ 1 ] . context ) else : raise ValueError ( "" unexpected token:  %s "" % nodelist [ i - 1 ] [ 0 ] ) return node",if nodelist [ i - 1 ] . type == token . PLUS :,if nodelist [ i - 1 ] . type == token . MINUS :,85.56921034,FALSE,98.18
4725,"def invert_index ( cls , index , length ) : if np . isscalar ( index ) : return length - index elif isinstance ( index , slice ) : start , stop = index . start , index . stop new_start , new_stop = None , None <MASK> new_stop = length - start if stop is not None : new_start = length - stop return slice ( new_start - 1 , new_stop - 1 ) elif isinstance ( index , Iterable ) : new_index = [ ] for ind in index : new_index . append ( length - ind ) return new_index",if start is not None :,if start is not None :,100,TRUE,100
4726,"def getRoots ( job ) : if job not in visited : visited . add ( job ) <MASK> list ( map ( lambda p : getRoots ( p ) , job . _directPredecessors ) ) else : roots . add ( job ) # The following call ensures we explore all successor edges. list ( map ( lambda c : getRoots ( c ) , job . _children + job . _followOns ) )",if len ( job . _directPredecessors ) > 0 :,if job . _directPredecessors :,61.59680484,FALSE,90.72
4727,"def visit_filter_projection ( self , node , value ) : base = self . visit ( node [ "" children "" ] [ 0 ] , value ) if not isinstance ( base , list ) : return None comparator_node = node [ "" children "" ] [ 2 ] collected = [ ] for element in base : <MASK> current = self . visit ( node [ "" children "" ] [ 1 ] , element ) if current is not None : collected . append ( current ) return collected","if self . _is_true ( self . visit ( comparator_node , element ) ) :","if self . visit ( comparator_node , element ) :",93.85951195,FALSE,92.04
4728,"def func ( x , y ) : try : if x > y : z = x + 2 * math . sin ( y ) return z * * 2 <MASK> return 4 else : return 2 * * 3 except ValueError : foo = 0 for i in range ( 4 ) : foo + = i return foo except TypeError : return 42 else : return 33 finally : print ( "" finished "" )",elif x == y :,elif y > y :,96.34130442,FALSE,94.53
4729,"def set_filter ( self , dataset_opt ) : """"""This function create and set the pre_filter to the obj as attributes"""""" self . pre_filter = None for key_name in dataset_opt . keys ( ) : <MASK> new_name = key_name . replace ( "" filters "" , "" filter "" ) try : filt = instantiate_filters ( getattr ( dataset_opt , key_name ) ) except Exception : log . exception ( "" Error trying to create  {} ,  {} "" . format ( new_name , getattr ( dataset_opt , key_name ) ) ) continue setattr ( self , new_name , filt )","if ""filter"" in key_name :","if key_name . startswith ( ""filters"" ) :",93.85989064,FALSE,93.52
4730,"def _add_states_to_lookup ( self , trackers_as_states , trackers_as_actions , domain , online = False ) : """"""Add states to lookup dict"""""" for states in trackers_as_states : active_form = self . _get_active_form_name ( states [ - 1 ] ) <MASK> # modify the states states = self . _modified_states ( states ) feature_key = self . _create_feature_key ( states ) # even if there are two identical feature keys # their form will be the same # because of `active_form_...` feature self . lookup [ feature_key ] = active_form",if active_form and self . _prev_action_listen_in_state ( states [ - 1 ] ) :,if active_form :,86.12396312,FALSE,86.46
4731,"def list_loaded_payloads ( self ) : print ( helpers . color ( "" \n  [*] Available Payloads: \n "" ) ) lastBase = None x = 1 for name in sorted ( self . active_payloads . keys ( ) ) : parts = name . split ( "" / "" ) <MASK> print ( ) lastBase = parts [ 0 ] print ( "" \t %s ) \t %s "" % ( x , "" {0: <24} "" . format ( name ) ) ) x + = 1 print ( "" \n "" ) return",if lastBase and parts [ 0 ] != lastBase :,if lastBase is not None and parts [ 0 ] != lastBase :,96.34027989,FALSE,96.46
4732,"def reprSmart ( vw , item ) : ptype = type ( item ) if ptype is int : if - 1024 < item < 1024 : return str ( item ) <MASK> return vw . reprPointer ( item ) else : return hex ( item ) elif ptype in ( list , tuple ) : return reprComplex ( vw , item ) # recurse elif ptype is dict : return "" { %s } "" % "" , "" . join ( [ "" %s : %s "" % ( reprSmart ( vw , k ) , reprSmart ( vw , v ) ) for k , v in item . items ( ) ] ) else : return repr ( item )",elif vw . isValidPointer ( item ) :,elif - 1 < item < 1 :,91.14653105,FALSE,94.51
4733,"def ConfigSectionMap ( section ) : config = ConfigParser . RawConfigParser ( ) configurations = config_manager ( ) # Class from mkchromecast.config configf = configurations . configf config . read ( configf ) dict1 = { } options = config . options ( section ) for option in options : try : dict1 [ option ] = config . get ( section , option ) <MASK> DebugPrint ( "" skip:  %s "" % option ) except : print ( "" Exception on  %s ! "" % option ) dict1 [ option ] = None return dict1",if dict1 [ option ] == - 1 :,if dict1 [ option ] is None :,97.84308048,FALSE,95.01
4734,"def on_success ( result ) : subtasks = { } if result : subtasks = { self . nodes_keys . inverse [ s [ "" node_id "" ] ] : s . get ( "" subtask_id "" ) for s in result <MASK> } if subtasks : print ( "" subtask finished "" ) self . next ( ) else : print ( "" waiting for a subtask to finish "" ) time . sleep ( 10 )","if s . get ( ""status"" ) == ""Failure""","if s [ ""subtask_id"" ] not in self . nodes_keys . inverse",84.97864581,FALSE,83.44
4735,"def redirect_aware_commmunicate ( p , sys = _sys ) : """"""Variant of process.communicate that works with in process I/O redirection."""""" assert sys is not None out , err = p . communicate ( ) if redirecting_io ( sys = sys ) : if out : # We don't unicodify in Python2 because sys.stdout may be a # cStringIO.StringIO object, which does not accept Unicode strings out = unicodify ( out ) sys . stdout . write ( out ) out = None <MASK> err = unicodify ( err ) sys . stderr . write ( err ) err = None return out , err",if err :,if err :,100,TRUE,100
4736,"def __exit__ ( self , * args , * * kwargs ) : self . _samples_cache = { } if is_validation_enabled ( ) and isinstance ( self . prior , dict ) : extra = set ( self . prior ) - self . _param_hits <MASK> warnings . warn ( "" pyro.module prior did not find params [ ' {} ' ].  "" "" Did you instead mean one of [ ' {} ' ]? "" . format ( "" ' ,  ' "" . join ( extra ) , "" ' ,  ' "" . join ( self . _param_misses ) ) ) return super ( ) . __exit__ ( * args , * * kwargs )",if extra :,if extra :,100,TRUE,100
4737,def __download_thread ( self ) : while True : <MASK> self . __current_download = self . __queue . get ( ) self . __download_file ( self . __current_download ) time . sleep ( 0.1 ),if not self . __queue . empty ( ) :,if self . __current_download is None :,84.27498441,FALSE,85.47
4738,"def plot_timer_command ( args ) : import nnabla . monitor as M format_unit = dict ( s = "" seconds "" , m = "" minutes "" , h = "" hours "" , d = "" days "" , ) if not args . ylabel : <MASK> args . ylabel = "" Total elapsed time [ {} ] "" . format ( format_unit [ args . time_unit ] ) else : args . ylabel = "" Elapsed time [ {} /iter] "" . format ( format_unit [ args . time_unit ] ) plot_any_command ( args , M . plot_time_elapsed , dict ( elapsed = args . elapsed , unit = args . time_unit ) ) return True",if args . elapsed :,if args . elapsed :,100,TRUE,100
4739,"def resolve_page ( root : ChannelContext [ models . MenuItem ] , info , * * kwargs ) : if root . node . page_id : requestor = get_user_or_app_from_context ( info . context ) requestor_has_access_to_all = requestor . is_active and requestor . has_perm ( PagePermissions . MANAGE_PAGES ) return ( PageByIdLoader ( info . context ) . load ( root . node . page_id ) . then ( lambda page : page <MASK> else None ) ) return None",if requestor_has_access_to_all or page . is_visible,if requestor_has_access_to_all,88.10502695,FALSE,93.81
4740,"def find ( self , pattern ) : """"""Find pages in database."""""" results = self . _search_keyword ( pattern ) pat = re . compile ( "" (.*?)( %s )(.*?)(  \ (.* \ ))?$ "" % re . escape ( pattern ) , re . I ) if results : for name , keyword , url in results : <MASK> keyword = pat . sub ( r "" \ 1 \ 033[1;31m \ 2 \ 033[0m \ 3 \ 033[1;33m \ 4 \ 033[0m "" , keyword ) print ( "" %s  -  %s "" % ( keyword , name ) ) else : raise RuntimeError ( "" %s : nothing appropriate. "" % pattern )",if os . isatty ( sys . stdout . fileno ( ) ) :,"if url . startswith ( ""http://www.w3.org/1999/02",57.90305148,FALSE,89.57
4741,"def _certonly_new_request_common ( self , mock_client , args = None ) : with mock . patch ( "" certbot._internal.main._find_lineage_for_domains_and_certname "" ) as mock_renewal : mock_renewal . return_value = ( "" newcert "" , None ) with mock . patch ( "" certbot._internal.main._init_le_client "" ) as mock_init : mock_init . return_value = mock_client <MASK> args = [ ] args + = "" -d foo.bar -a standalone certonly "" . split ( ) self . _call ( args )",if args is None :,if args is None :,100,TRUE,100
4742,"def __init__ ( self , * args , * * kw ) : if len ( args ) > 1 : raise TypeError ( "" MultiDict can only be called with one positional  "" "" argument "" ) if args : <MASK> items = list ( args [ 0 ] . iteritems ( ) ) elif hasattr ( args [ 0 ] , "" items "" ) : items = list ( args [ 0 ] . items ( ) ) else : items = list ( args [ 0 ] ) self . _items = items else : self . _items = [ ] if kw : self . _items . extend ( kw . items ( ) )","if hasattr ( args [ 0 ] , ""iteritems"" ) :","if hasattr ( args [ 0 ] , ""iteritems"" ) :",100,TRUE,100
4743,"def test08_ExceptionTypes ( self ) : self . assertTrue ( issubclass ( db . DBError , Exception ) ) for i , j in db . __dict__ . items ( ) : <MASK> self . assertTrue ( issubclass ( j , db . DBError ) , msg = i ) if i not in ( "" DBKeyEmptyError "" , "" DBNotFoundError "" ) : self . assertFalse ( issubclass ( j , KeyError ) , msg = i ) # This two exceptions have two bases self . assertTrue ( issubclass ( db . DBKeyEmptyError , KeyError ) ) self . assertTrue ( issubclass ( db . DBNotFoundError , KeyError ) )","if i . startswith ( ""DB"" ) and i . endswith ( ""Error"" ) :","if issubclass ( j , Exception ) :",85.81316249,FALSE,86.92
4744,"def _delegate_to_sinks ( self , value : Any ) - > None : for sink in self . _sinks : if isinstance ( sink , AgentT ) : await sink . send ( value = value ) <MASK> await cast ( TopicT , sink ) . send ( value = value ) else : await maybe_async ( cast ( Callable , sink ) ( value ) )","elif isinstance ( sink , ChannelT ) :","elif isinstance ( sink , TopicT ) :",73.2858975,FALSE,96.86
4745,"def _select_block ( str_in , start_tag , end_tag ) : """"""Select first block delimited by start_tag and end_tag"""""" start_pos = str_in . find ( start_tag ) if start_pos < 0 : raise ValueError ( "" start_tag not found "" ) depth = 0 for pos in range ( start_pos , len ( str_in ) ) : if str_in [ pos ] == start_tag : depth + = 1 <MASK> depth - = 1 if depth == 0 : break sel = str_in [ start_pos + 1 : pos ] return sel",elif str_in [ pos ] == end_tag :,if str_in [ pos ] == end_tag :,94.13316637,FALSE,98.26
4746,"def confirm ( request ) : details = request . session . get ( "" reauthenticate "" ) if not details : return redirect ( "" home "" ) # Monkey patch request request . user = User . objects . get ( pk = details [ "" user_pk "" ] ) if request . method == "" POST "" : confirm_form = PasswordConfirmForm ( request , request . POST ) <MASK> request . session . pop ( "" reauthenticate "" ) request . session [ "" reauthenticate_done "" ] = True return redirect ( "" social:complete "" , backend = details [ "" backend "" ] ) else : confirm_form = PasswordConfirmForm ( request ) context = { "" confirm_form "" : confirm_form } context . update ( details ) return render ( request , "" accounts/confirm.html "" , context )",if confirm_form . is_valid ( ) :,if confirm_form . is_valid ( ) :,100,TRUE,100
4747,"def verify_credentials ( self ) : if self . enabled : response = requests . get ( "" https://api.exotel.com/v1/Accounts/ {sid} "" . format ( sid = self . account_sid ) , auth = ( self . api_key , self . api_token ) , ) <MASK> frappe . throw ( _ ( "" Invalid credentials "" ) )",if response . status_code != 200 :,if response . status_code != 401 :,97.95637532,FALSE,97.12
4748,"def pixbufrenderer ( self , column , crp , model , it ) : tok = model . get_value ( it , 0 ) if tok . type == "" class "" : icon = "" class "" else : <MASK> icon = "" method_priv "" elif tok . visibility == "" protected "" : icon = "" method_prot "" else : icon = "" method "" crp . set_property ( "" pixbuf "" , imagelibrary . pixbufs [ icon ] )","if tok . visibility == ""private"" :","if tok . visibility == ""private"" :",100,TRUE,100
4749,"def _omit_keywords ( self , context ) : omitted_kws = 0 for event , elem in context : # Teardowns aren't omitted to allow checking suite teardown status. omit = elem . tag == "" kw "" and elem . get ( "" type "" ) != "" teardown "" start = event == "" start "" if omit and start : omitted_kws + = 1 if not omitted_kws : yield event , elem <MASK> elem . clear ( ) if omit and not start : omitted_kws - = 1",elif not start :,if omit and start :,97.53044534,FALSE,95.8
4750,"def on_double_click ( self , event ) : # TODO: don't act when the click happens below last item path = self . get_selected_path ( ) kind = self . get_selected_kind ( ) name = self . get_selected_name ( ) if kind == "" file "" : <MASK> self . open_file ( path ) else : self . open_path_with_system_app ( path ) elif kind == "" dir "" : self . request_focus_into ( path ) return "" break """,if self . should_open_name_in_thonny ( name ) :,if self . is_file ( path ) :,72.49944151,FALSE,90.36
4751,"def search_cve ( db : DatabaseInterface , product : Product ) - > dict : result = { } for query_result in db . fetch_multiple ( QUERIES [ "" cve_lookup "" ] ) : cve_entry = CveDbEntry ( * query_result ) <MASK> result [ cve_entry . cve_id ] = { "" score2 "" : cve_entry . cvss_v2_score , "" score3 "" : cve_entry . cvss_v3_score , "" cpe_version "" : build_version_string ( cve_entry ) , } return result","if _product_matches_cve ( product , cve_entry ) :",if cve_entry . product == product :,90.17380075,FALSE,90.04
4752,"def find_go_files_mtime ( app_files ) : files , mtime = [ ] , 0 for f , mt in app_files . items ( ) : if not f . endswith ( "" .go "" ) : continue <MASK> continue files . append ( f ) mtime = max ( mtime , mt ) return files , mtime",if APP_CONFIG . nobuild_files . match ( f ) :,if not os . path . exists ( f ) :,92.62774727,FALSE,87.35
4753,"def wrapper ( filename ) : mtime = getmtime ( filename ) with lock : <MASK> old_mtime , result = cache . pop ( filename ) if old_mtime == mtime : # Move to the end cache [ filename ] = old_mtime , result return result result = function ( filename ) with lock : cache [ filename ] = mtime , result # at the end if len ( cache ) > max_size : cache . popitem ( last = False ) return result",if filename in cache :,if filename in cache :,100,TRUE,100
4754,"def Tokenize ( s ) : # type: (str) -> Iterator[Token] for item in TOKEN_RE . findall ( s ) : # The type checker can't know the true type of item! item = cast ( TupleStr4 , item ) <MASK> typ = "" number "" val = item [ 0 ] elif item [ 1 ] : typ = "" name "" val = item [ 1 ] elif item [ 2 ] : typ = item [ 2 ] val = item [ 2 ] elif item [ 3 ] : typ = item [ 3 ] val = item [ 3 ] yield Token ( typ , val )",if item [ 0 ] :,if item [ 0 ] :,75,TRUE,100
4755,"def _show_encoders ( self , * args , * * kwargs ) : if issubclass ( self . current_module . __class__ , BasePayload ) : encoders = self . current_module . get_encoders ( ) <MASK> headers = ( "" Encoder "" , "" Name "" , "" Description "" ) print_table ( headers , * encoders , max_column_length = 100 ) return print_error ( "" No encoders available "" )",if encoders :,if encoders :,100,TRUE,100
4756,"def __init__ ( self ) : Builder . __init__ ( self , commandName = "" VCExpress.exe "" , formatName = "" msvcProject "" ) for key in [ "" VS90COMNTOOLS "" , "" VC80COMNTOOLS "" , "" VC71COMNTOOLS "" ] : <MASK> self . programDir = os . path . join ( os . environ [ key ] , "" .. "" , "" IDE "" ) if self . programDir is None : for version in [ "" 9.0 "" , "" 8 "" , "" .NET 2003 "" ] : msvcDir = ( "" C: \\ Program Files \\ Microsoft Visual Studio  %s \\ Common7 \\ IDE "" % version ) if os . path . exists ( msvcDir ) : self . programDir = msvcDir",if os . environ . has_key ( key ) :,if os . path . exists ( os . environ [ key ] ) :,93.44524258,FALSE,93.67
4757,"def _inner ( * args , * * kwargs ) : component_manager = args [ 0 ] . component_manager for condition_name in condition_names : condition_result , err_msg = component_manager . evaluate_condition ( condition_name ) <MASK> raise ComponentStartConditionNotMetError ( err_msg ) if not component_manager . all_components_running ( * components ) : raise ComponentsNotStartedError ( f "" the following required components have not yet started:  { json . dumps ( components ) } "" ) return method ( * args , * * kwargs )",if not condition_result :,if not condition_result :,100,TRUE,100
4758,"def _gridconvvalue ( self , value ) : if isinstance ( value , ( str , _tkinter . Tcl_Obj ) ) : try : svalue = str ( value ) if not svalue : return None <MASK> return self . tk . getdouble ( svalue ) else : return self . tk . getint ( svalue ) except ( ValueError , TclError ) : pass return value","elif ""."" in svalue :","if svalue == ""Double"" :",64.12711475,FALSE,89.77
4759,"def check_songs ( ) : desc = numeric_phrase ( "" %d  song "" , "" %d  songs "" , len ( songs ) ) with Task ( _ ( "" Rescan songs "" ) , desc ) as task : task . copool ( check_songs ) for i , song in enumerate ( songs ) : song = song . _song <MASK> app . library . reload ( song ) task . update ( ( float ( i ) + 1 ) / len ( songs ) ) yield",if song in app . library :,if song . _is_reload :,93.39752198,FALSE,93.93
4760,"def initialize ( self ) : nn . init . xavier_uniform_ ( self . linear . weight . data ) if self . linear . bias is not None : self . linear . bias . data . uniform_ ( - 1.0 , 1.0 ) if self . self_layer : nn . init . xavier_uniform_ ( self . linear_self . weight . data ) <MASK> self . linear_self . bias . data . uniform_ ( - 1.0 , 1.0 )",if self . linear_self . bias is not None :,if self . linear_self . bias is not None :,100,TRUE,100
4761,"def test_row ( self , row ) : for idx , test in self . patterns . items ( ) : try : value = row [ idx ] except IndexError : value = "" "" result = test ( value ) <MASK> if result : return not self . inverse # True else : if not result : return self . inverse # False <MASK> return self . inverse # False else : return not self . inverse # True",if self . any_match :,elif self . inverse :,66.35170162,FALSE,85.39
4762,"def toterminal ( self , tw ) : for element in self . chain : element [ 0 ] . toterminal ( tw ) <MASK> tw . line ( "" "" ) tw . line ( element [ 2 ] , yellow = True ) super ( ExceptionChainRepr , self ) . toterminal ( tw )",if element [ 2 ] is not None :,if element [ 1 ] :,90.73681077,FALSE,89.92
4763,"def runMainLoop ( self ) : """"""The curses gui main loop."""""" # pylint: disable=no-member # # Do NOT change g.app! self . curses_app = LeoApp ( ) stdscr = curses . initscr ( ) if 1 : # Must follow initscr. self . dump_keys ( ) try : self . curses_app . run ( ) # run calls CApp.main(), which calls CGui.run(). finally : curses . nocbreak ( ) stdscr . keypad ( 0 ) curses . echo ( ) curses . endwin ( ) <MASK> g . pr ( "" Exiting Leo... "" )","if ""shutdown"" in g . app . debug :",if g . get_debug ( ) :,71.02677485,FALSE,93.02
4764,"def test_chunkcoding ( self ) : for native , utf8 in zip ( * [ StringIO ( f ) . readlines ( ) for f in self . tstring ] ) : u = self . decode ( native ) [ 0 ] self . assertEqual ( u , utf8 . decode ( "" utf-8 "" ) ) <MASK> self . assertEqual ( native , self . encode ( u ) [ 0 ] )",if self . roundtriptest :,if PY3 :,68.33332534,FALSE,94.32
4765,"def reload_sanitize_allowlist ( self , explicit = True ) : self . sanitize_allowlist = [ ] try : with open ( self . sanitize_allowlist_file ) as f : for line in f . readlines ( ) : <MASK> self . sanitize_allowlist . append ( line . strip ( ) ) except OSError : if explicit : log . warning ( "" Sanitize log file explicitly specified as  ' %s '  but does not exist, continuing with no tools allowlisted. "" , self . sanitize_allowlist_file , )","if not line . startswith ( ""#"" ) :","if line . startswith ( ""Allow:"" ) :",92.27687867,FALSE,95.55
4766,"def get_all_extensions ( subtree = None ) : if subtree is None : subtree = full_extension_tree ( ) result = [ ] if isinstance ( subtree , dict ) : for value in subtree . values ( ) : if isinstance ( value , dict ) : result + = get_all_extensions ( value ) <MASK> result + = value . extensions elif isinstance ( value , ( list , tuple ) ) : result + = value elif isinstance ( subtree , ( ContentTypeMapping , ContentTypeDetector ) ) : result = subtree . extensions elif isinstance ( subtree , ( list , tuple ) ) : result = subtree return result","elif isinstance ( value , ( ContentTypeMapping , ContentTypeDetector ) ) :","elif isinstance ( value , ContentTypeMapping ) :",94.34515514,FALSE,95.26
4767,"def _configuration_dict_to_commandlist ( name , config_dict ) : command_list = [ "" config: %s "" % name ] for key , value in config_dict . items ( ) : <MASK> if value : b = "" true "" else : b = "" false "" command_list . append ( "" %s : %s "" % ( key , b ) ) else : command_list . append ( "" %s : %s "" % ( key , value ) ) return command_list",if type ( value ) is bool :,"if key == ""config_type"" :",91.55590983,FALSE,91.83
4768,"def _RewriteModinfo ( self , modinfo , obj_kernel_version , this_kernel_version , info_strings = None , to_remove = None , ) : new_modinfo = "" "" for line in modinfo . split ( "" \x00 "" ) : <MASK> continue if to_remove and line . split ( "" = "" ) [ 0 ] == to_remove : continue if info_strings is not None : info_strings . add ( line . split ( "" = "" ) [ 0 ] ) if line . startswith ( "" vermagic "" ) : line = line . replace ( obj_kernel_version , this_kernel_version ) new_modinfo + = line + "" \x00 "" return new_modinfo",if not line :,if not line :,100,TRUE,100
4769,"def zip_random_open_test ( self , f , compression ) : self . make_test_archive ( f , compression ) # Read the ZIP archive with zipfile . ZipFile ( f , "" r "" , compression ) as zipfp : zipdata1 = [ ] with zipfp . open ( TESTFN ) as zipopen1 : while True : read_data = zipopen1 . read ( randint ( 1 , 1024 ) ) <MASK> break zipdata1 . append ( read_data ) testdata = "" "" . join ( zipdata1 ) self . assertEqual ( len ( testdata ) , len ( self . data ) ) self . assertEqual ( testdata , self . data )",if not read_data :,if not read_data :,100,TRUE,100
4770,"def _memoized ( * args ) : now = time . time ( ) try : value , last_update = self . cache [ args ] age = now - last_update <MASK> self . _call_count = 0 raise AttributeError if self . ctl : self . _call_count + = 1 return value except ( KeyError , AttributeError ) : value = func ( * args ) if value : self . cache [ args ] = ( value , now ) return value except TypeError : return func ( * args )",if self . _call_count > self . ctl or age > self . ttl :,if age <= 0 :,69.46834242,FALSE,85.05
4771,"def on_data ( res ) : if terminate . is_set ( ) : return if args . strings and not args . no_content : if type ( res ) == tuple : f , v = res <MASK> f = f . encode ( "" utf-8 "" ) if type ( v ) == unicode : v = v . encode ( "" utf-8 "" ) self . success ( "" {} :  {} "" . format ( f , v ) ) elif not args . content_only : self . success ( res ) else : self . success ( res )",if type ( f ) == unicode :,if type ( f ) == unicode :,100,TRUE,100
4772,"def _finalize_setup_keywords ( self ) : for ep in pkg_resources . iter_entry_points ( "" distutils.setup_keywords "" ) : value = getattr ( self , ep . name , None ) <MASK> ep . require ( installer = self . fetch_build_egg ) ep . load ( ) ( self , ep . name , value )",if value is not None :,if value is not None :,100,TRUE,100
4773,"def test_attributes_types ( self ) : if not self . connection . strategy . pooled : <MASK> self . connection . refresh_server_info ( ) self . assertEqual ( type ( self . connection . server . schema . attribute_types [ "" cn "" ] ) , AttributeTypeInfo )",if not self . connection . server . info :,if self . connection . server . schema . attribute_types :,63.29427752,FALSE,88.2
4774,"def to_key ( literal_or_identifier ) : """"""returns string representation of this object"""""" if literal_or_identifier [ "" type "" ] == "" Identifier "" : return literal_or_identifier [ "" name "" ] elif literal_or_identifier [ "" type "" ] == "" Literal "" : k = literal_or_identifier [ "" value "" ] if isinstance ( k , float ) : return unicode ( float_repr ( k ) ) elif "" regex "" in literal_or_identifier : return compose_regex ( k ) <MASK> return "" true "" if k else "" false "" elif k is None : return "" null "" else : return unicode ( k )","elif isinstance ( k , bool ) :","elif isinstance ( k , bool ) :",100,TRUE,100
4775,"def list2rec ( x , test = False ) : if test : vid = "" {} _ {:06d} _ {:06d} "" . format ( x [ 0 ] , int ( x [ 1 ] ) , int ( x [ 2 ] ) ) label = - 1 # label unknown return vid , label else : vid = "" {} _ {:06d} _ {:06d} "" . format ( x [ 1 ] , int ( x [ 2 ] ) , int ( x [ 3 ] ) ) <MASK> vid = "" {} / {} "" . format ( convert_label ( x [ 0 ] ) , vid ) else : assert level == 1 label = class_mapping [ convert_label ( x [ 0 ] ) ] return vid , label",if level == 2 :,if level == 0 :,99.09493808,FALSE,98.44
4776,"def _expand_env ( self , snapcraft_yaml ) : environment_keys = [ "" name "" , "" version "" ] for key in snapcraft_yaml : <MASK> continue replacements = environment_to_replacements ( get_snapcraft_global_environment ( self . project ) ) snapcraft_yaml [ key ] = replace_attr ( snapcraft_yaml [ key ] , replacements ) return snapcraft_yaml",if any ( ( key == env_key for env_key in environment_keys ) ) :,if key not in environment_keys :,76.72085577,FALSE,81.26
4777,"def enableCtrls ( self ) : # Check if each ctrl has a requirement or an incompatibility, # look it up, and enable/disable if so for data in self . storySettingsData : name = data [ "" name "" ] <MASK> if "" requires "" in data : set = self . getSetting ( data [ "" requires "" ] ) for i in self . ctrls [ name ] : i . Enable ( set not in [ "" off "" , "" false "" , "" 0 "" ] )",if name in self . ctrls :,if name in self . ctrls :,75,TRUE,100
4778,"def __init__ ( self , * args , * * kwargs ) : super ( ChallengePhaseCreateSerializer , self ) . __init__ ( * args , * * kwargs ) context = kwargs . get ( "" context "" ) if context : challenge = context . get ( "" challenge "" ) <MASK> kwargs [ "" data "" ] [ "" challenge "" ] = challenge . pk test_annotation = context . get ( "" test_annotation "" ) if test_annotation : kwargs [ "" data "" ] [ "" test_annotation "" ] = test_annotation",if challenge :,if challenge :,100,TRUE,100
4779,def set_inactive ( self ) : for title in self . gramplet_map : if self . gramplet_map [ title ] . pui : <MASK> self . gramplet_map [ title ] . pui . active = False,"if self . gramplet_map [ title ] . gstate != ""detached"" :",if self . gramplet_map [ title ] . pui . active :,90.44245524,FALSE,87.11
4780,"def authenticate ( username , password ) : try : u = User . objects . get ( username = username ) <MASK> userLogger . info ( "" User logged in :  %s "" , username ) return u else : userLogger . warn ( "" Attempt to log in to :  %s "" , username ) return False except DoesNotExist : return False","if check_password_hash ( u . password , password ) :",if u . verify ( password ) :,84.57512318,FALSE,86.21
4781,def _check_date ( self ) : if not self . value : return None if not self . allow_date_in_past : if self . value < self . date_or_datetime ( ) . today ( ) : <MASK> self . value = self . date_or_datetime ( ) . today ( ) else : self . value = self . date_or_datetime ( ) . today ( ) + datetime . timedelta ( 1 ),if self . allow_todays_date :,if self . value > self . date_or_datetime ( ) . today ( ) :,91.06980266,FALSE,85.43
4782,"def update ( self , E = None , * * F ) : if E : <MASK> # Update with `E` dictionary for k in E : self [ k ] = E [ k ] else : # Update with `E` items for ( k , v ) in E : self [ k ] = v # Update with `F` dictionary for k in F : self [ k ] = F [ k ]","if hasattr ( E , ""keys"" ) :","if isinstance ( E , dict ) :",88.19970188,FALSE,92.3
4783,"def _get_quota_availability ( self ) : quotas_ok = defaultdict ( int ) qa = QuotaAvailability ( ) qa . queue ( * [ k for k , v in self . _quota_diff . items ( ) if v > 0 ] ) qa . compute ( now_dt = self . now_dt ) for quota , count in self . _quota_diff . items ( ) : <MASK> quotas_ok [ quota ] = 0 break avail = qa . results [ quota ] if avail [ 1 ] is not None and avail [ 1 ] < count : quotas_ok [ quota ] = min ( count , avail [ 1 ] ) else : quotas_ok [ quota ] = count return quotas_ok",if count <= 0 :,if count == 0 :,98.99749334,FALSE,98.32
4784,"def gen_env_vars ( ) : for fd_id , fd in zip ( STDIO_DESCRIPTORS , ( stdin , stdout , stderr ) ) : is_atty = fd . isatty ( ) yield ( cls . TTY_ENV_TMPL . format ( fd_id ) , cls . encode_env_var_value ( int ( is_atty ) ) ) <MASK> yield ( cls . TTY_PATH_ENV . format ( fd_id ) , os . ttyname ( fd . fileno ( ) ) or b "" "" )",if is_atty :,if fd . fileno ( ) :,69.04657656,FALSE,94.21
4785,"def _convertDict ( self , d ) : r = { } for k , v in d . items ( ) : if isinstance ( v , bytes ) : v = str ( v , "" utf-8 "" ) elif isinstance ( v , list ) or isinstance ( v , tuple ) : v = self . _convertList ( v ) elif isinstance ( v , dict ) : v = self . _convertDict ( v ) <MASK> k = str ( k , "" utf-8 "" ) r [ k ] = v return r","if isinstance ( k , bytes ) :","elif isinstance ( k , bytes ) :",94.60233684,FALSE,97.66
4786,"def get_attribute_value ( self , nodeid , attr ) : with self . _lock : self . logger . debug ( "" get attr val:  %s   %s "" , nodeid , attr ) if nodeid not in self . _nodes : dv = ua . DataValue ( ) dv . StatusCode = ua . StatusCode ( ua . StatusCodes . BadNodeIdUnknown ) return dv node = self . _nodes [ nodeid ] <MASK> dv = ua . DataValue ( ) dv . StatusCode = ua . StatusCode ( ua . StatusCodes . BadAttributeIdInvalid ) return dv attval = node . attributes [ attr ] if attval . value_callback : return attval . value_callback ( ) return attval . value",if attr not in node . attributes :,if attr not in node . attributes :,100,TRUE,100
4787,"def conninfo_parse ( dsn ) : ret = { } length = len ( dsn ) i = 0 while i < length : <MASK> i + = 1 continue param_match = PARAMETER_RE . match ( dsn [ i : ] ) if not param_match : return param = param_match . group ( 1 ) i + = param_match . end ( ) if i > = length : return value , end = read_param_value ( dsn [ i : ] ) if value is None : return i + = end ret [ param ] = value return ret",if dsn [ i ] . isspace ( ) :,"if dsn [ i ] == ""mysql"" :",94.12162645,FALSE,94.77
4788,"def connect ( self , buttons ) : for button in buttons : assert button is not None handled = False for handler_idx in range ( 0 , len ( self . __signal_handlers ) ) : ( obj_class , signal , handler , handler_id ) = self . __signal_handlers [ handler_idx ] <MASK> handler_id = button . connect ( signal , handler ) handled = True self . __signal_handlers [ handler_idx ] = ( obj_class , signal , handler , handler_id , ) assert handled","if isinstance ( button , obj_class ) :",if obj_class is None :,87.2557312,FALSE,93.32
4789,"def _parse_display ( display ) : """"""Parse an X11 display value"""""" try : host , dpynum = display . rsplit ( "" : "" , 1 ) if host . startswith ( "" [ "" ) and host . endswith ( "" ] "" ) : host = host [ 1 : - 1 ] idx = dpynum . find ( "" . "" ) <MASK> screen = int ( dpynum [ idx + 1 : ] ) dpynum = dpynum [ : idx ] else : screen = 0 except ( ValueError , UnicodeEncodeError ) : raise ValueError ( "" Invalid X11 display "" ) from None return host , dpynum , screen",if idx >= 0 :,if idx >= 0 :,100,TRUE,100
4790,"def delete_all ( path ) : ppath = os . getcwd ( ) os . chdir ( path ) for fn in glob . glob ( "" * "" ) : fn_full = os . path . join ( path , fn ) if os . path . isdir ( fn ) : delete_all ( fn_full ) <MASK> os . remove ( fn_full ) elif fn . endswith ( "" .md "" ) : os . remove ( fn_full ) elif DELETE_ALL_OLD : os . remove ( fn_full ) os . chdir ( ppath ) os . rmdir ( path )","elif fn . endswith ( "".png"" ) :","elif fn . endswith ( "".py"" ) :",98.82942566,FALSE,98.04
4791,"def _sync_get ( self , identifier , * args , * * kw ) : self . _mutex . acquire ( ) try : try : <MASK> return self . _values [ identifier ] else : self . _values [ identifier ] = value = self . creator ( identifier , * args , * * kw ) return value except KeyError : self . _values [ identifier ] = value = self . creator ( identifier , * args , * * kw ) return value finally : self . _mutex . release ( )",if identifier in self . _values :,if identifier in self . _values :,100,TRUE,100
4792,"def _query_fd ( self ) : if self . stream is None : self . _last_stat = None , None else : try : st = os . stat ( self . _filename ) except OSError : e = sys . exc_info ( ) [ 1 ] <MASK> raise self . _last_stat = None , None else : self . _last_stat = st [ stat . ST_DEV ] , st [ stat . ST_INO ]",if e . errno != errno . ENOENT :,if e . errno != errno . ENOENT :,100,TRUE,100
4793,"def get_place_name ( self , place_handle ) : """"""Obtain a place name"""""" text = "" "" if place_handle : place = self . dbstate . db . get_place_from_handle ( place_handle ) <MASK> place_title = place_displayer . display ( self . dbstate . db , place ) if place_title != "" "" : if len ( place_title ) > 25 : text = place_title [ : 24 ] + "" ... "" else : text = place_title return text",if place :,if place :,100,TRUE,100
4794,"def test_decoder_state ( self ) : # Check that getstate() and setstate() handle the state properly u = "" abc123 "" for encoding in all_unicode_encodings : <MASK> self . check_state_handling_decode ( encoding , u , u . encode ( encoding ) ) self . check_state_handling_encode ( encoding , u , u . encode ( encoding ) )",if encoding not in broken_unicode_with_stateful :,"if encoding in ( ""abc123"" , ""abc123"" ) :",70.13995594,FALSE,87.67
4795,"def cleanup ( self ) : if os . path . exists ( self . meta_gui_dir ) : for f in os . listdir ( self . meta_gui_dir ) : <MASK> os . remove ( os . path . join ( self . meta_gui_dir , f ) )","if os . path . splitext ( f ) [ 1 ] == "".desktop"" :","if f . endswith ( "".py"" ) and f . endswith ( "".py"" ) :",59.32337313,FALSE,76.47
4796,"def _have_applied_incense ( self ) : for applied_item in inventory . applied_items ( ) . all ( ) : self . logger . info ( applied_item ) <MASK> mins = format_time ( applied_item . expire_ms * 1000 ) self . logger . info ( "" Not applying incense, currently active:  %s ,  %s  minutes remaining "" , applied_item . item . name , mins , ) return True else : self . logger . info ( "" "" ) return False return False",if applied_item . expire_ms > 0 :,if applied_item . active :,67.48659543,FALSE,94.29
4797,"def get_closest_point ( self , point ) : point = to_point ( point ) cp , cd = None , None for p0 , p1 in iter_pairs ( self . pts , self . connected ) : diff = p1 - p0 l = diff . length d = diff / l pp = p0 + d * max ( 0 , min ( l , ( point - p0 ) . dot ( d ) ) ) dist = ( point - pp ) . length <MASK> cp , cd = pp , dist return cp",if not cp or dist < cd :,if dist > cd :,90.33758272,FALSE,94.32
4798,"def process_return ( lines ) : for line in lines : m = re . fullmatch ( r "" (?P<param> \ w+) \ s+: \ s+(?P<type>[ \ w.]+) "" , line ) <MASK> # Once this is in scanpydoc, we can use the fancy hover stuff yield f ' ** { m [ "" param "" ] } ** : :class:`~ { m [ "" type "" ] } ` ' else : yield line",if m :,if m :,100,TRUE,100
4799,"def _classify ( nodes_by_level ) : missing , invalid , downloads = [ ] , [ ] , [ ] for level in nodes_by_level : for node in level : if node . binary == BINARY_MISSING : missing . append ( node ) <MASK> invalid . append ( node ) elif node . binary in ( BINARY_UPDATE , BINARY_DOWNLOAD ) : downloads . append ( node ) return missing , invalid , downloads",elif node . binary == BINARY_INVALID :,"elif node . binary in ( BINARY_INVALID , BINARY_UPDATE ) :",93.9520907,FALSE,90.09
4800,"def safe_parse_date ( date_hdr ) : """"""Parse a Date: or Received: header into a unix timestamp."""""" try : <MASK> date_hdr = date_hdr . split ( "" ; "" ) [ - 1 ] . strip ( ) msg_ts = long ( rfc822 . mktime_tz ( rfc822 . parsedate_tz ( date_hdr ) ) ) if ( msg_ts > ( time . time ( ) + 24 * 3600 ) ) or ( msg_ts < 1 ) : return None else : return msg_ts except ( ValueError , TypeError , OverflowError ) : return None","if "";"" in date_hdr :","if "";"" in date_hdr :",100,TRUE,100
4801,"def _on_change ( self ) : changed = False self . save ( ) for key , value in self . data . items ( ) : if isinstance ( value , bool ) : if value : changed = True break if isinstance ( value , int ) : <MASK> changed = True break elif value is None : continue elif len ( value ) != 0 : changed = True break self . _reset_button . disabled = not changed",if value != 1 :,if value != 0 :,97.20797126,FALSE,97.24
4802,"def _rewrite_prepend_append ( self , string , prepend , append = None ) : if append is None : append = prepend if not isinstance ( string , StringElem ) : string = StringElem ( string ) string . sub . insert ( 0 , prepend ) if unicode ( string ) . endswith ( u "" \n "" ) : # Try and remove the last character from the tree try : lastnode = string . flatten ( ) [ - 1 ] <MASK> lastnode . sub [ - 1 ] = lastnode . sub [ - 1 ] . rstrip ( u "" \n "" ) except IndexError : pass string . sub . append ( append + u "" \n "" ) else : string . sub . append ( append ) return string","if isinstance ( lastnode . sub [ - 1 ] , unicode ) :","if lastnode . sub [ - 1 ] . startswith ( u""\n"" ) :",97.74872782,FALSE,93.28
4803,"def parse_indentless_sequence_entry ( self ) : if self . check_token ( BlockEntryToken ) : token = self . get_token ( ) <MASK> self . states . append ( self . parse_indentless_sequence_entry ) return self . parse_block_node ( ) else : self . state = self . parse_indentless_sequence_entry return self . process_empty_scalar ( token . end_mark ) token = self . peek_token ( ) event = SequenceEndEvent ( token . start_mark , token . start_mark ) self . state = self . states . pop ( ) return event","if not self . check_token ( BlockEntryToken , KeyToken , ValueToken , BlockEndToken ) :",if token . end_mark is None :,77.04178671,FALSE,88.67
4804,"def walk_directory ( directory , verbose = False ) : """"""Iterates a directory's text files and their contents."""""" for dir_path , _ , filenames in os . walk ( directory ) : for filename in filenames : file_path = os . path . join ( dir_path , filename ) if os . path . isfile ( file_path ) and not filename . startswith ( "" . "" ) : with io . open ( file_path , "" r "" , encoding = "" utf-8 "" ) as file : <MASK> print ( "" Reading  {} "" . format ( filename ) ) doc_text = file . read ( ) yield filename , doc_text",if verbose :,if verbose :,100,TRUE,100
4805,"def set_bounds ( self , x , y , width , height ) : if self . native : # Root level widgets may require vertical adjustment to # account for toolbars, etc. <MASK> vertical_shift = self . frame . vertical_shift else : vertical_shift = 0 self . native . Size = Size ( width , height ) self . native . Location = Point ( x , y + vertical_shift )",if self . interface . parent is None :,if self . frame :,95.47282223,FALSE,92.77
4806,"def _check_x11 ( self , command = None , * , exc = None , exit_status = None , * * kwargs ) : """"""Check requesting X11 forwarding"""""" with ( yield from self . connect ( ) ) as conn : <MASK> with self . assertRaises ( exc ) : yield from _create_x11_process ( conn , command , * * kwargs ) else : proc = yield from _create_x11_process ( conn , command , * * kwargs ) yield from proc . wait ( ) self . assertEqual ( proc . exit_status , exit_status ) yield from conn . wait_closed ( )",if exc :,if exc is not None :,96.01632578,FALSE,96.59
4807,"def repr ( self ) : try : <MASK> from infogami . infobase . utils import prepr return prepr ( self . obj ) else : return repr ( self . obj ) except : return "" failed "" return render_template ( "" admin/memory/object "" , self . obj )","if isinstance ( self . obj , ( dict , web . threadeddict ) ) :",if self . obj . is_prepr :,85.54493617,FALSE,79.8
4808,"def add ( self , tag , values ) : if tag not in self . different : if tag not in self : self [ tag ] = values <MASK> self . different . add ( tag ) self [ tag ] = [ "" "" ] self . counts [ tag ] + = 1",elif self [ tag ] != values :,if self . counts [ tag ] == values [ 0 ] :,66.76110963,FALSE,82.23
4809,"def _on_geturl ( self , event ) : selected = self . _status_list . get_selected ( ) if selected != - 1 : object_id = self . _status_list . GetItemData ( selected ) download_item = self . _download_list . get_item ( object_id ) url = download_item . url <MASK> clipdata = wx . TextDataObject ( ) clipdata . SetText ( url ) wx . TheClipboard . Open ( ) wx . TheClipboard . SetData ( clipdata ) wx . TheClipboard . Close ( )",if not wx . TheClipboard . IsOpened ( ) :,if url :,83.04303274,FALSE,91.73
4810,"def escape2null ( text ) : """"""Return a string with escape-backslashes converted to nulls."""""" parts = [ ] start = 0 while True : found = text . find ( "" \\ "" , start ) <MASK> parts . append ( text [ start : ] ) return "" "" . join ( parts ) parts . append ( text [ start : found ] ) parts . append ( "" \x00 "" + text [ found + 1 : found + 2 ] ) start = found + 2 # skip character after escape",if found == - 1 :,if found == - 1 :,100,TRUE,100
4811,def _process_inner_views ( self ) : for view in self . baseviews : for inner_class in view . get_uninit_inner_views ( ) : for v in self . baseviews : <MASK> view . get_init_inner_views ( ) . append ( v ),"if isinstance ( v , inner_class ) and v not in view . get_init_inner_views ( ) :",if inner_class . _is_in_view ( v ) :,57.17407062,FALSE,74.93
4812,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 10 : self . set_url ( d . getPrefixedString ( ) ) continue <MASK> self . set_app_version_id ( d . getPrefixedString ( ) ) continue if tt == 26 : self . set_method ( d . getPrefixedString ( ) ) continue if tt == 34 : self . set_queue ( d . getPrefixedString ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 18 :,if tt == 18 :,100,TRUE,100
4813,"def test_sample_output ( ) : comment = "" SAMPLE OUTPUT "" skip_files = [ "" __init__.py "" ] errors = [ ] for _file in sorted ( MODULE_PATH . iterdir ( ) ) : if _file . suffix == "" .py "" and _file . name not in skip_files : with _file . open ( ) as f : <MASK> errors . append ( ( comment , _file ) ) if errors : line = "" Missing sample error(s) detected! \n \n "" for error in errors : line + = "" ` {} ` is not in module ` {} ` \n "" . format ( * error ) print ( line [ : - 1 ] ) assert False",if comment not in f . read ( ) :,"if ""sample_output"" in f . read ( ) :",95.92027119,FALSE,96.02
4814,"def _get_planner ( name , path , source ) : for klass in _planners : <MASK> LOG . debug ( "" %r  accepted  %r  (filename  %r ) "" , klass , name , path ) return klass LOG . debug ( "" %r  rejected  %r "" , klass , name ) raise ansible . errors . AnsibleError ( NO_METHOD_MSG + repr ( invocation ) )","if klass . detect ( path , source ) :","if klass . _match ( name , source ) :",97.08084927,FALSE,93.96
4815,"def _to_string_infix ( self , ostream , idx , verbose ) : if verbose : ostream . write ( ""  ,  "" ) else : hasConst = not ( self . _const . __class__ in native_numeric_types and self . _const == 0 ) <MASK> idx - = 1 _l = self . _coef [ id ( self . _args [ idx ] ) ] _lt = _l . __class__ if _lt is _NegationExpression or ( _lt in native_numeric_types and _l < 0 ) : ostream . write ( ""  -  "" ) else : ostream . write ( ""  +  "" )",if hasConst :,if hasConst :,100,TRUE,100
4816,"def cluster_info_query ( self ) : if self . _major_version > = 90600 : extra = ( "" , CASE WHEN latest_end_lsn IS NULL THEN NULL ELSE received_tli END, "" ""  slot_name, conninfo FROM pg_catalog.pg_stat_get_wal_receiver() "" ) <MASK> extra = "" timeline_id "" + extra + "" , pg_catalog.pg_control_checkpoint() "" else : extra = "" 0 "" + extra else : extra = "" 0, NULL, NULL, NULL "" return ( "" SELECT  "" + self . TL_LSN + "" ,  {2} "" ) . format ( self . wal_name , self . lsn_name , extra )","if self . role == ""standby_leader"" :",if self . _major_version >= 90600 :,88.35183004,FALSE,94.38
4817,"def __init__ ( self , * args , * * kwargs ) : self . country = kwargs . pop ( "" country "" ) self . fields_needed = kwargs . pop ( "" fields_needed "" , [ ] ) super ( DynamicManagedAccountForm , self ) . __init__ ( * args , * * kwargs ) # build our form using the country specific fields and falling # back to our default set for f in self . fields_needed : <MASK> # pragma: no branch field_name , field = FIELDS_BY_COUNTRY [ self . country ] [ f ] self . fields [ field_name ] = field","if f in FIELDS_BY_COUNTRY . get ( self . country , { } ) :",if f not in FIELDS_BY_COUNTRY [ self . country ] :,95.27639911,FALSE,92.04
4818,"def delete_map ( self , query = None ) : query_map = self . interpolated_map ( query = query ) for alias , drivers in six . iteritems ( query_map . copy ( ) ) : for driver , vms in six . iteritems ( drivers . copy ( ) ) : for vm_name , vm_details in six . iteritems ( vms . copy ( ) ) : <MASK> query_map [ alias ] [ driver ] . pop ( vm_name ) if not query_map [ alias ] [ driver ] : query_map [ alias ] . pop ( driver ) if not query_map [ alias ] : query_map . pop ( alias ) return query_map","if vm_details == ""Absent"" :","if vm_details [ ""type"" ] == ""vm"" :",96.56887768,FALSE,94.11
4819,"def on_strokes_edited ( self ) : strokes = self . _strokes ( ) if strokes : translation = self . _engine . raw_lookup ( strokes ) <MASK> fmt = _ ( "" {strokes}  maps to  {translation} "" ) else : fmt = _ ( "" {strokes}  is not in the dictionary "" ) info = self . _format_label ( fmt , ( strokes , ) , translation ) else : info = "" "" self . strokes_info . setText ( info )",if translation is not None :,if translation :,76.69366424,FALSE,95.68
4820,"def release ( self ) : tid = _thread . get_ident ( ) with self . lock : if self . owner != tid : raise RuntimeError ( "" cannot release un-acquired lock "" ) assert self . count > 0 self . count - = 1 <MASK> self . owner = None if self . waiters : self . waiters - = 1 self . wakeup . release ( )",if self . count == 0 :,if self . count == 0 :,100,TRUE,100
4821,"def _cat_blob ( self , gcs_uri ) : """""":py:meth:`cat_file`, minus decompression."""""" blob = self . _get_blob ( gcs_uri ) if not blob : return # don't cat nonexistent files start = 0 while True : end = start + _CAT_CHUNK_SIZE try : chunk = blob . download_as_string ( start = start , end = end ) except google . api_core . exceptions . RequestRangeNotSatisfiable : return yield chunk <MASK> return start = end",if len ( chunk ) < _CAT_CHUNK_SIZE :,if not chunk :,95.49018182,FALSE,89.72
4822,"def device_iter ( * * kwargs ) : for dev in backend . enumerate_devices ( ) : d = Device ( dev , backend ) tests = ( val == _try_getattr ( d , key ) for key , val in kwargs . items ( ) ) <MASK> yield d",if _interop . _all ( tests ) and ( custom_match is None or custom_match ( d ) ) :,if tests :,74.04813051,FALSE,66.58
4823,"def _get_vtkjs ( self ) : if self . _vtkjs is None and self . object is not None : if isinstance ( self . object , string_types ) and self . object . endswith ( "" .vtkjs "" ) : if isfile ( self . object ) : with open ( self . object , "" rb "" ) as f : vtkjs = f . read ( ) else : data_url = urlopen ( self . object ) vtkjs = data_url . read ( ) <MASK> vtkjs = self . object . read ( ) self . _vtkjs = vtkjs return self . _vtkjs","elif hasattr ( self . object , ""read"" ) :",if not vtkjs :,78.8270129,FALSE,90.14
4824,"def _execute_with_error ( command , error , message ) : try : cli . invocation = cli . invocation_cls ( cli_ctx = cli , parser_cls = cli . parser_cls , commands_loader_cls = cli . commands_loader_cls , help_cls = cli . help_cls , ) cli . invocation . execute ( command . split ( ) ) except CLIError as ex : <MASK> raise AssertionError ( "" {} \n Expected:  {} \n Actual:  {} "" . format ( message , error , ex ) ) return except Exception as ex : raise ex raise AssertionError ( "" exception not raised for  ' {0} ' "" . format ( message ) )",if error not in str ( ex ) :,if error != ex . args [ 0 ] :,93.91668921,FALSE,93.88
4825,"def ray_intersection ( self , p , line ) : p = Vector ( center ( line . sites ) ) min_r = BIG_FLOAT nearest = None for v_i , v_j in self . edges : bound = LineEquation2D . from_two_points ( v_i , v_j ) intersection = bound . intersect_with_line ( line ) if intersection is not None : r = ( p - intersection ) . length # info(""INT: [%s - %s] X [%s] => %s (%s)"", v_i, v_j, line, intersection, r) <MASK> nearest = intersection min_r = r return nearest",if r < min_r :,if r < min_r :,100,TRUE,100
4826,"def CalculateChecksum ( data ) : # The checksum is just a sum of all the bytes. I swear. if isinstance ( data , bytearray ) : total = sum ( data ) elif isinstance ( data , bytes ) : <MASK> # Python 2 bytes (str) index as single-character strings. total = sum ( map ( ord , data ) ) else : # Python 3 bytes index as numbers (and PY2 empty strings sum() to 0) total = sum ( data ) else : # Unicode strings (should never see?) total = sum ( map ( ord , data ) ) return total & 0xFFFFFFFF","if data and isinstance ( data [ 0 ] , bytes ) :","if isinstance ( data , str ) :",70.75149182,FALSE,92.4
4827,"def __mul__ ( self , other : Union [ "" Tensor "" , float ] ) - > "" Tensor "" : if isinstance ( other , Tensor ) : <MASK> errstr = ( f "" Given backens are inconsistent. Found  ' { self . backend . name } ' "" f "" and  ' { other . backend . name } ' "" ) raise ValueError ( errstr ) other = other . array array = self . backend . multiply ( self . array , other ) return Tensor ( array , backend = self . backend )",if self . backend . name != other . backend . name :,if self . backend . name != other . backend . name :,100,TRUE,100
4828,"def next_item ( self , direction ) : """"""Selects next menu item, based on self._direction"""""" start , i = - 1 , 0 try : start = self . items . index ( self . _selected ) i = start + direction except : pass while True : <MASK> # Cannot find valid menu item self . select ( start ) break if i > = len ( self . items ) : i = 0 continue if i < 0 : i = len ( self . items ) - 1 continue if self . select ( i ) : break i + = direction if start < 0 : start = 0",if i == start :,if i == - 1 :,93.93762056,FALSE,97.33
4829,"def resolve_none ( self , data ) : # replace None to '_' for tok_idx in range ( len ( data ) ) : for feat_idx in range ( len ( data [ tok_idx ] ) ) : <MASK> data [ tok_idx ] [ feat_idx ] = "" _ "" return data",if data [ tok_idx ] [ feat_idx ] is None :,"if data [ tok_idx ] [ feat_idx ] == ""_"" :",71.66855889,FALSE,91.99
4830,"def distinct ( expr , * on ) : fields = frozenset ( expr . fields ) _on = [ ] append = _on . append for n in on : if isinstance ( n , Field ) : if n . _child . isidentical ( expr ) : n = n . _name else : raise ValueError ( "" {0}  is not a field of  {1} "" . format ( n , expr ) ) if not isinstance ( n , _strtypes ) : raise TypeError ( "" on must be a name or field, not:  {0} "" . format ( n ) ) <MASK> raise ValueError ( "" {0}  is not a field of  {1} "" . format ( n , expr ) ) append ( n ) return Distinct ( expr , tuple ( _on ) )",elif n not in fields :,if n not in fields :,97.10584058,FALSE,98.44
4831,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) <MASK> length = d . getVarInt32 ( ) tmp = ProtocolBuffer . Decoder ( d . buffer ( ) , d . pos ( ) , d . pos ( ) + length ) d . skip ( length ) self . mutable_cost ( ) . TryMerge ( tmp ) continue if tt == 24 : self . add_version ( d . getVarInt64 ( ) ) continue if tt == 0 : raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 10 :,if tt == 10 :,100,TRUE,100
4832,"def func_std_string ( func_name ) : # match what old profile produced if func_name [ : 2 ] == ( "" ~ "" , 0 ) : # special case for built-in functions name = func_name [ 2 ] <MASK> return "" { %s } "" % name [ 1 : - 1 ] else : return name else : return "" %s : %d ( %s ) "" % func_name","if name . startswith ( ""<"" ) and name . endswith ( "">"" ) :","if name [ - 1 ] == ( ""~"" , 0 ) :",67.67779982,FALSE,85.11
4833,"def f ( ) : try : # Intra-buffer read then buffer-flushing read for n in cycle ( [ 1 , 19 ] ) : s = bufio . read ( n ) <MASK> break # list.append() is atomic results . append ( s ) except Exception as e : errors . append ( e ) raise",if not s :,"if s == b"""" :",71.59215181,FALSE,89.03
4834,"def stop ( self ) : # Try to shut the connection down, but if we get any sort of # errors, go ahead and ignore them.. as we're shutting down anyway try : self . rpcserver . stop ( ) if self . backend_rpcserver : self . backend_rpcserver . stop ( ) <MASK> self . cluster_rpcserver . stop ( ) except Exception : pass if self . coordination : try : coordination . COORDINATOR . stop ( ) except Exception : pass super ( Service , self ) . stop ( graceful = True )",if self . cluster_rpcserver :,if self . cluster_rpcserver :,75,TRUE,100
4835,"def download ( cls , architecture , path = "" ./ "" ) : if cls . sanity_check ( architecture ) : architecture_file = download_file ( cls . architecture_map [ architecture ] , directory = path ) <MASK> return None print ( "" Coreml model  {}  is saved in [ {} ] "" . format ( architecture , path ) ) return architecture_file else : return None",if not architecture_file :,if not architecture_file :,100,TRUE,100
4836,"def opps_output_converter ( kpt_list ) : kpts = [ ] mpii_keys = to_opps_converter . keys ( ) for mpii_idx in range ( 0 , 16 ) : <MASK> model_idx = to_opps_converter [ mpii_idx ] x , y = kpt_list [ model_idx ] if x < 0 or y < 0 : kpts + = [ 0.0 , 0.0 , - 1.0 ] else : kpts + = [ x , y , 1.0 ] else : kpts + = [ 0.0 , 0.0 , - 1.0 ] return kpts",if mpii_idx in mpii_keys :,if mpii_idx in mpii_keys :,100,TRUE,100
4837,"def _get_headers ( self , headers = None ) : request_headers = headers or { } # Auth headers if access_token is present if self . _client . client . config : config = self . _client . client . config if "" Authorization "" not in request_headers and config . token : request_headers . update ( { "" Authorization "" : "" {}   {} "" . format ( config . authentication_type , config . token ) } ) <MASK> request_headers . update ( { config . header : config . header_service } ) return request_headers",if config . header and config . header_service :,if config . header_service :,97.83942519,FALSE,96.85
4838,"def get_last_traded_prices ( cls , trading_pairs : List [ str ] ) - > Dict [ str , float ] : results = dict ( ) async with aiohttp . ClientSession ( ) as client : resp = await client . get ( f "" { constants . REST_URL } /tickers "" ) resp_json = await resp . json ( ) for trading_pair in trading_pairs : resp_record = [ o for o in resp_json <MASK> ] [ 0 ] results [ trading_pair ] = float ( resp_record [ "" price "" ] ) return results","if o [ ""symbol"" ] == convert_to_exchange_trading_pair ( trading_pair )","if o [ ""type"" ] == ""trading_pair""",90.07243359,FALSE,88.07
4839,"def reset_two_factor_hotp ( ) : uid = request . form [ "" uid "" ] otp_secret = request . form . get ( "" otp_secret "" , None ) if otp_secret : user = Journalist . query . get ( uid ) <MASK> return render_template ( "" admin_edit_hotp_secret.html "" , uid = uid ) db . session . commit ( ) return redirect ( url_for ( "" admin.new_user_two_factor "" , uid = uid ) ) else : return render_template ( "" admin_edit_hotp_secret.html "" , uid = uid )","if not validate_hotp_secret ( user , otp_secret ) :",if user . secret == otp_secret :,75.55624644,FALSE,91.16
4840,"def ctx_for_video ( self , vurl ) : "" Get a context dict for a given video URL "" ctx = self . get_context_dict ( ) for portal , match , context_fn in self . PORTALS : <MASK> try : ctx . update ( context_fn ( vurl ) ) ctx [ "" portal "" ] = portal break except AttributeError : continue return ctx",if match . search ( vurl ) :,if match . lower ( ) == vurl . lower ( ) :,91.15587982,FALSE,88.51
4841,"def get ( self ) : name = request . args . get ( "" filename "" ) if name is not None : opts = dict ( ) opts [ "" type "" ] = "" episode "" result = guessit ( name , options = opts ) res = dict ( ) if "" episode "" in result : res [ "" episode "" ] = result [ "" episode "" ] else : res [ "" episode "" ] = 0 if "" season "" in result : res [ "" season "" ] = result [ "" season "" ] else : res [ "" season "" ] = 0 <MASK> res [ "" subtitle_language "" ] = str ( result [ "" subtitle_language "" ] ) return jsonify ( data = res ) else : return "" "" , 400","if ""subtitle_language"" in result :","if ""subtitle_language"" in result :",100,TRUE,100
4842,"def package_files ( package_path , directory_name ) : paths = [ ] directory_path = os . path . join ( package_path , directory_name ) for ( path , directories , filenames ) in os . walk ( directory_path ) : relative_path = os . path . relpath ( path , package_path ) for filename in filenames : <MASK> continue paths . append ( os . path . join ( relative_path , filename ) ) return paths","if filename [ 0 ] == ""."" :","if filename . endswith ( "".py"" ) :",93.15683821,FALSE,91.81
4843,"def parse_simple ( d , data ) : units = { } for v in data [ d ] : key = v [ "" name "" ] if not key : continue key_to_insert = make_key ( key ) <MASK> index = 2 tmp = f "" { key_to_insert } _ { index } "" while tmp in units : index + = 1 tmp = f "" { key_to_insert } _ { index } "" key_to_insert = tmp units [ key_to_insert ] = v [ "" id "" ] return units",if key_to_insert in units :,if key_to_insert not in units :,98.9379608,FALSE,97.96
4844,"def parse_clademodelc ( branch_type_no , line_floats , site_classes ) : """"""Parse results specific to the clade model C."""""" if not site_classes or len ( line_floats ) == 0 : return for n in range ( len ( line_floats ) ) : <MASK> site_classes [ n ] [ "" branch types "" ] = { } site_classes [ n ] [ "" branch types "" ] [ branch_type_no ] = line_floats [ n ] return site_classes","if site_classes [ n ] . get ( ""branch types"" ) is None :","if ""branch types"" not in site_classes [ n ] :",88.31218251,FALSE,91.62
4845,"def track_modules ( self , * modules ) : """"""Add module names to the tracked list."""""" already_tracked = self . session . GetParameter ( "" autodetect_build_local_tracked "" ) or [ ] needed = set ( modules ) if not needed . issubset ( already_tracked ) : needed . update ( already_tracked ) with self . session as session : session . SetParameter ( "" autodetect_build_local_tracked "" , needed ) for module_name in modules : module_obj = self . GetModuleByName ( module_name ) <MASK> # Clear the module's profile. This will force it to # reload a new profile. module_obj . profile = None",if module_obj :,if module_obj :,100,TRUE,100
4846,"def set_job_on_hold ( self , value , blocking = True ) : trigger = False # don't run any locking code beyond this... if not self . _job_on_hold . acquire ( blocking = blocking ) : return False try : <MASK> self . _job_on_hold . set ( ) else : self . _job_on_hold . clear ( ) if self . _job_on_hold . counter == 0 : trigger = True finally : self . _job_on_hold . release ( ) # locking code is now safe to run again if trigger : self . _continue_sending ( ) return True",if value :,if value :,100,TRUE,100
4847,"def moveToThreadNext ( self ) : """"""Move a position to threadNext position."""""" p = self if p . v : <MASK> p . moveToFirstChild ( ) elif p . hasNext ( ) : p . moveToNext ( ) else : p . moveToParent ( ) while p : if p . hasNext ( ) : p . moveToNext ( ) break # found p . moveToParent ( ) # not found. return p",if p . v . children :,if p . firstChild :,90.70718089,FALSE,94.72
4848,"def best_image ( width , height ) : # A heuristic for finding closest sized image to required size. image = images [ 0 ] for img in images : <MASK> # Exact match always used return img elif img . width > = width and img . width * img . height > image . width * image . height : # At least wide enough, and largest area image = img return image",if img . width == width and img . height == height :,if img . width == width and img . height == height :,75,TRUE,100
4849,"def _check_input_types ( self ) : if len ( self . base_features ) == 0 : return True input_types = self . primitive . input_types if input_types is not None : <MASK> input_types = [ input_types ] for t in input_types : zipped = list ( zip ( t , self . base_features ) ) if all ( [ issubclass ( f . variable_type , v ) for v , f in zipped ] ) : return True else : return True return False",if type ( input_types [ 0 ] ) != list :,"if not isinstance ( input_types , list ) :",90.50286163,FALSE,91.15
4850,"def get_result ( self ) : result_list = [ ] exc_info = None for f in self . children : try : result_list . append ( f . get_result ( ) ) except Exception as e : if exc_info is None : exc_info = sys . exc_info ( ) else : <MASK> app_log . error ( "" Multiple exceptions in yield list "" , exc_info = True ) if exc_info is not None : raise_exc_info ( exc_info ) if self . keys is not None : return dict ( zip ( self . keys , result_list ) ) else : return list ( result_list )","if not isinstance ( e , self . quiet_exceptions ) :",if len ( result_list ) > 1 :,92.31491316,FALSE,92.01
4851,"def _update_learning_params ( self ) : model = self . model hparams = self . hparams fd = self . runner . feed_dict step_num = self . step_num if hparams . model_type == "" resnet_tf "" : if step_num < hparams . lrn_step : lrn_rate = hparams . mom_lrn <MASK> lrn_rate = hparams . mom_lrn / 10 elif step_num < 35000 : lrn_rate = hparams . mom_lrn / 100 else : lrn_rate = hparams . mom_lrn / 1000 fd [ model . lrn_rate ] = lrn_rate",elif step_num < 30000 :,elif step_num < 10000 :,98.58936733,FALSE,98.1
4852,"def topic_exists ( self , arn ) : response = self . _conn . get_all_topics ( ) topics = response [ "" ListTopicsResponse "" ] [ "" ListTopicsResult "" ] [ "" Topics "" ] current_topics = [ ] if len ( topics ) > 0 : for topic in topics : topic_arn = topic [ "" TopicArn "" ] current_topics . append ( topic_arn ) <MASK> return True return False",if arn in current_topics :,if arn in topic_arn and arn in current_topics :,97.15049463,FALSE,93.52
4853,"def assertStartsWith ( self , expectedPrefix , text , msg = None ) : if not text . startswith ( expectedPrefix ) : <MASK> text = text [ : len ( expectedPrefix ) + 5 ] + "" ... "" standardMsg = "" {}  not found at the start of  {} "" . format ( repr ( expectedPrefix ) , repr ( text ) ) self . fail ( self . _formatMessage ( msg , standardMsg ) )",if len ( expectedPrefix ) + 5 < len ( text ) :,if len ( text ) > 5 :,90.90415337,FALSE,90.81
4854,"def validate_memory ( self , value ) : for k , v in value . viewitems ( ) : <MASK> # use NoneType to unset a value continue if not re . match ( PROCTYPE_MATCH , k ) : raise serializers . ValidationError ( "" Process types can only contain [a-z] "" ) if not re . match ( MEMLIMIT_MATCH , str ( v ) ) : raise serializers . ValidationError ( "" Limit format: <number><unit>, where unit = B, K, M or G "" ) return value",if v is None :,if v is None :,75,TRUE,100
4855,"def open ( self ) - > "" KeyValueJsonDb "" : """"""Create a new data base or open existing one"""""" if os . path . exists ( self . _name ) : <MASK> raise IOError ( "" %s  exists and is not a file "" % self . _name ) try : with open ( self . _name , "" r "" ) as _in : self . set_records ( json . load ( _in ) ) except json . JSONDecodeError : # file corrupted, reset it. self . commit ( ) else : # make sure path exists mkpath ( os . path . dirname ( self . _name ) ) self . commit ( ) return self",if not os . path . isfile ( self . _name ) :,if not os . path . isfile ( self . _name ) :,100,TRUE,100
4856,"def _calculate ( self ) : before = self . before . data after = self . after . data self . deleted = { } self . updated = { } self . created = after . copy ( ) for path , f in before . items ( ) : <MASK> self . deleted [ path ] = f continue del self . created [ path ] if f . mtime < after [ path ] . mtime : self . updated [ path ] = after [ path ]",if path not in after :,if path in self . created :,78.86943993,FALSE,94.58
4857,"def cache_sqs_queues_across_accounts ( ) - > bool : function : str = f "" { __name__ } . { sys . _getframe ( ) . f_code . co_name } "" # First, get list of accounts accounts_d : list = async_to_sync ( get_account_id_to_name_mapping ) ( ) # Second, call tasks to enumerate all the roles across all accounts for account_id in accounts_d . keys ( ) : <MASK> cache_sqs_queues_for_account . delay ( account_id ) else : if account_id in config . get ( "" celery.test_account_ids "" , [ ] ) : cache_sqs_queues_for_account . delay ( account_id ) stats . count ( f "" { function } .success "" ) return True","if config . get ( ""environment"" ) == ""prod"" :","if account_id in config . get ( ""celery.test_account_ids""",96.66501556,FALSE,92.4
4858,"def remove ( self , path , config = None , error_on_path = False , defaults = None ) : if not path : if error_on_path : raise NoSuchSettingsPath ( ) return if config is not None or defaults is not None : <MASK> config = self . _config if defaults is None : defaults = dict ( self . _map . parents ) chain = HierarchicalChainMap ( config , defaults ) else : chain = self . _map try : chain . del_by_path ( path ) self . _mark_dirty ( ) except KeyError : if error_on_path : raise NoSuchSettingsPath ( ) pass",if config is None :,if config is None :,100,TRUE,100
4859,"def PopulateProjectId ( project_id = None ) : """"""Fills in a project_id from the boto config file if one is not provided."""""" if not project_id : default_id = boto . config . get_value ( "" GSUtil "" , "" default_project_id "" ) <MASK> raise ProjectIdException ( "" MissingProjectId "" ) return default_id return project_id",if not default_id :,if default_id is None :,92.69660633,FALSE,93.93
4860,"def set ( self , name , value ) : with self . _object_cache_lock : old_value = self . _object_cache . get ( name ) ret = not old_value or int ( old_value . metadata . resource_version ) < int ( value . metadata . resource_version ) <MASK> self . _object_cache [ name ] = value return ret , old_value",if ret :,if ret :,100,TRUE,100
4861,"def remove ( self , url ) : try : i = self . items . index ( url ) except ( ValueError , IndexError ) : pass else : was_selected = i in self . selectedindices ( ) self . list . delete ( i ) del self . items [ i ] if not self . items : self . mp . hidepanel ( self . name ) <MASK> if i > = len ( self . items ) : i = len ( self . items ) - 1 self . list . select_set ( i )",elif was_selected :,if was_selected :,97.3235295,FALSE,97.64
4862,"def add_directory_csv_files ( dir_path , paths = None ) : if not paths : paths = [ ] for p in listdir ( dir_path ) : path = join ( dir_path , p ) <MASK> # call recursively for each dir paths = add_directory_csv_files ( path , paths ) elif isfile ( path ) and path . endswith ( "" .csv "" ) : # add every file to the list paths . append ( path ) return paths",if isdir ( path ) :,if isdir ( path ) :,100,TRUE,100
4863,"def _get_client ( rp_mapping , resource_provider ) : for key , value in rp_mapping . items ( ) : <MASK> if isinstance ( value , dict ) : return GeneralPrivateEndpointClient ( key , value [ "" api_version "" ] , value [ "" support_list_or_not "" ] , value [ "" resource_get_api_version "" ] , ) return value ( ) raise CLIError ( "" Resource type must be one of  {} "" . format ( "" ,  "" . join ( rp_mapping . keys ( ) ) ) )",if str . lower ( key ) == str . lower ( resource_provider ) :,if resource_provider == key :,89.30615951,FALSE,88.15
4864,"def compute_rule_hash ( self , rule ) : buf = "" %d - %d - %s - "" % ( rule . get ( "" FromPort "" , 0 ) or 0 , rule . get ( "" ToPort "" , 0 ) or 0 , rule . get ( "" IpProtocol "" , "" -1 "" ) or "" -1 "" , ) for a , ke in self . RULE_ATTRS : <MASK> continue ev = [ e [ ke ] for e in rule [ a ] ] ev . sort ( ) for e in ev : buf + = "" %s - "" % e # mask to generate the same numeric value across all Python versions return zlib . crc32 ( buf . encode ( "" ascii "" ) ) & 0xFFFFFFFF",if a not in rule :,if a not in rule :,100,TRUE,100
4865,"def analysis_sucess_metrics ( analysis_time : float , allow_exception = False ) : try : anchore_engine . subsys . metrics . counter_inc ( name = "" anchore_analysis_success "" ) anchore_engine . subsys . metrics . histogram_observe ( "" anchore_analysis_time_seconds "" , analysis_time , buckets = ANALYSIS_TIME_SECONDS_BUCKETS , status = "" success "" , ) except : <MASK> raise else : logger . exception ( "" Unexpected exception during metrics update for a successful analysis. Swallowing error and continuing "" )",if allow_exception :,if allow_exception :,100,TRUE,100
4866,"def decide_file_icon ( file ) : if file . state == File . ERROR : return FileItem . icon_error elif isinstance ( file . parent , Track ) : <MASK> return FileItem . icon_saved elif file . state == File . PENDING : return FileItem . match_pending_icons [ int ( file . similarity * 5 + 0.5 ) ] else : return FileItem . match_icons [ int ( file . similarity * 5 + 0.5 ) ] elif file . state == File . PENDING : return FileItem . icon_file_pending else : return FileItem . icon_file",if file . state == File . NORMAL :,if file . state == File . Saved :,98.7918803,FALSE,98.01
4867,"def deleteMenu ( self , menuName ) : try : menu = self . getMenu ( menuName ) <MASK> self . destroy ( menu ) self . destroyMenu ( menuName ) else : g . es ( "" can ' t delete menu: "" , menuName ) except Exception : g . es ( "" exception deleting "" , menuName , "" menu "" ) g . es_exception ( )",if menu :,if menu :,100,TRUE,100
4868,"def parser ( cls , buf ) : ( type_ , code , csum ) = struct . unpack_from ( cls . _PACK_STR , buf ) msg = cls ( type_ , code , csum ) offset = cls . _MIN_LEN if len ( buf ) > offset : cls_ = cls . _ICMPV6_TYPES . get ( type_ , None ) <MASK> msg . data = cls_ . parser ( buf , offset ) else : msg . data = buf [ offset : ] return msg , None , None",if cls_ :,if cls_ :,100,TRUE,100
4869,"def _load_dataset_area ( self , dsid , file_handlers , coords ) : """"""Get the area for *dsid*."""""" try : return self . _load_area_def ( dsid , file_handlers ) except NotImplementedError : if any ( x is None for x in coords ) : logger . warning ( "" Failed to load coordinates for  ' {} ' "" . format ( dsid ) ) return None area = self . _make_area_from_coords ( coords ) <MASK> logger . debug ( "" No coordinates found for  %s "" , str ( dsid ) ) return area",if area is None :,if area is None :,100,TRUE,100
4870,"def __getattr__ ( self , name ) : if Popen . verbose : sys . stdout . write ( "" Getattr:  %s ... "" % name ) if name in Popen . __slots__ : return object . __getattribute__ ( self , name ) else : if self . popen is not None : if Popen . verbose : print ( "" from Popen "" ) return getattr ( self . popen , name ) else : <MASK> return self . emu_wait else : raise Exception ( "" subprocess emulation: not implemented:  %s "" % name )","if name == ""wait"" :",if self . emu_wait is not None :,95.12633173,FALSE,92.55
4871,"def update ( self , time_delta ) : super ( ) . update ( time_delta ) n = self . menu . selected_option if n == self . last : return self . last = n s = "" "" for i in range ( len ( self . files ) ) : <MASK> for l in open ( self . files [ i ] [ 1 ] ) : x = l . strip ( ) if len ( x ) > 1 and x [ 0 ] == "" # "" : x = "" <b><u> "" + x [ 1 : ] + ""  </u></b> "" s + = x + "" <br> "" self . set_text ( s )",if self . files [ i ] [ 0 ] == n :,if self . files [ i ] [ 0 ] == n :,100,TRUE,100
4872,"def wrapper ( * args , * * kwargs ) : list_args , empty = _apply_defaults ( func , args , kwargs ) if len ( dimensions ) > len ( list_args ) : raise TypeError ( "" %s  takes  %i  parameters, but  %i  dimensions were passed "" % ( func . __name__ , len ( list_args ) , len ( dimensions ) ) ) for dim , value in zip ( dimensions , list_args ) : if dim is None : continue <MASK> val_dim = ureg . get_dimensionality ( value ) raise DimensionalityError ( value , "" a quantity of "" , val_dim , dim ) return func ( * args , * * kwargs )",if not ureg . Quantity ( value ) . check ( dim ) :,"if not empty and dim not in [ ""quantity"" , ""quantity"" ] :",90.01506377,FALSE,90.24
4873,"def _check ( self , name , size = None , * extra ) : func = getattr ( imageop , name ) for height in VALUES : for width in VALUES : strlen = abs ( width * height ) if size : strlen * = size <MASK> data = "" A "" * strlen else : data = AAAAA if size : arguments = ( data , size , width , height ) + extra else : arguments = ( data , width , height ) + extra try : func ( * arguments ) except ( ValueError , imageop . error ) : pass",if strlen < MAX_LEN :,if len ( size ) > 1 :,83.73105848,FALSE,93.38
4874,"def wait_send_all_might_not_block ( self ) - > None : with self . _send_conflict_detector : <MASK> raise trio . ClosedResourceError ( "" file was already closed "" ) try : await trio . lowlevel . wait_writable ( self . _fd_holder . fd ) except BrokenPipeError as e : # kqueue: raises EPIPE on wait_writable instead # of sending, which is annoying raise trio . BrokenResourceError from e",if self . _fd_holder . closed :,if self . _fd_holder . fd is not None :,94.13175253,FALSE,94.51
4875,"def parse_win_proxy ( val ) : proxies = [ ] for p in val . split ( "" ; "" ) : if "" = "" in p : tab = p . split ( "" = "" , 1 ) <MASK> tab [ 0 ] = "" SOCKS4 "" proxies . append ( ( tab [ 0 ] . upper ( ) , tab [ 1 ] , None , None ) ) # type, addr:port, username, password else : proxies . append ( ( "" HTTP "" , p , None , None ) ) return proxies","if tab [ 0 ] == ""socks"" :","if tab [ 0 ] == ""SOCKS4"" :",96.43202343,FALSE,97.84
4876,"def _super_function ( args ) : passed_class , passed_self = args . get_arguments ( [ "" type "" , "" self "" ] ) if passed_self is None : return passed_class else : # pyclass = passed_self.get_type() pyclass = passed_class if isinstance ( pyclass , pyobjects . AbstractClass ) : supers = pyclass . get_superclasses ( ) <MASK> return pyobjects . PyObject ( supers [ 0 ] ) return passed_self",if supers :,if len ( supers ) == 1 :,96.77356551,FALSE,92
4877,"def update_output_mintime ( job ) : try : return output_mintime [ job ] except KeyError : for job_ in chain ( [ job ] , self . depending [ job ] ) : try : t = output_mintime [ job_ ] except KeyError : t = job_ . output_mintime <MASK> output_mintime [ job ] = t return output_mintime [ job ] = None",if t is not None :,if t is not None :,100,TRUE,100
4878,"def get_list_of_strings_to_mongo_objects ( self , notifications_list = None ) : result = [ ] if len ( notifications_list ) > 0 : for x in notifications_list : split_provider_id = x . split ( "" : "" ) # email:id if len ( split_provider_id ) == 2 : _id = split_provider_id [ 1 ] cursor = self . get_by_id ( _id ) <MASK> # Append if exists result . append ( cursor ) return result",if cursor :,if cursor :,100,TRUE,100
4879,"def stop ( self ) : with self . lock : <MASK> return self . task_queue . put ( None ) self . result_queue . put ( None ) process = self . process self . process = None self . task_queue = None self . result_queue = None process . join ( timeout = 0.1 ) if process . exitcode is None : os . kill ( process . pid , signal . SIGKILL ) process . join ( )",if not self . process :,if self . process is None :,83.64306685,FALSE,94.59
4880,"def on_api_command ( self , command , data ) : if command == "" select "" : if not Permissions . PLUGIN_ACTION_COMMAND_PROMPT_INTERACT . can ( ) : return flask . abort ( 403 , "" Insufficient permissions "" ) <MASK> return flask . abort ( 409 , "" No active prompt "" ) choice = data [ "" choice "" ] if not isinstance ( choice , int ) or not self . _prompt . validate_choice ( choice ) : return flask . abort ( 400 , "" {!r}  is not a valid value for choice "" . format ( choice ) ) self . _answer_prompt ( choice )",if self . _prompt is None :,"if ""choice"" not in data :",87.14098941,FALSE,94.49
4881,"def application_openFiles_ ( self , nsapp , filenames ) : # logging.info('[osx] file open') # logging.info('[osx] file : %s' % (filenames)) for filename in filenames : logging . info ( "" [osx] receiving from macOS :  %s "" , filename ) <MASK> if sabnzbd . filesystem . get_ext ( filename ) in VALID_ARCHIVES + VALID_NZB_FILES : sabnzbd . add_nzbfile ( filename , keep = True )",if os . path . exists ( filename ) :,if os . path . exists ( filename ) :,75,TRUE,100
4882,"def test_error_through_destructor ( self ) : # Test that the exception state is not modified by a destructor, # even if close() fails. rawio = self . CloseFailureIO ( ) with support . catch_unraisable_exception ( ) as cm : with self . assertRaises ( AttributeError ) : self . tp ( rawio ) . xyzzy <MASK> self . assertIsNone ( cm . unraisable ) elif cm . unraisable is not None : self . assertEqual ( cm . unraisable . exc_type , OSError )",if not IOBASE_EMITS_UNRAISABLE :,"if hasattr ( cm , ""unraisable"" ) :",71.74331018,FALSE,91.14
4883,"def http_wrapper ( self , url , postdata = { } ) : try : <MASK> f = urllib . urlopen ( url , postdata ) else : f = urllib . urlopen ( url ) response = f . read ( ) except : import traceback import logging , sys cla , exc , tb = sys . exc_info ( ) logging . error ( url ) if postdata : logging . error ( "" with post data "" ) else : logging . error ( "" without post data "" ) logging . error ( exc . args ) logging . error ( traceback . format_tb ( tb ) ) response = "" "" return response",if postdata != { } :,if postdata :,96.63780496,FALSE,95.7
4884,"def check_single_file ( fn , fetchuri ) : """"""Determine if a single downloaded file is something we can't handle"""""" with open ( fn , "" r "" , errors = "" surrogateescape "" ) as f : <MASK> logger . error ( ' Fetching  "" %s ""  returned a single HTML page - check the URL is correct and functional ' % fetchuri ) sys . exit ( 1 )","if ""<html"" in f . read ( 100 ) . lower ( ) :","if fetchuri not in ( ""http://www.w3.org/1999/",78.34746525,FALSE,82.26
4885,"def update_properties ( self , update_dict ) : signed_attribute_changed = False for k , value in update_dict . items ( ) : if getattr ( self , k ) != value : setattr ( self , k , value ) signed_attribute_changed = signed_attribute_changed or ( k in self . payload_arguments ) if signed_attribute_changed : <MASK> self . status = UPDATED self . timestamp = clock . tick ( ) self . sign ( ) return self",if self . status != NEW :,if self . status == REAL :,97.8593591,FALSE,95.98
4886,"def clean_items ( event , items , variations ) : for item in items : if event != item . event : raise ValidationError ( _ ( "" One or more items do not belong to this event. "" ) ) if item . has_variations : <MASK> raise ValidationError ( _ ( "" One or more items has variations but none of these are in the variations list. "" ) )",if not any ( var . item == item for var in variations ) :,if variations and item . variations not in variations :,81.48348411,FALSE,84.49
4887,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 10 : length = d . getVarInt32 ( ) tmp = ProtocolBuffer . Decoder ( d . buffer ( ) , d . pos ( ) , d . pos ( ) + length ) d . skip ( length ) self . add_status ( ) . TryMerge ( tmp ) continue if tt == 18 : self . add_doc_id ( d . getPrefixedString ( ) ) continue <MASK> raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 0 :,if tt == 0 :,100,TRUE,100
4888,"def connections ( self ) : # Connections look something like this: # socket:[102422] fds = self . open_files socket = "" socket:[ "" result = [ ] functions = [ pwndbg . net . tcp , pwndbg . net . unix , pwndbg . net . netlink ] for fd , path in fds . items ( ) : if socket not in path : continue inode = path [ len ( socket ) : - 1 ] inode = int ( inode ) for func in functions : for x in func ( ) : <MASK> x . fd = fd result . append ( x ) return tuple ( result )",if x . inode == inode :,if x . inode == inode :,75,TRUE,100
4889,"def _movement_finished ( self ) : if self . in_ship_map : # if the movement somehow stops, the position sticks, and the unit isn't at next_target any more <MASK> ship = self . session . world . ship_map . get ( self . _next_target . to_tuple ( ) ) if ship is not None and ship ( ) is self : del self . session . world . ship_map [ self . _next_target . to_tuple ( ) ] super ( ) . _movement_finished ( )",if self . _next_target is not None :,"if self . _next_target . to_tuple ( ) not in [ ""sticks",95.8391888,FALSE,90.45
4890,"def print_addresses ( self ) : p = 3 tmp_str = "" [ "" if self . get_len ( ) > = 7 : # at least one complete IP address while 1 : if p + 1 == self . get_ptr ( ) : tmp_str + = "" # "" tmp_str + = self . get_ip_address ( p ) p + = 4 <MASK> break else : tmp_str + = "" ,  "" tmp_str + = "" ]  "" if self . get_ptr ( ) % 4 : # ptr field should be a multiple of 4 tmp_str + = "" nonsense ptr field:  %d   "" % self . get_ptr ( ) return tmp_str",if p >= self . get_len ( ) :,elif p == 7 :,96.10455843,FALSE,92.23
4891,"def source_shapes ( self ) : """"""Prints debug information about the sources in this provider."""""" if logger . isEnabledFor ( logging . DEBUG ) : for i , source in enumerate ( self . sources ) : <MASK> name = "" anonymous "" else : name = self . keys [ i ] try : shape = source . shape ( ) except NotImplementedError : shape = "" N/A "" logger . debug ( ' Data source  "" %s "" : entries= %s , shape= %s ' , name , len ( source ) , shape )",if self . keys is None :,if i == 0 :,90.51870771,FALSE,94.46
4892,def swap_actions ( actions ) : for mutexgroup in mutex_groups : mutex_actions = mutexgroup . _group_actions <MASK> # make a best guess as to where we should store the group targetindex = actions . index ( mutexgroup . _group_actions [ 0 ] ) # insert the _ArgumentGroup container actions [ targetindex ] = mutexgroup # remove the duplicated individual actions actions = [ action for action in actions if action not in mutex_actions ] return actions,"if contains_actions ( mutex_actions , actions ) :",if mutex_actions :,78.21235636,FALSE,89.86
4893,"def rec_deps ( services , container_by_name , cnt , init_service ) : deps = cnt [ "" _deps "" ] for dep in deps . copy ( ) : dep_cnts = services . get ( dep ) if not dep_cnts : continue dep_cnt = container_by_name . get ( dep_cnts [ 0 ] ) if dep_cnt : # TODO: avoid creating loops, A->B->A <MASK> continue new_deps = rec_deps ( services , container_by_name , dep_cnt , init_service ) deps . update ( new_deps ) return deps","if init_service and init_service in dep_cnt [ ""_deps"" ] :",if dep_cnt == 0 :,94.72140344,FALSE,87.92
4894,"def make_dump_list_by_name_list ( name_list ) : info_list = [ ] for info_name in name_list : info = next ( ( x for x in DUMP_LIST if x . info_name == info_name ) , None ) <MASK> raise RuntimeError ( ' Unknown info name:  "" {} "" ' . format ( info_name ) ) info_list . append ( info ) return info_list",if not info :,if info is None :,93.67798613,FALSE,95.56
4895,"def create ( self , private = False ) : try : if private : log . info ( "" Creating private channel  %s . "" , self ) self . _bot . api_call ( "" conversations.create "" , data = { "" name "" : self . name , "" is_private "" : True } ) else : log . info ( "" Creating channel  %s . "" , self ) self . _bot . api_call ( "" conversations.create "" , data = { "" name "" : self . name } ) except SlackAPIResponseError as e : <MASK> raise RoomError ( f "" Unable to create channel.  { USER_IS_BOT_HELPTEXT } "" ) else : raise RoomError ( e )","if e . error == ""user_is_bot"" :",if e . error_code == 400 :,95.50160821,FALSE,93.78
4896,"def talk ( self , words ) : if self . writeSentence ( words ) == 0 : return r = [ ] while 1 : i = self . readSentence ( ) <MASK> continue reply = i [ 0 ] attrs = { } for w in i [ 1 : ] : j = w . find ( "" = "" , 1 ) if j == - 1 : attrs [ w ] = "" "" else : attrs [ w [ : j ] ] = w [ j + 1 : ] r . append ( ( reply , attrs ) ) if reply == "" !done "" : return r",if len ( i ) == 0 :,if len ( i ) < 2 :,97.65519015,FALSE,96.44
4897,"def _load_logfile ( self , lfn ) : enc_key = self . decryption_key_func ( ) with open ( os . path . join ( self . logdir , lfn ) ) as fd : <MASK> with DecryptingStreamer ( fd , mep_key = enc_key , name = "" EventLog/DS( %s ) "" % lfn ) as streamer : lines = streamer . read ( ) streamer . verify ( _raise = IOError ) else : lines = fd . read ( ) if lines : for line in lines . splitlines ( ) : event = Event . Parse ( line . strip ( ) ) self . _events [ event . event_id ] = event",if enc_key :,if enc_key :,100,TRUE,100
4898,"def set_ok_port ( self , cookie , request ) : if cookie . port_specified : req_port = request_port ( request ) if req_port is None : req_port = "" 80 "" else : req_port = str ( req_port ) for p in cookie . port . split ( "" , "" ) : try : int ( p ) except ValueError : debug ( ""    bad port  %s  (not numeric) "" , p ) return False <MASK> break else : debug ( ""    request port ( %s ) not found in  %s "" , req_port , cookie . port ) return False return True",if p == req_port :,if req_port == int ( p ) :,95.43761843,FALSE,94.13
4899,"def get_attribute_value ( self , nodeid , attr ) : with self . _lock : self . logger . debug ( "" get attr val:  %s   %s "" , nodeid , attr ) <MASK> dv = ua . DataValue ( ) dv . StatusCode = ua . StatusCode ( ua . StatusCodes . BadNodeIdUnknown ) return dv node = self . _nodes [ nodeid ] if attr not in node . attributes : dv = ua . DataValue ( ) dv . StatusCode = ua . StatusCode ( ua . StatusCodes . BadAttributeIdInvalid ) return dv attval = node . attributes [ attr ] if attval . value_callback : return attval . value_callback ( ) return attval . value",if nodeid not in self . _nodes :,if nodeid not in self . _nodes :,100,TRUE,100
4900,"def data_logging_status ( self , trail_name , trail_details , api_client ) : for es in api_client . get_event_selectors ( TrailName = trail_name ) [ "" EventSelectors "" ] : has_wildcard = { u "" Values "" : [ u "" arn:aws:s3::: "" ] , u "" Type "" : u "" AWS::S3::Object "" , } in es [ "" DataResources "" ] is_logging = trail_details [ "" IsLogging "" ] <MASK> return True return False",if has_wildcard and is_logging and self . is_fresh ( trail_details ) :,if has_wildcard and is_logging :,78.29715983,FALSE,90.07
4901,"def pytest_deselected ( items ) : if sb_config . dashboard : sb_config . item_count - = len ( items ) for item in items : test_id , display_id = _get_test_ids_ ( item ) <MASK> sb_config . _results . pop ( test_id )",if test_id in sb_config . _results . keys ( ) :,if test_id in sb_config . _results :,82.32587175,FALSE,92.93
4902,"def _visit ( self , func ) : fname = func [ 0 ] if fname in self . _flags : if self . _flags [ fname ] == 1 : logger . critical ( "" Fatal error! network ins not Dag. "" ) import sys sys . exit ( - 1 ) else : return else : if fname not in self . _flags : self . _flags [ fname ] = 1 for output in func [ 3 ] : for f in self . _orig : for input in f [ 2 ] : <MASK> self . _visit ( f ) self . _flags [ fname ] = 2 self . _sorted . insert ( 0 , func )",if output == input :,if input == output :,98.4775151,FALSE,96.87
4903,"def printWiki ( ) : firstHeading = False for m in protocol : <MASK> if firstHeading : output ( "" |} "" ) __printWikiHeader ( m [ 1 ] , m [ 2 ] ) firstHeading = True else : output ( "" |- "" ) output ( ' | <span style= "" white-space:nowrap; "" ><tt> ' + m [ 0 ] + "" </tt></span> || ||  "" + m [ 1 ] ) output ( "" |} "" )","if m [ 0 ] == """" :","if m [ 0 ] == ""wiki"" :",98.54348298,FALSE,97.72
4904,"def test_getitem ( self ) : n = 200 d = deque ( range ( n ) ) l = list ( range ( n ) ) for i in range ( n ) : d . popleft ( ) l . pop ( 0 ) <MASK> d . append ( i ) l . append ( i ) for j in range ( 1 - len ( l ) , len ( l ) ) : assert d [ j ] == l [ j ] d = deque ( "" superman "" ) self . assertEqual ( d [ 0 ] , "" s "" ) self . assertEqual ( d [ - 1 ] , "" n "" ) d = deque ( ) self . assertRaises ( IndexError , d . __getitem__ , 0 ) self . assertRaises ( IndexError , d . __getitem__ , - 1 )",if random . random ( ) < 0.5 :,if len ( l ) == 1 :,95.30170359,FALSE,95.27
4905,"def get_num ( line , char_ptr , num_chars ) : char_ptr = char_ptr + 1 numstr = "" "" good = "" -.0123456789 "" while char_ptr < num_chars : digit = line [ char_ptr ] <MASK> numstr = numstr + digit char_ptr = char_ptr + 1 else : break return numstr",if good . find ( digit ) != - 1 :,if digit in good :,58.12456727,FALSE,86.41
4906,"def read_digits ( source , start , first_code ) : body = source . body position = start code = first_code if code is not None and 48 < = code < = 57 : # 0 - 9 while True : position + = 1 code = char_code_at ( body , position ) <MASK> break return position raise GraphQLSyntaxError ( source , position , u "" Invalid number, expected digit but got:  {} . "" . format ( print_char_code ( code ) ) , )",if not ( code is not None and 48 <= code <= 57 ) :,if code is None :,92.84118317,FALSE,86.38
4907,"def get_aws_metadata ( headers , provider = None ) : if not provider : provider = boto . provider . get_default ( ) metadata_prefix = provider . metadata_prefix metadata = { } for hkey in headers . keys ( ) : <MASK> val = urllib . unquote_plus ( headers [ hkey ] ) try : metadata [ hkey [ len ( metadata_prefix ) : ] ] = unicode ( val , "" utf-8 "" ) except UnicodeDecodeError : metadata [ hkey [ len ( metadata_prefix ) : ] ] = val del headers [ hkey ] return metadata",if hkey . lower ( ) . startswith ( metadata_prefix ) :,if hkey . startswith ( metadata_prefix ) :,94.28120227,FALSE,96.19
4908,"def _process_rtdest ( self ) : LOG . debug ( "" Processing RT NLRI destination... "" ) if self . _rtdest_queue . is_empty ( ) : return else : processed_any = False while not self . _rtdest_queue . is_empty ( ) : # We process the first destination in the queue. next_dest = self . _rtdest_queue . pop_first ( ) if next_dest : next_dest . process ( ) processed_any = True <MASK> # Since RT destination were updated we update RT filters self . _core_service . update_rtfilters ( )",if processed_any :,if processed_any :,100,TRUE,100
4909,"def _get_header ( self , requester , header_name ) : hits = sum ( [ header_name in headers for _ , headers in requester . requests ] ) self . assertEquals ( hits , 2 if self . revs_enabled else 1 ) for url , headers in requester . requests : if header_name in headers : <MASK> self . assertTrue ( url . endswith ( "" /latest "" ) , msg = url ) else : self . assertTrue ( url . endswith ( "" /download_urls "" ) , msg = url ) return headers . get ( header_name )",if self . revs_enabled :,if self . revs_enabled :,100,TRUE,100
4910,"def add_external_deps ( self , deps ) : for dep in deps : if hasattr ( dep , "" el "" ) : dep = dep . el <MASK> raise InvalidArguments ( "" Argument is not an external dependency "" ) self . external_deps . append ( dep ) if isinstance ( dep , dependencies . Dependency ) : self . process_sourcelist ( dep . get_sources ( ) )","if not isinstance ( dep , dependencies . Dependency ) :","if not isinstance ( dep , ( dependencies . ExternalDependency , dependencies . ExternalDependency ) ) :",93.60956492,FALSE,90.05
4911,"def _consume_msg ( self ) : ws = self . _ws try : while True : r = await ws . recv ( ) if isinstance ( r , bytes ) : r = r . decode ( "" utf-8 "" ) msg = json . loads ( r ) stream = msg . get ( "" stream "" ) <MASK> await self . _dispatch ( stream , msg ) except websockets . WebSocketException as wse : logging . warn ( wse ) await self . close ( ) asyncio . ensure_future ( self . _ensure_ws ( ) )",if stream is not None :,if stream :,95.91512929,FALSE,95.98
4912,"def generate_and_check_random ( ) : random_size = 256 while True : random = os . urandom ( random_size ) a = int . from_bytes ( random , "" big "" ) A = pow ( g , a , p ) <MASK> a_for_hash = big_num_for_hash ( A ) u = int . from_bytes ( sha256 ( a_for_hash , b_for_hash ) , "" big "" ) if u > 0 : return ( a , a_for_hash , u )","if is_good_mod_exp_first ( A , p ) :",if A > 0 :,83.06238304,FALSE,87.76
4913,"def write ( self , datagram , address ) : """"""Write a datagram."""""" try : return self . socket . sendto ( datagram , address ) except OSError as se : no = se . args [ 0 ] if no == EINTR : return self . write ( datagram , address ) elif no == EMSGSIZE : raise error . MessageLengthError ( "" message too long "" ) <MASK> # oh, well, drop the data. The only difference from UDP # is that UDP won't ever notice. # TODO: add TCP-like buffering pass else : raise",elif no == EAGAIN :,elif no == EWOULDBLOCK :,98.75651053,FALSE,97.8
4914,"def doDir ( elem ) : for child in elem . childNodes : <MASK> continue if child . tagName == "" Directory "" : doDir ( child ) elif child . tagName == "" Component "" : for grandchild in child . childNodes : if not isinstance ( grandchild , minidom . Element ) : continue if grandchild . tagName != "" File "" : continue files . add ( grandchild . getAttribute ( "" Source "" ) . replace ( os . sep , "" / "" ) )","if not isinstance ( child , minidom . Element ) :","if not isinstance ( child , minidom . Element ) :",100,TRUE,100
4915,"def add_reversed_tensor ( i , X , reversed_X ) : # Do not keep tensors that should stop the mapping. if X in stop_mapping_at_tensors : return if X not in reversed_tensors : reversed_tensors [ X ] = { "" id "" : ( nid , i ) , "" tensor "" : reversed_X } else : tmp = reversed_tensors [ X ] if "" tensor "" in tmp and "" tensors "" in tmp : raise Exception ( "" Wrong order, tensors already aggregated! "" ) <MASK> tmp [ "" tensors "" ] = [ tmp [ "" tensor "" ] , reversed_X ] del tmp [ "" tensor "" ] else : tmp [ "" tensors "" ] . append ( reversed_X )","if ""tensor"" in tmp :","if ""tensors"" not in tmp :",73.83095283,FALSE,97.27
4916,"def walk ( source , path , default , delimiter = "" . "" ) : """"""Walk the sourch hash given the path and return the value or default if not found"""""" if not isinstance ( source , dict ) : raise RuntimeError ( "" The source is not a walkable dict:  {}  path:  {} "" . format ( source , path ) ) keys = path . split ( delimiter ) max_depth = len ( keys ) cur_depth = 0 while cur_depth < max_depth : <MASK> source = source [ keys [ cur_depth ] ] cur_depth = cur_depth + 1 else : return default return source",if keys [ cur_depth ] in source :,if keys [ cur_depth ] in source :,100,TRUE,100
4917,"def _from_txt_get_vulns ( self ) : file_vulns = [ ] vuln_regex = ( ' SQL injection in a .*? was found at:  "" (.*?) "" ' ' , using HTTP method (.*?). The sent .*?data was:  "" (.*?) "" ' ) vuln_re = re . compile ( vuln_regex ) for line in file ( self . OUTPUT_FILE ) : mo = vuln_re . search ( line ) <MASK> v = MockVuln ( "" TestCase "" , None , "" High "" , 1 , "" plugin "" ) v . set_url ( URL ( mo . group ( 1 ) ) ) v . set_method ( mo . group ( 2 ) ) file_vulns . append ( v ) return file_vulns",if mo :,if mo :,100,TRUE,100
4918,"def __get__ ( self , instance , instance_type = None ) : if instance : if self . att_name not in instance . _obj_cache : rel_obj = self . get_obj ( instance ) <MASK> instance . _obj_cache [ self . att_name ] = rel_obj return instance . _obj_cache . get ( self . att_name ) return self",if rel_obj :,if rel_obj :,100,TRUE,100
4919,"def get_ranges_from_func_set ( support_set ) : pos_start = 0 pos_end = 0 ranges = [ ] for pos , func in enumerate ( network . function ) : <MASK> pos_end = pos else : if pos_end > = pos_start : ranges . append ( ( pos_start , pos_end ) ) pos_start = pos + 1 if pos_end > = pos_start : ranges . append ( ( pos_start , pos_end ) ) return ranges",if func . type in support_set :,if func in support_set :,93.16228997,FALSE,96.91
4920,"def get_all_active_plugins ( self ) - > List [ BotPlugin ] : """"""This returns the list of plugins in the callback ordered defined from the config."""""" all_plugins = [ ] for name in self . plugins_callback_order : # None is a placeholder for any plugin not having a defined order if name is None : all_plugins + = [ plugin for name , plugin in self . plugins . items ( ) <MASK> ] else : plugin = self . plugins [ name ] if plugin . is_activated : all_plugins . append ( plugin ) return all_plugins",if name not in self . plugins_callback_order and plugin . is_activated,"if isinstance ( plugin , BotPlugin )",94.49088288,FALSE,87.36
4921,"def render_token_list ( self , tokens ) : result = [ ] vars = [ ] for token in tokens : <MASK> result . append ( token . contents . replace ( "" % "" , "" %% "" ) ) elif token . token_type == TOKEN_VAR : result . append ( "" %% ( %s )s "" % token . contents ) vars . append ( token . contents ) msg = "" "" . join ( result ) if self . trimmed : msg = translation . trim_whitespace ( msg ) return msg , vars",if token . token_type == TOKEN_TEXT :,if token . token_type == TOKEN_VAR_STRING :,98.77174961,FALSE,96.34
4922,"def test_build_root_config_overwrite ( self ) : cfg = build_root_config ( "" tests.files.settings_overwrite "" ) for key , val in DEFAULT_SPIDER_GLOBAL_CONFIG . items ( ) : <MASK> self . assertEqual ( cfg [ "" global "" ] [ key ] , [ "" zzz "" ] ) else : self . assertEqual ( cfg [ "" global "" ] [ key ] , val )","if key == ""spider_modules"" :","if key == ""zzz"" :",98.12383872,FALSE,95.28
4923,"def get_limit ( self , request ) : if self . limit_query_param : try : limit = int ( request . query_params [ self . limit_query_param ] ) if limit < 0 : raise ValueError ( ) # Enforce maximum page size, if defined <MASK> if limit == 0 : return settings . MAX_PAGE_SIZE else : return min ( limit , settings . MAX_PAGE_SIZE ) return limit except ( KeyError , ValueError ) : pass return self . default_limit",if settings . MAX_PAGE_SIZE :,if settings . MAX_PAGE_SIZE > settings . MAX_PAGE_SIZE :,98.8302044,FALSE,93.19
4924,"def track_handler ( handler ) : tid = handler . request . tid for event in events_monitored : <MASK> e = Event ( event , handler . request . execution_time ) State . tenant_state [ tid ] . RecentEventQ . append ( e ) State . tenant_state [ tid ] . EventQ . append ( e ) break","if event [ ""handler_check"" ] ( handler ) :","if event . type == ""event"" :",88.71453391,FALSE,85.92
4925,"def TryMerge ( self , d ) : while d . avail ( ) > 0 : tt = d . getVarInt32 ( ) if tt == 10 : length = d . getVarInt32 ( ) tmp = ProtocolBuffer . Decoder ( d . buffer ( ) , d . pos ( ) , d . pos ( ) + length ) d . skip ( length ) self . add_subscription ( ) . TryMerge ( tmp ) continue <MASK> raise ProtocolBuffer . ProtocolBufferDecodeError d . skipData ( tt )",if tt == 0 :,if tt == 0 :,100,TRUE,100
4926,"def GetCreateInstanceBinder ( self , info ) : with self . _lock : <MASK> return self . _createInstanceBinders [ info ] b = runtime . SymplCreateInstanceBinder ( info ) self . _createInstanceBinders [ info ] = b return b",if self . _createInstanceBinders . ContainsKey ( info ) :,if info in self . _createInstanceBinders :,84.99964575,FALSE,84.36
4927,"def process_task ( self , body , message ) : if "" control "" in body : try : return self . control ( body , message ) except Exception : logger . exception ( "" Exception handling control message: "" ) return if len ( self . pool ) : <MASK> try : queue = UUID ( body [ "" uuid "" ] ) . int % len ( self . pool ) except Exception : queue = self . total_messages % len ( self . pool ) else : queue = self . total_messages % len ( self . pool ) else : queue = 0 self . pool . write ( queue , body ) self . total_messages + = 1 message . ack ( )","if ""uuid"" in body and body [ ""uuid"" ] :","if ""uuid"" in body :",95.23235403,FALSE,94.24
4928,"def is_defined_in_base_class ( self , var : Var ) - > bool : if var . info : for base in var . info . mro [ 1 : ] : if base . get ( var . name ) is not None : return True <MASK> return True return False",if var . info . fallback_to_any :,if base . get ( var . name ) is not None :,91.29992825,FALSE,83.32
4929,"def ant_map ( m ) : tmp = "" rows  %s \n cols  %s \n "" % ( len ( m ) , len ( m [ 0 ] ) ) players = { } for row in m : tmp + = "" m  "" for col in row : if col == LAND : tmp + = "" . "" <MASK> tmp + = "" % "" elif col == FOOD : tmp + = "" * "" elif col == UNSEEN : tmp + = "" ? "" else : players [ col ] = True tmp + = chr ( col + 97 ) tmp + = "" \n "" tmp = ( "" players  %s \n "" % len ( players ) ) + tmp return tmp",elif col == BARRIER :,elif col == LIKE :,89.61766885,FALSE,98.3
4930,"def prompt_for_resume ( config ) : logger = logging . getLogger ( "" changeme "" ) logger . error ( "" A previous scan was interrupted. Type R to resume or F to start a fresh scan "" ) answer = "" "" while not ( answer == "" R "" or answer == "" F "" ) : prompt = "" (R/F)>  "" answer = "" "" try : answer = raw_input ( prompt ) except NameError : answer = input ( prompt ) <MASK> logger . debug ( "" Forcing a fresh scan "" ) elif answer . upper ( ) == "" R "" : logger . debug ( "" Resuming previous scan "" ) config . resume = True return config . resume","if answer . upper ( ) == ""F"" :","if answer == ""F"" :",95.29228069,FALSE,96.35
4931,"def f ( view , s ) : if mode == modes . INTERNAL_NORMAL : <MASK> if view . line ( s . b ) . size ( ) > 0 : eol = view . line ( s . b ) . b return R ( s . b , eol ) return s return s",if count == 1 :,if view . line ( s . b ) . size ( ) > 0 :,89.77047539,FALSE,78.86
4932,"def flush ( self ) : if not self . cuts : return for move , ( x , y , z ) , cent in douglas ( self . cuts , self . tolerance , self . plane ) : <MASK> self . write ( "" %s  X %.4f  Y %.4f  Z %.4f   %s "" % ( move , x , y , z , cent ) ) self . lastgcode = None self . lastx = x self . lasty = y self . lastz = z else : self . move_common ( x , y , z , gcode = "" G1 "" ) self . cuts = [ ]",if cent :,if cent :,100,TRUE,100
4933,"def copy_shell ( self ) : cls = self . __class__ old_id = cls . id new_i = cls ( ) # create a new group new_i . id = self . id # with the same id cls . id = old_id # Reset the Class counter # Copy all properties for prop in cls . properties : <MASK> if self . has ( prop ) : val = getattr ( self , prop ) setattr ( new_i , prop , val ) # but no members new_i . members = [ ] return new_i","if prop is not ""members"" :","if hasattr ( self , prop ) :",96.56571741,FALSE,94.01
4934,"def find_region_by_value ( key , value ) : for region in cognitoidp_backends : backend = cognitoidp_backends [ region ] for user_pool in backend . user_pools . values ( ) : if key == "" client_id "" and value in user_pool . clients : return region <MASK> return region # If we can't find the `client_id` or `access_token`, we just pass # back a default backend region, which will raise the appropriate # error message (e.g. NotAuthorized or NotFound). return list ( cognitoidp_backends ) [ 0 ]","if key == ""access_token"" and value in user_pool . access_tokens :","if key == ""access_token"" and value in user_pool . clients :",98.69366945,FALSE,96.79
4935,"def __init__ ( self , fixed : MQTTFixedHeader = None , variable_header : PacketIdVariableHeader = None ) : if fixed is None : header = MQTTFixedHeader ( PUBREL , 0x02 ) # [MQTT-3.6.1-1] else : <MASK> raise HBMQTTException ( "" Invalid fixed packet type  %s  for PubrelPacket init "" % fixed . packet_type ) header = fixed super ( ) . __init__ ( header ) self . variable_header = variable_header self . payload = None",if fixed . packet_type is not PUBREL :,if fixed . packet_type not in PUBREL :,97.80531889,FALSE,96.83
4936,"def _on_event_MetadataStatisticsUpdated ( self , event , data ) : with self . _selectedFileMutex : <MASK> self . _setJobData ( self . _selectedFile [ "" filename "" ] , self . _selectedFile [ "" filesize "" ] , self . _selectedFile [ "" sd "" ] , self . _selectedFile [ "" user "" ] , )",if self . _selectedFile :,if self . _selectedFile :,75,TRUE,100
4937,"def _validate_parameter_range ( self , value_hp , parameter_range ) : """"""Placeholder docstring"""""" for ( parameter_range_key , parameter_range_value , ) in parameter_range . __dict__ . items ( ) : <MASK> continue # Categorical ranges if isinstance ( parameter_range_value , list ) : for categorical_value in parameter_range_value : value_hp . validate ( categorical_value ) # Continuous, Integer ranges else : value_hp . validate ( parameter_range_value )","if parameter_range_key == ""scaling_type"" :","if parameter_range_key == ""categorical"" :",73.12871072,FALSE,96.41
4938,"def visit_filter_projection ( self , node , value ) : base = self . visit ( node [ "" children "" ] [ 0 ] , value ) if not isinstance ( base , list ) : return None comparator_node = node [ "" children "" ] [ 2 ] collected = [ ] for element in base : if self . _is_true ( self . visit ( comparator_node , element ) ) : current = self . visit ( node [ "" children "" ] [ 1 ] , element ) <MASK> collected . append ( current ) return collected",if current is not None :,if current is not None :,100,TRUE,100
4939,"def _getSubstrings ( self , va , size , ltyp ) : # rip through the desired memory range to populate any substrings subs = set ( ) end = va + size for offs in range ( va , end , 1 ) : loc = self . getLocation ( offs , range = True ) <MASK> subs . add ( ( loc [ L_VA ] , loc [ L_SIZE ] ) ) if loc [ L_TINFO ] : subs = subs . union ( set ( loc [ L_TINFO ] ) ) return list ( subs )",if loc and loc [ L_LTYPE ] == LOC_STRING and loc [ L_VA ] > va :,if loc [ L_VA ] :,68.87427484,FALSE,85.26
4940,"def run ( self ) : while not self . _stopped : try : try : test_name = next ( self . pending ) except StopIteration : break mp_result = self . _runtest ( test_name ) self . output . put ( ( False , mp_result ) ) <MASK> break except ExitThread : break except BaseException : self . output . put ( ( True , traceback . format_exc ( ) ) ) break","if must_stop ( mp_result . result , self . ns ) :",if mp_result :,61.7393762,FALSE,85.44
4941,"def get_in_inputs ( key , data ) : if isinstance ( data , dict ) : for k , v in data . items ( ) : if k == key : return v elif isinstance ( v , ( list , tuple , dict ) ) : out = get_in_inputs ( key , v ) <MASK> return out elif isinstance ( data , ( list , tuple ) ) : out = [ get_in_inputs ( key , x ) for x in data ] out = [ x for x in out if x ] <MASK> return out [ 0 ]",if out :,if len ( out ) == 1 :,65.36530288,FALSE,87.21
4942,"def act_mapping ( self , items , actions , mapping ) : """"""Executes all the actions on the list of pods."""""" success = True for action in actions : for key , method in mapping . items ( ) : if key in action : params = action . get ( key ) ret = method ( items , params ) <MASK> success = False return success",if not ret :,if ret is False :,77.03429188,FALSE,94.54
4943,"def _apply ( self , plan ) : desired = plan . desired changes = plan . changes self . log . debug ( "" _apply: zone= %s , len(changes)= %d "" , desired . name , len ( changes ) ) domain_name = desired . name [ : - 1 ] try : nsone_zone = self . _client . loadZone ( domain_name ) except ResourceException as e : <MASK> raise self . log . debug ( "" _apply:   no matching zone, creating "" ) nsone_zone = self . _client . createZone ( domain_name ) for change in changes : class_name = change . __class__ . __name__ getattr ( self , "" _apply_ {} "" . format ( class_name ) ) ( nsone_zone , change )",if e . message != self . ZONE_NOT_FOUND_MESSAGE :,if e . args [ 0 ] != 404 :,95.94685576,FALSE,92.95
4944,"def split_artists ( self , json ) : if len ( json ) == 0 : ( [ ] , [ ] ) elif len ( json ) == 1 : artist = Artist . query . filter_by ( name = json [ 0 ] [ "" name "" ] ) . first ( ) return ( [ artist ] , [ ] ) my_artists = [ ] other_artists = [ ] for artist_dict in json : artist = Artist . query . filter_by ( name = artist_dict [ "" name "" ] ) <MASK> my_artists . append ( artist . first ( ) ) else : del artist_dict [ "" thumb_url "" ] other_artists . append ( artist_dict ) return ( my_artists , other_artists )",if artist . count ( ) :,if artist :,93.82767048,FALSE,96.56
4945,"def update_metadata ( self ) : for attrname in dir ( self ) : if attrname . startswith ( "" __ "" ) : continue attrvalue = getattr ( self , attrname , None ) <MASK> continue if attrname == "" salt_version "" : attrname = "" version "" if hasattr ( self . metadata , "" set_ {0} "" . format ( attrname ) ) : getattr ( self . metadata , "" set_ {0} "" . format ( attrname ) ) ( attrvalue ) elif hasattr ( self . metadata , attrname ) : try : setattr ( self . metadata , attrname , attrvalue ) except AttributeError : pass",if attrvalue == 0 :,if not attrvalue :,70.49078109,FALSE,95.87
4946,"def close ( self , code = errno . ECONNRESET ) : with self . shutdown_lock : <MASK> super ( RemoteIPRoute , self ) . close ( code = code ) self . closed = True try : self . _mitogen_call . get ( ) except mitogen . core . ChannelError : pass if self . _mitogen_broker is not None : self . _mitogen_broker . shutdown ( ) self . _mitogen_broker . join ( )",if not self . closed :,if self . closed :,94.1614405,FALSE,97.29
4947,"def untokenize ( self , iterable ) : for t in iterable : <MASK> self . compat ( t , iterable ) break tok_type , token , start , end , line = t self . add_whitespace ( start ) self . tokens . append ( token ) self . prev_row , self . prev_col = end if tok_type in ( NEWLINE , NL ) : self . prev_row + = 1 self . prev_col = 0 return "" "" . join ( self . tokens )",if len ( t ) == 2 :,if len ( t ) == 1 :,98.65376697,FALSE,97.66
4948,"def __call__ ( self , x , uttid = None ) : if self . utt2spk is not None : spk = self . utt2spk [ uttid ] else : spk = uttid if not self . reverse : if self . norm_means : x = np . add ( x , self . bias [ spk ] ) <MASK> x = np . multiply ( x , self . scale [ spk ] ) else : <MASK> x = np . divide ( x , self . scale [ spk ] ) if self . norm_means : x = np . subtract ( x , self . bias [ spk ] ) return x",if self . norm_vars :,if self . scale is not None :,93.52003216,FALSE,91.85
4949,"def get_party_total ( self , args ) : self . party_total = frappe . _dict ( ) for d in self . receivables : self . init_party_total ( d ) # Add all amount columns for k in list ( self . party_total [ d . party ] ) : <MASK> self . party_total [ d . party ] [ k ] + = d . get ( k , 0.0 ) # set territory, customer_group, sales person etc self . set_party_details ( d )","if k not in [ ""currency"" , ""sales_person"" ] :",if k not in self . party_total [ d . party ] :,96.45356555,FALSE,90.92
4950,"def get_databases ( request ) : dbs = { } global_env = globals ( ) for ( key , value ) in global_env . items ( ) : try : cond = isinstance ( value , GQLDB ) except : cond = isinstance ( value , SQLDB ) <MASK> dbs [ key ] = value return dbs",if cond :,if cond :,100,TRUE,100
4951,"def check_twobit_file ( dbkey , GALAXY_DATA_INDEX_DIR ) : twobit_file = "" %s /twobit.loc "" % GALAXY_DATA_INDEX_DIR twobit_path = "" "" twobits = { } for i , line in enumerate ( open ( twobit_file ) ) : line = line . rstrip ( "" \r \n "" ) if line and not line . startswith ( "" # "" ) : fields = line . split ( "" \t "" ) <MASK> continue twobits [ ( fields [ 0 ] ) ] = fields [ 1 ] if dbkey in twobits : twobit_path = twobits [ ( dbkey ) ] return twobit_path",if len ( fields ) < 2 :,if len ( fields ) != 2 :,98.84500964,FALSE,97.52
4952,"def action ( scheduler , _ ) : nonlocal state nonlocal has_result nonlocal result nonlocal first nonlocal time <MASK> observer . on_next ( result ) try : if first : first = False else : state = iterate ( state ) has_result = condition ( state ) <MASK> result = state time = time_mapper ( state ) except Exception as e : # pylint: disable=broad-except observer . on_error ( e ) return <MASK> mad . disposable = scheduler . schedule_relative ( time , action ) else : observer . on_completed ( )",if has_result :,if has_result :,100,TRUE,100
4953,def orthogonalEnd ( self ) : if self . type == Segment . LINE : O = self . AB . orthogonal ( ) O . norm ( ) return O else : O = self . B - self . C O . norm ( ) <MASK> return - O else : return O,if self . type == Segment . CCW :,if self . type == Segment . CCW :,100,TRUE,100
4954,"def remove ( self , values ) : if not isinstance ( values , ( list , tuple , set ) ) : values = [ values ] for v in values : v = str ( v ) if isinstance ( self . _definition , dict ) : self . _definition . pop ( v , None ) elif self . _definition == "" ANY "" : <MASK> self . _definition = [ ] elif v in self . _definition : self . _definition . remove ( v ) if ( self . _value is not None and self . _value not in self . _definition and self . _not_any ( ) ) : raise ConanException ( bad_value_msg ( self . _name , self . _value , self . values_range ) )","if v == ""ANY"" :",if not self . _not_any ( ) :,95.05691086,FALSE,93.53
4955,"def __enter__ ( self ) - > None : try : <MASK> signal . signal ( signal . SIGALRM , self . handle_timeout ) signal . alarm ( self . seconds ) except ValueError as ex : logger . warning ( "" timeout can ' t be used in the current context "" ) logger . exception ( ex )",if threading . current_thread ( ) == threading . main_thread ( ) :,if self . seconds is not None :,60.71517639,FALSE,78.3
4956,"def __init__ ( self , fixed : MQTTFixedHeader = None ) : if fixed is None : header = MQTTFixedHeader ( PINGRESP , 0x00 ) else : <MASK> raise HBMQTTException ( "" Invalid fixed packet type  %s  for PingRespPacket init "" % fixed . packet_type ) header = fixed super ( ) . __init__ ( header ) self . variable_header = None self . payload = None",if fixed . packet_type is not PINGRESP :,if fixed . packet_type not in PINGRESP :,97.57129766,FALSE,96.25
4957,"def _put_nowait ( self , data , * , sender ) : if not self . _running : logger . warning ( "" Pub/Sub listener message after stop:  %r ,  %r "" , sender , data ) return self . _queue . put_nowait ( ( sender , data ) ) if self . _waiter is not None : fut , self . _waiter = self . _waiter , None <MASK> assert fut . cancelled ( ) , ( "" Waiting future is in wrong state "" , self , fut ) return fut . set_result ( None )",if fut . done ( ) :,if fut . done ( ) :,100,TRUE,100
4958,"def OnAssignBuiltin ( self , cmd_val ) : # type: (cmd_value__Assign) -> None buf = self . _ShTraceBegin ( ) if not buf : return for i , arg in enumerate ( cmd_val . argv ) : <MASK> buf . write ( ""   "" ) buf . write ( arg ) for pair in cmd_val . pairs : buf . write ( ""   "" ) buf . write ( pair . var_name ) buf . write ( "" = "" ) if pair . rval : _PrintShValue ( pair . rval , buf ) buf . write ( "" \n "" ) self . f . write ( buf . getvalue ( ) )",if i != 0 :,if i > 0 :,73.93406054,FALSE,97.45
4959,"def convertDict ( obj ) : obj = dict ( obj ) for k , v in obj . items ( ) : del obj [ k ] if not ( isinstance ( k , str ) or isinstance ( k , unicode ) ) : k = dumps ( k ) # Keep track of which keys need to be decoded when loading. <MASK> obj [ Types . KEYS ] = [ ] obj [ Types . KEYS ] . append ( k ) obj [ k ] = convertObjects ( v ) return obj",if Types . KEYS not in obj :,if k not in obj :,97.82932122,FALSE,95.6
4960,"def _ArgumentListHasDictionaryEntry ( self , token ) : """"""Check if the function argument list has a dictionary as an arg."""""" if _IsArgumentToFunction ( token ) : while token : if token . value == "" { "" : length = token . matching_bracket . total_length - token . total_length return length + self . stack [ - 2 ] . indent > self . column_limit if token . ClosesScope ( ) : break <MASK> token = token . matching_bracket token = token . next_token return False",if token . OpensScope ( ) :,"if token . value == ""}"" :",95.29696936,FALSE,93.49
4961,"def get_editable_dict ( self ) : ret = { } for ref , ws_package in self . _workspace_packages . items ( ) : path = ws_package . root_folder <MASK> path = os . path . join ( path , CONANFILE ) ret [ ref ] = { "" path "" : path , "" layout "" : ws_package . layout } return ret",if os . path . isdir ( path ) :,if not os . path . isabs ( path ) :,80.49311474,FALSE,94.15
4962,"def serialize ( self , name = None ) : data = super ( WebLink , self ) . serialize ( name ) data [ "" contentType "" ] = self . contentType if self . width : <MASK> raise InvalidWidthException ( self . width ) data [ "" inputOptions "" ] = { } data [ "" width "" ] = self . width data . update ( { "" content "" : { "" url "" : self . linkUrl , "" text "" : self . linkText } } ) return data","if self . width not in [ 100 , 50 , 33 , 25 ] :","if self . width not in ( ""0x"" , ""0x7E"" )",91.41576571,FALSE,89.61
4963,"def callback ( lexer , match , context ) : text = match . group ( ) extra = "" "" if start : context . next_indent = len ( text ) <MASK> while context . next_indent < context . indent : context . indent = context . indent_stack . pop ( ) if context . next_indent > context . indent : extra = text [ context . indent : ] text = text [ : context . indent ] else : context . next_indent + = len ( text ) if text : yield match . start ( ) , TokenClass , text if extra : yield match . start ( ) + len ( text ) , TokenClass . Error , extra context . pos = match . end ( )",if context . next_indent < context . indent :,if context . indent_stack :,94.03260363,FALSE,95.02
4964,"def _handle_unsubscribe ( self , web_sock ) : index = None with await self . _subscriber_lock : for i , ( subscriber_web_sock , _ ) in enumerate ( self . _subscribers ) : if subscriber_web_sock == web_sock : index = i break <MASK> del self . _subscribers [ index ] if not self . _subscribers : asyncio . ensure_future ( self . _unregister_subscriptions ( ) )",if index is not None :,if index :,84.54517028,FALSE,95.32
4965,"def test_missing_dict_param ( ) : expected_err = "" params dictionary did not contain value for placeholder "" try : substitute_params ( "" SELECT * FROM cust WHERE salesrep =  %(name)s "" , { "" foobar "" : "" John Doe "" } ) assert False , "" expected exception b/c dict did not contain replacement value "" except ValueError as exc : <MASK> raise",if expected_err not in str ( exc ) :,if expected_err not in str ( exc ) :,100,TRUE,100
4966,"def one_gpr_reg_one_mem_scalable ( ii ) : n , r = 0 , 0 for op in _gen_opnds ( ii ) : <MASK> n + = 1 elif op_gprv ( op ) : r + = 1 else : return False return n == 1 and r == 1","if op_agen ( op ) or ( op_mem ( op ) and op . oc2 in [ ""v"" ] ) :",if op_mem ( op ) :,79.91872115,FALSE,75.68
4967,"def on_enter ( self ) : """"""Fired when mouse enter the bbox of the widget."""""" if hasattr ( self , "" md_bg_color "" ) and self . focus_behavior : if hasattr ( self , "" theme_cls "" ) and not self . focus_color : self . md_bg_color = self . theme_cls . bg_normal else : <MASK> self . md_bg_color = App . get_running_app ( ) . theme_cls . bg_normal else : self . md_bg_color = self . focus_color",if not self . focus_color :,"if hasattr ( App , ""get_running_app"" ) and not self . focus_",69.60395914,FALSE,88.5
4968,"def __init__ ( self , * args , * * kwargs ) : BaseCellExporter . __init__ ( self , * args , * * kwargs ) self . comment = "" # "" for key in [ "" cell_marker "" ] : <MASK> self . metadata [ key ] = self . unfiltered_metadata [ key ] if self . fmt . get ( "" rst2md "" ) : raise ValueError ( "" The  ' rst2md '  option is a read only option. The reverse conversion is not  "" "" implemented. Please either deactivate the option, or save to another format. "" ) # pragma: no cover",if key in self . unfiltered_metadata :,if key in self . unfiltered_metadata :,100,TRUE,100
4969,"def sendQueryQueueByAfterNate ( self ) : for i in range ( 10 ) : queryQueueByAfterNateRsp = self . session . httpClint . send ( urls . get ( "" queryQueue "" ) ) <MASK> print ( "" "" . join ( queryQueueByAfterNateRsp . get ( "" messages "" ) ) or queryQueueByAfterNateRsp . get ( "" validateMessages "" ) ) time . sleep ( 1 ) else : sendEmail ( ticket . WAIT_ORDER_SUCCESS ) sendServerChan ( ticket . WAIT_ORDER_SUCCESS ) raise ticketIsExitsException ( ticket . WAIT_AFTER_NATE_SUCCESS )","if not queryQueueByAfterNateRsp . get ( ""status"" ) :","if queryQueueByAfterNateRsp . get ( ""status"" ) == ticket .",87.8127253,FALSE,93.68
4970,"def filter_errors ( self , errors : List [ str ] ) - > List [ str ] : real_errors : List [ str ] = list ( ) current_file = __file__ current_path = os . path . split ( current_file ) for line in errors : line = line . strip ( ) <MASK> continue fn , lno , lvl , msg = self . parse_trace_line ( line ) if fn is not None : _path = os . path . split ( fn ) if _path [ - 1 ] != current_path [ - 1 ] : continue real_errors . append ( line ) return real_errors",if not line :,if not line :,100,TRUE,100
4971,"def pretty ( self , n , comment = True ) : if isinstance ( n , ( str , bytes , list , tuple , dict ) ) : r = repr ( n ) if not comment : # then it can be inside a comment! r = r . replace ( "" */ "" , r "" \ x2a/ "" ) return r if not isinstance ( n , six . integer_types ) : return n if isinstance ( n , constants . Constant ) : <MASK> return "" %s  /*  %s  */ "" % ( n , self . pretty ( int ( n ) ) ) else : return "" %s  ( %s ) "" % ( n , self . pretty ( int ( n ) ) ) elif abs ( n ) < 10 : return str ( n ) else : return hex ( n )",if comment :,if abs ( n ) == 0 :,98.42842672,FALSE,95.04
4972,"def get_pricings ( self , subscription_id : str ) : try : client = self . get_client ( subscription_id ) pricings_list = await run_concurrently ( lambda : client . pricings . list ( ) ) <MASK> return pricings_list . value else : return [ ] except Exception as e : print_exception ( f "" Failed to retrieve pricings:  { e } "" ) return [ ]","if hasattr ( pricings_list , ""value"" ) :",if pricings_list . value :,86.12480134,FALSE,89.32
4973,"def add_doc ( target , variables , body_lines ) : if isinstance ( target , ast . Name ) : # if it is a variable name add it to the doc name = target . id if name not in variables : doc = find_doc_for ( target , body_lines ) <MASK> variables [ name ] = doc elif isinstance ( target , ast . Tuple ) : # if it is a tuple then iterate the elements # this can happen like this: # a, b = 1, 2 for e in target . elts : add_doc ( e , variables , body_lines )",if doc is not None :,if doc :,97.54640306,FALSE,96.4
4974,"def find_word_bounds ( self , text , index , allowed_chars ) : right = left = index done = False while not done : if left == 0 : done = True elif not self . word_boundary_char ( text [ left - 1 ] ) : left - = 1 else : done = True done = False while not done : <MASK> done = True elif not self . word_boundary_char ( text [ right ] ) : right + = 1 else : done = True return left , right",if right == len ( text ) :,if right == 0 and allowed_chars in text :,93.74684451,FALSE,92.83
4975,"def pxrun_nodes ( self , * args , * * kwargs ) : cell = self . _px_cell if re . search ( r "" ^ \ s* %a utopx \ b "" , cell ) : self . _disable_autopx ( ) return False else : try : result = self . view . execute ( cell , silent = False , block = False ) except : self . shell . showtraceback ( ) return True else : <MASK> try : result . get ( ) except : self . shell . showtraceback ( ) return True else : result . display_outputs ( ) return False",if self . view . block :,if result :,95.85192,FALSE,94.71
4976,"def candidates ( ) - > Generator [ "" Symbol "" , None , None ] : s = self if Symbol . debug_lookup : Symbol . debug_print ( "" searching in self: "" ) print ( s . to_string ( Symbol . debug_indent + 1 ) , end = "" "" ) while True : if matchSelf : yield s if recurseInAnon : yield from s . children_recurse_anon else : yield from s . _children <MASK> break s = s . siblingAbove if Symbol . debug_lookup : Symbol . debug_print ( "" searching in sibling: "" ) print ( s . to_string ( Symbol . debug_indent + 1 ) , end = "" "" )",if s . siblingAbove is None :,if s . siblingAbove is None :,100,TRUE,100
4977,"def decTaskGen ( ) : cnt = intbv ( 0 , min = - n , max = n ) while 1 : yield clock . posedge , reset . negedge <MASK> cnt [ : ] = 0 count . next = 0 else : # print count decTaskFunc ( cnt , enable , reset , n ) count . next = cnt",if reset == ACTIVE_LOW :,if enable == ACTIVE_LOW :,93.08919406,FALSE,96.31
4978,"def __call__ ( self , * args , * * kwargs ) : if not NET_INITTED : return self . raw ( * args , * * kwargs ) for stack in traceback . walk_stack ( None ) : <MASK> layer = stack [ 0 ] . f_locals [ "" self "" ] if layer in layer_names : log . pytorch_layer_name = layer_names [ layer ] print ( layer_names [ layer ] ) break out = self . obj ( self . raw , * args , * * kwargs ) # if isinstance(out,Variable): #     out=[out] return out","if ""self"" in stack [ 0 ] . f_locals :",if stack :,89.69034994,FALSE,90.26
4979,"def to_json_dict ( self ) : d = super ( ) . to_json_dict ( ) d [ "" bullet_list "" ] = RenderedContent . rendered_content_list_to_json ( self . bullet_list ) if self . header is not None : if isinstance ( self . header , RenderedContent ) : d [ "" header "" ] = self . header . to_json_dict ( ) else : d [ "" header "" ] = self . header if self . subheader is not None : <MASK> d [ "" subheader "" ] = self . subheader . to_json_dict ( ) else : d [ "" subheader "" ] = self . subheader return d","if isinstance ( self . subheader , RenderedContent ) :","if isinstance ( self . subheader , RenderedContent ) :",100,TRUE,100
4980,"def add ( request ) : form_type = "" servers "" if request . method == "" POST "" : form = BookMarkForm ( request . POST ) <MASK> form_type = form . save ( ) messages . add_message ( request , messages . INFO , "" Bookmark created "" ) else : messages . add_message ( request , messages . INFO , form . errors ) if form_type == "" server "" : url = reverse ( "" servers "" ) else : url = reverse ( "" metrics "" ) return redirect ( url ) else : return redirect ( reverse ( "" servers "" ) )",if form . is_valid ( ) :,if form . is_valid ( ) :,100,TRUE,100
4981,"def fee_amount_in_quote ( self , trading_pair : str , price : Decimal , order_amount : Decimal ) : fee_amount = Decimal ( "" 0 "" ) if self . percent > 0 : fee_amount = ( price * order_amount ) * self . percent base , quote = trading_pair . split ( "" - "" ) for flat_fee in self . flat_fees : if interchangeable ( flat_fee [ 0 ] , base ) : fee_amount + = flat_fee [ 1 ] * price <MASK> fee_amount + = flat_fee [ 1 ] return fee_amount","elif interchangeable ( flat_fee [ 0 ] , quote ) :","elif interchangeable ( flat_fee [ 0 ] , quote ) :",100,TRUE,100
4982,"def load_batch ( fpath ) : with open ( fpath , "" rb "" ) as f : <MASK> # Python3 d = pickle . load ( f , encoding = "" latin1 "" ) else : # Python2 d = pickle . load ( f ) data = d [ "" data "" ] labels = d [ "" labels "" ] return data , labels","if sys . version_info > ( 3 , 0 ) :","if sys . version_info < ( 3 , 0 ) :",98.34584377,FALSE,96.82
4983,"def clear_entries ( options ) : """"""Clear pending entries"""""" with Session ( ) as session : query = session . query ( db . PendingEntry ) . filter ( db . PendingEntry . approved == False ) <MASK> query = query . filter ( db . PendingEntry . task_name == options . task_name ) deleted = query . delete ( ) console ( "" Successfully deleted  %i  pending entries "" % deleted )",if options . task_name :,if options . task_name :,100,TRUE,100
4984,"def attribute_table ( self , attribute ) : """"""Return a tuple (schema, table) for attribute."""""" dimension = attribute . dimension if dimension : schema = self . naming . dimension_schema or self . naming . schema <MASK> table = self . fact_name else : table = self . naming . dimension_table_name ( dimension ) else : table = self . fact_name schema = self . naming . schema return ( schema , table )",if dimension . is_flat and not dimension . has_details :,if self . naming . is_fact :,72.38564466,FALSE,88.97
4985,"def remove_rating ( self , songs , librarian ) : count = len ( songs ) if count > 1 and config . getboolean ( "" browsers "" , "" rating_confirm_multiple "" ) : parent = qltk . get_menu_item_top_parent ( self ) dialog = ConfirmRateMultipleDialog ( parent , _ ( "" _Remove Rating "" ) , count , None ) if dialog . run ( ) != Gtk . ResponseType . YES : return reset = [ ] for song in songs : <MASK> del song [ "" ~#rating "" ] reset . append ( song ) librarian . changed ( reset )","if ""~#rating"" in song :","if ""#rating"" in song :",98.82315401,FALSE,98.03
4986,"def find_word_bounds ( self , text , index , allowed_chars ) : right = left = index done = False while not done : if left == 0 : done = True elif not self . word_boundary_char ( text [ left - 1 ] ) : left - = 1 else : done = True done = False while not done : if right == len ( text ) : done = True <MASK> right + = 1 else : done = True return left , right",elif not self . word_boundary_char ( text [ right ] ) :,elif allowed_chars and not self . word_boundary_char ( text [ right ],86.44648308,FALSE,93.85
4987,"def handle_read ( self ) : """"""Called when there is data waiting to be read."""""" try : chunk = self . recv ( self . ac_in_buffer_size ) except RetryError : pass except socket . error : self . handle_error ( ) else : self . tot_bytes_received + = len ( chunk ) if not chunk : self . transfer_finished = True # self.close()  # <-- asyncore.recv() already do that... return <MASK> chunk = self . _data_wrapper ( chunk ) try : self . file_obj . write ( chunk ) except OSError as err : raise _FileReadWriteError ( err )",if self . _data_wrapper is not None :,if self . _data_wrapper :,97.49758317,FALSE,96.96
4988,"def toggle ( self , event = None ) : if self . absolute : if self . save == self . split : self . save = 100 if self . split > 20 : self . save = self . split self . split = 1 else : self . split = self . save else : if self . save == self . split : self . save = 0.3 if self . split < = self . min or self . split > = self . max : self . split = self . save <MASK> self . split = self . min else : self . split = self . max self . placeChilds ( )",elif self . split < 0.5 :,elif self . split < self . min :,75.7448802,FALSE,96.51
4989,"def readAtOffset ( self , offset , size , shortok = False ) : ret = b "" "" self . fd . seek ( offset ) while len ( ret ) != size : rlen = size - len ( ret ) x = self . fd . read ( rlen ) if x == b "" "" : <MASK> return None return ret ret + = x return ret",if not shortok :,if shortok :,95.6929421,FALSE,96.63
4990,"def webfinger ( environ , start_response , _ ) : query = parse_qs ( environ [ "" QUERY_STRING "" ] ) try : rel = query [ "" rel "" ] resource = query [ "" resource "" ] [ 0 ] except KeyError : resp = BadRequest ( "" Missing parameter in request "" ) else : <MASK> resp = BadRequest ( "" Bad issuer in request "" ) else : wf = WebFinger ( ) resp = Response ( wf . response ( subject = resource , base = OAS . baseurl ) ) return resp ( environ , start_response )",if rel != [ OIC_ISSUER ] :,"if rel != ""issuer"" :",95.73651761,FALSE,94.51
4991,"def _tokenize ( self , text ) : if format_text ( text ) == EMPTY_TEXT : return [ self . additional_special_tokens [ 0 ] ] split_tokens = [ ] if self . do_basic_tokenize : for token in self . basic_tokenizer . tokenize ( text , never_split = self . all_special_tokens ) : # If the token is part of the never_split set <MASK> split_tokens . append ( token ) else : split_tokens + = self . wordpiece_tokenizer . tokenize ( token ) else : split_tokens = self . wordpiece_tokenizer . tokenize ( text ) return split_tokens",if token in self . basic_tokenizer . never_split :,if token in self . never_split :,98.69237361,FALSE,96.7
4992,"def send_packed_command ( self , command , check_health = True ) : if not self . _sock : self . connect ( ) try : if isinstance ( command , str ) : command = [ command ] for item in command : self . _sock . sendall ( item ) except socket . error as e : self . disconnect ( ) <MASK> _errno , errmsg = "" UNKNOWN "" , e . args [ 0 ] else : _errno , errmsg = e . args raise ConnectionError ( "" Error  %s  while writing to socket.  %s . "" % ( _errno , errmsg ) ) except Exception : self . disconnect ( ) raise",if len ( e . args ) == 1 :,if check_health :,77.46356617,FALSE,92.36
4993,"def to_value ( self , value ) : # Tip: 'value' is the object returned by #      taiga.projects.history.models.HistoryEntry.values_diff() ret = { } for key , val in value . items ( ) : if key in [ "" attachments "" , "" custom_attributes "" , "" description_diff "" ] : ret [ key ] = val <MASK> ret [ key ] = { k : { "" from "" : v [ 0 ] , "" to "" : v [ 1 ] } for k , v in val . items ( ) } else : ret [ key ] = { "" from "" : val [ 0 ] , "" to "" : val [ 1 ] } return ret","elif key == ""points"" :","elif key == ""attachments"" :",74.01193811,FALSE,98.32
4994,"def to_child ( cls , key = None , process = None ) : if process is not None : if type ( process ) is not dict : raise ValueError ( ' Invalid value provided for  "" process ""  parameter, expected a dictionary ' ) <MASK> # Merge class `__process__` parameters with provided parameters result = { } result . update ( deepcopy ( cls . __process__ ) ) result . update ( process ) process = result class Child ( cls ) : __key__ = key __process__ = process __root__ = False Child . __name__ = cls . __name__ return Child",if cls . __process__ :,if key is not None :,73.01321129,FALSE,94.03
4995,"def _super_function ( args ) : passed_class , passed_self = args . get_arguments ( [ "" type "" , "" self "" ] ) if passed_self is None : return passed_class else : # pyclass = passed_self.get_type() pyclass = passed_class <MASK> supers = pyclass . get_superclasses ( ) if supers : return pyobjects . PyObject ( supers [ 0 ] ) return passed_self","if isinstance ( pyclass , pyobjects . AbstractClass ) :","if pyclass . __name__ == ""pyobjects.PyClass"" :",93.74779778,FALSE,85.93
4996,"def get_data ( row ) : data = [ ] for field_name , field_xpath in fields : result = row . xpath ( field_xpath ) <MASK> result = ""   "" . join ( text for text in map ( six . text_type . strip , map ( six . text_type , map ( unescape , result ) ) ) if text ) else : result = None data . append ( result ) return data",if result :,if result :,100,TRUE,100
4997,"def say ( jarvis , s ) : """"""Reads what is typed."""""" if not s : jarvis . say ( "" What should I say? "" ) else : voice_state = jarvis . is_voice_enabled ( ) jarvis . enable_voice ( ) jarvis . say ( s ) <MASK> jarvis . disable_voice ( )",if not voice_state :,if voice_state :,93.13788442,FALSE,96.63
4998,"def __import__ ( name , globals = None , locals = None , fromlist = ( ) , level = 0 ) : module = orig___import__ ( name , globals , locals , fromlist , level ) if fromlist and module . __name__ in modules : <MASK> fromlist = list ( fromlist ) fromlist . remove ( "" * "" ) fromlist . extend ( getattr ( module , "" __all__ "" , [ ] ) ) for x in fromlist : if isinstance ( getattr ( module , x , None ) , types . ModuleType ) : from_name = "" {} . {} "" . format ( module . __name__ , x ) if from_name in modules : importlib . import_module ( from_name ) return module","if ""*"" in fromlist :","if ""*"" in fromlist :",100,TRUE,100
4999,"def _read_pricing_file ( self , region = None , pricing_file = None ) : if not self . __pricing_file_cache : <MASK> logging . info ( "" Reading pricing file... "" ) with open ( pricing_file ) as data_file : self . __pricing_file_cache = json . load ( data_file ) else : self . __pricing_file_cache = self . _download_pricing_file ( region ) return self . __pricing_file_cache",if pricing_file :,if pricing_file :,100,TRUE,100
