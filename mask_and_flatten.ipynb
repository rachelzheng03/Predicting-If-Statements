{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9VzRxQ6IlAzJ3LV0DZoIu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["'''\n","This notebook was tokenizes (based on a Python tokenizer), flattens, and masks the target if statement with the token <MASK>\n","for the training, validation, and test datasets.\n","'''"],"metadata":{"id":"Y9o7Zk0y8MCZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zd9miFmU-5o8"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","source":["df_train = pd.read_csv('ft_train.csv')\n","df_valid = pd.read_csv('ft_valid.csv')\n","df_test = pd.read_csv('ft_test.csv')"],"metadata":{"id":"AxjVbGZK_B_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pygments.lexers import PythonLexer\n","from pygments.token import Token\n","from pygments import lex"],"metadata":{"id":"mqdxnJBXqPuL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_code_with_tab(code):\n","    tokens = []\n","    for ttype, value in lex(code, PythonLexer()):\n","        # take care of tabs\n","        if ttype in Token.Text and len(value)%4 == 0:\n","            num_tabs = len(value)//4 # I consider each tab to be 4 spaces\n","            assert(value == len(value)*\" \") # make sure it's actually all spaces\n","            tokens.append(\"<TAB>\"*num_tabs)\n","            continue\n","        # ignore regular spaces and new lines\n","        elif ttype in Token.Text:\n","            continue\n","        tokens.append(value)\n","    return tokens"],"metadata":{"id":"-lLnLa6cqmQ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_code_no_tab(code):\n","    tokens = []\n","    for ttype, value in lex(code, PythonLexer()):\n","        # ignore regular spaces and new lines\n","        if ttype in Token.Text:\n","            continue\n","        tokens.append(value)\n","    return tokens"],"metadata":{"id":"LDJOBs-BvJ9k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mask_if_statement(method, target, with_tab):\n","    # flatten cleaned method\n","    if with_tab:\n","        tokenized_method = tokenize_code_with_tab(method)\n","    else:\n","        tokenized_method = tokenize_code_no_tab(method)\n","    joined_tokens = \" \".join(tokenized_method) # join tokens into a string separate by a space\n","\n","    tokenized_target = \" \".join(tokenize_code_no_tab(target))\n","    assert(tokenized_target in joined_tokens) # make sure the target if statement is found in the cleaned method\n","    return joined_tokens.replace(tokenized_target, \"<MASK>\") # replace if"],"metadata":{"id":"-xojQltopxdK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["masked_method_with_tab_list = []\n","masked_method_no_tab_list = []\n","for i in range(len(df_train)):\n","    try:\n","      method = df_train.iloc[i][\"cleaned_method\"]\n","      target = df_train.iloc[i][\"target_block\"]\n","\n","      # mask and flatten with tab token\n","      masked_method_with_tab = mask_if_statement(method, target, True)\n","      masked_method_with_tab_list.append(masked_method_with_tab)\n","\n","      # mask and flatten without tab token\n","      masked_method_no_tab = mask_if_statement(method, target, False)\n","      masked_method_no_tab_list.append(masked_method_no_tab)\n","    except Exception as e:\n","      print(i)\n","      raise e"],"metadata":{"id":"qfFmBYWolw7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# add masked and flattenen methods as columns in the df\n","df_train[\"masked_with_tab\"] = masked_method_with_tab_list\n","df_train[\"masked_no_tab\"] = masked_method_no_tab_list"],"metadata":{"id":"MglmTDN16P2i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save df to csv\n","df_train.to_csv(\"ft_train_masked.csv\")"],"metadata":{"id":"TaGoFu2v6Ybk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["masked_method_with_tab_list = []\n","masked_method_no_tab_list = []\n","for i in range(len(df_valid)):\n","    method = df_valid.iloc[i][\"cleaned_method\"]\n","    target = df_valid.iloc[i][\"target_block\"]\n","    masked_method_with_tab = mask_if_statement(method, target, True)\n","    masked_method_with_tab_list.append(masked_method_with_tab)\n","    masked_method_no_tab = mask_if_statement(method, target, False)\n","    masked_method_no_tab_list.append(masked_method_no_tab)"],"metadata":{"id":"KRZz-exJxSDD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_valid[\"masked_with_tab\"] = masked_method_with_tab_list\n","df_valid[\"masked_no_tab\"] = masked_method_no_tab_list\n","df_valid.to_csv(\"ft_valid_masked.csv\")"],"metadata":{"id":"iNS5Jwlj7MLd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["masked_method_with_tab_list = []\n","masked_method_no_tab_list = []\n","for i in range(len(df_test)):\n","    method = df_test.iloc[i][\"cleaned_method\"]\n","    target = df_test.iloc[i][\"target_block\"]\n","    masked_method_with_tab = mask_if_statement(method, target, True)\n","    masked_method_with_tab_list.append(masked_method_with_tab)\n","    masked_method_no_tab = mask_if_statement(method, target, False)\n","    masked_method_no_tab_list.append(masked_method_no_tab)"],"metadata":{"id":"bDNS7gUlxT7P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test[\"masked_with_tab\"] = masked_method_with_tab_list\n","df_test[\"masked_no_tab\"] = masked_method_no_tab_list"],"metadata":{"id":"-f3_fMy2uqJy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test.to_csv(\"ft_test_masked.csv\")"],"metadata":{"id":"qCUKXxSkw3zy"},"execution_count":null,"outputs":[]}]}